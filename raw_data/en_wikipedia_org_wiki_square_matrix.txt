Title: Square matrix

URL Source: https://en.wikipedia.org/wiki/Square_matrix

Published Time: 2003-01-06T16:17:01Z

Markdown Content:
From Wikipedia, the free encyclopedia

[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Arbitrary_square_matrix.gif/250px-Arbitrary_square_matrix.gif)](https://en.wikipedia.org/wiki/File:Arbitrary_square_matrix.gif)

A square matrix of order 4. The entries ![Image 2: {\displaystyle a_{ii}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ecac28d5c61c4a592a926295db6ad1d87f76abbb) form the [main diagonal](https://en.wikipedia.org/wiki/Main_diagonal "Main diagonal") of a square matrix. For instance, the main diagonal of the 4×4 matrix above contains the elements _a_ 11 = 9, _a_ 22 = 11, _a_ 33 = 4, _a_ 44 = 10.

In [mathematics](https://en.wikipedia.org/wiki/Mathematics "Mathematics"), a **square matrix** is a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)") with the same number of rows and columns. An _n_-by-_n_ matrix is known as a square matrix of order ![Image 3: {\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b). Any two square matrices of the same order can be added and multiplied.

Square matrices are often used to represent simple [linear transformations](https://en.wikipedia.org/wiki/Linear_transformation "Linear transformation"), such as [shearing](https://en.wikipedia.org/wiki/Shear_mapping "Shear mapping") or [rotation](https://en.wikipedia.org/wiki/Rotation_(mathematics) "Rotation (mathematics)"). For example, if ![Image 4: {\displaystyle R}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4b0bfb3769bf24d80e15374dc37b0441e2616e33) is a square matrix representing a rotation ([rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix "Rotation matrix")) and ![Image 5: {\displaystyle \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c1866e359fbfd2e0f606c725ba5cc37a5195d6) is a [column vector](https://en.wikipedia.org/wiki/Column_vector "Column vector") describing the [position](https://en.wikipedia.org/wiki/Position_(vector) "Position (vector)") of a point in space, the product ![Image 6: {\displaystyle R\mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/9cc2042c4215356f186f7c384e2a536e9958cb59) yields another column vector describing the position of that point after that rotation. If ![Image 7: {\displaystyle \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c1866e359fbfd2e0f606c725ba5cc37a5195d6) is a [row vector](https://en.wikipedia.org/wiki/Row_vector "Row vector"), the same transformation can be obtained using ![Image 8: {\displaystyle \mathbf {v} R^{\mathsf {T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/32ba0f3b5309fc859bc599a86ffd358a3679a1c6), where ![Image 9: {\displaystyle R^{\mathsf {T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/541749388f73662ea97c3c92a59715ce11350b15) is the [transpose](https://en.wikipedia.org/wiki/Transpose "Transpose") of ![Image 10: {\displaystyle R}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4b0bfb3769bf24d80e15374dc37b0441e2616e33).

The entries ![Image 11: {\displaystyle a_{ii}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ecac28d5c61c4a592a926295db6ad1d87f76abbb) (_i_ = 1, ..., _n_) form the [main diagonal](https://en.wikipedia.org/wiki/Main_diagonal "Main diagonal") of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. For instance, the main diagonal of the 4×4 matrix above contains the elements _a_ 11 = 9, _a_ 22 = 11, _a_ 33 = 4, _a_ 44 = 10.

The diagonal of a square matrix from the top right to the bottom left corner is called _antidiagonal_ or _counterdiagonal_.

| Name | Example with _n_ = 3 |
| --- | --- |
| [Diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix "Diagonal matrix") | ![Image 12: {\displaystyle {\begin{bmatrix}a_{11}&0&0\\0&a_{22}&0\\0&0&a_{33}\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3e8c94dfee8f7a36fda8f66421d001cb82d9f94f) |
| [Lower triangular matrix](https://en.wikipedia.org/wiki/Lower_triangular_matrix "Lower triangular matrix") | ![Image 13: {\displaystyle {\begin{bmatrix}a_{11}&0&0\\a_{21}&a_{22}&0\\a_{31}&a_{32}&a_{33}\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d26f986ed81f1ed71a281003c50e6035be158611) |
| [Upper triangular matrix](https://en.wikipedia.org/wiki/Upper_triangular_matrix "Upper triangular matrix") | ![Image 14: {\displaystyle {\begin{bmatrix}a_{11}&a_{12}&a_{13}\\0&a_{22}&a_{23}\\0&0&a_{33}\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e3aa5c761ffecfd963328bd2d50cec67bba6feb) |

### Diagonal or triangular matrix

[[edit](https://en.wikipedia.org/w/index.php?title=Square_matrix&action=edit&section=3 "Edit section: Diagonal or triangular matrix")]

If all entries outside the main diagonal are zero, ![Image 15: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called a [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix "Diagonal matrix"). If all entries below (resp. above) the main diagonal are zero, ![Image 16: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called an upper (resp. lower) [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix "Triangular matrix").

The [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix "Identity matrix")![Image 17: {\displaystyle I_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/aba34f081d776e30204f3458e4f50b403b09e5c6) of size ![Image 18: {\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) is the ![Image 19: {\displaystyle n\times n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/59d2b4cb72e304526cf5b5887147729ea259da78) matrix in which all the elements on the [main diagonal](https://en.wikipedia.org/wiki/Main_diagonal "Main diagonal") are equal to 1 and all other elements are equal to 0, e.g. ![Image 20: {\displaystyle I_{1}={\begin{bmatrix}1\end{bmatrix}},\ I_{2}={\begin{bmatrix}1&0\\0&1\end{bmatrix}},\ \ldots ,\ I_{n}={\begin{bmatrix}1&0&\cdots &0\\0&1&\cdots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\cdots &1\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a506ee7a75250d2fecf45fa9cc015b06cf4c414a) It is a square matrix of order ![Image 21: {\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b), and also a special kind of [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix "Diagonal matrix"). The term _identity matrix_ refers to the property of matrix multiplication that ![Image 22: {\displaystyle I_{m}A=AI_{n}=A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/77d86bdd107dbe6c7d7185a85aeff1a85420d9ba) for any ![Image 23: {\displaystyle m\times n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/12b23d207d23dd430b93320539abbb0bde84870d) matrix ![Image 24: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3).

### Invertible matrix and its inverse

[[edit](https://en.wikipedia.org/w/index.php?title=Square_matrix&action=edit&section=5 "Edit section: Invertible matrix and its inverse")]

A square matrix ![Image 25: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called _[invertible](https://en.wikipedia.org/wiki/Invertible\_matrix "Invertible matrix")_ or _non-singular_ if there exists a matrix ![Image 26: {\displaystyle B}](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a) such that[[1]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-1)[[2]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-2)![Image 27: {\displaystyle AB=BA=I_{n}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/168794a263102ab93410d65c28bdc8bb955c8d2f) If ![Image 28: {\displaystyle B}](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a) exists, it is unique and is called the _[inverse matrix](https://en.wikipedia.org/wiki/Inverse\_matrix "Inverse matrix")_ of ![Image 29: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3), denoted ![Image 30: {\displaystyle A^{-1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/83ba3a7118652cffd5de466dc439ee9184371d50).

### Symmetric or skew-symmetric matrix

[[edit](https://en.wikipedia.org/w/index.php?title=Square_matrix&action=edit&section=6 "Edit section: Symmetric or skew-symmetric matrix")]

A square matrix ![Image 31: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) that is equal to its transpose, i.e., ![Image 32: {\displaystyle A^{\mathsf {T}}=A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c1b6a1388ee9834c01cffbb46c9326e51a25d5f2), is a [symmetric matrix](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix"). If instead ![Image 33: {\displaystyle A^{\mathsf {T}}=-A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8f020a5cc94f0fd27929041f589affa0e575a083), then ![Image 34: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called a [skew-symmetric matrix](https://en.wikipedia.org/wiki/Skew-symmetric_matrix "Skew-symmetric matrix").

For a complex square matrix ![Image 35: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3), often the appropriate analogue of the transpose is the [conjugate transpose](https://en.wikipedia.org/wiki/Conjugate_transpose "Conjugate transpose")![Image 36: {\displaystyle A^{*}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/44e23745a51c2c2d8d91fd98c1cf721573747ece), defined as the transpose of the [complex conjugate](https://en.wikipedia.org/wiki/Complex_conjugate "Complex conjugate") of ![Image 37: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3). A complex square matrix ![Image 38: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) satisfying ![Image 39: {\displaystyle A^{*}=A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/73bdf631dc8d439d50cd96e570f94266310c0c8a) is called a [Hermitian matrix](https://en.wikipedia.org/wiki/Hermitian_matrix "Hermitian matrix"). If instead ![Image 40: {\displaystyle A^{*}=-A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/88eb1212c50319661f8771d179aea34dca699026), then ![Image 41: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called a [skew-Hermitian matrix](https://en.wikipedia.org/wiki/Skew-Hermitian_matrix "Skew-Hermitian matrix").

By the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem "Spectral theorem"), real symmetric (or complex Hermitian) matrices have an orthogonal (or unitary) [eigenbasis](https://en.wikipedia.org/wiki/Eigenbasis "Eigenbasis"); i.e., every vector is expressible as a [linear combination](https://en.wikipedia.org/wiki/Linear_combination "Linear combination") of eigenvectors. In both cases, all eigenvalues are real.[[3]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-3)

| [Positive definite](https://en.wikipedia.org/wiki/Positive_definite_matrix "Positive definite matrix") | [Indefinite](https://en.wikipedia.org/wiki/Indefinite_matrix "Indefinite matrix") |
| --- | --- |
| ![Image 42: {\displaystyle {\begin{bmatrix}1/4&0\\0&1\\\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f0a047a6d8124b837a45d8244f139042c9715dbf) | ![Image 43: {\displaystyle {\begin{bmatrix}1/4&0\\0&-1/4\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/383f3d0ff0a2d58dfc825a2a17fe2a0d0657db69) |
| _Q_(_x_,_y_) = 1/4 _x_ 2 + _y_ 2 | _Q_(_x_,_y_) = 1/4 _x_ 2 − 1/4 _y_ 2 |
| [![Image 44](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Ellipse_in_coordinate_system_with_semi-axes_labelled.svg/250px-Ellipse_in_coordinate_system_with_semi-axes_labelled.svg.png)](https://en.wikipedia.org/wiki/File:Ellipse_in_coordinate_system_with_semi-axes_labelled.svg) Points such that _Q_(_x_, _y_) = 1 ([Ellipse](https://en.wikipedia.org/wiki/Ellipse "Ellipse")). | [![Image 45](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Hyperbola2_SVG.svg/120px-Hyperbola2_SVG.svg.png)](https://en.wikipedia.org/wiki/File:Hyperbola2_SVG.svg) Points such that _Q_(_x_, _y_) = 1 ([Hyperbola](https://en.wikipedia.org/wiki/Hyperbola "Hyperbola")). |

A symmetric _n_×_n_-matrix is called _[positive-definite](https://en.wikipedia.org/wiki/Positive-definite\_matrix "Positive-definite matrix")_ (respectively negative-definite; indefinite), if for all nonzero vectors ![Image 46: {\displaystyle x\in \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c520ee2cb6ccf8a93c89a8c58a8378796bd52e53) the associated [quadratic form](https://en.wikipedia.org/wiki/Quadratic_form "Quadratic form") given by ![Image 47: {\displaystyle Q(\mathbf {x} )=\mathbf {x} ^{\mathsf {T}}A\mathbf {x} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/7e4a0fb16f8331ffcdbd451766b42f0558c0c173) takes only positive values (respectively only negative values; both some negative and some positive values).[[4]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-4) If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.

A symmetric matrix is positive-definite if and only if all its eigenvalues are positive.[[5]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-5) The table at the right shows two possibilities for 2×2 matrices.

Allowing as input two different vectors instead yields the [bilinear form](https://en.wikipedia.org/wiki/Bilinear_form "Bilinear form") associated to A:[[6]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-6)![Image 48: {\displaystyle B_{A}(\mathbf {x} ,\mathbf {y} )=\mathbf {x} ^{\mathsf {T}}A\mathbf {y} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/74d9818c84fe6bf724a93d97a53c1a780cbe6b55)

An _[orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal\_matrix "Orthogonal matrix")_ is a [square matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)#Square_matrices "Matrix (mathematics)") with [real](https://en.wikipedia.org/wiki/Real_number "Real number") entries whose columns and rows are [orthogonal](https://en.wikipedia.org/wiki/Orthogonal "Orthogonal")[unit vectors](https://en.wikipedia.org/wiki/Unit_vector "Unit vector") (i.e., [orthonormal](https://en.wikipedia.org/wiki/Orthonormality "Orthonormality") vectors). Equivalently, a matrix _A_ is orthogonal if its [transpose](https://en.wikipedia.org/wiki/Transpose "Transpose") is equal to its [inverse](https://en.wikipedia.org/wiki/Inverse_matrix "Inverse matrix"): ![Image 49: {\displaystyle A^{\textsf {T}}=A^{-1},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f15017277eae596688718f8f90455af59b439f) which entails ![Image 50: {\displaystyle A^{\textsf {T}}A=AA^{\textsf {T}}=I,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2adf7d58366492128780d2e23352cb4df22977ff) where _I_ is the [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix "Identity matrix").

An orthogonal matrix A is necessarily [invertible](https://en.wikipedia.org/wiki/Invertible_matrix "Invertible matrix") (with inverse _A_−1 = _A_ T), [unitary](https://en.wikipedia.org/wiki/Unitary_matrix "Unitary matrix") (_A_−1 = _A_*), and [normal](https://en.wikipedia.org/wiki/Normal_matrix "Normal matrix") (_A_*_A_ = _AA_*). The [determinant](https://en.wikipedia.org/wiki/Determinant "Determinant") of any orthogonal matrix is either +1 or −1. The [special orthogonal group](https://en.wikipedia.org/wiki/Special_orthogonal_group "Special orthogonal group")![Image 51: {\displaystyle \operatorname {SO} (n)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/79c49648527ab4aacf6c03c15633727606cc7d22) consists of the _n_ × _n_ orthogonal matrices with [determinant](https://en.wikipedia.org/wiki/Determinant "Determinant") +1.

The [complex](https://en.wikipedia.org/wiki/Complex_number "Complex number") analogue of an orthogonal matrix is a [unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix "Unitary matrix").

A real or complex square matrix ![Image 52: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is called _[normal](https://en.wikipedia.org/wiki/Normal\_matrix "Normal matrix")_ if ![Image 53: {\displaystyle A^{*}A=AA^{*}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/aa3137720f5159df0c9fac990a9d7399b0f966c8). If a real square matrix is symmetric, skew-symmetric, or orthogonal, then it is normal. If a complex square matrix is Hermitian, skew-Hermitian, or unitary, then it is normal. Normal matrices are of interest mainly because they include the types of matrices just listed and form the broadest class of matrices for which the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem "Spectral theorem") holds.[[7]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-7)

The [trace](https://en.wikipedia.org/wiki/Trace_of_a_matrix "Trace of a matrix"), tr(_A_) of a square matrix _A_ is the sum of its diagonal entries. While matrix multiplication is not commutative, the trace of the product of two matrices is independent of the order of the factors: ![Image 54: {\displaystyle \operatorname {tr} (AB)=\operatorname {tr} (BA).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d74d6691b68f54834c01a71278c83e1814f178c4) This is immediate from the definition of matrix multiplication: ![Image 55: {\displaystyle \operatorname {tr} (AB)=\sum _{i=1}^{m}\sum _{j=1}^{n}A_{ij}B_{ji}=\operatorname {tr} (BA).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/dad02f904e4bb045b9c09c5fb2ca3dec526d2b6c) Also, the trace of a matrix is equal to that of its transpose, i.e., ![Image 56: {\displaystyle \operatorname {tr} (A)=\operatorname {tr} (A^{\mathrm {T} }).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb628f4fd3eaa41e0741d3b0b4ba2262e4d12f1a)

[![Image 57](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Determinant_example.svg/330px-Determinant_example.svg.png)](https://en.wikipedia.org/wiki/File:Determinant_example.svg)

A linear transformation on ![Image 58: {\displaystyle \mathbb {R} ^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e150115ab9f63023215109595b76686a1ff890fd) given by the indicated matrix. The determinant of this matrix is −1, as the area of the green parallelogram at the right is 1, but the map reverses the [orientation](https://en.wikipedia.org/wiki/Orientation_(mathematics) "Orientation (mathematics)"), since it turns the counterclockwise orientation of the vectors to a clockwise one.

The _determinant_![Image 59: {\displaystyle \det(A)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/135eb8f635a86d87cfd1386bc58e3c70a3f8a42a) or ![Image 60: {\displaystyle |A|}](https://wikimedia.org/api/rest_v1/media/math/render/svg/648fce92f29d925f04d39244ccfe435320dfc6de) of a square matrix ![Image 61: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is a number encoding certain properties of the matrix. A matrix is invertible [if and only if](https://en.wikipedia.org/wiki/If_and_only_if "If and only if") its determinant is nonzero. Its [absolute value](https://en.wikipedia.org/wiki/Absolute_value "Absolute value") equals the area (in ![Image 62: {\displaystyle \mathbb {R} ^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e150115ab9f63023215109595b76686a1ff890fd)) or volume (in ![Image 63: {\displaystyle \mathbb {R} ^{3}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f936ddf584f8f3dd2a0ed08917001b7a404c10b5)) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.

The determinant of 2×2 matrices is given by ![Image 64: {\displaystyle \det {\begin{bmatrix}a&b\\c&d\end{bmatrix}}=ad-bc.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8138b2141d7b9406cebd3eb82b3cf05b238ca851) The determinant of 3×3 matrices involves 6 terms ([rule of Sarrus](https://en.wikipedia.org/wiki/Rule_of_Sarrus "Rule of Sarrus")). The more lengthy [Leibniz formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants "Leibniz formula for determinants") generalizes these two formulae to all dimensions.[[8]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-8)

The determinant of a product of square matrices equals the product of their determinants:[[9]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-9)![Image 65: {\displaystyle \det(AB)=\det(A)\cdot \det(B)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cc066daa550c425ada4768baf93c2dcc2bb47194) Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[[10]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-10) Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the [Laplace expansion](https://en.wikipedia.org/wiki/Laplace_expansion "Laplace expansion") expresses the determinant in terms of [minors](https://en.wikipedia.org/wiki/Minor_(linear_algebra) "Minor (linear algebra)"), i.e., determinants of smaller matrices.[[11]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-11) This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1×1 matrix, which is its unique entry, or even the determinant of a 0×0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve [linear systems](https://en.wikipedia.org/wiki/Linear_system "Linear system") using [Cramer's rule](https://en.wikipedia.org/wiki/Cramer%27s_rule "Cramer's rule"), where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[[12]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-12)

### Eigenvalues and eigenvectors

[[edit](https://en.wikipedia.org/w/index.php?title=Square_matrix&action=edit&section=13 "Edit section: Eigenvalues and eigenvectors")]

A number λ and a non-zero vector ![Image 66: {\displaystyle \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c1866e359fbfd2e0f606c725ba5cc37a5195d6) satisfying ![Image 67: {\displaystyle A\mathbf {v} =\lambda \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/1a14308bca76c95b0c802c13524713ae532adb4a) are called an _eigenvalue_ and an _eigenvector_ of ![Image 68: {\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3), respectively.[[13]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-13)[[14]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-14) The number λ is an eigenvalue of an _n_×_n_-matrix A if and only if _A_ − λ _I_ _n_ is not invertible, which is [equivalent](https://en.wikipedia.org/wiki/Logical_equivalence "Logical equivalence") to[[15]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-15)![Image 69: {\displaystyle \det(A-\lambda I)=0.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb858d14d5e880fa2345d11515f4c3b1bb6e1a49) The polynomial _p_ _A_ in an [indeterminate](https://en.wikipedia.org/wiki/Indeterminate_(variable) "Indeterminate (variable)")_X_ given by evaluation of the determinant det(_XI_ _n_ − _A_) is called the [characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial "Characteristic polynomial") of A. It is a [monic polynomial](https://en.wikipedia.org/wiki/Monic_polynomial "Monic polynomial") of [degree](https://en.wikipedia.org/wiki/Degree_of_a_polynomial "Degree of a polynomial")_n_. Therefore the polynomial equation _p_ _A_(λ) = 0 has at most _n_ different solutions, i.e., eigenvalues of the matrix.[[16]](https://en.wikipedia.org/wiki/Square_matrix#cite_note-16) They may be complex even if the entries of A are real. According to the [Cayley–Hamilton theorem](https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem "Cayley–Hamilton theorem"), _p_ _A_(_A_) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the [zero matrix](https://en.wikipedia.org/wiki/Zero_matrix "Zero matrix").

*   [Cartan matrix](https://en.wikipedia.org/wiki/Cartan_matrix "Cartan matrix")
*   [Cayley-Hamilton theorem](https://en.wikipedia.org/wiki/Cayley-Hamilton_theorem "Cayley-Hamilton theorem")

1.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-1 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Definition I.2.28
2.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-2 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Definition I.5.13
3.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-3 "Jump up")**Horn & Johnson[1985](https://en.wikipedia.org/wiki/Square_matrix#CITEREFHornJohnson1985), Theorem 2.5.6
4.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-4 "Jump up")**Horn & Johnson[1985](https://en.wikipedia.org/wiki/Square_matrix#CITEREFHornJohnson1985), Chapter 7
5.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-5 "Jump up")**Horn & Johnson[1985](https://en.wikipedia.org/wiki/Square_matrix#CITEREFHornJohnson1985), Theorem 7.2.1
6.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-6 "Jump up")**Horn & Johnson[1985](https://en.wikipedia.org/wiki/Square_matrix#CITEREFHornJohnson1985), Example 4.0.6, p. 169
7.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-7 "Jump up")**Artin, _Algebra_, 2nd edition, Pearson, 2018, section 8.6.
8.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-8 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Definition III.2.1
9.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-9 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Theorem III.2.12
10.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-10 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Corollary III.2.16
11.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-11 "Jump up")**Mirsky[1990](https://en.wikipedia.org/wiki/Square_matrix#CITEREFMirsky1990), Theorem 1.4.1
12.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-12 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Theorem III.3.18
13.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-13 "Jump up")**_Eigen_ means "own" in [German](https://en.wikipedia.org/wiki/German_language "German language") and in [Dutch](https://en.wikipedia.org/wiki/Dutch_language "Dutch language").
14.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-14 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Definition III.4.1
15.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-15 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Definition III.4.9
16.   **[^](https://en.wikipedia.org/wiki/Square_matrix#cite_ref-16 "Jump up")**Brown[1991](https://en.wikipedia.org/wiki/Square_matrix#CITEREFBrown1991), Corollary III.4.10

*   Brown, William C. (1991), [_Matrices and vector spaces_](https://archive.org/details/matricesvectorsp0000brow), New York, NY: [Marcel Dekker](https://en.wikipedia.org/wiki/Marcel_Dekker "Marcel Dekker"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8247-8419-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8247-8419-5 "Special:BookSources/978-0-8247-8419-5")
*   [Horn, Roger A.](https://en.wikipedia.org/wiki/Roger_Horn "Roger Horn"); [Johnson, Charles R.](https://en.wikipedia.org/wiki/Charles_Royal_Johnson "Charles Royal Johnson") (1985), _Matrix Analysis_, Cambridge University Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-521-38632-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-38632-6 "Special:BookSources/978-0-521-38632-6")
*   [Mirsky, Leonid](https://en.wikipedia.org/wiki/Leon_Mirsky "Leon Mirsky") (1990), [_An Introduction to Linear Algebra_](https://books.google.com/books?id=ULMmheb26ZcC&dq=linear+algebra+determinant&pg=PA1), Courier Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-66434-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-66434-7 "Special:BookSources/978-0-486-66434-7")

*   [![Image 70](https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png)](https://en.wikipedia.org/wiki/File:Commons-logo.svg) Media related to [Square matrices](https://commons.wikimedia.org/wiki/Category:Square_matrices "commons:Category:Square matrices") at Wikimedia Commons
