Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Main diagonal 2 Special kinds Toggle Special kinds subsection 2.1 Diagonal or triangular matrix 2.2 Identity matrix 2.3 Invertible matrix and its inverse 2.4 Symmetric or skew-symmetric matrix 2.5 Definite matrix 2.6 Orthogonal matrix 2.7 Normal matrix 3 Operations Toggle Operations subsection 3.1 Trace 3.2 Determinant 3.3 Eigenvalues and eigenvectors 4 See also 5 Notes 6 References 7 External links Toggle the table of contents Square matrix 39 languages العربية 閩南語 / Bân-lâm-gí Bosanski Català Чӑвашла Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Galego Հայերեն हिन्दी Bahasa Indonesia Italiano עברית Lombard Magyar മലയാളം မြန်မာဘာသာ Nederlands 日本語 Português Română Русский Shqip کوردی Srpskohrvatski / српскохрватски Suomi தமிழ் Türkçe Українська اردو Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Matrix with the same number of rows and columns A square matrix of order 4. The entries a i i {\displaystyle a_{ii}} form the main diagonal of a square matrix. For instance, the main diagonal of the 4×4 matrix above contains the elements a 11 = 9 , a 22 = 11 , a 33 = 4 , a 44 = 10 .

In mathematics , a square matrix is a matrix with the same number of rows and columns. An n -by- n matrix is known as a square matrix of order n {\displaystyle n} .

Any two square matrices of the same order can be added and multiplied.

Square matrices are often used to represent simple linear transformations , such as shearing or rotation . For example, if R {\displaystyle R} is a square matrix representing a rotation ( rotation matrix ) and v {\displaystyle \mathbf {v} } is a column vector describing the position of a point in space, the product R v {\displaystyle R\mathbf {v} } yields another column vector describing the position of that point after that rotation. If v {\displaystyle \mathbf {v} } is a row vector , the same transformation can be obtained using v R T {\displaystyle \mathbf {v} R^{\mathsf {T}}} , where R T {\displaystyle R^{\mathsf {T}}} is the transpose of R {\displaystyle R} .

Main diagonal [ edit ] Main article: Main diagonal The entries a i i {\displaystyle a_{ii}} ( i = 1, ..., n ) form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. For instance, the main diagonal of the 4×4 matrix above contains the elements a 11 = 9 , a 22 = 11 , a 33 = 4 , a 44 = 10 .

The diagonal of a square matrix from the top right to the bottom left corner is called antidiagonal or counterdiagonal .

Special kinds [ edit ] Name Example with n = 3 Diagonal matrix [ a 11 0 0 0 a 22 0 0 0 a 33 ] {\displaystyle {\begin{bmatrix}a_{11}&0&0\\0&a_{22}&0\\0&0&a_{33}\end{bmatrix}}} Lower triangular matrix [ a 11 0 0 a 21 a 22 0 a 31 a 32 a 33 ] {\displaystyle {\begin{bmatrix}a_{11}&0&0\\a_{21}&a_{22}&0\\a_{31}&a_{32}&a_{33}\end{bmatrix}}} Upper triangular matrix [ a 11 a 12 a 13 0 a 22 a 23 0 0 a 33 ] {\displaystyle {\begin{bmatrix}a_{11}&a_{12}&a_{13}\\0&a_{22}&a_{23}\\0&0&a_{33}\end{bmatrix}}} Diagonal or triangular matrix [ edit ] If all entries outside the main diagonal are zero, A {\displaystyle A} is called a diagonal matrix . If all entries below (resp. above) the main diagonal are zero, A {\displaystyle A} is called an upper (resp. lower) triangular matrix .

Identity matrix [ edit ] The identity matrix I n {\displaystyle I_{n}} of size n {\displaystyle n} is the n × × n {\displaystyle n\times n} matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.

I 1 = [ 1 ] , I 2 = [ 1 0 0 1 ] , … … , I n = [ 1 0 ⋯ ⋯ 0 0 1 ⋯ ⋯ 0 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 0 ⋯ ⋯ 1 ] .

{\displaystyle I_{1}={\begin{bmatrix}1\end{bmatrix}},\ I_{2}={\begin{bmatrix}1&0\\0&1\end{bmatrix}},\ \ldots ,\ I_{n}={\begin{bmatrix}1&0&\cdots &0\\0&1&\cdots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\cdots &1\end{bmatrix}}.} It is a square matrix of order n {\displaystyle n} , and also a special kind of diagonal matrix . The term identity matrix refers to the property of matrix multiplication that I m A = A I n = A {\displaystyle I_{m}A=AI_{n}=A} for any m × × n {\displaystyle m\times n} matrix A {\displaystyle A} .

Invertible matrix and its inverse [ edit ] A square matrix A {\displaystyle A} is called invertible or non-singular if there exists a matrix B {\displaystyle B} such that [ 1 ] [ 2 ] A B = B A = I n .

{\displaystyle AB=BA=I_{n}.} If B {\displaystyle B} exists, it is unique and is called the inverse matrix of A {\displaystyle A} , denoted A − − 1 {\displaystyle A^{-1}} .

Symmetric or skew-symmetric matrix [ edit ] A square matrix A {\displaystyle A} that is equal to its transpose, i.e., A T = A {\displaystyle A^{\mathsf {T}}=A} , is a symmetric matrix .  If instead A T = − − A {\displaystyle A^{\mathsf {T}}=-A} , then A {\displaystyle A} is called a skew-symmetric matrix .

For a complex square matrix A {\displaystyle A} , often the appropriate analogue of the transpose is the conjugate transpose A ∗ ∗ {\displaystyle A^{*}} , defined as the transpose of the complex conjugate of A {\displaystyle A} .

A complex square matrix A {\displaystyle A} satisfying A ∗ ∗ = A {\displaystyle A^{*}=A} is called a Hermitian matrix .  If instead A ∗ ∗ = − − A {\displaystyle A^{*}=-A} , then A {\displaystyle A} is called a skew-Hermitian matrix .

By the spectral theorem , real symmetric (or complex Hermitian) matrices have an orthogonal (or unitary) eigenbasis ; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.

[ 3 ] Definite matrix [ edit ] Positive definite Indefinite [ 1 / 4 0 0 1 ] {\displaystyle {\begin{bmatrix}1/4&0\\0&1\\\end{bmatrix}}} [ 1 / 4 0 0 − − 1 / 4 ] {\displaystyle {\begin{bmatrix}1/4&0\\0&-1/4\end{bmatrix}}} Q ( x , y ) = 1/4 x 2 + y 2 Q ( x , y ) = 1/4 x 2 − 1/4 y 2 Points such that Q ( x , y ) = 1 ( Ellipse ).

Points such that Q ( x , y ) = 1 ( Hyperbola ).

A symmetric n × n -matrix is called positive-definite (respectively negative-definite; indefinite), if for all nonzero vectors x ∈ ∈ R n {\displaystyle x\in \mathbb {R} ^{n}} the associated quadratic form given by Q ( x ) = x T A x {\displaystyle Q(\mathbf {x} )=\mathbf {x} ^{\mathsf {T}}A\mathbf {x} } takes only positive values (respectively only negative values; both some negative and some positive values).

[ 4 ] If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.

A symmetric matrix is positive-definite if and only if all its eigenvalues are positive.

[ 5 ] The table at the right shows two possibilities for 2×2 matrices.

Allowing as input two different vectors instead yields the bilinear form associated to A : [ 6 ] B A ( x , y ) = x T A y .

{\displaystyle B_{A}(\mathbf {x} ,\mathbf {y} )=\mathbf {x} ^{\mathsf {T}}A\mathbf {y} .} Orthogonal matrix [ edit ] An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse : A T = A − − 1 , {\displaystyle A^{\textsf {T}}=A^{-1},} which entails A T A = A A T = I , {\displaystyle A^{\textsf {T}}A=AA^{\textsf {T}}=I,} where I is the identity matrix .

An orthogonal matrix A is necessarily invertible (with inverse A −1 = A T ), unitary ( A −1 = A * ), and normal ( A * A = AA * ). The determinant of any orthogonal matrix is either +1 or −1.  The special orthogonal group SO ⁡ ⁡ ( n ) {\displaystyle \operatorname {SO} (n)} consists of the n × n orthogonal matrices with determinant +1.

The complex analogue of an orthogonal matrix is a unitary matrix .

Normal matrix [ edit ] A real or complex square matrix A {\displaystyle A} is called normal if A ∗ ∗ A = A A ∗ ∗ {\displaystyle A^{*}A=AA^{*}} .

If a real square matrix is symmetric, skew-symmetric, or orthogonal, then it is normal. If a complex square matrix is Hermitian, skew-Hermitian, or unitary, then it is normal. Normal matrices are of interest mainly because they include the types of matrices just listed and form the broadest class of matrices for which the spectral theorem holds.

[ 7 ] Operations [ edit ] Trace [ edit ] The trace , tr( A ) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative, the trace of the product of two matrices is independent of the order of the factors: tr ⁡ ⁡ ( A B ) = tr ⁡ ⁡ ( B A ) .

{\displaystyle \operatorname {tr} (AB)=\operatorname {tr} (BA).} This is immediate from the definition of matrix multiplication: tr ⁡ ⁡ ( A B ) = ∑ ∑ i = 1 m ∑ ∑ j = 1 n A i j B j i = tr ⁡ ⁡ ( B A ) .

{\displaystyle \operatorname {tr} (AB)=\sum _{i=1}^{m}\sum _{j=1}^{n}A_{ij}B_{ji}=\operatorname {tr} (BA).} Also, the trace of a matrix is equal to that of its transpose, i.e., tr ⁡ ⁡ ( A ) = tr ⁡ ⁡ ( A T ) .

{\displaystyle \operatorname {tr} (A)=\operatorname {tr} (A^{\mathrm {T} }).} Determinant [ edit ] Main article: Determinant A linear transformation on R 2 {\displaystyle \mathbb {R} ^{2}} given by the indicated matrix. The determinant of this matrix is −1, as the area of the green parallelogram at the right is 1, but the map reverses the orientation , since it turns the counterclockwise orientation of the vectors to a clockwise one.

The determinant det ( A ) {\displaystyle \det(A)} or | A | {\displaystyle |A|} of a square matrix A {\displaystyle A} is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R 2 {\displaystyle \mathbb {R} ^{2}} ) or volume (in R 3 {\displaystyle \mathbb {R} ^{3}} ) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.

The determinant of 2×2 matrices is given by det [ a b c d ] = a d − − b c .

{\displaystyle \det {\begin{bmatrix}a&b\\c&d\end{bmatrix}}=ad-bc.} The determinant of 3×3 matrices involves 6 terms ( rule of Sarrus ). The more lengthy Leibniz formula generalizes these two formulae to all dimensions.

[ 8 ] The determinant of a product of square matrices equals the product of their determinants: [ 9 ] det ( A B ) = det ( A ) ⋅ ⋅ det ( B ) {\displaystyle \det(AB)=\det(A)\cdot \det(B)} Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.

[ 10 ] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors , i.e., determinants of smaller matrices.

[ 11 ] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1×1 matrix, which is its unique entry, or even the determinant of a 0×0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule , where the division of the determinants of two related square matrices equates to the value of each of the system's variables.

[ 12 ] Eigenvalues and eigenvectors [ edit ] Main article: Eigenvalues and eigenvectors A number λ and a non-zero vector v {\displaystyle \mathbf {v} } satisfying A v = λ λ v {\displaystyle A\mathbf {v} =\lambda \mathbf {v} } are called an eigenvalue and an eigenvector of A {\displaystyle A} , respectively.

[ 13 ] [ 14 ] The number λ is an eigenvalue of an n × n -matrix A if and only if A − λ I n is not invertible, which is equivalent to [ 15 ] det ( A − − λ λ I ) = 0.

{\displaystyle \det(A-\lambda I)=0.} The polynomial p A in an indeterminate X given by evaluation of the determinant det( XI n − A ) is called the characteristic polynomial of A . It is a monic polynomial of degree n . Therefore the polynomial equation p A (λ) = 0 has at most n different solutions, i.e., eigenvalues of the matrix.

[ 16 ] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem , p A ( A ) = 0 , that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix .

See also [ edit ] Cartan matrix Cayley-Hamilton theorem Notes [ edit ] ^ Brown 1991 , Definition I.2.28 ^ Brown 1991 , Definition I.5.13 ^ Horn & Johnson 1985 , Theorem 2.5.6 ^ Horn & Johnson 1985 , Chapter 7 ^ Horn & Johnson 1985 , Theorem 7.2.1 ^ Horn & Johnson 1985 , Example 4.0.6, p. 169 ^ Artin, Algebra , 2nd edition, Pearson, 2018, section 8.6.

^ Brown 1991 , Definition III.2.1 ^ Brown 1991 , Theorem III.2.12 ^ Brown 1991 , Corollary III.2.16 ^ Mirsky 1990 , Theorem 1.4.1 ^ Brown 1991 , Theorem III.3.18 ^ Eigen means "own" in German and in Dutch .

^ Brown 1991 , Definition III.4.1 ^ Brown 1991 , Definition III.4.9 ^ Brown 1991 , Corollary III.4.10 References [ edit ] Brown, William C. (1991), Matrices and vector spaces , New York, NY: Marcel Dekker , ISBN 978-0-8247-8419-5 Horn, Roger A.

; Johnson, Charles R.

(1985), Matrix Analysis , Cambridge University Press, ISBN 978-0-521-38632-6 Mirsky, Leonid (1990), An Introduction to Linear Algebra , Courier Dover Publications, ISBN 978-0-486-66434-7 External links [ edit ] Media related to Square matrices at Wikimedia Commons v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐46fs6
Cached time: 20250812002611
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.344 seconds
Real time usage: 0.469 seconds
Preprocessor visited node count: 5088/1000000
Revision size: 15948/2097152 bytes
Post‐expand include size: 31282/2097152 bytes
Template argument size: 4873/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 4/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 25250/5000000 bytes
Lua time usage: 0.168/10.000 seconds
Lua memory usage: 4753436/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  304.558      1 -total
 23.18%   70.603      3 Template:Citation
 22.32%   67.968      1 Template:Linear_algebra
 21.85%   66.555      1 Template:Navbox
 21.43%   65.275      1 Template:Short_description
 12.75%   38.819      2 Template:Pagetype
  9.52%   29.006      1 Template:Reflist
  7.02%   21.380     28 Template:Main_other
  6.72%   20.460     25 Template:Math
  6.52%   19.856      1 Template:Commonscat-inline Saved in parser cache with key enwiki:pcache:166022:|#|:idhash:canonical and timestamp 20250812002611 and revision id 1303259512. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Square_matrix&oldid=1303259512 " Category : Matrices (mathematics) Hidden categories: Articles with short description Short description matches Wikidata Commons category link from Wikidata This page was last edited on 29 July 2025, at 23:27 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Square matrix 39 languages Add topic

