Title: History of entropy

URL Source: https://en.wikipedia.org/wiki/History_of_entropy

Published Time: 2006-04-11T00:07:06Z

Markdown Content:
In the [history of physics](https://en.wikipedia.org/wiki/History_of_physics "History of physics"), the concept of [entropy](https://en.wikipedia.org/wiki/Entropy "Entropy") developed in response to the observation that a certain amount of functional energy released from [combustion reactions](https://en.wikipedia.org/wiki/Combustion_reactions "Combustion reactions") is always lost to dissipation or friction and is thus not transformed into [useful work](https://en.wikipedia.org/wiki/Work_(thermodynamics) "Work (thermodynamics)"). Early heat-powered engines such as [Thomas Savery](https://en.wikipedia.org/wiki/Thomas_Savery "Thomas Savery")'s (1698), the [Newcomen engine](https://en.wikipedia.org/wiki/Newcomen_engine "Newcomen engine") (1712) and [Nicolas-Joseph Cugnot](https://en.wikipedia.org/wiki/Nicolas-Joseph_Cugnot "Nicolas-Joseph Cugnot")'s [steam tricycle](https://en.wikipedia.org/wiki/Steam_tricycle "Steam tricycle") (1769) were inefficient, converting about 0.5% of the input energy into useful [work output](https://en.wikipedia.org/wiki/Work_output "Work output").[[1]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-1) Over the next two centuries, physicists investigated this puzzle of lost energy; the result was the concept of entropy.

In the early 1850s, [Rudolf Clausius](https://en.wikipedia.org/wiki/Rudolf_Clausius "Rudolf Clausius") set forth the concept of the [thermodynamic system](https://en.wikipedia.org/wiki/Thermodynamic_system "Thermodynamic system") and posited the argument that in any [irreversible process](https://en.wikipedia.org/wiki/Irreversible_process "Irreversible process") a small amount of [heat](https://en.wikipedia.org/wiki/Heat "Heat") energy _δQ_ is incrementally dissipated across the system boundary. Clausius continued to develop his ideas of lost energy, and coined the term _entropy_.

Since the mid-20th century the concept of entropy has found application in the field of [information theory](https://en.wikipedia.org/wiki/Information_theory "Information theory"), describing an analogous loss of data in information transmission systems.

Classical thermodynamic views
-----------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=History_of_entropy&action=edit&section=1 "Edit section: Classical thermodynamic views")]

In 1803, mathematician [Lazare Carnot](https://en.wikipedia.org/wiki/Lazare_Carnot "Lazare Carnot") published a work entitled _Fundamental Principles of Equilibrium and Movement_. This work includes a discussion on the efficiency of fundamental machines, i.e. pulleys and inclined planes. Carnot saw through all the details of the mechanisms to develop a general discussion on the conservation of mechanical energy. Over the next three decades, Carnot's theorem was taken as a statement that in any machine the accelerations and shocks of the moving parts all represent losses of _moment of activity_, i.e. the [useful work](https://en.wikipedia.org/wiki/Work_(thermodynamics) "Work (thermodynamics)") done. From this Carnot drew the inference that [perpetual motion](https://en.wikipedia.org/wiki/Perpetual_motion "Perpetual motion") was impossible. This _loss of moment of activity_ was the first-ever rudimentary statement of the [second law of thermodynamics](https://en.wikipedia.org/wiki/Second_law_of_thermodynamics "Second law of thermodynamics") and the concept of 'transformation-energy' or _entropy_, i.e. energy lost to dissipation and friction.[[2]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-2)

Carnot died in exile in 1823. During the following year his son [Sadi Carnot](https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot "Nicolas Léonard Sadi Carnot"), having graduated from the [École Polytechnique](https://en.wikipedia.org/wiki/%C3%89cole_Polytechnique "École Polytechnique") training school for engineers, but now living on half-pay with his brother Hippolyte in a small apartment in Paris, wrote _[Reflections on the Motive Power of Fire](https://en.wikipedia.org/wiki/Reflections\_on\_the\_Motive\_Power\_of\_Fire "Reflections on the Motive Power of Fire")_. In this book, Sadi visualized an [ideal engine](https://en.wikipedia.org/wiki/Carnot_heat_engine "Carnot heat engine") in which any heat (i.e., [caloric](https://en.wikipedia.org/wiki/Caloric_theory "Caloric theory")) converted into [work](https://en.wikipedia.org/wiki/Work_(thermodynamics) "Work (thermodynamics)"), could be reinstated by reversing the motion of the cycle, a concept subsequently known as [thermodynamic reversibility](https://en.wikipedia.org/wiki/Thermodynamic_reversibility "Thermodynamic reversibility"). Building on his father's work, Sadi postulated the concept that "some caloric is always lost" in the conversion into work, even in his idealized reversible heat engine, which excluded frictional losses and other losses due to the imperfections of any real machine. He also discovered that this idealized efficiency was dependent only on the [temperatures](https://en.wikipedia.org/wiki/Temperature "Temperature") of the heat reservoirs between which the engine was working, and not on the types of [working fluids](https://en.wikipedia.org/wiki/Working_fluid "Working fluid"). Any real [heat engine](https://en.wikipedia.org/wiki/Heat_engine "Heat engine") could not realize the [Carnot cycle](https://en.wikipedia.org/wiki/Carnot_cycle "Carnot cycle")'s reversibility, and was condemned to be even less efficient. This loss of usable caloric was a precursory form of the increase in entropy as we now know it. Though formulated in terms of caloric, rather than entropy, this was an early insight into the second law of thermodynamics.

Clausius' definitions
---------------------

[[edit](https://en.wikipedia.org/w/index.php?title=History_of_entropy&action=edit&section=2 "Edit section: Clausius' definitions")]

[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Clausius.jpg/250px-Clausius.jpg)](https://en.wikipedia.org/wiki/File:Clausius.jpg)

[Rudolf Clausius](https://en.wikipedia.org/wiki/Rudolf_Clausius "Rudolf Clausius") - originator of the concept of **"entropy"**

In his 1854 memoir, Clausius first develops the concepts of _interior work_, i.e. that "which the atoms of the body exert upon each other", and _exterior work_, i.e. that "which arise from foreign influences [to] which the body may be exposed", which may act on a working body of fluid or gas, typically functioning to work a piston. He then discusses the three categories into which heat _Q_ may be divided:

1.   Heat employed in increasing the heat actually existing in the body.
2.   Heat employed in producing the interior work.
3.   Heat employed in producing the exterior work.

Building on this logic, and following a mathematical presentation of the _first fundamental theorem_, Clausius then presented the first-ever mathematical formulation of entropy, although at this point in the development of his theories he called it "equivalence-value", perhaps referring to the concept of the [mechanical equivalent of heat](https://en.wikipedia.org/wiki/Mechanical_equivalent_of_heat "Mechanical equivalent of heat") which was developing at the time rather than entropy, a term which was to come into use later.[[3]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-3) He stated:[[4]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-4)

> the _second fundamental theorem_ in the mechanical [theory of heat](https://en.wikipedia.org/wiki/Theory_of_heat "Theory of heat") may thus be enunciated:
> 
> 
> If two transformations which, without necessitating any other permanent change, can mutually replace one another, be called equivalent, then the generations of the quantity of heat _Q_ from work at the temperature _T_, has the _equivalence-value_:
> 
> ![Image 2: {\displaystyle {\frac {Q}{T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2d8c893027e3b73ad7b5835c5372a4745a33927e)
> and the passage of the quantity of heat _Q_ from the temperature _T 1_ to the temperature _T 2_, has the equivalence-value:
> 
> ![Image 3: {\displaystyle Q\left({\frac {1}{T_{2}}}-{\frac {1}{T_{1}}}\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f8df14da60b0138fa54aeb2ff99ebfc2ee8206a8)
> wherein _T_ is a function of the temperature, independent of the nature of the process by which the transformation is effected.

In modern terminology, that is, the terminology introduced by Clausius himself in 1865, we think of this equivalence-value as "entropy", symbolized by _S_. Thus, using the above description, we can calculate the entropy change Δ _S_ for the passage of the quantity of heat _Q_ from the temperature _T 1_, through the "working body" of fluid, which was typically a body of steam, to the temperature _T 2_ as shown below:

[![Image 4](https://upload.wikimedia.org/wikipedia/en/thumb/d/dc/Entropy-diagram.png/330px-Entropy-diagram.png)](https://en.wikipedia.org/wiki/File:Entropy-diagram.png)

Diagram of Sadi Carnot's [heat engine](https://en.wikipedia.org/wiki/Heat_engine "Heat engine"), 1824

If we make the assignment:

![Image 5: {\displaystyle S={\frac {Q}{T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ce6a0e29fad3257f7eb3598923fbc62651093cb3)
Then, the entropy change or "equivalence-value" for this transformation is:

![Image 6: {\displaystyle \Delta S=S_{\rm {final}}-S_{\rm {initial}}\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8d2c9e3257d79b4fa39c1afad1a2ffe494ea14fa)
which equals:

![Image 7: {\displaystyle \Delta S=\left({\frac {Q}{T_{2}}}-{\frac {Q}{T_{1}}}\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3086ec10615c8a3583032bf7820d1af639a8b91f)
and by factoring out Q, we have the following form, as was derived by Clausius:

![Image 8: {\displaystyle \Delta S=Q\left({\frac {1}{T_{2}}}-{\frac {1}{T_{1}}}\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0e57adf9d3b323f781db7b559a1c4d24cb658013)
In 1856, Clausius stated what he called the "second fundamental theorem in the [mechanical theory of heat](https://en.wikipedia.org/wiki/Mechanical_theory_of_heat "Mechanical theory of heat")" in the following form:

![Image 9: {\displaystyle \int {\frac {\delta Q}{T}}=-N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/788c11865876c1029258d9f6c21a942a0909650f)
where _N_ is the "equivalence-value" of all uncompensated transformations involved in a cyclical process. This equivalence-value was a precursory formulation of entropy.[[5]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-5)

In 1862, Clausius stated what he calls the "theorem respecting the equivalence-values of the transformations" or what is now known as the [second law of thermodynamics](https://en.wikipedia.org/wiki/Second_law_of_thermodynamics "Second law of thermodynamics"), as such:

> The algebraic sum of all the transformations occurring in a cyclical process can only be positive, or, as an extreme case, equal to nothing.

Quantitatively, Clausius states the mathematical expression for this theorem is follows.

> Let _δQ_ be an element of the heat given up by the body to any reservoir of heat during its own changes, heat which it may absorb from a reservoir being here reckoned as negative, and _T_ the [absolute temperature](https://en.wikipedia.org/wiki/Absolute_temperature "Absolute temperature") of the body at the moment of giving up this heat, then the equation:
> 
> ![Image 10: {\displaystyle \int {\frac {\delta Q}{T}}=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/71cf0c2127238ee7dc489b3ee631c7ac3ad3c2dd)
> must be true for every reversible cyclical process, and the relation:
> 
> ![Image 11: {\displaystyle \int {\frac {\delta Q}{T}}\leq 0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d356388568be96fd69079f0aed387cfde1ccd42)
> must hold good for every cyclical process which is in any way possible.

This was an early formulation of the second law and one of the original forms of the concept of entropy.

In 1865, Clausius gave irreversible heat loss, or what he had previously been calling "equivalence-value", a name:[[6]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-6)[[7]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-7)[[8]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-8)

> I propose that _S_ be taken from the Greek words, 'en-tropie' [intrinsic direction]. I have deliberately chosen the word entropy to be as similar as possible to the word energy: the two quantities to be named by these words are so closely related in physical significance that a certain similarity in their names appears to be appropriate.[[9]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-Clausius1865-9)

Clausius did not specify why he chose the symbol _S_ to represent entropy, and it is almost certainly untrue that Clausius chose _S_ in honor of [Sadi Carnot](https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot "Nicolas Léonard Sadi Carnot"); the given names of scientists are rarely if ever used this way.[[10]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-10)

In 1876, physicist [J. Willard Gibbs](https://en.wikipedia.org/wiki/J._Willard_Gibbs "J. Willard Gibbs"), building on the work of Clausius, [Hermann von Helmholtz](https://en.wikipedia.org/wiki/Hermann_von_Helmholtz "Hermann von Helmholtz") and others, proposed that the measurement of "available energy" Δ _G_ in a thermodynamic system could be mathematically accounted for by subtracting the "energy loss" _T_ Δ _S_ from total energy change of the system Δ _H_. These concepts were further developed by [James Clerk Maxwell](https://en.wikipedia.org/wiki/James_Clerk_Maxwell "James Clerk Maxwell") [1871] [[11]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-11) and [Max Planck](https://en.wikipedia.org/wiki/Max_Planck "Max Planck") [1903] [[12]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-12)

Statistical mechanics
---------------------

[[edit](https://en.wikipedia.org/w/index.php?title=History_of_entropy&action=edit&section=8 "Edit section: Statistical mechanics")]

In 1877, [Ludwig Boltzmann](https://en.wikipedia.org/wiki/Ludwig_Boltzmann "Ludwig Boltzmann") developed a [statistical mechanics](https://en.wikipedia.org/wiki/Statistical_mechanics "Statistical mechanics") evaluation of the entropy _S_, of a body in its own given macrostate of internal thermodynamic equilibrium. It may be written as:

![Image 12: {\displaystyle S=k_{\rm {B}}\ln \Omega \!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1350a56e364438177d82c1d84965a63ff8757770)
where

_k_ B denotes the [Boltzmann constant](https://en.wikipedia.org/wiki/Boltzmann_constant "Boltzmann constant") and Ω denotes the number of microstates consistent with the given equilibrium macrostate.
Boltzmann himself did not actually write this formula expressed with the named constant _k_ B, which is due to Planck's reading of Boltzmann.[[13]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-13)

Boltzmann saw entropy as a measure of statistical "mixedupness" or disorder. This concept was soon refined by Gibbs, and is now regarded as one of the cornerstones of the theory of [statistical mechanics](https://en.wikipedia.org/wiki/Statistical_mechanics "Statistical mechanics").

Relation to living systems
--------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=History_of_entropy&action=edit&section=9 "Edit section: Relation to living systems")]

[Erwin Schrödinger](https://en.wikipedia.org/wiki/Erwin_Schr%C3%B6dinger "Erwin Schrödinger") made use of Boltzmann's work in his 1944 book _[What is Life?](https://en.wikipedia.org/wiki/What\_Is\_Life%3F "What Is Life?")_[[14]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-14) to explain why living systems have far fewer replication errors than would be predicted from statistical mechanics. Schrödinger used the Boltzmann equation in a different form to show increase of entropy

![Image 13: {\displaystyle S=k_{\rm {B}}\ln D\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/68a49bc77ecdcfdf6fc116190f6915d9e682045d)
where _D_ is the number of possible energy states in the system that can be randomly filled with energy. He postulated a local decrease of entropy for living systems when (1/D) represents the number of states that are prevented from randomly distributing, such as occurs in replication of the genetic code.

![Image 14: {\displaystyle -S=k_{\rm {B}}\ln(1/D)\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e20fcc4b34b5b3e0eb663e3441995b3d5d9e6110)[_[clarification needed](https://en.wikipedia.org/wiki/Wikipedia:Please\_clarify "Wikipedia:Please clarify")_]
Without this correction Schrödinger claimed that statistical mechanics would predict one thousand mutations per million replications, and ten mutations per hundred replications following the rule for square root of _n_, far more mutations than actually occur.

Schrödinger's separation of random and non-random energy states is one of the few explanations for why entropy could be low in the past, but continually increasing now. It has been proposed as an explanation of localized decrease of entropy[[15]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-15) in [radiant energy](https://en.wikipedia.org/wiki/Radiant_energy "Radiant energy") focusing in parabolic reflectors and during [dark current](https://en.wikipedia.org/wiki/Dark_current_(physics) "Dark current (physics)") in diodes, which would otherwise be in violation of statistical mechanics.

An analog to _thermodynamic entropy_ is **information entropy**. In 1948, while working at [Bell Telephone](https://en.wikipedia.org/wiki/Bell_Telephone_Company "Bell Telephone Company") Laboratories, electrical engineer [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon "Claude Shannon") set out to mathematically quantify the statistical nature of "lost information" in phone-line signals. To do this, Shannon developed the very general concept of [information entropy](https://en.wikipedia.org/wiki/Information_entropy "Information entropy"), a fundamental cornerstone of [information theory](https://en.wikipedia.org/wiki/Information_theory "Information theory"). Although the story varies, initially it seems that Shannon was not particularly aware of the close similarity between his new quantity and earlier work in thermodynamics. In 1939, however, when Shannon had been working on his equations for some time, he happened to visit the mathematician [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann "John von Neumann"). During their discussions, regarding what Shannon should call the "measure of uncertainty" or attenuation in phone-line signals with reference to his new information theory, according to one source:[[16]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-16)

> My greatest concern was what to call it. I thought of calling it ‘information’, but the word was overly used, so I decided to call it ‘uncertainty’. When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, ‘You should call it entropy, for two reasons: In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, nobody knows what entropy really is, so in a debate you will always have the advantage.

According to another source, when von Neumann asked him how he was getting on with his information theory, Shannon replied:[[17]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-17)

> The theory was in excellent shape, except that he needed a good name for "missing information". "Why don’t you call it entropy", von Neumann suggested. "In the first place, a mathematical development very much like yours already exists in Boltzmann's statistical mechanics, and in the second place, no one understands entropy very well, so in any discussion you will be in a position of advantage.

In 1948 Shannon published his seminal paper _[A Mathematical Theory of Communication](https://en.wikipedia.org/wiki/A\_Mathematical\_Theory\_of\_Communication "A Mathematical Theory of Communication")_, in which he devoted a section to what he calls Choice, Uncertainty, and Entropy.[[18]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-18) In this section, Shannon introduces an _H function_ of the following form:

![Image 15: {\displaystyle H=-K\sum _{i=1}^{k}p(i)\log p(i),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/26b927150acd4dd0cf7e67fa35ca5f0431c2bf6b)
where _K_ is a positive constant. Shannon then states that "any quantity of this form, where _K_ merely amounts to a choice of a unit of measurement, plays a central role in information theory as measures of information, choice, and uncertainty." Then, as an example of how this expression applies in a number of different fields, he references [Richard C. Tolman](https://en.wikipedia.org/wiki/Richard_C._Tolman "Richard C. Tolman")'s 1938 _Principles of Statistical Mechanics_, stating that

> the form of _H_ will be recognized as that of entropy as defined in certain formulations of statistical mechanics where _p i_ is the probability of a system being in cell _i_ of its phase space ... _H_ is then, for example, the _H_ in Boltzmann's famous [_H_ theorem](https://en.wikipedia.org/wiki/H-theorem "H-theorem").

As such, over the last fifty years, ever since this statement was made, people have been overlapping the two concepts or even stating that they are exactly the same.

Shannon's information entropy is a much more general concept than statistical thermodynamic entropy. Information entropy is present whenever there are unknown quantities that can be described only by a probability distribution. In a series of papers by [E. T. Jaynes](https://en.wikipedia.org/wiki/E._T._Jaynes "E. T. Jaynes") starting in 1957,[[19]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-19)[[20]](https://en.wikipedia.org/wiki/History_of_entropy#cite_note-20) the statistical thermodynamic entropy can be seen as just a particular application of Shannon's information entropy to the probabilities of particular microstates of a system occurring in order to produce a particular macrostate.

The term entropy is often used in popular language to denote a variety of unrelated phenomena. One example is the concept of **corporate entropy** as put forward somewhat humorously by authors [Tom DeMarco](https://en.wikipedia.org/wiki/Tom_DeMarco "Tom DeMarco") and Timothy Lister in their 1987 classic publication _[Peopleware](https://en.wikipedia.org/wiki/Peopleware "Peopleware")_, a book on growing and managing productive teams and successful software projects. Here, they view energy waste as red tape and business team inefficiency as a form of entropy, i.e. energy lost to waste. This concept has caught on and is now common jargon in business schools.

In another example, entropy is the central theme in [Isaac Asimov](https://en.wikipedia.org/wiki/Isaac_Asimov "Isaac Asimov")'s short story "[The Last Question](https://en.wikipedia.org/wiki/The_Last_Question "The Last Question")" (first copyrighted in 1956). The story plays with the idea that the most important question is how to stop the increase of entropy.

Terminology overlap
-------------------

[[edit](https://en.wikipedia.org/w/index.php?title=History_of_entropy&action=edit&section=12 "Edit section: Terminology overlap")]

When necessary, to disambiguate between the statistical thermodynamic concept of entropy, and entropy-like formulae put forward by different researchers, the statistical thermodynamic entropy is most properly referred to as the [Gibbs entropy](https://en.wikipedia.org/wiki/Gibbs_entropy "Gibbs entropy"). The terms _Boltzmann–Gibbs entropy_ or _BG entropy_, and _Boltzmann–Gibbs–Shannon entropy_ or _BGS entropy_ are also seen in the literature.

*   [Entropy](https://en.wikipedia.org/wiki/Entropy "Entropy")
*   [Enthalpy](https://en.wikipedia.org/wiki/Enthalpy "Enthalpy")
*   [History of thermodynamics](https://en.wikipedia.org/wiki/History_of_thermodynamics "History of thermodynamics")
*   [History of perpetual motion machines](https://en.wikipedia.org/wiki/History_of_perpetual_motion_machines "History of perpetual motion machines")

1.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-1)**"A single-atom heat engine: The power of an engine". _Physics Today_. **73** (5): 66–72. 2020.
2.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-2)**Mendoza, E. (1988). _Reflections on the Motive Power of Fire – and other Papers on the Second Law of Thermodynamics by E. Clapeyron and R. Clausius_. New York: Dover Publications. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-486-44641-7](https://en.wikipedia.org/wiki/Special:BookSources/0-486-44641-7 "Special:BookSources/0-486-44641-7").
3.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-3)**_Mechanical Theory of Heat_, by [Rudolf Clausius](https://en.wikipedia.org/wiki/Rudolf_Clausius "Rudolf Clausius"), 1850-1865
4.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-4)**Published in _Poggendoff's Annalen_, December 1854, vol. xciii. p. 481; translated in the _Journal de Mathematiques_, vol. xx. Paris, 1855, and in the _Philosophical Magazine_, August 1856, s. 4. vol. xii, p. 81
5.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-5)**Clausius, Rudolf. (1856). "_On the Application of the Mechanical theory of Heat to the Steam-Engine_." as found in: Clausius, R. (1865). [The Mechanical Theory of Heat – with its Applications to the Steam Engine and to Physical Properties of Bodies](https://archive.org/details/mechanicaltheor04claugoog). London: John van Voorst, 1 Paternoster Row. MDCCCLXVII.
6.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-6)**Laidler, Keith J. (1995). _The Physical World of Chemistry_. Oxford University Press. pp.104–105. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-19-855919-4](https://en.wikipedia.org/wiki/Special:BookSources/0-19-855919-4 "Special:BookSources/0-19-855919-4").
7.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-7)**[OED](https://en.wikipedia.org/wiki/OED "OED"), Second Edition, 1989, "_Clausius (Pogg. Ann. CXXV. 390), assuming (unhistorically) the etymological sense of energy to be ‘work-contents’ (werk-inhalt), devised the term entropy as a corresponding designation for the ‘transformation-contents’ (Verwandlungsinhalt) of a system"_
8.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-8)**Baierlein, Ralph (December 1992). "How entropy got its name". _American Journal of Physics_. **60** (12): 1151. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[1992AmJPh..60.1151B](https://ui.adsabs.harvard.edu/abs/1992AmJPh..60.1151B). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1119/1.16966](https://doi.org/10.1119%2F1.16966).
9.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-Clausius1865_9-0)**Clausius, Rudolf (1865). ["Ueber verschiedene für die Anwendung bequeme Formen der Hauptgleichungen der mechanischen Wärmetheorie (Vorgetragen in der naturforsch. Gesellschaft zu Zürich den 24. April 1865)"](https://zenodo.org/record/1423700). _Annalen der Physik und Chemie_. **125** (7): 353–400. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[1865AnP...201..353C](https://ui.adsabs.harvard.edu/abs/1865AnP...201..353C). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1002/andp.18652010702](https://doi.org/10.1002%2Fandp.18652010702). "Sucht man für _S_ einen bezeichnenden Namen, so könnte man, ähnlich wie von der Gröſse _U_ gesagt ist, sie sey der _Wärme- und Werkinhalt_ des Körpers, von der Gröſse _S_ sagen, sie sey der _Verwandlungsinhalt_ des Körpers. Da ich es aber für besser halte, die Namen derartiger für die Wissenschaft wichtiger Gröſsen aus den alten Sprachen zu entnehmen, damit sie unverändert in allen neuen Sprachen angewandt werden können, so schlage ich vor, die Gröſse _S_ nach dem griechischen Worte ἡ τροπή, die Verwandlung, die _Entropie_ des Körpers zu nennen. Das Wort _Entropie_ habei ich absichtlich dem Worte _Energie_ möglichst ähnlich gebildet, denn die beiden Gröſsen, welche durch diese Worte benannt werden sollen, sind ihren physikalischen Bedeutungen nach einander so nahe verwandt, daſs eine gewisse Gleichartigkeit in der Benennung mir zweckmäſsig zu seyn scheint." (p. 390).
10.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-10)**Girolami, G. S. (2020). "A Brief History of Thermodynamics, As Illustrated by Books and People". _J. Chem. Eng. Data_. **65** (2): 298–311. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1021/acs.jced.9b00515](https://doi.org/10.1021%2Facs.jced.9b00515). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[203146340](https://api.semanticscholar.org/CorpusID:203146340).
11.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-11)**["Maxwell's demon | Entropy, Thermodynamics, Heat | Britannica"](https://www.britannica.com/science/Maxwells-demon). _www.britannica.com_. Retrieved 2025-05-27.
12.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-12)**[Planck, M.](https://en.wikipedia.org/wiki/Max_Planck "Max Planck") (1903). [_Treatise on Thermodynamics_](https://archive.org/stream/treatiseonthermo00planuoft#page/n7/mode/2up). Ogg, A. (transl.). London: Longmans, Green & Co. [OL](https://en.wikipedia.org/wiki/OL_(identifier) "OL (identifier)")[7246691M](https://openlibrary.org/books/OL7246691M).
13.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-13)**[Partington, J.R.](https://en.wikipedia.org/wiki/J.R._Partington "J.R. Partington") (1949), _An Advanced Treatise on Physical Chemistry_, vol.1, _Fundamental Principles_, _The Properties of Gases_, London: [Longmans, Green and Co.](https://en.wikipedia.org/wiki/Longman "Longman"), p.300
14.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-14)**Schrödinger, Erwin (2004). _What is Life?_ (11th reprinting ed.). Cambridge: Canto. pp.72–73. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-521-42708-8](https://en.wikipedia.org/wiki/Special:BookSources/0-521-42708-8 "Special:BookSources/0-521-42708-8").
15.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-15)**["Random and Non Random States"](http://www.science20.com/rational_mystic_amateur_astronomer/blog/science_questions_new_year). 27 August 2014.
16.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-16)**M. Tribus, E.C. McIrvine, "Energy and information", _Scientific American_, 224 (September 1971).
17.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-17)**Avery, John (2003). _Information Theory and Evolution_. World Scientific. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[981-238-400-6](https://en.wikipedia.org/wiki/Special:BookSources/981-238-400-6 "Special:BookSources/981-238-400-6").
18.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-18)**C.E. Shannon, "A Mathematical Theory of Communication", _[Bell System Technical Journal](https://en.wikipedia.org/wiki/Bell\_System\_Technical\_Journal "Bell System Technical Journal")_, vol. 27, pp. 379-423, 623-656, July, October, 1948, [Eprint](http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html)[Archived](https://web.archive.org/web/19980131083455/http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html) 1998-01-31 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine "Wayback Machine"), [PDF](http://webarchive.loc.gov/all/20020913071303/http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf)
19.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-19)**E. T. Jaynes (1957) [Information theory and statistical mechanics](http://bayes.wustl.edu/etj/articles/theory.1.pdf), _Physical Review_**106**:620
20.   **[^](https://en.wikipedia.org/wiki/History_of_entropy#cite_ref-20)**E. T. Jaynes (1957) [Information theory and statistical mechanics II](http://bayes.wustl.edu/etj/articles/theory.2.pdf), _Physical Review_**108**:171
