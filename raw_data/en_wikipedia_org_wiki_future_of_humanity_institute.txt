Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Existential risk 3 Anthropic reasoning 4 Human enhancement and rationality 5 Selected publications 6 See also 7 References 8 Further reading 9 External links Toggle the table of contents Future of Humanity Institute 11 languages Català Deutsch Español فارسی Français Nederlands Polski Português Русский Svenska Тоҷикӣ Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Defunct Oxford interdisciplinary research centre Not to be confused with Future of Life Institute .

Future of Humanity Institute Formation 2005 ; 20 years ago ( 2005 ) Dissolved 16 April 2024 ; 16 months ago ( 2024-04-16 ) Purpose Research big-picture questions about humanity and its prospects Headquarters Oxford , England Director Nick Bostrom Parent organization Faculty of Philosophy , University of Oxford Website futureofhumanityinstitute.org The Future of Humanity Institute ( FHI ) was an interdisciplinary research centre at the University of Oxford investigating big-picture questions about humanity and its prospects. It was founded in 2005 as part of the Faculty of Philosophy and the Oxford Martin School .

[ 1 ] Its director was philosopher Nick Bostrom , and its research staff included futurist Anders Sandberg and Giving What We Can founder Toby Ord .

[ 2 ] Sharing an office and working closely with the Centre for Effective Altruism , the institute's stated objective was to focus research where it can make the greatest positive difference for humanity in the long term.

[ 3 ] [ 4 ] It engaged in a mix of academic and outreach activities, seeking to promote informed discussion and public engagement in government, businesses, universities, and other organizations. The centre's largest research funders included Amlin , Elon Musk , the European Research Council , Future of Life Institute , and Leverhulme Trust .

[ 5 ] On 16 April 2024 the University of Oxford closed the Institute, which said it had "faced increasing administrative headwinds within the Faculty of Philosophy ".

[ 6 ] [ 7 ] History [ edit ] Nick Bostrom established the institute in November 2005 as part of the Oxford Martin School, then the James Martin 21st Century School.

[ 1 ] Between 2008 and 2010, FHI hosted the Global Catastrophic Risks conference, wrote 22 academic journal articles, and published 34 chapters in academic volumes. FHI researchers have given policy advice at the World Economic Forum , to the private and non-profit sector (such as the Macarthur Foundation , and the World Health Organization ), as well as to governmental bodies in Sweden, Singapore, Belgium, the United Kingdom, and the United States.

Bostrom and bioethicist Julian Savulescu also published the book Human Enhancement in March 2009.

[ 8 ] Most recently, FHI has focused on the dangers of advanced artificial intelligence (AI). In 2014, its researchers published several books on AI risk, including Stuart Armstrong's Smarter Than Us and Bostrom's Superintelligence: Paths, Dangers, Strategies .

[ 9 ] [ 10 ] In 2018, Open Philanthropy recommended a grant of up to approximately £13.4 million to FHI over three years, with a large portion conditional on successful hiring.

[ 11 ] Existential risk [ edit ] Main article: Existential risk The largest topic FHI has spent time exploring is global catastrophic risk , and in particular existential risk. In a 2002 paper, Bostrom defined an "existential risk" as one "where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential".

[ 12 ] This includes scenarios where humanity is not directly harmed, but it fails to colonize space and make use of the observable universe's available resources in humanly valuable projects, as discussed in Bostrom's 2003 paper, "Astronomical Waste: The Opportunity Cost of Delayed Technological Development".

[ 13 ] Bostrom and Milan Ćirković 's 2008 book Global Catastrophic Risks collects essays on a variety of such risks, both natural and anthropogenic. Possible catastrophic risks from nature include super-volcanism , impact events , and energetic astronomical events such as gamma-ray bursts , cosmic rays , solar flares , and supernovae . These dangers are characterized as relatively small and relatively well understood, though pandemics may be exceptions as a result of being more common, and of dovetailing with technological trends.

[ 14 ] [ 4 ] Synthetic pandemics via weaponized biological agents are given more attention by FHI. Technological outcomes the institute is particularly interested in include anthropogenic climate change , nuclear warfare and nuclear terrorism , molecular nanotechnology , and artificial general intelligence . In expecting the largest risks to stem from future technologies, and from advanced artificial intelligence in particular, FHI agrees with other existential risk reduction organizations, such as the Centre for the Study of Existential Risk and the Machine Intelligence Research Institute .

[ 15 ] [ 16 ] FHI researchers have also studied the impact of technological progress on social and institutional risks, such as totalitarianism , automation-driven unemployment , and information hazards.

[ 17 ] In 2020, FHI Senior Research Fellow Toby Ord published his book The Precipice: Existential Risk and the Future of Humanity , in which he argues that safeguarding humanity's future is among the most important moral issues of our time.

[ 18 ] [ 19 ] Anthropic reasoning [ edit ] Main article: Anthropic principle FHI devotes much of its attention to exotic threats that have been little explored by other organizations, and to methodological considerations that inform existential risk reduction and forecasting. The institute has particularly emphasized anthropic reasoning in its research, as an under-explored area with general epistemological implications.

Anthropic arguments FHI has studied include the doomsday argument , which claims that humanity is likely to go extinct soon because it is unlikely that one is observing a point in human history that is extremely early. Instead, present-day humans are likely to be near the middle of the distribution of humans that will ever live.

[ 14 ] Bostrom has also popularized the simulation argument .

A recurring theme in FHI's research is the Fermi paradox , the surprising absence of observable alien civilizations. Robin Hanson has argued that there must be a " Great Filter " preventing space colonization to account for the paradox. That filter may lie in the past, if intelligence is much more rare than current biology would predict; or it may lie in the future, if existential risks are even larger than is currently recognized.

Human enhancement and rationality [ edit ] Closely linked to FHI's work on risk assessment, astronomical waste, and the dangers of future technologies is its work on the promise and risks of human enhancement . The modifications in question may be biological, digital, or sociological, and an emphasis is placed on the most radical hypothesized changes, rather than on the likeliest short-term innovations. FHI's bioethics research focuses on the potential consequences of gene therapy , life extension , brain implants and brain–computer interfaces , and mind uploading .

[ 20 ] FHI's focus has been on methods for assessing and enhancing human intelligence and rationality, as a way of shaping the speed and direction of technological and social progress. FHI's work on human irrationality, as exemplified in cognitive heuristics and biases , includes an ongoing collaboration with Amlin to study the systemic risk arising from biases in modeling.

[ 21 ] [ 22 ] Selected publications [ edit ] Toby Ord : The Precipice: Existential Risk and the Future of Humanity , 2020.

ISBN 1526600218 Nick Bostrom : Superintelligence: Paths, Dangers, Strategies , 2014.

ISBN 0-415-93858-9 Nick Bostrom and Milan Ćirković : Global Catastrophic Risks , 2011.

ISBN 978-0-19-857050-9 Nick Bostrom and Julian Savulescu : Human Enhancement , 2011.

ISBN 0-19-929972-2 Nick Bostrom: Anthropic Bias: Observation Selection Effects in Science and Philosophy , 2010.

ISBN 0-415-93858-9 Nick Bostrom and Anders Sandberg: Brain Emulation Roadmap , 2008.

See also [ edit ] Future of Life Institute Futures studies Leverhulme Centre for the Future of Intelligence Effective altruism References [ edit ] ^ a b "Humanity's Future: Future of Humanity Institute" .

Oxford Martin School . Archived from the original on 17 March 2014 . Retrieved 28 March 2014 .

^ "Staff" .

Future of Humanity Institute . Retrieved 28 March 2014 .

^ "About FHI" .

Future of Humanity Institute . Archived from the original on 1 December 2015 . Retrieved 28 March 2014 .

^ a b Ross Andersen (25 February 2013).

"Omens" .

Aeon Magazine . Archived from the original on 9 February 2014 . Retrieved 28 March 2014 .

^ "Support FHI" .

Future of Humanity Institute . 2021.

Archived from the original on 20 October 2021 . Retrieved 23 July 2022 .

^ "Future of Humanity Institute" . 17 April 2024. Archived from the original on 17 April 2024 . Retrieved 17 April 2024 .

{{ cite web }} :  CS1 maint: bot: original URL status unknown ( link ) ^ Maiberg, Emanuel (17 April 2024).

"Institute That Pioneered AI 'Existential Risk' Research Shuts Down" .

404 Media . Retrieved 17 April 2024 .

^ Nick Bostrom (18 July 2007).

Achievements Report: 2008-2010 (PDF) (Report). Future of Humanity Institute. Archived from the original (PDF) on 21 December 2012 . Retrieved 31 March 2014 .

^ Mark Piesing (17 May 2012).

"AI uprising: humans will be outsourced, not obliterated" .

Wired . Retrieved 31 March 2014 .

^ Coughlan, Sean (24 April 2013).

"How are humans going to become extinct?" .

BBC News . Retrieved 29 March 2014 .

^ Open Philanthropy Project (July 2018).

"Future of Humanity Institute — Work on Global Catastrophic Risks" .

^ Nick Bostrom (March 2002).

"Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards" .

Journal of Evolution and Technology .

15 (3): 308– 314 . Retrieved 31 March 2014 .

^ Nick Bostrom (November 2003).

"Astronomical Waste: The Opportunity Cost of Delayed Technological Development" .

Utilitas .

15 (3): 308– 314.

CiteSeerX 10.1.1.429.2849 .

doi : 10.1017/s0953820800004076 .

S2CID 15860897 . Retrieved 31 March 2014 .

^ a b Ross Andersen (6 March 2012).

"We're Underestimating the Risk of Human Extinction" .

The Atlantic . Retrieved 29 March 2014 .

^ Kate Whitehead (16 March 2014).

"Cambridge University study centre focuses on risks that could annihilate mankind" .

South China Morning Post . Retrieved 29 March 2014 .

^ Jenny Hollander (September 2012).

"Oxford Future of Humanity Institute knows what will make us extinct" .

Bustle . Retrieved 31 March 2014 .

^ Nick Bostrom.

"Information Hazards: A Typology of Potential Harms from Knowledge" (PDF) .

Future of Humanity Institute . Retrieved 31 March 2014 .

^ Ord, Toby.

"The Precipice: Existential Risk and the Future of Humanity" .

The Precipice Website . Retrieved 18 October 2020 .

^ Chivers, Tom (7 March 2020).

"How close is humanity to destroying itself?" .

The Spectator . Retrieved 18 October 2020 .

^ Anders Sandberg and Nick Bostrom.

"Whole Brain Emulation: A Roadmap" (PDF) .

Future of Humanity Institute . Retrieved 31 March 2014 .

^ "Amlin and Oxford University launch major research project into the Systemic Risk of Modelling" (Press release). Amlin. 11 February 2014. Archived from the original on 13 April 2014 . Retrieved 31 March 2014 .

^ "Amlin and Oxford University to collaborate on modelling risk study" .

Continuity, Insurance & Risk Magazine . 11 February 2014 . Retrieved 31 March 2014 .

Further reading [ edit ] Anthony, Andrew (28 April 2024).

" 'Eugenics on steroids': the toxic and contested legacy of Oxford's Future of Humanity Institute" .

The Observer .

External links [ edit ] FHI official website v t e Future of Humanity Institute People Nick Bostrom K. Eric Drexler Robin Hanson Toby Ord Anders Sandberg Rebecca Roache Concepts Differential technological development Global catastrophic risk Great Filter Pascal's mugging Reversal test Self-indication assumption Self-sampling assumption Simulation hypothesis Singleton Works Anthropic Bias Global Catastrophic Risks Human Enhancement The Precipice Superintelligence: Paths, Dangers, Strategies v t e Effective altruism Concepts Aid effectiveness Charity assessment Demandingness objection Disability-adjusted life year Disease burden Distributional cost-effectiveness analysis Earning to give Equal consideration of interests Incremental cost-effectiveness ratio Longtermism Marginal utility Moral circle expansion Psychological barriers to effective altruism Quality-adjusted life year Utilitarianism Venture philanthropy Key figures Sam Bankman-Fried Liv Boeree Nick Bostrom Hilary Greaves Holden Karnofsky William MacAskill Dustin Moskovitz Yew-Kwang Ng Toby Ord Derek Parfit Kelsey Piper Peter Singer Brian Tomasik Cari Tuna Eliezer Yudkowsky Organizations 80,000 Hours Against Malaria Foundation Animal Charity Evaluators Animal Ethics Centre for Effective Altruism Centre for Enabling EA Learning & Research Center for High Impact Philanthropy Centre for the Study of Existential Risk Development Media International Evidence Action Faunalytics Fistula Foundation Future of Humanity Institute Future of Life Institute Founders Pledge GiveDirectly GiveWell Giving Multiplier Giving What We Can Good Food Fund The Good Food Institute Good Ventures The Humane League Mercy for Animals Machine Intelligence Research Institute Malaria Consortium Open Philanthropy Raising for Effective Giving Sentience Institute Unlimit Health Wild Animal Initiative Focus areas Biotechnology risk Climate change Cultured meat Economic stability Existential risk from artificial general intelligence Global catastrophic risk Global health Global poverty Intensive animal farming Land use reform Life extension Malaria prevention Mass deworming Neglected tropical diseases Risk of astronomical suffering Wild animal suffering Literature Doing Good Better The End of Animal Farming Famine, Affluence, and Morality The Life You Can Save Living High and Letting Die The Most Good You Can Do Practical Ethics The Precipice Superintelligence: Paths, Dangers, Strategies What We Owe the Future Events Effective Altruism Global v t e Global catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Human extinction risk estimates Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Ransomware Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeover Existential risk from artificial intelligence Technological singularity Transhumanism Sociological Anthropogenic hazard Collapsology Doomsday argument Self-indication assumption doomsday argument rebuttal Self-referencing doomsday argument rebuttal Economic collapse Malthusian catastrophe New World Order (conspiracy theory) Nuclear holocaust cobalt famine winter Riots Social crisis Societal collapse State collapse World War III Ecological Climate change Anoxic event Biodiversity loss Mass mortality event Cascade effect Cataclysmic pole shift hypothesis Deforestation Desertification Plant or animal species extinctions Civilizational collapse Tipping points Climate sensitivity Flood basalt Global dimming Global terrestrial stilling Global warming Hypercane Ice age Ecocide Ecological collapse Environmental degradation Habitat destruction Human impact on the environment coral reefs on marine life Land degradation Land consumption Land surface effects on climate Ocean acidification Ozone depletion Resource depletion Sea level rise Supervolcano winter Verneshot Water pollution Water scarcity Earth Overshoot Day Overexploitation Overpopulation Human overpopulation Biological Extinction Extinction event Holocene extinction Human extinction List of extinction events Genetic erosion Genetic pollution Others Biodiversity loss Decline in amphibian populations Decline in insect populations Biotechnology risk Biological agent Biological warfare Bioterrorism Colony collapse disorder Defaunation Dysgenics Interplanetary contamination Pandemic Pollinator decline Overfishing Astronomical Big Crunch Big Rip Coronal mass ejection Cosmological phase transition Geomagnetic storm False vacuum decay Gamma-ray burst Heat death of the universe Proton decay Virtual black hole Impact event Asteroid impact avoidance Asteroid impact prediction Potentially hazardous object Near-Earth object winter Rogue planet Rogue star Near-Earth supernova Hypernova Micronova Solar flare Stellar collision Eschatological Buddhist Maitreya Three Ages Hindu Kalki Kali Yuga Last Judgement Second Coming 1 Enoch Daniel Abomination of desolation Prophecy of Seventy Weeks Messiah Christian Futurism Historicism Interpretations of Revelation Idealism Preterism 2 Esdras 2 Thessalonians Man of sin Katechon Antichrist Book of Revelation Events Four Horsemen of the Apocalypse Seven bowls Seven seals The Beast Two witnesses War in Heaven Whore of Babylon Great Apostasy New Earth New Jerusalem Olivet Discourse Great Tribulation Son of perdition Sheep and Goats Islamic Al-Qa'im Beast of the Earth Dhu al-Qarnayn Dhul-Suwayqatayn Dajjal Israfil Mahdi Sufyani Jewish Messiah War of Gog and Magog Third Temple Norse Zoroastrian Saoshyant Others 2011 end times prediction 2012 phenomenon Apocalypse Apocalyptic literature Apocalypticism Armageddon Blood moon prophecy Earth Changes End time Gog and Magog List of dates predicted for apocalyptic events Messianism Messianic Age Millenarianism Millennialism Premillennialism Amillennialism Postmillennialism Nemesis (hypothetical star) Nibiru cataclysm Rapture Prewrath Posttribulation rapture Resurrection of the dead Vulnerable world hypothesis World to come Fictional Alien invasion Apocalyptic and post-apocalyptic fiction List of apocalyptic and post-apocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films Zombie apocalypse Zombie Organizations Centre for the Study of Existential Risk Future of Humanity Institute Future of Life Institute Nuclear Threat Initiative General Disaster Depression Financial crisis Survivalism World portal Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios v t e Existential risk from artificial intelligence Concepts AGI AI alignment AI boom AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial intelligence Friendly artificial intelligence Instrumental convergence Vulnerable world hypothesis Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI Safe Superintelligence People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Ilya Sutskever Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Artificial Intelligence Act Do You Trust This Computer?

Human Compatible Open letter on artificial intelligence (2015) Our Final Invention Roko's basilisk Statement on AI risk of extinction Superintelligence: Paths, Dangers, Strategies The Precipice Category v t e LessWrong People Aella Scott Alexander Paul Christiano Wei Dai Robin Hanson Zvi Mowshowitz Eliezer Yudkowsky Organizations Center for Applied Rationality Future of Humanity Institute Machine Intelligence Research Institute MetaMed Zizians Works Harry Potter and the Methods of Rationality Rationality: From AI to Zombies Concepts Friendly artificial intelligence Pascal's mugging Roko's basilisk Waluigi effect v t e Molecular nanotechnology Concepts Molecular assembler Molecular machine Mechanosynthesis Mechanochemistry Nanorobotics Self-replicating machine Productive nanosystems Gray goo Exploratory engineering Carbon nanotube nanomotor Utility fog Ecophagy Starseed launcher Organizations Foresight Institute Future of Humanity Institute Zyvex Works Engines of Creation Great Mambo Chicken and the Transhuman Condition " There's Plenty of Room at the Bottom " Other Feynman Prize in Nanotechnology Drexler–Smalley debate on molecular nanotechnology People James C. Bennett K. Eric Drexler Robert Freitas J. Storrs Hall Ralph Merkle Carlo Montemagno Christine Peterson Related topics Nanotechnology Nanomedicine Transhumanism Cryonics Technological singularity Impact of nanotechnology Societal v t e Transhumanism Overviews Transhuman Transhumanism in fiction Currents Accelerationism Effective Antinaturalism Cypherpunk Dataism Eradication of suffering Extropianism Immortalism Postgenderism Posthumanism Postpoliticism Russian cosmism Singularitarianism Technogaianism Technolibertarianism Technological utopianism Techno-progressivism Organizations Foresight Institute Humanity+ Institute for Ethics and Emerging Technologies Future of Humanity Institute LessWrong US Transhumanist Party People Andrews Bostrom Church José Luis Cordeiro K. Eric Drexler Fahy FM-2030 Freitas Fuller Fyodorov de Garis Gasson David Gobel Ben Goertzel de Grey Haldane Hanson Harari Harbisson Harris Huxley Hughes Zoltan Istvan Ray Kurzweil Land Ole Martin Moen Hans Moravec Max More Elon Musk Osborn David Pearce Martine Rothblatt Anders Sandberg Savulescu Sorgner Spencer Stock Gennady Stolyarov II Teilhard de Chardin Vernor Vinge Natasha Vita-More Mark Alan Walker Warwick Eliezer Yudkowsky Category NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐dnzqg
Cached time: 20250817054517
Cache expiry: 1275286
Reduced expiry: true
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.625 seconds
Real time usage: 0.780 seconds
Preprocessor visited node count: 3385/1000000
Revision size: 17122/2097152 bytes
Post‐expand include size: 157747/2097152 bytes
Template argument size: 2339/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 151674/5000000 bytes
Lua time usage: 0.356/10.000 seconds
Lua memory usage: 7590954/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  614.266      1 -total
 30.49%  187.292      1 Template:Reflist
 18.78%  115.356      9 Template:Navbox
 17.03%  104.599      1 Template:Infobox_organization
 15.86%   97.413      1 Template:Infobox
 13.86%   85.145     12 Template:Cite_web
 12.01%   73.796      1 Template:Future_of_Humanity_Institute
 10.06%   61.796      1 Template:Short_description
  9.05%   55.584      5 Template:ISBN
  8.71%   53.482      7 Template:Cite_news Saved in parser cache with key enwiki:pcache:8481853:|#|:idhash:canonical and timestamp 20250817054517 and revision id 1285773733. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Future_of_Humanity_Institute&oldid=1285773733 " Categories : Departments of the University of Oxford Futures studies organizations Research institutes in Oxford Research institutes established in 2005 2005 establishments in England Transhumanist organizations Existential risk organizations Organizations associated with effective altruism Nick Bostrom Hidden categories: CS1 maint: bot: original URL status unknown Articles with short description Short description is different from Wikidata Use dmy dates from April 2022 This page was last edited on 15 April 2025, at 18:12 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Future of Humanity Institute 11 languages Add topic

