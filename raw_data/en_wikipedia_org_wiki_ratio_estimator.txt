Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Statistical properties Toggle Statistical properties subsection 2.1 Correction of the mean's bias 3 Jackknife estimation Toggle Jackknife estimation subsection 3.1 Other methods of estimation 4 Estimate of total 5 Variance estimates Toggle Variance estimates subsection 5.1 Variance of total 5.2 Variance of mean 6 Skewness Toggle Skewness subsection 6.1 Effect on confidence intervals 7 Alternative methods of bias reduction Toggle Alternative methods of bias reduction subsection 7.1 Lahiri's method 7.2 Midzuno-Sen's method 7.3 Other ratio estimators 8 Ordinary least squares regression 9 Uses 10 History 11 See also 12 References Toggle the table of contents Ratio estimator 1 language فارسی Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical estimator for ratio of means The ratio estimator is a statistical estimator for the ratio of means of two random variables. Ratio estimates are biased and corrections must be made when they are used in experimental or survey work. The ratio estimates are asymmetrical so symmetrical tests such as the t test should not be used to generate confidence intervals.

The bias is of the order O (1/ n ) (see big O notation ) so as the sample size ( n ) increases, the bias will asymptotically approach 0. Therefore, the estimator is approximately unbiased for large sample sizes.

Definition [ edit ] Assume there are two characteristics – x and y – that can be observed for each sampled element in the data set. The ratio R is R = μ μ ¯ ¯ y / μ μ ¯ ¯ x {\displaystyle R={\bar {\mu }}_{y}/{\bar {\mu }}_{x}} The ratio estimate of a value of the y variate ( θ y ) is θ θ y = R θ θ x {\displaystyle \theta _{y}=R\theta _{x}} where θ x is the corresponding value of the x variate.

θ y is known to be asymptotically normally distributed.

[ 1 ] Statistical properties [ edit ] See also: ratio distribution The sample ratio ( r ) is estimated from the sample r = y ¯ ¯ x ¯ ¯ = ∑ ∑ i = 1 n y i ∑ ∑ i = 1 n x i {\displaystyle r={\frac {\bar {y}}{\bar {x}}}={\frac {\sum _{i=1}^{n}y_{i}}{\sum _{i=1}^{n}x_{i}}}} That the ratio is biased can be shown with Jensen's inequality as follows (assuming independence between x ¯ ¯ {\displaystyle {\bar {x}}} and y ¯ ¯ {\displaystyle {\bar {y}}} ): E ( y ¯ ¯ x ¯ ¯ ) = E ( y ¯ ¯ 1 x ¯ ¯ ) = E ( y ¯ ¯ ) E ( 1 x ¯ ¯ ) ≥ ≥ E ( y ¯ ¯ ) 1 E ( x ¯ ¯ ) = E ( y ¯ ¯ ) E ( x ¯ ¯ ) = E ( y ) E ( x ) = m y m x {\displaystyle E\left({\frac {\bar {y}}{\bar {x}}}\right)=E\left({\bar {y}}{\frac {1}{\bar {x}}}\right)=E({\bar {y}})E\left({\frac {1}{\bar {x}}}\right)\geq E({\bar {y}}){\frac {1}{E({\bar {x}})}}={\frac {E({\bar {y}})}{E({\bar {x}})}}={\frac {E(y)}{E(x)}}={\frac {m_{y}}{m_{x}}}} where m x {\displaystyle m_{x}} is the mean of the variate x {\displaystyle x} and m y {\displaystyle m_{y}} is the mean of the variate y {\displaystyle y} .

Under simple random sampling the bias is of the order O ( n −1 ). An upper bound on the relative bias of the estimate is provided by the coefficient of variation (the ratio of the standard deviation to the mean ).

[ 2 ] Under simple random sampling the relative bias is O ( n −1/2 ).

Correction of the mean's bias [ edit ] The correction methods, depending on the distributions of the x and y variates, differ in their efficiency making it difficult to recommend an overall best method. Because the estimates of r are biased a corrected version should be used in all subsequent calculations.

A correction of the bias accurate to the first order is [ citation needed ] r c o r r = r − − s x y m x {\displaystyle r_{\mathrm {corr} }=r-{\frac {s_{xy}}{m_{x}}}} where m x is the mean of the variate x and s xy is the covariance between x and y .

To simplify the notation s xy will be used subsequently to denote the covariance between the variates x and y .

Another estimator based on the Taylor expansion is [ 3 ] r c o r r = r − − ( 1 − − n − − 1 N − − 1 ) r s x 2 − − s x y n m x 2 {\displaystyle r_{\mathrm {corr} }=r-(1-{\frac {n-1}{N-1}}){\frac {rs_{x}^{2}-s_{xy}}{nm_{x}^{2}}}} where n is the sample size, N is the population size, m x is the mean of the x variate and s x 2 and s y 2 are the sample variances of the x and y variates respectively.

A computationally simpler but slightly less accurate version of this estimator is r c o r r = r − − N − − n N r s x 2 − − s x y n m x 2 {\displaystyle r_{\mathrm {corr} }=r-{\frac {N-n}{N}}{\frac {rs_{x}^{2}-s_{xy}}{nm_{x}^{2}}}} where N is the population size, n is the sample size, m x is the mean of the x variate and s x 2 and s y 2 are the sample variances of the x and y variates respectively. These versions differ only in the factor in the denominator ( N - 1). For a large N the difference is negligible.

If x and y are unitless counts with Poisson distribution a second-order correction is [ 4 ] r c o r r = r [ 1 + 1 n ( 1 m x − − s x y m x m y ) + 1 n 2 ( 2 m x 2 − − s x y m x m y [ 2 + 3 m x ] + s x 2 y m x 2 m y ) ] {\displaystyle r_{\mathrm {corr} }=r\left[1+{\frac {1}{n}}\left({\frac {1}{m_{x}}}-{\frac {s_{xy}}{m_{x}m_{y}}}\right)+{\frac {1}{n^{2}}}\left({\frac {2}{m_{x}^{2}}}-{\frac {s_{xy}}{m_{x}m_{y}}}\left[2+{\frac {3}{m_{x}}}\right]+{\frac {s_{x^{2}y}}{m_{x}^{2}m_{y}}}\right)\right]} Other methods of bias correction have also been proposed. To simplify the notation the following variables will be used θ θ = 1 n − − 1 N {\displaystyle \theta ={\frac {1}{n}}-{\frac {1}{N}}} c x 2 = s x 2 m x 2 {\displaystyle c_{x}^{2}={\frac {s_{x}^{2}}{m_{x}^{2}}}} c x y = s x y m x m y {\displaystyle c_{xy}={\frac {s_{xy}}{m_{x}m_{y}}}} Pascual's estimator: [ 5 ] r c o r r = r + N − − 1 N m y − − r m x n − − 1 {\displaystyle r_{\mathrm {corr} }=r+{\frac {N-1}{N}}{\frac {m_{y}-rm_{x}}{n-1}}} Beale's estimator: [ 6 ] r c o r r = r 1 + θ θ c x y 1 + θ θ c x 2 {\displaystyle r_{\mathrm {corr} }=r{\frac {1+\theta c_{xy}}{1+\theta c_{x}^{2}}}} Tin's estimator: [ 7 ] r c o r r = r ( 1 + θ θ ( c x y − − c x 2 ) ) {\displaystyle r_{\mathrm {corr} }=r\left(1+\theta \left(c_{xy}-c_{x}^{2}\right)\right)} Sahoo's estimator: [ 8 ] r c o r r = r 1 + θ θ ( c x 2 − − c x y ) {\displaystyle r_{\mathrm {corr} }={\frac {r}{1+\theta (c_{x}^{2}-c_{xy})}}} Sahoo has also proposed a number of additional estimators: [ 9 ] r c o r r = r ( 1 + θ θ c x y ) ( 1 − − θ θ c x 2 ) {\displaystyle r_{\mathrm {corr} }=r(1+\theta c_{xy})(1-\theta c_{x}^{2})} r c o r r = r ( 1 − − θ θ c x 2 ) 1 − − θ θ c x y {\displaystyle r_{\mathrm {corr} }={\frac {r(1-\theta c_{x}^{2})}{1-\theta c_{xy}}}} r c o r r = r ( 1 − − θ θ c x y ) ( 1 + θ θ c x 2 ) {\displaystyle r_{\mathrm {corr} }={\frac {r}{(1-\theta c_{xy})(1+\theta c_{x}^{2})}}} If x and y are unitless counts with Poisson distribution and m x and m y are both greater than 10, then the following approximation is correct to order O( n −3 ).

[ 4 ] r c o r r = r [ 1 − − 2 n 2 m x ( 1 m x − − s x y m x m y ) ( 1 + 13 2 n + 8 n m x ) ] {\displaystyle r_{\mathrm {corr} }=r\left[1-{\frac {2}{n^{2}m_{x}}}\left({\frac {1}{m_{x}}}-{\frac {s_{xy}}{m_{x}m_{y}}}\right)\left(1+{\frac {13}{2n}}+{\frac {8}{nm_{x}}}\right)\right]} An asymptotically correct estimator is [ 3 ] r c o r r = r + c x 2 m y m x − − s x y m x 2 {\displaystyle r_{\mathrm {corr} }=r+c_{x}^{2}{\frac {m_{y}}{m_{x}}}-{\frac {s_{xy}}{m_{x}^{2}}}} Jackknife estimation [ edit ] A jackknife estimate of the ratio is less biased than the naive form. A jackknife estimator of the ratio is r c o r r = n r − − n − − 1 n ∑ ∑ i ≠ ≠ j = 1 n r i {\displaystyle r_{\mathrm {corr} }=nr-{\frac {n-1}{n}}\sum _{i\neq j=1}^{n}r_{i}} where n is the size of the sample and the r i are estimated with the omission of one pair of variates at a time.

[ 10 ] An alternative method is to divide the sample into g groups each of size p with n = pg .

[ 11 ] Let r i be the estimate of the i th group. Then the estimator r c o r r = g r − − g − − 1 g ∑ ∑ i = 1 g r i = g ( r − − r ¯ ¯ ) + r ¯ ¯ {\displaystyle r_{\mathrm {corr} }=gr-{\frac {g-1}{g}}\sum _{i=1}^{g}r_{i}=g\left(r-{\bar {r}}\right)+{\bar {r}}} where r ¯ ¯ {\displaystyle {\bar {r}}} is the mean of the ratios r g of the g groups, has a bias of at most O ( n −2 ).

Other estimators based on the division of the sample into g groups are: [ 12 ] r c o r r = g g + 1 r − − 1 g ( g − − 1 ) ∑ ∑ i = 1 g r i {\displaystyle r_{\mathrm {corr} }={\frac {g}{g+1}}r-{\frac {1}{g(g-1)}}\sum _{i=1}^{g}r_{i}} r c o r r = r ¯ ¯ + n n − − 1 m y − − r ¯ ¯ m x m x {\displaystyle r_{\mathrm {corr} }={\bar {r}}+{\frac {n}{n-1}}{\frac {m_{y}-{\bar {r}}m_{x}}{m_{x}}}} r c o r r = r g ¯ ¯ + g ( m y − − r g ¯ ¯ m x ) m x {\displaystyle r_{\mathrm {corr} }={\bar {r_{g}}}+{\frac {g(m_{y}-{\bar {r_{g}}}m_{x})}{m_{x}}}} where r ¯ ¯ {\displaystyle {\bar {r}}} is the mean of the ratios r g of the g groups and r g ¯ ¯ = ∑ ∑ r i ′ g {\displaystyle {\bar {r_{g}}}=\sum {\frac {r_{i}'}{g}}} where r i ' is the value of the sample ratio with the i th group omitted.

Other methods of estimation [ edit ] Other methods of estimating a ratio estimator include maximum likelihood and bootstrapping .

[ 10 ] Estimate of total [ edit ] The estimated total of the y variate ( τ y ) is τ τ y = r τ τ x {\displaystyle \tau _{y}=r\tau _{x}} where ( τ x ) is the total of the x variate.

Variance estimates [ edit ] The variance of the sample ratio is approximately: var ⁡ ⁡ ( r ) = 1 s x 2 + m x 2 [ ( s y 2 − − s x 2 [ y 2 / x 2 ] ) − − ( s x [ y / x ] ) 2 + 2 m y s x [ y / x ] − − s x 2 m x 2 ( m y − − s x [ y / x ] 2 ) ] {\displaystyle \operatorname {var} (r)={\frac {1}{s_{x}^{2}+m_{x}^{2}}}\left[(s_{y}^{2}-s_{x^{2}[y^{2}/x^{2}]})-(s_{x[y/x]})^{2}+2m_{y}s_{x[y/x]}-{\frac {s_{x}^{2}}{m_{x}^{2}}}(m_{y}-s_{x[y/x]}^{2})\right]} where s x 2 and s y 2 are the variances of the x and y variates respectively, m x and m y are the means of the x and y variates respectively and s xy is the covariance of x and y .

Although the approximate variance estimator of the ratio given below is biased, if the sample size is large, the bias in this estimator is negligible.

var ⁡ ⁡ ( r ) = 1 n N − − n N 1 m x 2 ∑ ∑ i = 1 n ( y i − − r x i ) 2 n − − 1 {\displaystyle \operatorname {var} (r)={\frac {1}{n}}{\frac {N-n}{N}}{\frac {1}{m_{x}^{2}}}{\frac {\sum _{i=1}^{n}(y_{i}-rx_{i})^{2}}{n-1}}} where N is the population size, n is the sample size and m x is the mean of the x variate.

Another estimator of the variance based on the Taylor expansion is var ⁡ ⁡ ( r ) = 1 n ( 1 − − n − − 1 N − − 1 ) r 2 s x 2 + s y 2 − − 2 r s x y m x 2 {\displaystyle \operatorname {var} (r)={\frac {1}{n}}(1-{\frac {n-1}{N-1}}){\frac {r^{2}s_{x}^{2}+s_{y}^{2}-2rs_{xy}}{m_{x}^{2}}}} where n is the sample size and N is the population size and s xy is the covariance of x and y .

An estimate accurate to O( n −2 ) is [ 3 ] var ⁡ ⁡ ( r ) = 1 n [ s y 2 m x 2 + m y 2 s x 2 m x 4 − − 2 m y s x y m x 3 ] {\displaystyle \operatorname {var} (r)={\frac {1}{n}}\left[{\frac {s_{y}^{2}}{m_{x}^{2}}}+{\frac {m_{y}^{2}s_{x}^{2}}{m_{x}^{4}}}-{\frac {2m_{y}s_{xy}}{m_{x}^{3}}}\right]} If the probability distribution is Poissonian, an estimator accurate to O( n −3 ) is [ 4 ] var ⁡ ⁡ ( r ) = r 2 [ 1 n ( 1 m x + 1 m y − − 2 s x y m x m y ) + 1 n 2 ( 6 m x 2 + 3 m x m y + s x y [ 4 m y 2 − − 8 m x m y − − 16 m x 2 m y + 5 s x y m x 2 m y 2 ] + 4 s x 2 y m x 2 m y − − 2 s x y 2 m x m y 2 ) ] {\displaystyle \operatorname {var} (r)=r^{2}\left[{\frac {1}{n}}\left({\frac {1}{m_{x}}}+{\frac {1}{m_{y}}}-{\frac {2s_{xy}}{m_{x}m_{y}}}\right)+{\frac {1}{n^{2}}}\left({\frac {6}{m_{x}^{2}}}+{\frac {3}{m_{x}m_{y}}}+s_{xy}\left[{\frac {4}{m_{y}^{2}}}-{\frac {8}{m_{x}m_{y}}}-{\frac {16}{m_{x}^{2}m_{y}}}+{\frac {5s_{xy}}{m_{x}^{2}m_{y}^{2}}}\right]+{\frac {4s_{x^{2}y}}{m_{x}^{2}m_{y}}}-{\frac {2s_{xy^{2}}}{m_{x}m_{y}^{2}}}\right)\right]} A jackknife estimator of the variance is var ⁡ ⁡ ( r ) = ( n − − 1 ) n ∑ ∑ i = 1 n ( r i − − r J ) 2 {\displaystyle \operatorname {var} (r)={\frac {(n-1)}{n}}\sum _{i=1}^{n}(r_{i}-r_{J})^{2}} where r i is the ratio with the i th pair of variates omitted and r J is the jackknife estimate of the ratio.

[ 10 ] Variance of total [ edit ] The variance of the estimated total is var ⁡ ⁡ ( τ τ y ) = τ τ y 2 var ⁡ ⁡ ( r ) {\displaystyle \operatorname {var} (\tau _{y})=\tau _{y}^{2}\operatorname {var} (r)} Variance of mean [ edit ] The variance of the estimated mean of the y variate is var ⁡ ⁡ ( y ¯ ¯ ) = m x 2 var ⁡ ⁡ ( r ) = N − − n N ∑ ∑ i = 1 n ( y i − − r x i ) 2 n − − 1 = N − − n N ( s y 2 + r 2 s x 2 − − 2 r s x y ) n {\displaystyle \operatorname {var} ({\bar {y}})=m_{x}^{2}\operatorname {var} (r)={\frac {N-n}{N}}{\frac {\sum _{i=1}^{n}(y_{i}-rx_{i})^{2}}{n-1}}={\frac {N-n}{N}}{\frac {(s_{y}^{2}+r^{2}s_{x}^{2}-2rs_{xy})}{n}}} where m x is the mean of the x variate, s x 2 and s y 2 are the sample variances of the x and y variates respectively and s xy is the covariance of x and y .

Skewness [ edit ] The skewness and the kurtosis of the ratio depend on the distributions of the x and y variates. Estimates have been made of these parameters for normally distributed x and y variates but for other distributions no expressions have yet been derived. It has been found that in general ratio variables are skewed to the right, are leptokurtic and their nonnormality is increased when magnitude of the denominator's coefficient of variation is increased.

For normally distributed x and y variates the skewness of the ratio is approximately [ 7 ] γ γ = ( m y ω ω n m x m y ω ω 2 + m x 2 m y ) ( 6 + 1 n m x [ 44 + 1 1 + ω ω 2 m y / m x ] ) {\displaystyle \gamma =\left({\frac {m_{y}\omega }{\sqrt {nm_{x}m_{y}\omega ^{2}+m_{x}^{2}m_{y}}}}\right)\left(6+{\frac {1}{nm_{x}}}\left[44+{\frac {1}{1+\omega ^{2}m_{y}/m_{x}}}\right]\right)} where ω ω = 1 − − m x cov ⁡ ⁡ ( x , y ) {\displaystyle \omega =1-m_{x}\operatorname {cov} (x,y)} Effect on confidence intervals [ edit ] Because the ratio estimate is generally skewed confidence intervals created with the variance and symmetrical tests such as the t test are incorrect.

[ 10 ] These confidence intervals tend to overestimate the size of the left confidence interval and underestimate the size of the right.

If the ratio estimator is unimodal (which is frequently the case) then a conservative estimate of the 95% confidence intervals can be made with the Vysochanskiï–Petunin inequality .

Alternative methods of bias reduction [ edit ] An alternative method of reducing or eliminating the bias in the ratio estimator is to alter the method of sampling. The variance of the ratio using these methods differs from the estimates given previously.  Note that while many applications such as those discussion in Lohr [ 13 ] are intended to be restricted to positive integers only, such as sizes of sample groups, the Midzuno-Sen method works for any sequence of positive numbers, integral or not. It's not clear what it means that Lahiri's method works since it returns a biased result.

Lahiri's method [ edit ] The first of these sampling schemes is a double use of a sampling method introduced by Lahiri in 1951.

[ 14 ] The algorithm here is based upon the description by Lohr.

[ 13 ] Choose a number M = max( x 1 , ..., x N ) where N is the population size.

Choose i at random from a uniform distribution on [1, N ].

Choose k at random from a uniform distribution on [1, M ].

If k ≤ x i , then x i is retained in the sample. If not then it is rejected.

Repeat this process from step 2 until the desired sample size is obtained.

The same procedure for the same desired sample size is carried out with the y variate.

Lahiri's scheme as described by Lohr is biased high and, so, is interesting only for historical reasons. The Midzuno-Sen technique described below is recommended instead.

Midzuno-Sen's method [ edit ] In 1952 Midzuno and Sen independently described a sampling scheme that provides an unbiased estimator of the ratio.

[ 15 ] [ 16 ] The first sample is chosen with probability proportional to the size of the x variate. The remaining n - 1 samples are chosen at random without replacement from the remaining N - 1 members in the population. The probability of selection under this scheme is P = ∑ ∑ x i ( N − − 1 n − − 1 ) X {\displaystyle P={\frac {\sum x_{i}}{{N-1 \choose n-1}X}}} where X is the sum of the N x variates and the x i are the n members of the sample. Then the ratio of the sum of the y variates and the sum of the x variates chosen in this fashion is an unbiased estimate of the ratio estimator.

In symbols we have r = ∑ ∑ y i ∑ ∑ x i {\displaystyle r={\frac {\sum y_{i}}{\sum x_{i}}}} where x i and y i are chosen according to the scheme described above.

The ratio estimator given by this scheme is unbiased.

Särndal, Swensson, and Wretman credit Lahiri, Midzuno and Sen for the insights leading to this method [ 17 ] but Lahiri's technique is biased high.

Other ratio estimators [ edit ] Tin (1965) [ 18 ] described and compared ratio estimators proposed by Beale (1962) [ 19 ] and Quenouille (1956) [ 20 ] and proposed a modified approach (now referred to as Tin's method).  These ratio estimators are commonly used to calculate pollutant loads from sampling of waterways, particularly where flow is measured more frequently than water quality.  For example see Quilbe et al., (2006) [ 21 ] Ordinary least squares regression [ edit ] If a linear relationship between the x and y variates exists and the regression equation passes through the origin then the estimated variance of the regression equation is always less than that of the ratio estimator [ citation needed ] . The precise relationship between the variances depends on the linearity of the relationship between the x and y variates: when the relationship is other than linear the ratio estimate may have a lower variance than that estimated by regression.

Uses [ edit ] Although the ratio estimator may be of use in a number of settings it is of particular use in two cases: when the variates x and y are highly correlated through the origin .

In survey methodology when estimating a weighted average in which the denominator indicates the sum of weights that reflect the total population size, but the total population size is unknown.

History [ edit ] The first known use of the ratio estimator was by John Graunt in England who in 1662 was the first to estimate the ratio y / x where y represented the total population and x the known total number of registered births in the same areas during the preceding year.

Later Messance (~1765) and Moheau (1778) published very carefully prepared estimates for France based on enumeration of population in certain districts and on the count of births, deaths and marriages as reported for the whole country. The districts from which the ratio of inhabitants to birth was determined only constituted a sample.

In 1802, Laplace wished to estimate the population of France. No population census had been carried out and Laplace lacked the resources to count every individual. Instead he sampled 30 parishes whose total number of inhabitants was 2,037,615. The parish baptismal registrations were considered to be reliable estimates of the number of live births so he used the total number of births over a three-year period. The sample estimate was 71,866.333 baptisms per year over this period giving a ratio of one registered baptism for every 28.35 persons. The total number of baptismal registrations for France was also available to him and he assumed that the ratio of live births to population was constant. He then used the ratio from his sample to estimate the population of France.

Karl Pearson said in 1897 that the ratio estimates are biased and cautioned against their use.

[ 22 ] See also [ edit ] Mark and recapture , another way of estimating population using a ratio.

Ratio distribution References [ edit ] ^ Scott AJ, Wu CFJ (1981) On the asymptotic distribution of ratio and regression
estimators. JASA 76: 98–102 ^ Cochran WG (1977) Sampling techniques. New York: John Wiley & Sons ^ a b c van Kempen GMP, van Vliet LJ (2000) Mean and variance of ratio estimators used in fluorescence ratio imaging. Cytometry 39:300–305 ^ a b c Ogliore RC, Huss GR, Nagashima K (2011) Ratio estimation in SIMS analysis. Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms 269 (17) 1910–1918 ^ Pascual JN (1961) Unbiased ratio estimators in stratified sampling. JASA 56(293):70–87 ^ Beale EML (1962) Some use of computers in operational research. Industrielle Organization 31: 27-28 ^ a b Tin M (1965) Comparison of some ratio estimators. JASA 60: 294–307 ^ Sahoo LN (1983). On a method of bias reduction in ratio estimation. J Statist Res 17:1—6 ^ Sahoo LN (1987) On a class of almost unbiased estimators for population ratio. Statistics 18: 119-121 ^ a b c d Choquet D, L'ecuyer P, Léger C (1999) Bootstrap confidence intervals for ratios of expectations. ACM Transactions on Modeling and Computer Simulation - TOMACS 9 (4) 326-348 doi : 10.1145/352222.352224 ^ Durbin J (1959) A note on the application of Quenouille's method of bias reduction to estimation of ratios. Biometrika 46: 477-480 ^ Mickey MR (1959) Some finite population unbiased ratio and regression estimators. JASA 54: 596–612 ^ a b Lohr S (2010) Sampling - Design and Analysis (2nd edition) ^ Lahiri DB (1951) A method of sample selection providing unbiased ratio estimates. Bull Int Stat Inst 33: 133–140 ^ Midzuno H (1952) On the sampling system with probability proportional to the sum of the sizes. Ann Inst Stat Math 3: 99-107 ^ Sen AR (1952) Present status of probability sampling and its use in the estimation of a characteristic. Econometrika 20-103 ^ Särndal, C-E, B Swensson J Wretman (1992) Model assisted survey sampling. Springer, §7.3.1 (iii) ^ Tin M (1965). Comparison of Some Ratio Estimators. Journal of the American Statistical Association, 60(309), 294–307.

https://doi.org/10.1080/01621459.1965.10480792 ^ Beale EML (1965) Some use of computers in operational research.  Industrielle organisation 31:27-8 ^ Quenouille R Rousseau AN Duchemin M Poulin A Gangbazo G Villeneuve J-P (2006) Selecting a calculation method to estimate sediment and nutrient loads in streams: application to the Beaurivage River (Quebec, Canada).  Journal of Hydrology 326:295-310 ^ Quilbé, R., Rousseau, A. N., Duchemin, M., Poulin, A., Gangbazo, G., & Villeneuve, J. P. (2006). Selecting a calculation method to estimate sediment and nutrient loads in streams: Application to the Beaurivage River (Québec, Canada). Journal of Hydrology, 326(1–4), 295–310.

https://doi.org/10.1016/j.jhydrol.2005.11.008 ^ Pearson K (1897) On a form of spurious correlation that may arise when indices are used for the measurement of organs. Proc Roy Soc Lond 60: 498 v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Ratio_estimator&oldid=1304589262 " Categories : Statistical deviation and dispersion Statistical ratios Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from May 2018 Articles with unsourced statements from July 2022 Articles containing proofs This page was last edited on 7 August 2025, at 00:02 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Ratio estimator 1 language Add topic

