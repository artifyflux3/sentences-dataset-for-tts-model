Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Intuitive explanation 2 History 3 Statement 4 Applications Toggle Applications subsection 4.1 Composites of more than two functions 4.2 Quotient rule 4.3 Derivatives of inverse functions 4.4 Back propagation 5 Higher derivatives 6 Proofs Toggle Proofs subsection 6.1 First proof 6.2 Second proof 6.3 Third proof 6.4 Proof via infinitesimals 7 Multivariable case Toggle Multivariable case subsection 7.1 Case of scalar-valued functions with multiple inputs 7.1.1 Example: arithmetic operations 7.2 General rule: Vector-valued functions with multiple inputs 7.2.1 Example 7.2.2 Higher derivatives of multivariable functions 8 Further generalizations 9 See also 10 References 11 Further reading 12 External links Toggle the table of contents Chain rule 43 languages Afrikaans العربية 閩南語 / Bân-lâm-gí Български Bosanski Català Чӑвашла Čeština Deutsch Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Bahasa Indonesia Íslenska Italiano עברית Magyar Nederlands 日本語 Norsk nynorsk Piemontèis Polski Português Română Русский Simple English Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Formula in calculus For other uses, see Chain rule (disambiguation) .

Part of a series of articles about Calculus ∫ ∫ a b f ′ ( t ) d t = f ( b ) − − f ( a ) {\displaystyle \int _{a}^{b}f'(t)\,dt=f(b)-f(a)} Fundamental theorem Limits Continuity Rolle's theorem Mean value theorem Inverse function theorem Differential Definitions Derivative ( generalizations ) Differential infinitesimal of a function total Concepts Differentiation notation Second derivative Implicit differentiation Logarithmic differentiation Related rates Taylor's theorem Rules and identities Sum Product Chain Power Quotient L'Hôpital's rule Inverse General Leibniz Faà di Bruno's formula Reynolds Integral Lists of integrals Integral transform Leibniz integral rule Definitions Antiderivative Integral ( improper ) Riemann integral Lebesgue integration Contour integration Integral of inverse functions Integration by Parts Discs Cylindrical shells Substitution ( trigonometric , tangent half-angle , Euler ) Euler's formula Partial fractions ( Heaviside's method ) Changing order Reduction formulae Differentiating under the integral sign Risch algorithm Series Geometric ( arithmetico-geometric ) Harmonic Alternating Power Binomial Taylor Convergence tests Summand limit (term test) Ratio Root Integral Direct comparison Limit comparison Alternating series Cauchy condensation Dirichlet Abel Vector Gradient Divergence Curl Laplacian Directional derivative Identities Theorems Gradient Green's Stokes' Divergence Generalized Stokes Helmholtz decomposition Multivariable Formalisms Matrix Tensor Exterior Geometric Definitions Partial derivative Multiple integral Line integral Surface integral Volume integral Jacobian Hessian Advanced Calculus on Euclidean space Generalized functions Limit of distributions Specialized Fractional Malliavin Stochastic Variations Miscellanea Precalculus History Glossary List of topics Integration Bee Mathematical analysis Nonstandard analysis v t e In calculus , the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g . More precisely, if h = f ∘ ∘ g {\displaystyle h=f\circ g} is the function such that h ( x ) = f ( g ( x ) ) {\displaystyle h(x)=f(g(x))} for every x , then the chain rule is, in Lagrange's notation , h ′ ( x ) = f ′ ( g ( x ) ) g ′ ( x ) .

{\displaystyle h'(x)=f'(g(x))g'(x).} or, equivalently, h ′ = ( f ∘ ∘ g ) ′ = ( f ′ ∘ ∘ g ) ⋅ ⋅ g ′ .

{\displaystyle h'=(f\circ g)'=(f'\circ g)\cdot g'.} The chain rule may also be expressed in Leibniz's notation . If a variable z depends on the variable y , which itself depends on the variable x (that is, y and z are dependent variables ), then z depends on x as well, via the intermediate variable y . In this case, the chain rule is expressed as d z d x = d z d y ⋅ ⋅ d y d x , {\displaystyle {\frac {dz}{dx}}={\frac {dz}{dy}}\cdot {\frac {dy}{dx}},} and d z d x | x = d z d y | y ( x ) ⋅ ⋅ d y d x | x , {\displaystyle \left.{\frac {dz}{dx}}\right|_{x}=\left.{\frac {dz}{dy}}\right|_{y(x)}\cdot \left.{\frac {dy}{dx}}\right|_{x},} for indicating at which points the derivatives have to be evaluated.

In integration , the counterpart to the chain rule is the substitution rule .

Intuitive explanation [ edit ] Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.

As put by George F. Simmons : "If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man." [ 1 ] The relationship between this example and the chain rule is as follows. Let z , y and x be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is d z d y = 2.

{\textstyle {\frac {dz}{dy}}=2.} Similarly, d y d x = 4.

{\textstyle {\frac {dy}{dx}}=4.} So, the rate of change of the relative positions of the car and the walking man is d z d x = d z d y ⋅ ⋅ d y d x = 2 ⋅ ⋅ 4 = 8.

{\displaystyle {\frac {dz}{dx}}={\frac {dz}{dy}}\cdot {\frac {dy}{dx}}=2\cdot 4=8.} The rate of change of positions is the ratio of the speeds, and the speed is the derivative of the position with respect to the time; that is, d z d x = d z d t d x d t , {\displaystyle {\frac {dz}{dx}}={\frac {\frac {dz}{dt}}{\frac {dx}{dt}}},} or, equivalently, d z d t = d z d x ⋅ ⋅ d x d t , {\displaystyle {\frac {dz}{dt}}={\frac {dz}{dx}}\cdot {\frac {dx}{dt}},} which is also an application of the chain rule.

History [ edit ] The chain rule seems to have first been used by Gottfried Wilhelm Leibniz . He used it to calculate the derivative of a + b z + c z 2 {\displaystyle {\sqrt {a+bz+cz^{2}}}} as the composite of the square root function and the function a + b z + c z 2 {\displaystyle a+bz+cz^{2}\!} . He first mentioned it in a 1676 memoir (with a sign error in the calculation).

[ 2 ] The common notation of the chain rule is due to Leibniz.

[ 3 ] Guillaume de l'Hôpital used the chain rule implicitly in his Analyse des infiniment petits . The chain rule does not appear in any of Leonhard Euler 's analysis books, even though they were written over a hundred years after Leibniz's discovery.

[ citation needed ] . It is believed that the first "modern" version of the chain rule appears in Lagrange's 1797 Théorie des fonctions analytiques ; it also appears in Cauchy's 1823 Résumé des Leçons données a L’École Royale Polytechnique sur Le Calcul Infinitesimal .

[ 3 ] Statement [ edit ] The simplest form of the chain rule is for real-valued functions of one real variable. It states that if g is a function that is differentiable at a point c (i.e. the derivative g ′( c ) exists) and f is a function that is differentiable at g ( c ) , then the composite function f ∘ ∘ g {\displaystyle f\circ g} is differentiable at c , and the derivative is [ 4 ] ( f ∘ ∘ g ) ′ ( c ) = f ′ ( g ( c ) ) ⋅ ⋅ g ′ ( c ) .

{\displaystyle (f\circ g)'(c)=f'(g(c))\cdot g'(c).} The rule is sometimes abbreviated as ( f ∘ ∘ g ) ′ = ( f ′ ∘ ∘ g ) ⋅ ⋅ g ′ .

{\displaystyle (f\circ g)'=(f'\circ g)\cdot g'.} If y = f ( u ) and u = g ( x ) , then this abbreviated form is written in Leibniz notation as: d y d x = d y d u ⋅ ⋅ d u d x .

{\displaystyle {\frac {dy}{dx}}={\frac {dy}{du}}\cdot {\frac {du}{dx}}.} The points where the derivatives are evaluated may also be stated explicitly: d y d x | x = c = d y d u | u = g ( c ) ⋅ ⋅ d u d x | x = c .

{\displaystyle \left.{\frac {dy}{dx}}\right|_{x=c}=\left.{\frac {dy}{du}}\right|_{u=g(c)}\cdot \left.{\frac {du}{dx}}\right|_{x=c}.} Carrying the same reasoning further, given n functions f 1 , … … , f n {\displaystyle f_{1},\ldots ,f_{n}\!} with the composite function f 1 ∘ ∘ ( f 2 ∘ ∘ ⋯ ⋯ ( f n − − 1 ∘ ∘ f n ) ) {\displaystyle f_{1}\circ (f_{2}\circ \cdots (f_{n-1}\circ f_{n}))\!} , if each function f i {\displaystyle f_{i}\!} is differentiable at its immediate input, then the composite function is also differentiable by the repeated application of Chain Rule, where the derivative is (in Leibniz's notation): d f 1 d x = d f 1 d f 2 d f 2 d f 3 ⋯ ⋯ d f n d x .

{\displaystyle {\frac {df_{1}}{dx}}={\frac {df_{1}}{df_{2}}}{\frac {df_{2}}{df_{3}}}\cdots {\frac {df_{n}}{dx}}.} Applications [ edit ] The chain rule in case of composites of more than two functions Composites of more than two functions [ edit ] The chain rule can be applied to composites of more than two functions. To take the derivative of a composite of more than two functions, notice that the composite of f , g , and h (in that order) is the composite of f with g ∘ h . The chain rule states that to compute the derivative of f ∘ g ∘ h , it is sufficient to compute the derivative of f and the derivative of g ∘ h . The derivative of f can be calculated directly, and the derivative of g ∘ h can be calculated by applying the chain rule again.

[ citation needed ] For concreteness, consider the function y = e sin ⁡ ⁡ ( x 2 ) .

{\displaystyle y=e^{\sin(x^{2})}.} This can be decomposed as the composite of three functions: y = f ( u ) = e u , u = g ( v ) = sin ⁡ ⁡ v , v = h ( x ) = x 2 .

{\displaystyle {\begin{aligned}y&=f(u)=e^{u},\\u&=g(v)=\sin v,\\v&=h(x)=x^{2}.\end{aligned}}} So that y = f ( g ( h ( x ) ) ) {\displaystyle y=f(g(h(x)))} .

Their derivatives are: d y d u = f ′ ( u ) = e u , d u d v = g ′ ( v ) = cos ⁡ ⁡ v , d v d x = h ′ ( x ) = 2 x .

{\displaystyle {\begin{aligned}{\frac {dy}{du}}&=f'(u)=e^{u},\\{\frac {du}{dv}}&=g'(v)=\cos v,\\{\frac {dv}{dx}}&=h'(x)=2x.\end{aligned}}} The chain rule states that the derivative of their composite at the point x = a is: ( f ∘ ∘ g ∘ ∘ h ) ′ ( a ) = f ′ ( ( g ∘ ∘ h ) ( a ) ) ⋅ ⋅ ( g ∘ ∘ h ) ′ ( a ) = f ′ ( ( g ∘ ∘ h ) ( a ) ) ⋅ ⋅ g ′ ( h ( a ) ) ⋅ ⋅ h ′ ( a ) = ( f ′ ∘ ∘ g ∘ ∘ h ) ( a ) ⋅ ⋅ ( g ′ ∘ ∘ h ) ( a ) ⋅ ⋅ h ′ ( a ) .

{\displaystyle {\begin{aligned}(f\circ g\circ h)'(a)&=f'((g\circ h)(a))\cdot (g\circ h)'(a)\\&=f'((g\circ h)(a))\cdot g'(h(a))\cdot h'(a)\\&=(f'\circ g\circ h)(a)\cdot (g'\circ h)(a)\cdot h'(a).\end{aligned}}} In Leibniz's notation , this is: d y d x = d y d u | u = g ( h ( a ) ) ⋅ ⋅ d u d v | v = h ( a ) ⋅ ⋅ d v d x | x = a , {\displaystyle {\frac {dy}{dx}}=\left.{\frac {dy}{du}}\right|_{u=g(h(a))}\cdot \left.{\frac {du}{dv}}\right|_{v=h(a)}\cdot \left.{\frac {dv}{dx}}\right|_{x=a},} or for short, d y d x = d y d u ⋅ ⋅ d u d v ⋅ ⋅ d v d x .

{\displaystyle {\frac {dy}{dx}}={\frac {dy}{du}}\cdot {\frac {du}{dv}}\cdot {\frac {dv}{dx}}.} The derivative function is therefore: d y d x = e sin ⁡ ⁡ ( x 2 ) ⋅ ⋅ cos ⁡ ⁡ ( x 2 ) ⋅ ⋅ 2 x .

{\displaystyle {\frac {dy}{dx}}=e^{\sin(x^{2})}\cdot \cos(x^{2})\cdot 2x.} Another way of computing this derivative is to view the composite function f ∘ g ∘ h as the composite of f ∘ g and h . Applying the chain rule in this manner would yield: ( f ∘ ∘ g ∘ ∘ h ) ′ ( a ) = ( f ∘ ∘ g ) ′ ( h ( a ) ) ⋅ ⋅ h ′ ( a ) = f ′ ( g ( h ( a ) ) ) ⋅ ⋅ g ′ ( h ( a ) ) ⋅ ⋅ h ′ ( a ) .

{\displaystyle {\begin{aligned}(f\circ g\circ h)'(a)&=(f\circ g)'(h(a))\cdot h'(a)\\&=f'(g(h(a)))\cdot g'(h(a))\cdot h'(a).\end{aligned}}} This is the same as what was computed above. This should be expected because ( f ∘ g ) ∘ h = f ∘ ( g ∘ h ) .

Sometimes, it is necessary to differentiate an arbitrarily long composition of the form f 1 ∘ ∘ f 2 ∘ ∘ ⋯ ⋯ ∘ ∘ f n − − 1 ∘ ∘ f n {\displaystyle f_{1}\circ f_{2}\circ \cdots \circ f_{n-1}\circ f_{n}\!} . In this case, define f a .

.

b = f a ∘ ∘ f a + 1 ∘ ∘ ⋯ ⋯ ∘ ∘ f b − − 1 ∘ ∘ f b {\displaystyle f_{a\,.\,.\,b}=f_{a}\circ f_{a+1}\circ \cdots \circ f_{b-1}\circ f_{b}} where f a .

.

a = f a {\displaystyle f_{a\,.\,.\,a}=f_{a}} and f a .

.

b ( x ) = x {\displaystyle f_{a\,.\,.\,b}(x)=x} when b < a {\displaystyle b<a} . Then the chain rule takes the form D f 1 .

.

n = ( D f 1 ∘ ∘ f 2 .

.

n ) ( D f 2 ∘ ∘ f 3 .

.

n ) ⋯ ⋯ ( D f n − − 1 ∘ ∘ f n .

.

n ) D f n = ∏ ∏ k = 1 n [ D f k ∘ ∘ f ( k + 1 ) .

.

n ] {\displaystyle {\begin{aligned}Df_{1\,.\,.\,n}&=(Df_{1}\circ f_{2\,.\,.\,n})(Df_{2}\circ f_{3\,.\,.\,n})\cdots (Df_{n-1}\circ f_{n\,.\,.\,n})Df_{n}\\&=\prod _{k=1}^{n}\left[Df_{k}\circ f_{(k+1)\,.\,.\,n}\right]\end{aligned}}} or, in the Lagrange notation, f 1 .

.

n ′ ( x ) = f 1 ′ ( f 2 .

.

n ( x ) ) f 2 ′ ( f 3 .

.

n ( x ) ) ⋯ ⋯ f n − − 1 ′ ( f n .

.

n ( x ) ) f n ′ ( x ) = ∏ ∏ k = 1 n f k ′ ( f ( k + 1 .

.

n ) ( x ) ) {\displaystyle {\begin{aligned}f_{1\,.\,.\,n}'(x)&=f_{1}'\left(f_{2\,.\,.\,n}(x)\right)\;f_{2}'\left(f_{3\,.\,.\,n}(x)\right)\cdots f_{n-1}'\left(f_{n\,.\,.\,n}(x)\right)\;f_{n}'(x)\\[1ex]&=\prod _{k=1}^{n}f_{k}'\left(f_{(k+1\,.\,.\,n)}(x)\right)\end{aligned}}} Quotient rule [ edit ] See also: Quotient rule The chain rule can be used to derive some well-known differentiation rules. For example, the quotient rule is a consequence of the chain rule and the product rule . To see this, write the function f ( x )/ g ( x ) as the product f ( x ) · 1/ g ( x ) . First apply the product rule: d d x ( f ( x ) g ( x ) ) = d d x ( f ( x ) ⋅ ⋅ 1 g ( x ) ) = f ′ ( x ) ⋅ ⋅ 1 g ( x ) + f ( x ) ⋅ ⋅ d d x ( 1 g ( x ) ) .

{\displaystyle {\begin{aligned}{\frac {d}{dx}}\left({\frac {f(x)}{g(x)}}\right)&={\frac {d}{dx}}\left(f(x)\cdot {\frac {1}{g(x)}}\right)\\&=f'(x)\cdot {\frac {1}{g(x)}}+f(x)\cdot {\frac {d}{dx}}\left({\frac {1}{g(x)}}\right).\end{aligned}}} To compute the derivative of 1/ g ( x ) , notice that it is the composite of g with the reciprocal function, that is, the function that sends x to 1/ x . The derivative of the reciprocal function is − − 1 / x 2 {\displaystyle -1/x^{2}\!} . By applying the chain rule, the last expression becomes: f ′ ( x ) ⋅ ⋅ 1 g ( x ) + f ( x ) ⋅ ⋅ ( − − 1 g ( x ) 2 ⋅ ⋅ g ′ ( x ) ) = f ′ ( x ) g ( x ) − − f ( x ) g ′ ( x ) g ( x ) 2 , {\displaystyle f'(x)\cdot {\frac {1}{g(x)}}+f(x)\cdot \left(-{\frac {1}{g(x)^{2}}}\cdot g'(x)\right)={\frac {f'(x)g(x)-f(x)g'(x)}{g(x)^{2}}},} which is the usual formula for the quotient rule.

Derivatives of inverse functions [ edit ] Main article: Inverse functions and differentiation Suppose that y = g ( x ) has an inverse function . Call its inverse function f so that we have x = f ( y ) . There is a formula for the derivative of f in terms of the derivative of g . To see this, note that f and g satisfy the formula f ( g ( x ) ) = x .

{\displaystyle f(g(x))=x.} And because the functions f ( g ( x ) ) {\displaystyle f(g(x))} and x are equal, their derivatives must be equal. The derivative of x is the constant function with value 1, and the derivative of f ( g ( x ) ) {\displaystyle f(g(x))} is determined by the chain rule. Therefore, we have that: f ′ ( g ( x ) ) g ′ ( x ) = 1.

{\displaystyle f'(g(x))g'(x)=1.} To express f' as a function of an independent variable y , we substitute f ( y ) {\displaystyle f(y)} for x wherever it appears. Then we can solve for f' .

f ′ ( g ( f ( y ) ) ) g ′ ( f ( y ) ) = 1 f ′ ( y ) g ′ ( f ( y ) ) = 1 f ′ ( y ) = 1 g ′ ( f ( y ) ) .

{\displaystyle {\begin{aligned}f'(g(f(y)))g'(f(y))&=1\\f'(y)g'(f(y))&=1\\f'(y)={\frac {1}{g'(f(y))}}.\end{aligned}}} For example, consider the function g ( x ) = e x . It has an inverse f ( y ) = ln y . Because g ′( x ) = e x , the above formula says that d d y ln ⁡ ⁡ y = 1 e ln ⁡ ⁡ y = 1 y .

{\displaystyle {\frac {d}{dy}}\ln y={\frac {1}{e^{\ln y}}}={\frac {1}{y}}.} This formula is true whenever g is differentiable and its inverse f is also differentiable. This formula can fail when one of these conditions is not true. For example, consider g ( x ) = x 3 . Its inverse is f ( y ) = y 1/3 , which is not differentiable at zero. If we attempt to use the above formula to compute the derivative of f at zero, then we must evaluate 1/ g ′( f (0)) . Since f (0) = 0 and g ′(0) = 0 , we must evaluate 1/0, which is undefined. Therefore, the formula fails in this case. This is not surprising because f is not differentiable at zero.

Back propagation [ edit ] The chain rule forms the basis of the back propagation algorithm, which is used in gradient descent of neural networks in deep learning ( artificial intelligence ).

[ 5 ] Higher derivatives [ edit ] Faà di Bruno's formula generalizes the chain rule to higher derivatives. Assuming that y = f ( u ) and u = g ( x ) , then the first few derivatives are: d y d x = d y d u d u d x d 2 y d x 2 = d 2 y d u 2 ( d u d x ) 2 + d y d u d 2 u d x 2 d 3 y d x 3 = d 3 y d u 3 ( d u d x ) 3 + 3 d 2 y d u 2 d u d x d 2 u d x 2 + d y d u d 3 u d x 3 d 4 y d x 4 = d 4 y d u 4 ( d u d x ) 4 + 6 d 3 y d u 3 ( d u d x ) 2 d 2 u d x 2 + d 2 y d u 2 ( 4 d u d x d 3 u d x 3 + 3 ( d 2 u d x 2 ) 2 ) + d y d u d 4 u d x 4 .

{\displaystyle {\begin{aligned}{\frac {dy}{dx}}&={\frac {dy}{du}}{\frac {du}{dx}}\\{\frac {d^{2}y}{dx^{2}}}&={\frac {d^{2}y}{du^{2}}}\left({\frac {du}{dx}}\right)^{2}+{\frac {dy}{du}}{\frac {d^{2}u}{dx^{2}}}\\{\frac {d^{3}y}{dx^{3}}}&={\frac {d^{3}y}{du^{3}}}\left({\frac {du}{dx}}\right)^{3}+3\,{\frac {d^{2}y}{du^{2}}}{\frac {du}{dx}}{\frac {d^{2}u}{dx^{2}}}+{\frac {dy}{du}}{\frac {d^{3}u}{dx^{3}}}\\{\frac {d^{4}y}{dx^{4}}}&={\frac {d^{4}y}{du^{4}}}\left({\frac {du}{dx}}\right)^{4}+6\,{\frac {d^{3}y}{du^{3}}}\left({\frac {du}{dx}}\right)^{2}{\frac {d^{2}u}{dx^{2}}}+{\frac {d^{2}y}{du^{2}}}\left(4\,{\frac {du}{dx}}{\frac {d^{3}u}{dx^{3}}}+3\,\left({\frac {d^{2}u}{dx^{2}}}\right)^{2}\right)+{\frac {dy}{du}}{\frac {d^{4}u}{dx^{4}}}.\end{aligned}}} Proofs [ edit ] First proof [ edit ] One proof of the chain rule begins by defining the derivative of the composite function f ∘ g , where we take the limit of the difference quotient for f ∘ g as x approaches a : ( f ∘ ∘ g ) ′ ( a ) = lim x → → a f ( g ( x ) ) − − f ( g ( a ) ) x − − a .

{\displaystyle (f\circ g)'(a)=\lim _{x\to a}{\frac {f(g(x))-f(g(a))}{x-a}}.} Assume for the moment that g ( x ) {\displaystyle g(x)\!} does not equal g ( a ) {\displaystyle g(a)} for any x {\displaystyle x} near a {\displaystyle a} . Then the previous expression is equal to the product of two factors: lim x → → a f ( g ( x ) ) − − f ( g ( a ) ) g ( x ) − − g ( a ) ⋅ ⋅ g ( x ) − − g ( a ) x − − a .

{\displaystyle \lim _{x\to a}{\frac {f(g(x))-f(g(a))}{g(x)-g(a)}}\cdot {\frac {g(x)-g(a)}{x-a}}.} If g {\displaystyle g} oscillates near a , then it might happen that no matter how close one gets to a , there is always an even closer x such that g ( x ) = g ( a ) . For example, this happens near a = 0 for the continuous function g defined by g ( x ) = 0 for x = 0 and g ( x ) = x 2 sin(1/ x ) otherwise. Whenever this happens, the above expression is undefined because it involves division by zero . To work around this, introduce a function Q {\displaystyle Q} as follows: Q ( y ) = { f ( y ) − − f ( g ( a ) ) y − − g ( a ) , y ≠ ≠ g ( a ) , f ′ ( g ( a ) ) , y = g ( a ) .

{\displaystyle Q(y)={\begin{cases}\displaystyle {\frac {f(y)-f(g(a))}{y-g(a)}},&y\neq g(a),\\f'(g(a)),&y=g(a).\end{cases}}} We will show that the difference quotient for f ∘ g is always equal to: Q ( g ( x ) ) ⋅ ⋅ g ( x ) − − g ( a ) x − − a .

{\displaystyle Q(g(x))\cdot {\frac {g(x)-g(a)}{x-a}}.} Whenever g ( x ) is not equal to g ( a ) , this is clear because the factors of g ( x ) − g ( a ) cancel. When g ( x ) equals g ( a ) , then the difference quotient for f ∘ g is zero because f ( g ( x )) equals f ( g ( a )) , and the above product is zero because it equals f ′( g ( a )) times zero. So the above product is always equal to the difference quotient, and to show that the derivative of f ∘ g at a exists and to determine its value, we need only show that the limit as x goes to a of the above product exists and determine its value.

To do this, recall that the limit of a product exists if the limits of its factors exist. When this happens, the limit of the product of these two factors will equal the product of the limits of the factors. The two factors are Q ( g ( x )) and ( g ( x ) − g ( a )) / ( x − a ) . The latter is the difference quotient for g at a , and because g is differentiable at a by assumption, its limit as x tends to a exists and equals g ′( a ) .

As for Q ( g ( x )) , notice that Q is defined wherever f is. Furthermore, f is differentiable at g ( a ) by assumption, so Q is continuous at g ( a ) , by definition of the derivative. The function g is continuous at a because it is differentiable at a , and therefore Q ∘ g is continuous at a . So its limit as x goes to a exists and equals Q ( g ( a )) , which is f ′( g ( a )) .

This shows that the limits of both factors exist and that they equal f ′( g ( a )) and g ′( a ) , respectively. Therefore, the derivative of f ∘ g at a exists and equals f ′( g ( a )) g ′( a ) .

Second proof [ edit ] Another way of proving the chain rule is to measure the error in the linear approximation determined by the derivative. This proof has the advantage that it generalizes to several variables. It relies on the following equivalent definition of differentiability at a point: A function g is differentiable at a if there exists a real number g ′( a ) and a function ε ( h ) that tends to zero as h tends to zero, and furthermore g ( a + h ) − − g ( a ) = g ′ ( a ) h + ε ε ( h ) h .

{\displaystyle g(a+h)-g(a)=g'(a)h+\varepsilon (h)h.} Here the left-hand side represents the true difference between the value of g at a and at a + h , whereas the right-hand side represents the approximation determined by the derivative plus an error term.

In the situation of the chain rule, such a function ε exists because g is assumed to be differentiable at a . Again by assumption, a similar function also exists for f at g ( a ). Calling this function η , we have f ( g ( a ) + k ) − − f ( g ( a ) ) = f ′ ( g ( a ) ) k + η η ( k ) k .

{\displaystyle f(g(a)+k)-f(g(a))=f'(g(a))k+\eta (k)k.} The above definition imposes no constraints on η (0), even though it is assumed that η ( k ) tends to zero as k tends to zero. If we set η (0) = 0 , then η is continuous at 0.

Proving the theorem requires studying the difference f ( g ( a + h )) − f ( g ( a )) as h tends to zero. The first step is to substitute for g ( a + h ) using the definition of differentiability of g at a : f ( g ( a + h ) ) − − f ( g ( a ) ) = f ( g ( a ) + g ′ ( a ) h + ε ε ( h ) h ) − − f ( g ( a ) ) .

{\displaystyle f(g(a+h))-f(g(a))=f(g(a)+g'(a)h+\varepsilon (h)h)-f(g(a)).} The next step is to use the definition of differentiability of f at g ( a ). This requires a term of the form f ( g ( a ) + k ) for some k . In the above equation, the correct k varies with h . Set k h = g ′( a ) h + ε ( h ) h and the right hand side becomes f ( g ( a ) + k h ) − f ( g ( a )) . Applying the definition of the derivative gives: f ( g ( a ) + k h ) − − f ( g ( a ) ) = f ′ ( g ( a ) ) k h + η η ( k h ) k h .

{\displaystyle f(g(a)+k_{h})-f(g(a))=f'(g(a))k_{h}+\eta (k_{h})k_{h}.} To study the behavior of this expression as h tends to zero, expand k h . After regrouping the terms, the right-hand side becomes: f ′ ( g ( a ) ) g ′ ( a ) h + [ f ′ ( g ( a ) ) ε ε ( h ) + η η ( k h ) g ′ ( a ) + η η ( k h ) ε ε ( h ) ] h .

{\displaystyle f'(g(a))g'(a)h+[f'(g(a))\varepsilon (h)+\eta (k_{h})g'(a)+\eta (k_{h})\varepsilon (h)]h.} Because ε ( h ) and η ( k h ) tend to zero as h tends to zero, the first two bracketed terms tend to zero as h tends to zero. Applying the same theorem on products of limits as in the first proof, the third bracketed term also tends zero. Because the above expression is equal to the difference f ( g ( a + h )) − f ( g ( a )) , by the definition of the derivative f ∘ g is differentiable at a and its derivative is f ′( g ( a )) g ′( a ).

The role of Q in the first proof is played by η in this proof. They are related by the equation: Q ( y ) = f ′ ( g ( a ) ) + η η ( y − − g ( a ) ) .

{\displaystyle Q(y)=f'(g(a))+\eta (y-g(a)).} The need to define Q at g ( a ) is analogous to the need to define η at zero.

Third proof [ edit ] Constantin Carathéodory 's alternative definition of the differentiability of a function can be used to give an elegant proof of the chain rule.

[ 6 ] Under this definition, a function f is differentiable at a point a if and only if there is a function q , continuous at a and such that f ( x ) − f ( a ) = q ( x )( x − a ) . There is at most one such function, and if f is differentiable at a then f ′( a ) = q ( a ) .

Given the assumptions of the chain rule and the fact that differentiable functions and compositions of continuous functions are continuous, we have that there exist functions q , continuous at g ( a ) , and r , continuous at a , and such that, f ( g ( x ) ) − − f ( g ( a ) ) = q ( g ( x ) ) ( g ( x ) − − g ( a ) ) {\displaystyle f(g(x))-f(g(a))=q(g(x))(g(x)-g(a))} and g ( x ) − − g ( a ) = r ( x ) ( x − − a ) .

{\displaystyle g(x)-g(a)=r(x)(x-a).} Therefore, f ( g ( x ) ) − − f ( g ( a ) ) = q ( g ( x ) ) r ( x ) ( x − − a ) , {\displaystyle f(g(x))-f(g(a))=q(g(x))r(x)(x-a),} but the function given by h ( x ) = q ( g ( x )) r ( x ) is continuous at a , and we get, for this a ( f ( g ( a ) ) ) ′ = q ( g ( a ) ) r ( a ) = f ′ ( g ( a ) ) g ′ ( a ) .

{\displaystyle (f(g(a)))'=q(g(a))r(a)=f'(g(a))g'(a).} A similar approach works for continuously differentiable (vector-)functions of many variables. This method of factoring also allows a unified approach to stronger forms of differentiability, when the derivative is required to be Lipschitz continuous , Hölder continuous , etc. Differentiation itself can be viewed as the polynomial remainder theorem (the little Bézout theorem, or factor theorem), generalized to an appropriate class of functions.

[ citation needed ] Proof via infinitesimals [ edit ] See also: Non-standard calculus If y = f ( x ) {\displaystyle y=f(x)} and x = g ( t ) {\displaystyle x=g(t)} then choosing infinitesimal Δ Δ t ≠ 0 {\displaystyle \Delta t\not =0} we compute the corresponding Δ Δ x = g ( t + Δ Δ t ) − − g ( t ) {\displaystyle \Delta x=g(t+\Delta t)-g(t)} and then the corresponding Δ Δ y = f ( x + Δ Δ x ) − − f ( x ) {\displaystyle \Delta y=f(x+\Delta x)-f(x)} , so that Δ Δ y Δ Δ t = Δ Δ y Δ Δ x Δ Δ x Δ Δ t {\displaystyle {\frac {\Delta y}{\Delta t}}={\frac {\Delta y}{\Delta x}}{\frac {\Delta x}{\Delta t}}} and applying the standard part we obtain d y d t = d y d x d x d t {\displaystyle {\frac {dy}{dt}}={\frac {dy}{dx}}{\frac {dx}{dt}}} which is the chain rule.

Multivariable case [ edit ] The full generalization of the chain rule to multi-variable functions (such as f : R m → → R n {\displaystyle f:\mathbb {R} ^{m}\to \mathbb {R} ^{n}} ) is rather technical. However, it is simpler to write in the case of functions of the form f ( g 1 ( x ) , … … , g k ( x ) ) , {\displaystyle f(g_{1}(x),\dots ,g_{k}(x)),} where f : R k → → R {\displaystyle f:\mathbb {R} ^{k}\to \mathbb {R} } , and g i : R → → R {\displaystyle g_{i}:\mathbb {R} \to \mathbb {R} } for each i = 1 , 2 , … … , k .

{\displaystyle i=1,2,\dots ,k.} As this case occurs often in the study of functions of a single variable, it is worth describing it separately.

Case of scalar-valued functions with multiple inputs [ edit ] Let f : R k → → R {\displaystyle f:\mathbb {R} ^{k}\to \mathbb {R} } , and g i : R → → R {\displaystyle g_{i}:\mathbb {R} \to \mathbb {R} } for each i = 1 , 2 , … … , k .

{\displaystyle i=1,2,\dots ,k.} To write the chain rule for the composition of functions x ↦ ↦ f ( g 1 ( x ) , … … , g k ( x ) ) , {\displaystyle x\mapsto f(g_{1}(x),\dots ,g_{k}(x)),} one needs the partial derivatives of f with respect to its k arguments. The usual notations for partial derivatives involve names for the arguments of the function. As these arguments are not named in the above formula, it is simpler and clearer to use D -Notation , and to denote by D i f {\displaystyle D_{i}f} the partial derivative of f with respect to its i th argument, and by D i f ( z ) {\displaystyle D_{i}f(z)} the value of this derivative at z .

With this notation, the chain rule is d d x f ( g 1 ( x ) , … … , g k ( x ) ) = ∑ ∑ i = 1 k ( d d x g i ( x ) ) D i f ( g 1 ( x ) , … … , g k ( x ) ) .

{\displaystyle {\frac {d}{dx}}f(g_{1}(x),\dots ,g_{k}(x))=\sum _{i=1}^{k}\left({\frac {d}{dx}}{g_{i}}(x)\right)D_{i}f(g_{1}(x),\dots ,g_{k}(x)).} Example: arithmetic operations [ edit ] If the function f is addition, that is, if f ( u , v ) = u + v , {\displaystyle f(u,v)=u+v,} then D 1 f = ∂ ∂ f ∂ ∂ u = 1 {\textstyle D_{1}f={\frac {\partial f}{\partial u}}=1} and D 2 f = ∂ ∂ f ∂ ∂ v = 1 {\textstyle D_{2}f={\frac {\partial f}{\partial v}}=1} . Thus, the chain rule gives d d x ( g ( x ) + h ( x ) ) = ( d d x g ( x ) ) D 1 f + ( d d x h ( x ) ) D 2 f = d d x g ( x ) + d d x h ( x ) .

{\displaystyle {\frac {d}{dx}}(g(x)+h(x))=\left({\frac {d}{dx}}g(x)\right)D_{1}f+\left({\frac {d}{dx}}h(x)\right)D_{2}f={\frac {d}{dx}}g(x)+{\frac {d}{dx}}h(x).} For multiplication f ( u , v ) = u v , {\displaystyle f(u,v)=uv,} the partials are D 1 f = v {\displaystyle D_{1}f=v} and D 2 f = u {\displaystyle D_{2}f=u} . Thus, d d x ( g ( x ) h ( x ) ) = h ( x ) d d x g ( x ) + g ( x ) d d x h ( x ) .

{\displaystyle {\frac {d}{dx}}(g(x)h(x))=h(x){\frac {d}{dx}}g(x)+g(x){\frac {d}{dx}}h(x).} The case of exponentiation f ( u , v ) = u v {\displaystyle f(u,v)=u^{v}} is slightly more complicated, as D 1 f = v u v − − 1 , {\displaystyle D_{1}f=vu^{v-1},} and, as u v = e v ln ⁡ ⁡ u , {\displaystyle u^{v}=e^{v\ln u},} D 2 f = u v ln ⁡ ⁡ u .

{\displaystyle D_{2}f=u^{v}\ln u.} It follows that d d x ( g ( x ) h ( x ) ) = h ( x ) g ( x ) h ( x ) − − 1 d d x g ( x ) + g ( x ) h ( x ) ln ⁡ ⁡ g ( x ) d d x h ( x ) .

{\displaystyle {\frac {d}{dx}}\left(g(x)^{h(x)}\right)=h(x)g(x)^{h(x)-1}{\frac {d}{dx}}g(x)+g(x)^{h(x)}\ln g(x)\,{\frac {d}{dx}}h(x).} General rule: Vector-valued functions with multiple inputs [ edit ] The simplest way for writing the chain rule in the general case is to use the total derivative , which is a linear transformation that captures all directional derivatives in a single formula. Consider differentiable functions f : R m → R k and g : R n → R m , and a point a in R n . Let D a g denote the total derivative of g at a and D g ( a ) f denote the total derivative of f at g ( a ) . These two derivatives are linear transformations R n → R m and R m → R k , respectively, so they can be composed. The chain rule for total derivatives is that their composite is the total derivative of f ∘ g at a : D a ( f ∘ ∘ g ) = D g ( a ) f ∘ ∘ D a g , {\displaystyle D_{\mathbf {a} }(f\circ g)=D_{g(\mathbf {a} )}f\circ D_{\mathbf {a} }g,} or for short, D ( f ∘ ∘ g ) = D f ∘ ∘ D g .

{\displaystyle D(f\circ g)=Df\circ Dg.} The higher-dimensional chain rule can be proved using a technique similar to the second proof given above.

[ 7 ] Because the total derivative is a linear transformation, the functions appearing in the formula can be rewritten as matrices. The matrix corresponding to a total derivative is called a Jacobian matrix , and the composite of two derivatives corresponds to the product of their Jacobian matrices. From this perspective the chain rule therefore says: J f ∘ ∘ g ( a ) = J f ( g ( a ) ) J g ( a ) , {\displaystyle J_{f\circ g}(\mathbf {a} )=J_{f}(g(\mathbf {a} ))J_{g}(\mathbf {a} ),} or for short, J f ∘ ∘ g = ( J f ∘ ∘ g ) J g .

{\displaystyle J_{f\circ g}=(J_{f}\circ g)J_{g}.} That is, the Jacobian of a composite function is the product of the Jacobians of the composed functions (evaluated at the appropriate points).

The higher-dimensional chain rule is a generalization of the one-dimensional chain rule. If k , m , and n are 1, so that f : R → R and g : R → R , then the Jacobian matrices of f and g are 1 × 1 . Specifically, they are: J g ( a ) = ( g ′ ( a ) ) , J f ( g ( a ) ) = ( f ′ ( g ( a ) ) ) .

{\displaystyle {\begin{aligned}J_{g}(a)&={\begin{pmatrix}g'(a)\end{pmatrix}},\\J_{f}(g(a))&={\begin{pmatrix}f'(g(a))\end{pmatrix}}.\end{aligned}}} The Jacobian of f ∘ g is the product of these 1 × 1 matrices, so it is f ′( g ( a ))⋅ g ′( a ) , as expected from the one-dimensional chain rule. In the language of linear transformations, D a ( g ) is the function which scales a vector by a factor of g ′( a ) and D g ( a ) ( f ) is the function which scales a vector by a factor of f ′( g ( a )) . The chain rule says that the composite of these two linear transformations is the linear transformation D a ( f ∘ g ) , and therefore it is the function that scales a vector by f ′( g ( a ))⋅ g ′( a ) .

Another way of writing the chain rule is used when f and g are expressed in terms of their components as y = f ( u ) = ( f 1 ( u ), …, f k ( u )) and u = g ( x ) = ( g 1 ( x ), …, g m ( x )) . In this case, the above rule for Jacobian matrices is usually written as: ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ ( x 1 , … … , x n ) = ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ ( u 1 , … … , u m ) ∂ ∂ ( u 1 , … … , u m ) ∂ ∂ ( x 1 , … … , x n ) .

{\displaystyle {\frac {\partial (y_{1},\ldots ,y_{k})}{\partial (x_{1},\ldots ,x_{n})}}={\frac {\partial (y_{1},\ldots ,y_{k})}{\partial (u_{1},\ldots ,u_{m})}}{\frac {\partial (u_{1},\ldots ,u_{m})}{\partial (x_{1},\ldots ,x_{n})}}.} The chain rule for total derivatives implies a chain rule for partial derivatives. Recall that when the total derivative exists, the partial derivative in the i -th coordinate direction is found by multiplying the Jacobian matrix by the i -th basis vector. By doing this to the formula above, we find: ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ x i = ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ ( u 1 , … … , u m ) ∂ ∂ ( u 1 , … … , u m ) ∂ ∂ x i .

{\displaystyle {\frac {\partial (y_{1},\ldots ,y_{k})}{\partial x_{i}}}={\frac {\partial (y_{1},\ldots ,y_{k})}{\partial (u_{1},\ldots ,u_{m})}}{\frac {\partial (u_{1},\ldots ,u_{m})}{\partial x_{i}}}.} Since the entries of the Jacobian matrix are partial derivatives, we may simplify the above formula to get: ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ x i = ∑ ∑ ℓ ℓ = 1 m ∂ ∂ ( y 1 , … … , y k ) ∂ ∂ u ℓ ℓ ∂ ∂ u ℓ ℓ ∂ ∂ x i .

{\displaystyle {\frac {\partial (y_{1},\ldots ,y_{k})}{\partial x_{i}}}=\sum _{\ell =1}^{m}{\frac {\partial (y_{1},\ldots ,y_{k})}{\partial u_{\ell }}}{\frac {\partial u_{\ell }}{\partial x_{i}}}.} More conceptually, this rule expresses the fact that a change in the x i direction may change all of g 1 through g m , and any of these changes may affect f .

In the special case where k = 1 , so that f is a real-valued function, then this formula simplifies even further: ∂ ∂ y ∂ ∂ x i = ∑ ∑ ℓ ℓ = 1 m ∂ ∂ y ∂ ∂ u ℓ ℓ ∂ ∂ u ℓ ℓ ∂ ∂ x i .

{\displaystyle {\frac {\partial y}{\partial x_{i}}}=\sum _{\ell =1}^{m}{\frac {\partial y}{\partial u_{\ell }}}{\frac {\partial u_{\ell }}{\partial x_{i}}}.} This can be rewritten as a dot product . Recalling that u = ( g 1 , …, g m ) , the partial derivative ∂ u / ∂ x i is also a vector, and the chain rule says that: ∂ ∂ y ∂ ∂ x i = ∇ ∇ y ⋅ ⋅ ∂ ∂ u ∂ ∂ x i .

{\displaystyle {\frac {\partial y}{\partial x_{i}}}=\nabla y\cdot {\frac {\partial \mathbf {u} }{\partial x_{i}}}.} Example [ edit ] Given u ( x , y ) = x 2 + 2 y where x ( r , t ) = r sin( t ) and y ( r , t ) = sin 2 ( t ) , determine the value of ∂ u / ∂ r and ∂ u / ∂ t using the chain rule.

[ citation needed ] ∂ ∂ u ∂ ∂ r = ∂ ∂ u ∂ ∂ x ∂ ∂ x ∂ ∂ r + ∂ ∂ u ∂ ∂ y ∂ ∂ y ∂ ∂ r = ( 2 x ) ( sin ⁡ ⁡ ( t ) ) + ( 2 ) ( 0 ) = 2 r sin 2 ⁡ ⁡ ( t ) , {\displaystyle {\frac {\partial u}{\partial r}}={\frac {\partial u}{\partial x}}{\frac {\partial x}{\partial r}}+{\frac {\partial u}{\partial y}}{\frac {\partial y}{\partial r}}=(2x)(\sin(t))+(2)(0)=2r\sin ^{2}(t),} and ∂ ∂ u ∂ ∂ t = ∂ ∂ u ∂ ∂ x ∂ ∂ x ∂ ∂ t + ∂ ∂ u ∂ ∂ y ∂ ∂ y ∂ ∂ t = ( 2 x ) ( r cos ⁡ ⁡ ( t ) ) + ( 2 ) ( 2 sin ⁡ ⁡ ( t ) cos ⁡ ⁡ ( t ) ) = ( 2 r sin ⁡ ⁡ ( t ) ) ( r cos ⁡ ⁡ ( t ) ) + 4 sin ⁡ ⁡ ( t ) cos ⁡ ⁡ ( t ) = 2 ( r 2 + 2 ) sin ⁡ ⁡ ( t ) cos ⁡ ⁡ ( t ) = ( r 2 + 2 ) sin ⁡ ⁡ ( 2 t ) .

{\displaystyle {\begin{aligned}{\frac {\partial u}{\partial t}}&={\frac {\partial u}{\partial x}}{\frac {\partial x}{\partial t}}+{\frac {\partial u}{\partial y}}{\frac {\partial y}{\partial t}}\\&=(2x)(r\cos(t))+(2)(2\sin(t)\cos(t))\\&=(2r\sin(t))(r\cos(t))+4\sin(t)\cos(t)\\&=2(r^{2}+2)\sin(t)\cos(t)\\&=(r^{2}+2)\sin(2t).\end{aligned}}} Higher derivatives of multivariable functions [ edit ] Main article: Faà di Bruno's formula § Multivariate version Faà di Bruno's formula for higher-order derivatives of single-variable functions generalizes to the multivariable case. If y = f ( u ) is a function of u = g ( x ) as above, then the second derivative of f ∘ g is: ∂ ∂ 2 y ∂ ∂ x i ∂ ∂ x j = ∑ ∑ k ( ∂ ∂ y ∂ ∂ u k ∂ ∂ 2 u k ∂ ∂ x i ∂ ∂ x j ) + ∑ ∑ k , ℓ ℓ ( ∂ ∂ 2 y ∂ ∂ u k ∂ ∂ u ℓ ℓ ∂ ∂ u k ∂ ∂ x i ∂ ∂ u ℓ ℓ ∂ ∂ x j ) .

{\displaystyle {\frac {\partial ^{2}y}{\partial x_{i}\partial x_{j}}}=\sum _{k}\left({\frac {\partial y}{\partial u_{k}}}{\frac {\partial ^{2}u_{k}}{\partial x_{i}\partial x_{j}}}\right)+\sum _{k,\ell }\left({\frac {\partial ^{2}y}{\partial u_{k}\partial u_{\ell }}}{\frac {\partial u_{k}}{\partial x_{i}}}{\frac {\partial u_{\ell }}{\partial x_{j}}}\right).} Further generalizations [ edit ] All extensions of calculus have a chain rule. In most of these, the formula remains the same, though the meaning of that formula may be vastly different.

One generalization is to manifolds . In this situation, the chain rule represents the fact that the derivative of f ∘ g is the composite of the derivative of f and the derivative of g . This theorem is an immediate consequence of the higher dimensional chain rule given above, and it has exactly the same formula.

The chain rule is also valid for Fréchet derivatives in Banach spaces . The same formula holds as before.

[ 8 ] This case and the previous one admit a simultaneous generalization to Banach manifolds .

In differential algebra , the derivative is interpreted as a morphism of modules of Kähler differentials . A ring homomorphism of commutative rings f : R → S determines a morphism of Kähler differentials Df : Ω R → Ω S which sends an element dr to d ( f ( r )) , the exterior differential of f ( r ) . The formula D ( f ∘ g ) = Df ∘ Dg holds in this context as well.

The common feature of these examples is that they are expressions of the idea that the derivative is part of a functor . A functor is an operation on spaces and functions between them. It associates to each space a new space and to each function between two spaces a new function between the corresponding new spaces. In each of the above cases, the functor sends each space to its tangent bundle and it sends each function to its derivative. For example, in the manifold case, the derivative sends a C r -manifold to a C r −1 -manifold (its tangent bundle) and a C r -function to its total derivative. There is one requirement for this to be a functor, namely that the derivative of a composite must be the composite of the derivatives. This is exactly the formula D ( f ∘ g ) = Df ∘ Dg .

There are also chain rules in stochastic calculus . One of these, Itō's lemma , expresses the composite of an Itō process (or more generally a semimartingale ) dX t with a twice-differentiable function f . In Itō's lemma, the derivative of the composite function depends not only on dX t and the derivative of f but also on the second derivative of f . The dependence on the second derivative is a consequence of the non-zero quadratic variation of the stochastic process, which broadly speaking means that the process can move up and down in a very rough way. This variant of the chain rule is not an example of a functor because the two functions being composed are of different types.

See also [ edit ] Automatic differentiation – Numerical calculations carrying along derivatives − a computational method that makes heavy use of the chain rule to compute exact numerical derivatives.

Differentiation rules – Rules for computing derivatives of functions Integration by substitution – Technique in integral evaluation Leibniz integral rule – Differentiation under the integral sign formula Product rule – Formula for the derivative of a product Quotient rule – Formula for the derivative of a ratio of functions Triple product rule – Relation between relative derivatives of three variables References [ edit ] ^ George F. Simmons , Calculus with Analytic Geometry (1985), p. 93.

^ Child, J. M. (1917).

"THE MANUSCRIPTS OF LEIBNIZ ON HIS DISCOVERY OF THE DIFFERENTIAL CALCULUS. PART II (Continued)" .

The Monist .

27 (3): 411– 454.

doi : 10.5840/monist191727324 .

ISSN 0026-9662 .

JSTOR 27900650 .

^ a b Rodríguez, Omar Hernández; López Fernández, Jorge M. (2010).

"A Semiotic Reflection on the Didactics of the Chain Rule" .

The Mathematics Enthusiast .

7 (2): 321– 332.

doi : 10.54870/1551-3440.1191 .

S2CID 29739148 . Retrieved 2019-08-04 .

^ Apostol, Tom (1974).

Mathematical analysis (2nd ed.). Addison Wesley. Theorem 5.5.

^ Goodfellow, Ian ; Bengio, Yoshua ; Courville, Aaron (2016), Deep learning , MIT , pp=197–217.

^ Kuhn, Stephen (1991). "The Derivative á la Carathéodory".

The American Mathematical Monthly .

98 (1): 40– 44.

doi : 10.2307/2324035 .

JSTOR 2324035 .

^ Spivak, Michael (1965).

Calculus on Manifolds . Boston: Addison-Wesley. pp.

19– 20.

ISBN 0-8053-9021-9 .

^ Cheney, Ward (2001). "The Chain Rule and Mean Value Theorems".

Analysis for Applied Mathematics . New York: Springer. pp.

121– 125.

ISBN 0-387-95279-9 .

Further reading [ edit ] Abou-Hayt, Imad; Dahl, Bettina (2025-03-29).

"Using a Realistic Context to Motivate and Teach Engineering Students the Chain Rule" .

Education Sciences .

15 (4): 433.

doi : 10.3390/educsci15040433 .

ISSN 2227-7102 .

External links [ edit ] "Leibniz rule" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Weisstein, Eric W.

"Chain Rule" .

MathWorld .

v t e Calculus Precalculus Binomial theorem Concave function Continuous function Factorial Finite difference Free variables and bound variables Graph of a function Linear function Radian Rolle's theorem Secant Slope Tangent Limits Indeterminate form Limit of a function One-sided limit Limit of a sequence Order of approximation (ε, δ)-definition of limit Differential calculus Derivative Second derivative Partial derivative Differential Differential operator Mean value theorem Notation Leibniz's notation Newton's notation Rules of differentiation linearity Power Sum Chain L'Hôpital's Product General Leibniz's rule Quotient Other techniques Implicit differentiation Inverse functions and differentiation Logarithmic derivative Related rates Stationary points First derivative test Second derivative test Extreme value theorem Maximum and minimum Further applications Newton's method Taylor's theorem Differential equation Ordinary differential equation Partial differential equation Stochastic differential equation Integral calculus Antiderivative Arc length Riemann integral Basic properties Constant of integration Fundamental theorem of calculus Differentiating under the integral sign Integration by parts Integration by substitution trigonometric Euler Tangent half-angle substitution Partial fractions in integration Quadratic integral Trapezoidal rule Volumes Washer method Shell method Integral equation Integro-differential equation Vector calculus Derivatives Curl Directional derivative Divergence Gradient Laplacian Basic theorems Line integrals Green's Stokes' Gauss' Multivariable calculus Divergence theorem Geometric Hessian matrix Jacobian matrix and determinant Lagrange multiplier Line integral Matrix Multiple integral Partial derivative Surface integral Volume integral Advanced topics Differential forms Exterior derivative Generalized Stokes' theorem Tensor calculus Sequences and series Arithmetico-geometric sequence Types of series Alternating Binomial Fourier Geometric Harmonic Infinite Power Maclaurin Taylor Telescoping Tests of convergence Abel's Alternating series Cauchy condensation Direct comparison Dirichlet's Integral Limit comparison Ratio Root Term Special functions and numbers Bernoulli numbers e (mathematical constant) Exponential function Natural logarithm Stirling's approximation History of calculus Adequality Brook Taylor Colin Maclaurin Generality of algebra Gottfried Wilhelm Leibniz Infinitesimal Infinitesimal calculus Isaac Newton Fluxion Law of Continuity Leonhard Euler Method of Fluxions The Method of Mechanical Theorems Lists Integrals rational functions irrational algebraic functions exponential functions logarithmic functions hyperbolic functions inverse trigonometric functions inverse Secant Secant cubed List of limits List of derivatives Miscellaneous topics Complex calculus Contour integral Differential geometry Manifold Curvature of curves of surfaces Tensor Euler–Maclaurin formula Gabriel's horn Integration Bee Proof that 22/7 exceeds π Regiomontanus' angle maximization problem Steinmetz solid Retrieved from " https://en.wikipedia.org/w/index.php?title=Chain_rule&oldid=1305550718 " Categories : Differentiation rules Theorems in mathematical analysis Theorems in calculus Hidden categories: Articles with short description Short description is different from Wikidata Pages using sidebar with the child parameter All articles with unsourced statements Articles with unsourced statements from September 2022 Articles with unsourced statements from November 2023 Articles with unsourced statements from February 2016 Articles containing proofs This page was last edited on 12 August 2025, at 18:39 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Chain rule 43 languages Add topic

