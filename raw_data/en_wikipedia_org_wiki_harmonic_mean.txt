Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Relationship with other means 3 Harmonic mean of two or three numbers Toggle Harmonic mean of two or three numbers subsection 3.1 Two numbers 3.2 Three numbers 4 Weighted harmonic mean 5 Examples Toggle Examples subsection 5.1 In analytic number theory 5.1.1 Prime number theory 5.2 In physics 5.2.1 Average speed 5.2.2 Density 5.2.3 Electricity 5.2.4 Optics 5.3 In finance 5.4 In geometry 5.5 In other sciences 6 Beta distribution 7 Lognormal distribution 8 Pareto distribution 9 Statistics Toggle Statistics subsection 9.1 Sample distributions of mean and variance 9.2 Delta method 9.3 Jackknife method 9.4 Size biased sampling 9.5 Shifted variables 9.6 Moments 9.7 Sampling properties 9.8 Bias and variance estimators 10 Notes 11 See also 12 Notes 13 References 14 External links Toggle the table of contents Harmonic mean 49 languages العربية Azərbaycanca Български Català Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Italiano עברית Қазақша Latina Lietuvių Magyar Македонски Nederlands 日本語 Norsk bokmål Norsk nynorsk Piemontèis Polski Português Română Русский Simple English Slovenčina Slovenščina کوردی Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska தமிழ் Türkçe Українська اردو Tiếng Việt 粵語 中文 Edit links Article Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Inverse of the average of the inverses of a set of numbers In mathematics , the harmonic mean is a kind of average , one of the Pythagorean means .

It is the most appropriate average for ratios and rates such as speeds, [ 1 ] [ 2 ] and is normally only used for positive arguments.

[ 3 ] The harmonic mean is the reciprocal of the arithmetic mean of the reciprocals of the numbers, that is, the generalized f-mean with f ( x ) = 1 x {\displaystyle f(x)={\frac {1}{x}}} . For example, the harmonic mean of 1, 4, and 4 is ( 1 − − 1 + 4 − − 1 + 4 − − 1 3 ) − − 1 = 3 1 1 + 1 4 + 1 4 = 3 1.5 = 2 .

{\displaystyle \left({\frac {1^{-1}+4^{-1}+4^{-1}}{3}}\right)^{-1}={\frac {3}{{\frac {1}{1}}+{\frac {1}{4}}+{\frac {1}{4}}}}={\frac {3}{1.5}}=2\,.} Definition The harmonic mean H of the positive real numbers x 1 , x 2 , … … , x n {\displaystyle x_{1},x_{2},\ldots ,x_{n}} is [ 4 ] H ( x 1 , x 2 , … … , x n ) = n 1 x 1 + 1 x 2 + ⋯ ⋯ + 1 x n = n ∑ ∑ i = 1 n 1 x i .

{\displaystyle H(x_{1},x_{2},\ldots ,x_{n})={\frac {n}{\displaystyle {\frac {1}{x_{1}}}+{\frac {1}{x_{2}}}+\cdots +{\frac {1}{x_{n}}}}}={\frac {n}{\displaystyle \sum _{i=1}^{n}{\frac {1}{x_{i}}}}}.} It is the reciprocal of the arithmetic mean of the reciprocals, and vice versa: H ( x 1 , x 2 , … … , x n ) = 1 A ( 1 x 1 , 1 x 2 , … … 1 x n ) , A ( x 1 , x 2 , … … , x n ) = 1 H ( 1 x 1 , 1 x 2 , … … 1 x n ) , {\displaystyle {\begin{aligned}H(x_{1},x_{2},\ldots ,x_{n})&={\frac {1}{\displaystyle A\left({\frac {1}{x_{1}}},{\frac {1}{x_{2}}},\ldots {\frac {1}{x_{n}}}\right)}},\\A(x_{1},x_{2},\ldots ,x_{n})&={\frac {1}{\displaystyle H\left({\frac {1}{x_{1}}},{\frac {1}{x_{2}}},\ldots {\frac {1}{x_{n}}}\right)}},\end{aligned}}} where the arithmetic mean is A ( x 1 , x 2 , … … , x n ) = 1 n ∑ ∑ i = 1 n x i .

{\textstyle A(x_{1},x_{2},\ldots ,x_{n})={\tfrac {1}{n}}\sum _{i=1}^{n}x_{i}.} The harmonic mean is a Schur-concave function, and is greater than or equal to the minimum of its arguments: for positive arguments, min ( x 1 … … x n ) ≤ ≤ H ( x 1 … … x n ) ≤ ≤ n min ( x 1 … … x n ) {\displaystyle \min(x_{1}\ldots x_{n})\leq H(x_{1}\ldots x_{n})\leq n\min(x_{1}\ldots x_{n})} . Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).

[ citation needed ] The harmonic mean is also concave for positive arguments, an even stronger property than Schur-concavity.

[ citation needed ] Relationship with other means Geometric proof without words that max ( a , b ) > root mean square ( RMS ) or quadratic mean ( QM ) > arithmetic mean ( AM ) > geometric mean ( GM ) > harmonic mean ( HM ) > min ( a , b ) of two distinct positive numbers a and b [ note 1 ] For all positive data sets containing at least one pair of nonequal values , the harmonic mean is always the least of the three Pythagorean means, [ 5 ] while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (If all values in a nonempty data set are equal, the three means are always equal.) It is the special case M −1 of the power mean : H ( x 1 , x 2 , … … , x n ) = M − − 1 ( x 1 , x 2 , … … , x n ) = n x 1 − − 1 + x 2 − − 1 + ⋯ ⋯ + x n − − 1 {\displaystyle H\left(x_{1},x_{2},\ldots ,x_{n}\right)=M_{-1}\left(x_{1},x_{2},\ldots ,x_{n}\right)={\frac {n}{x_{1}^{-1}+x_{2}^{-1}+\cdots +x_{n}^{-1}}}} Since the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.

The arithmetic mean is often mistakenly used in places calling for the harmonic mean.

[ 6 ] In the speed example below for instance, the arithmetic mean of 40 is incorrect, and too big.

The harmonic mean is related to the other Pythagorean means, as seen in the equation below. This can be seen by interpreting the denominator to be the arithmetic mean of the product of numbers n times but each time omitting the j -th term. That is, for the first term, we multiply all n numbers except the first; for the second, we multiply all n numbers except the second; and so on. The numerator, excluding the n , which goes with the arithmetic mean, is the geometric mean to the power n . Thus the n -th harmonic mean is related to the n -th geometric and arithmetic means. The general formula is H ( x 1 , … … , x n ) = ( G ( x 1 , … … , x n ) ) n A ( x 2 x 3 ⋯ ⋯ x n , x 1 x 3 ⋯ ⋯ x n , … … , x 1 x 2 ⋯ ⋯ x n − − 1 ) = ( G ( x 1 , … … , x n ) ) n A ( 1 x 1 ∏ ∏ i = 1 n x i , 1 x 2 ∏ ∏ i = 1 n x i , … … , 1 x n ∏ ∏ i = 1 n x i ) .

{\displaystyle H\left(x_{1},\ldots ,x_{n}\right)={\frac {\left(G\left(x_{1},\ldots ,x_{n}\right)\right)^{n}}{A\left(x_{2}x_{3}\cdots x_{n},x_{1}x_{3}\cdots x_{n},\ldots ,x_{1}x_{2}\cdots x_{n-1}\right)}}={\frac {\left(G\left(x_{1},\ldots ,x_{n}\right)\right)^{n}}{A\left({\frac {1}{x_{1}}}{\prod \limits _{i=1}^{n}x_{i}},{\frac {1}{x_{2}}}{\prod \limits _{i=1}^{n}x_{i}},\ldots ,{\frac {1}{x_{n}}}{\prod \limits _{i=1}^{n}x_{i}}\right)}}.} If a set of non-identical numbers is subjected to a mean-preserving spread — that is, two or more elements of the set are "spread apart" from each other while leaving the arithmetic mean unchanged — then the harmonic mean always decreases.

[ 7 ] Harmonic mean of two or three numbers Two numbers A geometric construction of the three Pythagorean means of two numbers, a and b . The harmonic mean is denoted by H in purple, while the arithmetic mean is A in red and the geometric mean is G in blue.

Q denotes a fourth mean, the quadratic mean . Since a hypotenuse is always longer than a leg of a right triangle , the diagram shows that H ≤ ≤ G ≤ ≤ A ≤ ≤ Q {\displaystyle H\leq G\leq A\leq Q} .

A graphical interpretation of the harmonic mean, z of two numbers, x and y , and a nomogram to calculate it. The blue line shows that the harmonic mean of 6 and 2 is 3. The magenta line shows that the harmonic mean of 6 and −2 is −6. The red line shows that the harmonic mean of a number and its negative is undefined as the line does not intersect the z axis.

For the special case of just two numbers, x 1 {\displaystyle x_{1}} and x 2 {\displaystyle x_{2}} , the harmonic mean can be written as: [ 4 ] H = 2 x 1 x 2 x 1 + x 2 {\displaystyle H={\frac {2x_{1}x_{2}}{x_{1}+x_{2}}}\qquad } or 1 H = ( 1 / x 1 ) + ( 1 / x 2 ) 2 .

{\displaystyle \qquad {\frac {1}{H}}={\frac {(1/x_{1})+(1/x_{2})}{2}}.} (Note that the harmonic mean is undefined if x 1 + x 2 = 0 {\displaystyle x_{1}+x_{2}=0} , i.e.

x 1 = − − x 2 {\displaystyle x_{1}=-x_{2}} .) In this special case, the harmonic mean is related to the arithmetic mean A = x 1 + x 2 2 {\displaystyle A={\frac {x_{1}+x_{2}}{2}}} and the geometric mean G = x 1 x 2 , {\displaystyle G={\sqrt {x_{1}x_{2}}},} by [ 4 ] H = G 2 A = G ( G A ) .

{\displaystyle H={\frac {G^{2}}{A}}=G\left({\frac {G}{A}}\right).} Since G A ≤ ≤ 1 {\displaystyle {\tfrac {G}{A}}\leq 1} by the inequality of arithmetic and geometric means , this shows for the n = 2 case that H ≤ G (a property that in fact holds for all n ). It also follows that G = A H {\displaystyle G={\sqrt {AH}}} , meaning the two numbers' geometric mean equals the geometric mean of their arithmetic and harmonic means.

Three numbers For the special case of three numbers, x 1 {\displaystyle x_{1}} , x 2 {\displaystyle x_{2}} and x 3 {\displaystyle x_{3}} , the harmonic mean can be written as: [ 4 ] H = 3 x 1 x 2 x 3 x 1 x 2 + x 1 x 3 + x 2 x 3 .

{\displaystyle H={\frac {3x_{1}x_{2}x_{3}}{x_{1}x_{2}+x_{1}x_{3}+x_{2}x_{3}}}.} Three positive numbers H , G , and A are respectively the harmonic, geometric, and arithmetic means of three positive numbers if and only if [ 8 ] : p.74, #1834 the following inequality holds A 3 G 3 + G 3 H 3 + 1 ≤ ≤ 3 4 ( 1 + A H ) 2 .

{\displaystyle {\frac {A^{3}}{G^{3}}}+{\frac {G^{3}}{H^{3}}}+1\leq {\frac {3}{4}}\left(1+{\frac {A}{H}}\right)^{2}.} Weighted harmonic mean If a set of weights w 1 {\displaystyle w_{1}} , ..., w n {\displaystyle w_{n}} is associated to the data set x 1 {\displaystyle x_{1}} , ..., x n {\displaystyle x_{n}} , the weighted harmonic mean is defined by [ 9 ] H = ∑ ∑ i = 1 n w i ∑ ∑ i = 1 n w i x i = ( ∑ ∑ i = 1 n w i x i − − 1 ∑ ∑ i = 1 n w i ) − − 1 .

{\displaystyle H={\frac {\sum \limits _{i=1}^{n}w_{i}}{\sum \limits _{i=1}^{n}{\frac {w_{i}}{x_{i}}}}}=\left({\frac {\sum \limits _{i=1}^{n}w_{i}x_{i}^{-1}}{\sum \limits _{i=1}^{n}w_{i}}}\right)^{-1}.} The unweighted harmonic mean can be regarded as the special case where all of the weights are equal.

Examples In analytic number theory Prime number theory The prime number theorem states that the number of primes less than or equal to n {\displaystyle n} is asymptotically equal to the harmonic mean of the first n {\displaystyle n} natural numbers .

[ 10 ] In physics Average speed In many situations involving rates and ratios , the harmonic mean provides the correct average . For instance, if a vehicle travels a certain distance d outbound at a speed x (e.g. 60 km/h) and returns the same distance at a speed y (e.g. 20 km/h), then its average speed is the harmonic mean of x and y (30 km/h), not the arithmetic mean (40 km/h). The total travel time is the same as if it had traveled the whole distance at that average speed. This can be proven as follows: [ 11 ] Average speed for the entire journey
= ⁠ Total distance traveled / Sum of time for each segment ⁠ = ⁠ 2 d / ⁠ d / x ⁠ + ⁠ d / y ⁠ ⁠ = ⁠ 2 xy / x + y ⁠ However, if the vehicle travels for a certain amount of time at a speed x and then the same amount of time at a speed y , then its average speed is the arithmetic mean of x and y , which in the above example is 40 km/h.

Average speed for the entire journey
= ⁠ Total distance traveled / Sum of time for each segment ⁠ = ⁠ xt + yt / 2t ⁠ = ⁠ x + y / 2 ⁠ The same principle applies to more than two segments: given a series of sub-trips at different speeds, if each sub-trip covers the same distance , then the average speed is the harmonic mean of all the sub-trip speeds; and if each sub-trip takes the same amount of time , then the average speed is the arithmetic mean of all the sub-trip speeds. (If neither is the case, then a weighted harmonic mean or weighted arithmetic mean is needed. For the arithmetic mean, the speed of each portion of the trip is weighted by the duration of that portion, while for the harmonic mean, the corresponding weight is the distance.  In both cases, the resulting formula reduces to dividing the total distance by the total time.) However, one may avoid the use of the harmonic mean for the case of "weighting by distance". Pose the problem as finding "slowness" of the trip where "slowness" (in hours per kilometre) is the inverse of speed. When trip slowness is found, invert it so as to find the "true" average trip speed. For each trip segment i, the slowness s i = 1/speed i . Then take the weighted arithmetic mean of the s i 's weighted by their respective distances (optionally with the weights normalized so they sum to 1 by dividing them by trip length). This gives the true average slowness (in time per kilometre). It turns out that this procedure, which can be done with no knowledge of the harmonic mean, amounts to the same mathematical operations as one would use in solving this problem by using the harmonic mean. Thus it illustrates why the harmonic mean works in this case.

Density This section does not cite any sources .

Please help improve this section by adding citations to reliable sources . Unsourced material may be challenged and removed .

( December 2019 ) ( Learn how and when to remove this message ) Similarly, if one wishes to estimate the density of an alloy given the densities of its constituent elements and their mass fractions (or, equivalently, percentages by mass), then the predicted density of the alloy (exclusive of typically minor volume changes due to atom packing effects) is the weighted harmonic mean of the individual densities, weighted by mass, rather than the weighted arithmetic mean as one might at first expect. To use the weighted arithmetic mean, the densities would have to be weighted by volume. Applying dimensional analysis to the problem while labeling the mass units by element and making sure that only like element-masses cancel makes this clear.

Electricity See also: Parallel (operator) If one connects two electrical resistors in parallel, one having resistance x (e.g., 60 Ω ) and one having resistance y (e.g., 40 Ω), then the effect is the same as if one had used two resistors with the same resistance, both equal to the harmonic mean of x and y (48 Ω): the equivalent resistance, in either case, is 24 Ω (one-half of the harmonic mean). This same principle applies to capacitors in series or to inductors in parallel.

However, if one connects the resistors in series, then the average resistance is the arithmetic mean of x and y (50 Ω), with total resistance equal to twice this, the sum of x and y (100 Ω). This principle applies to capacitors in parallel or to inductors in series.

As with the previous example, the same principle applies when more than two resistors, capacitors or inductors are connected, provided that all are in parallel or all are in series.

The "conductivity effective mass" of a semiconductor is also defined as the harmonic mean of the effective masses along the three crystallographic directions.

[ 12 ] Optics As for other optic equations , the thin lens equation ⁠ 1 / f ⁠ = ⁠ 1 / u ⁠ + ⁠ 1 / v ⁠ can be rewritten such that the focal length f is one-half of the harmonic mean of the distances of the subject u and object v from the lens.

[ 13 ] Two thin lenses of focal length f 1 and f 2 in series is equivalent to two thin lenses of focal length f hm , their harmonic mean, in series. Expressed as optical power , two thin lenses of optical powers P 1 and P 2 in series is equivalent to two thin lenses of optical power P am , their arithmetic mean, in series.

In finance The weighted harmonic mean is the preferable method for averaging multiples, such as the price–earnings ratio (P/E). If these ratios are averaged using a weighted arithmetic mean, high data points are given greater weights than low data points. The weighted harmonic mean, on the other hand, correctly weights each data point.

[ 14 ] The simple weighted arithmetic mean when applied to non-price normalized ratios such as the P/E is biased upwards and cannot be numerically justified, since it is based on equalized earnings; just as vehicles speeds cannot be averaged for a roundtrip journey (see above).

[ 15 ] In geometry In any triangle , the radius of the incircle is one-third of the harmonic mean of the altitudes .

For any point P on the minor arc BC of the circumcircle of an equilateral triangle ABC, with distances q and t from B and C respectively, and with the intersection of PA and BC being at a distance y from point P, we have that y is half the harmonic mean of q and t .

[ 16 ] In a right triangle with legs a and b and altitude h from the hypotenuse to the right angle, h 2 is half the harmonic mean of a 2 and b 2 .

[ 17 ] [ 18 ] Let t and s ( t > s ) be the sides of the two inscribed squares in a right triangle with hypotenuse c . Then s 2 equals half the harmonic mean of c 2 and t 2 .

Let a trapezoid have vertices A, B, C, and D in sequence and have parallel sides AB and CD. Let E be the intersection of the diagonals , and let F be on side DA and G be on side BC such that FEG is parallel to AB and CD. Then FG is the harmonic mean of AB and DC. (This is provable using similar triangles.) Crossed ladders.

h is half the harmonic mean of A and B One application of this trapezoid result is in the crossed ladders problem , where two ladders lie oppositely across an alley, each with feet at the base of one sidewall, with one leaning against a wall at height A and the other leaning against the opposite wall at height B , as shown. The ladders cross at a height of h above the alley floor. Then h is half the harmonic mean of A and B . This result still holds if the walls are slanted but still parallel and the "heights" A , B , and h are measured as distances from the floor along lines parallel to the walls. This can be proved easily using the area formula of a trapezoid and area addition formula.

In an ellipse , the semi-latus rectum (the distance from a focus to the ellipse along a line parallel to the minor axis) is the harmonic mean of the maximum and minimum distances of the ellipse from a focus.

In other sciences In computer science , specifically information retrieval and machine learning , the harmonic mean of the precision (true positives per predicted positive) and the recall (true positives per real positive) is often used as an aggregated performance score for the evaluation of algorithms and systems: the F-score (or F-measure). This is used in information retrieval because only the positive class is of relevance , while number of negatives, in general, is large and unknown.

[ 19 ] It is thus a trade-off as to whether the correct positive predictions should be measured in relation to the number of predicted positives or the number of real positives, so it is measured versus a putative number of positives that is an arithmetic mean of the two possible denominators.

A consequence arises from basic algebra in problems where people or systems work together. As an example, if a gas-powered pump can drain a pool in 4 hours and a battery-powered pump can drain the same pool in 6 hours, then it will take both pumps ⁠ 6·4 / 6 + 4 ⁠ , which is equal to 2.4 hours, to drain the pool together. This is one-half of the harmonic mean of 6 and 4: ⁠ 2·6·4 / 6 + 4 ⁠ = 4.8 . That is, the appropriate average for the two types of pump is the harmonic mean, and with one pair of pumps (two pumps), it takes half this harmonic mean time, while with two pairs of pumps (four pumps) it would take a quarter of this harmonic mean time.

In hydrology , the harmonic mean is similarly used to average hydraulic conductivity values for a flow that is perpendicular to layers (e.g., geologic or soil) - flow parallel to layers uses the arithmetic mean. This apparent difference in averaging is explained by the fact that hydrology uses conductivity, which is the inverse of resistivity.

In sabermetrics , a baseball player's Power–speed number is the harmonic mean of their home run and stolen base totals.

In population genetics , the harmonic mean is used when calculating the effects of fluctuations in the census population size on the effective population size. The harmonic mean takes into account the fact that events such as population bottleneck increase the rate genetic drift and reduce the amount of genetic variation in the population.  This is a result of the fact that following a bottleneck very few individuals contribute to the gene pool limiting the genetic variation present in the population for many generations to come.

When considering fuel economy in automobiles two measures are commonly used – miles per gallon (mpg), and litres per 100 km. As the dimensions of these quantities are the inverse of each other (one is distance per volume, the other volume per distance) when taking the mean value of the fuel economy of a range of cars one measure will produce the harmonic mean of the other – i.e., converting the mean value of fuel economy expressed in litres per 100 km to miles per gallon will produce the harmonic mean of the fuel economy expressed in miles per gallon. For calculating the average fuel consumption of a fleet of vehicles from the individual fuel consumptions, the harmonic mean should be used if the fleet uses miles per gallon, whereas the arithmetic mean should be used if the fleet uses litres per 100 km. In the USA the CAFE standards (the federal automobile fuel consumption standards) make use of the harmonic mean.

In chemistry and nuclear physics the average mass per particle of a mixture consisting of different species (e.g., molecules or isotopes) is given by the harmonic mean of the individual species' masses weighted by their respective mass fraction.

Beta distribution This section does not cite any sources .

Please help improve this section by adding citations to reliable sources . Unsourced material may be challenged and removed .

( December 2019 ) ( Learn how and when to remove this message ) Harmonic mean for Beta distribution for 0 < α < 5 and 0 < β < 5 (Mean - HarmonicMean) for Beta distribution versus alpha and beta from 0 to 2 Harmonic Means for Beta distribution Purple=H(X), Yellow=H(1-X), smaller values alpha and beta in front Harmonic Means for Beta distribution Purple=H(X), Yellow=H(1-X), larger values alpha and beta in front The harmonic mean of a beta distribution with shape parameters α and β is: H = α α − − 1 α α + β β − − 1 conditional on α α > 1 & & β β > 0 {\displaystyle H={\frac {\alpha -1}{\alpha +\beta -1}}{\text{ conditional on }}\alpha >1\,\,\&\,\,\beta >0} The harmonic mean with α < 1 is undefined because its defining expression is not bounded in [0, 1].

Letting α = β H = α α − − 1 2 α α − − 1 {\displaystyle H={\frac {\alpha -1}{2\alpha -1}}} showing that for α = β the harmonic mean ranges from 0 for α = β = 1, to 1/2 for α = β → ∞.

The following are the limits with one parameter finite (non-zero) and the other parameter approaching these limits: lim α α → → 0 H = undefined lim α α → → 1 H = lim β β → → ∞ ∞ H = 0 lim β β → → 0 H = lim α α → → ∞ ∞ H = 1 {\displaystyle {\begin{aligned}\lim _{\alpha \to 0}H&={\text{ undefined }}\\\lim _{\alpha \to 1}H&=\lim _{\beta \to \infty }H=0\\\lim _{\beta \to 0}H&=\lim _{\alpha \to \infty }H=1\end{aligned}}} With the geometric mean the harmonic mean may be useful in maximum likelihood estimation in the four parameter case.

A second harmonic mean ( H 1 − X ) also exists for this distribution H 1 − − X = β β − − 1 α α + β β − − 1 conditional on β β > 1 & & α α > 0 {\displaystyle H_{1-X}={\frac {\beta -1}{\alpha +\beta -1}}{\text{ conditional on }}\beta >1\,\,\&\,\,\alpha >0} This harmonic mean with β < 1 is undefined because its defining expression is not bounded in [ 0, 1 ].

Letting α = β in the above expression H 1 − − X = β β − − 1 2 β β − − 1 {\displaystyle H_{1-X}={\frac {\beta -1}{2\beta -1}}} showing that for α = β the harmonic mean ranges from 0, for α = β = 1, to 1/2, for α = β → ∞.

The following are the limits with one parameter finite (non zero) and the other approaching these limits: lim β β → → 0 H 1 − − X = undefined lim β β → → 1 H 1 − − X = lim α α → → ∞ ∞ H 1 − − X = 0 lim α α → → 0 H 1 − − X = lim β β → → ∞ ∞ H 1 − − X = 1 {\displaystyle {\begin{aligned}\lim _{\beta \to 0}H_{1-X}&={\text{ undefined }}\\\lim _{\beta \to 1}H_{1-X}&=\lim _{\alpha \to \infty }H_{1-X}=0\\\lim _{\alpha \to 0}H_{1-X}&=\lim _{\beta \to \infty }H_{1-X}=1\end{aligned}}} Although both harmonic means are asymmetric, when α = β the two means are equal.

Lognormal distribution The harmonic mean ( H ) of the lognormal distribution of a random variable X is [ 20 ] H = exp ⁡ ⁡ ( μ μ − − 1 2 σ σ 2 ) , {\displaystyle H=\exp \left(\mu -{\frac {1}{2}}\sigma ^{2}\right),} where μ and σ 2 are the parameters of the distribution, i.e. the mean and variance of the distribution of the natural logarithm of X .

The harmonic and arithmetic means of the distribution are related by μ μ ∗ ∗ H = 1 + C v 2 , {\displaystyle {\frac {\mu ^{*}}{H}}=1+C_{v}^{2}\,,} where C v and μ * are the coefficient of variation and the mean of the distribution respectively..

The geometric ( G ), arithmetic and harmonic means of the distribution are related by [ 21 ] H μ μ ∗ ∗ = G 2 .

{\displaystyle H\mu ^{*}=G^{2}.} Pareto distribution The harmonic mean of type 1 Pareto distribution is [ 22 ] H = k ( 1 + 1 α α ) {\displaystyle H=k\left(1+{\frac {1}{\alpha }}\right)} where k is the scale parameter and α is the shape parameter.

Statistics For a random sample, the harmonic mean is calculated as above. Both the mean and the variance may be infinite (if it includes at least one term of the form 1/0).

Sample distributions of mean and variance The mean of the sample m is  asymptotically distributed normally with variance s 2 .

s 2 = m [ E ⁡ ⁡ ( 1 x − − 1 ) ] m 2 n {\displaystyle s^{2}={\frac {m\left[\operatorname {E} \left({\frac {1}{x}}-1\right)\right]}{m^{2}n}}} The variance of the mean itself is [ 23 ] Var ⁡ ⁡ ( 1 x ) = m [ E ⁡ ⁡ ( 1 x − − 1 ) ] n m 2 {\displaystyle \operatorname {Var} \left({\frac {1}{x}}\right)={\frac {m\left[\operatorname {E} \left({\frac {1}{x}}-1\right)\right]}{nm^{2}}}} where m is the arithmetic mean of the reciprocals, x are the variates, n is the population size and E is the expectation operator.

Delta method This section does not cite any sources .

Please help improve this section by adding citations to reliable sources . Unsourced material may be challenged and removed .

( December 2019 ) ( Learn how and when to remove this message ) Assuming that the variance is not infinite and that the central limit theorem applies to the sample then using the delta method , the variance is Var ⁡ ⁡ ( H ) = 1 n s 2 m 4 {\displaystyle \operatorname {Var} (H)={\frac {1}{n}}{\frac {s^{2}}{m^{4}}}} where H is the harmonic mean, m is the arithmetic mean of the reciprocals m = 1 n ∑ ∑ 1 x .

{\displaystyle m={\frac {1}{n}}\sum {\frac {1}{x}}.} s 2 is the variance of the reciprocals of the data s 2 = Var ⁡ ⁡ ( 1 x ) {\displaystyle s^{2}=\operatorname {Var} \left({\frac {1}{x}}\right)} and n is the number of data points in the sample.

Jackknife method A jackknife method of estimating the variance is possible if the mean is known.

[ 24 ] This method is the usual 'delete 1' rather than the 'delete m' version.

This method first requires the computation of the mean of the sample ( m ) m = n ∑ ∑ 1 x {\displaystyle m={\frac {n}{\sum {\frac {1}{x}}}}} where x are the sample values.

A series of value w i is then computed where w i = n − − 1 ∑ ∑ j ≠ ≠ i 1 x .

{\displaystyle w_{i}={\frac {n-1}{\sum _{j\neq i}{\frac {1}{x}}}}.} The mean ( h ) of the w i is then taken: h = 1 n ∑ ∑ w i {\displaystyle h={\frac {1}{n}}\sum {w_{i}}} The variance of the mean is n − − 1 n ∑ ∑ ( m − − w i ) 2 .

{\displaystyle {\frac {n-1}{n}}\sum {(m-w_{i})}^{2}.} Significance testing and confidence intervals for the mean can then be estimated with the t test .

Size biased sampling Assume a random variate has a distribution f ( x ). Assume also that the likelihood of a variate being chosen is proportional to its value. This is known as length based or size biased sampling.

Let μ be the mean of the population. Then the probability density function f *( x ) of the size biased population is f ∗ ∗ ( x ) = x f ( x ) μ μ {\displaystyle f^{*}(x)={\frac {xf(x)}{\mu }}} The expectation of this length biased distribution E * ( x ) is [ 23 ] E ∗ ∗ ⁡ ⁡ ( x ) = μ μ [ 1 + σ σ 2 μ μ 2 ] {\displaystyle \operatorname {E} ^{*}(x)=\mu \left[1+{\frac {\sigma ^{2}}{\mu ^{2}}}\right]} where σ 2 is the variance.

The expectation of the harmonic mean is the same as the non-length biased version E( x ) E ∗ ∗ ( x − − 1 ) = E ( x ) − − 1 {\displaystyle E^{*}(x^{-1})=E(x)^{-1}} The problem of length biased sampling arises in a number of areas including textile manufacture [ 25 ] pedigree analysis [ 26 ] and survival analysis [ 27 ] Akman et al.

have developed a test for the detection of length based bias in samples.

[ 28 ] Shifted variables If X is a positive random variable and q > 0 then for all ε > 0 [ 29 ] Var ⁡ ⁡ [ 1 ( X + ϵ ϵ ) q ] < Var ⁡ ⁡ ( 1 X q ) .

{\displaystyle \operatorname {Var} \left[{\frac {1}{(X+\epsilon )^{q}}}\right]<\operatorname {Var} \left({\frac {1}{X^{q}}}\right).} Moments Assuming that X and E( X ) are > 0 then [ 29 ] E ⁡ ⁡ [ 1 X ] ≥ ≥ 1 E ⁡ ⁡ ( X ) {\displaystyle \operatorname {E} \left[{\frac {1}{X}}\right]\geq {\frac {1}{\operatorname {E} (X)}}} This follows from Jensen's inequality .

Gurland has shown that [ 30 ] for a distribution that takes only positive values, for any n > 0 E ⁡ ⁡ ( X − − 1 ) ≥ ≥ E ⁡ ⁡ ( X n − − 1 ) E ⁡ ⁡ ( X n ) .

{\displaystyle \operatorname {E} \left(X^{-1}\right)\geq {\frac {\operatorname {E} \left(X^{n-1}\right)}{\operatorname {E} \left(X^{n}\right)}}.} Under some conditions [ 31 ] E ⁡ ⁡ ( a + X ) − − n ∼ ∼ E ⁡ ⁡ ( a + X − − n ) {\displaystyle \operatorname {E} (a+X)^{-n}\sim \operatorname {E} \left(a+X^{-n}\right)} where ~ means approximately equal to.

Sampling properties Assuming that the variates ( x ) are drawn from a lognormal distribution there are several possible estimators for H : H 1 = n ∑ ∑ ( 1 x ) H 2 = ( exp ⁡ ⁡ [ 1 n ∑ ∑ log e ⁡ ⁡ ( x ) ] ) 2 1 n ∑ ∑ ( x ) H 3 = exp ⁡ ⁡ ( m − − 1 2 s 2 ) {\displaystyle {\begin{aligned}H_{1}&={\frac {n}{\sum \left({\frac {1}{x}}\right)}}\\H_{2}&={\frac {\left(\exp \left[{\frac {1}{n}}\sum \log _{e}(x)\right]\right)^{2}}{{\frac {1}{n}}\sum (x)}}\\H_{3}&=\exp \left(m-{\frac {1}{2}}s^{2}\right)\end{aligned}}} where m = 1 n ∑ ∑ log e ⁡ ⁡ ( x ) {\displaystyle m={\frac {1}{n}}\sum \log _{e}(x)} s 2 = 1 n ∑ ∑ ( log e ⁡ ⁡ ( x ) − − m ) 2 {\displaystyle s^{2}={\frac {1}{n}}\sum \left(\log _{e}(x)-m\right)^{2}} Of these H 3 is probably the best estimator for samples of 25 or more.

[ 32 ] Bias and variance estimators A first order approximation to the bias and variance of H 1 are [ 33 ] bias ⁡ ⁡ [ H 1 ] = H C v n Var ⁡ ⁡ [ H 1 ] = H 2 C v n {\displaystyle {\begin{aligned}\operatorname {bias} \left[H_{1}\right]&={\frac {HC_{v}}{n}}\\\operatorname {Var} \left[H_{1}\right]&={\frac {H^{2}C_{v}}{n}}\end{aligned}}} where C v is the coefficient of variation.

Similarly a first order approximation to the bias and variance of H 3 are [ 33 ] H log e ⁡ ⁡ ( 1 + C v ) 2 n [ 1 + 1 + C v 2 2 ] H log e ⁡ ⁡ ( 1 + C v ) n [ 1 + 1 + C v 2 4 ] {\displaystyle {\begin{aligned}{\frac {H\log _{e}\left(1+C_{v}\right)}{2n}}\left[1+{\frac {1+C_{v}^{2}}{2}}\right]\\{\frac {H\log _{e}\left(1+C_{v}\right)}{n}}\left[1+{\frac {1+C_{v}^{2}}{4}}\right]\end{aligned}}} In numerical experiments H 3 is generally a superior estimator of the harmonic mean than H 1 .

[ 33 ] H 2 produces estimates that are largely similar to H 1 .

Notes The Environmental Protection Agency recommends the use of the harmonic mean in setting maximum toxin levels in water.

[ 34 ] In geophysical reservoir engineering studies, the harmonic mean is widely used.

[ 35 ] See also Mathematics portal Rate (mathematics) Contraharmonic mean Generalized mean Weighted mean Weighted geometric mean HM-GM-AM-QM inequalities Harmonic mean p-value Harmonic number Parallel (operator) , whose result is half of the harmonic mean Mediant (mathematics) , a fraction which lies between two fractions Notes ^ If NM = a and PM = b . AM = AM of a and b , and radius r = AQ = AG.

Using Pythagoras' theorem , QM² = AQ² + AM² ∴ QM = √ AQ² + AM² = QM .

Using Pythagoras' theorem, AM² = AG² + GM² ∴ GM = √ AM² − AG² = GM .

Using similar triangles , ⁠ HM / GM ⁠ = ⁠ GM / AM ⁠ ∴ HM = ⁠ GM² / AM ⁠ = HM .

References ^ Course Archived 2022-07-11 at the Wayback Machine ^ Srivastava, U. K.; Shenoy, G. V.; Sharma, S. C. (1989).

Quantitative Techniques for Managerial Decisions . New Age International. p. 63.

ISBN 978-81-224-0189-9 .

^ Jones, Alan (2018-10-09).

Probability, Statistics and Other Frightening Stuff . Routledge. p. 42.

ISBN 978-1-351-66138-6 .

^ a b c d Weisstein, Eric W.

"Harmonic Mean" .

mathworld.wolfram.com . Retrieved 2023-05-31 .

^ Da-Feng Xia, Sen-Lin Xu, and Feng Qi, "A proof of the arithmetic mean-geometric mean-harmonic mean inequalities", RGMIA Research Report Collection, vol. 2, no. 1, 1999, http://ajmaa.org/RGMIA/papers/v2n1/v2n1-10.pdf Archived 2015-12-22 at the Wayback Machine ^ * Statistical Analysis , Ya-lun Chou, Holt International, 1969, ISBN 0030730953 ^ Mitchell, Douglas W., "More on spreads and non-arithmetic means," The Mathematical Gazette 88, March 2004, 142–144.

^ Inequalities proposed in " Crux Mathematicorum " , "Archived copy" (PDF) .

Archived (PDF) from the original on 2014-10-15 . Retrieved 2014-09-09 .

{{ cite web }} :  CS1 maint: archived copy as title ( link ) .

^ Ferger F (1931) The nature and use of the harmonic mean. Journal of the
American Statistical Association 26(173) 36-40 ^ Deveci, Sinan (2022). "On a Double Series Representation of the Natural Logarithm, the Asymptotic Behavior of Hölder Means, and an Elementary Estimate for the Prime Counting Function". p. 2.

arXiv : 2211.10751 [ math.NT ].

^ "Average: How to calculate Average, Formula, Weighted average" .

learningpundits.com .

Archived from the original on 29 December 2017 . Retrieved 8 May 2018 .

^ "Effective mass in semiconductors" .

ecee.colorado.edu . Archived from the original on 20 October 2017 . Retrieved 8 May 2018 .

^ Hecht, Eugene (2002).

Optics (4th ed.).

Addison Wesley . p. 168.

ISBN 978-0805385663 .

^ "Fairness Opinions: Common Errors and Omissions".

The Handbook of Business Valuation and Intellectual Property Analysis . McGraw Hill. 2004.

ISBN 0-07-142967-0 .

^ Agrrawal, Pankaj; Borgman, Richard; Clark, John M.; Strong, Robert (2010). "Using the Price-to-Earnings Harmonic Mean to Improve Firm Valuation Estimates".

Journal of Financial Education .

36 ( 3– 4): 98– 110.

ISSN 0093-3961 .

JSTOR 41948650 .

SSRN 2621087 .

^ Posamentier, Alfred S.; Salkind, Charles T. (1996).

Challenging Problems in Geometry (Second ed.). Dover. p.

172 .

ISBN 0-486-69154-3 .

^ Voles, Roger, "Integer solutions of a − − 2 + b − − 2 = d − − 2 {\displaystyle a^{-2}+b^{-2}=d^{-2}} ," Mathematical Gazette 83, July 1999, 269–271.

^ Richinick, Jennifer, "The upside-down Pythagorean Theorem," Mathematical Gazette 92, July 2008, 313–;317.

^ Van Rijsbergen, C. J. (1979).

Information Retrieval (2nd ed.). Butterworth.

Archived from the original on 2005-04-06.

^ Aitchison J, Brown JAC (1969). The lognormal distribution with special reference to its uses in economics. Cambridge University Press, New York ^ Rossman LA (1990) Design stream flows based on harmonic means. J Hydr Eng ASCE 116(7) 946–950 ^ Johnson NL, Kotz S, Balakrishnan N (1994) Continuous univariate distributions Vol 1. Wiley Series in Probability and Statistics.

^ a b Zelen M (1972) Length-biased sampling and biomedical problems. In: Biometric Society Meeting, Dallas, Texas ^ Lam FC (1985) Estimate of variance for harmonic mean half lives. J Pharm Sci 74(2) 229-231 ^ Cox DR (1969) Some sampling problems in technology. In: New developments in survey sampling. U.L. Johnson, H Smith eds. New York: Wiley Interscience ^ Davidov O, Zelen M (2001) Referent sampling, family history and relative risk: the role of length-biased sampling. Biostat 2(2): 173-181 doi : 10.1093/biostatistics/2.2.173 ^ Zelen M, Feinleib M (1969) On the theory of screening for chronic diseases. Biometrika 56: 601-614 ^ Akman O, Gamage J, Jannot J, Juliano S, Thurman A, Whitman D (2007) A simple test for detection of length-biased sampling. J Biostats 1 (2) 189-195 ^ a b Chuen-Teck See, Chen J (2008) Convex functions of random variables. J Inequal Pure Appl Math 9 (3) Art 80 ^ Gurland J (1967) An inequality satisfied by the expectation of the reciprocal of a random variable. The American Statistician. 21 (2) 24 ^ Sung SH (2010) On inverse moments for a class of nonnegative random variables. J Inequal Applic doi : 10.1155/2010/823767 ^ Stedinger JR (1980) Fitting lognormal distributions to hydrologic data. Water Resour Res 16(3) 481–490 ^ a b c Limbrunner JF, Vogel RM, Brown LC (2000) Estimation of harmonic mean of a lognormal variable. J Hydrol Eng 5(1) 59-66 "Archived copy" (PDF) . Archived from the original (PDF) on 2010-06-11 . Retrieved 2012-09-16 .

{{ cite web }} :  CS1 maint: archived copy as title ( link ) ^ EPA (1991) Technical support document for water quality-based toxics control. EPA/505/2-90-001. Office of Water ^ Muskat M (1937) The flow of homogeneous fluids through porous media. McGraw-Hill, New York External links Weisstein, Eric W.

"Harmonic Mean" .

MathWorld .

Averages, Arithmetic and Harmonic Means at cut-the-knot v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Harmonic_mean&oldid=1294506202 " Category : Means Hidden categories: Webarchive template wayback links CS1 maint: archived copy as title Wikipedia semi-protected pages Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from May 2023 Articles needing additional references from December 2019 All articles needing additional references This page was last edited on 8 June 2025, at 03:40 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Harmonic mean 49 languages Add topic

