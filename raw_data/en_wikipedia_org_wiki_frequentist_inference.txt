Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History of frequentist statistics 2 Definition 3 Fisherian reduction and Neyman-Pearson operational criteria 4 Experimental design and methodology 5 The statistical philosophy of frequentism 6 Relationship with other approaches Toggle Relationship with other approaches subsection 6.1 Bayesian inference 7 See also 8 References 9 Bibliography Toggle the table of contents Frequentist inference 9 languages Català فارسی 한국어 Italiano Polski Português Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability Theory This article has multiple issues.

Please help improve it or discuss these issues on the talk page .

( Learn how and when to remove these messages ) This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Frequentist inference" – news · newspapers · books · scholar · JSTOR ( April 2021 ) ( Learn how and when to remove this message ) This article is written like a personal reflection, personal essay, or argumentative essay that states a Wikipedia editor's personal feelings or presents an original argument about a topic.

Please help improve it by rewriting it in an encyclopedic style .

( December 2024 ) ( Learn how and when to remove this message ) ( Learn how and when to remove this message ) Statistics Outline Statisticians Glossary Notation Journals Lists of topics Articles Category Mathematics portal v t e Frequentist inference is a type of statistical inference based in frequentist probability , which treats “probability” in equivalent terms to “frequency” and draws conclusions from sample-data by means of emphasizing the frequency or proportion of findings in the data. Frequentist inference underlies frequentist statistics , in which the well-established methodologies of statistical hypothesis testing and confidence intervals are founded.

History of frequentist statistics [ edit ] Frequentism is based on the presumption that statistics represent probabilistic frequencies. This view was primarily developed by Ronald Fisher and the team of Jerzy Neyman and Egon Pearson . Ronald Fisher contributed to frequentist statistics by developing the frequentist concept of "significance testing", which is the study of the significance of a measure of a statistic when compared to the hypothesis.

Neyman-Pearson extended Fisher's ideas to apply to multiple hypotheses. They posed that the ratio of probabilities of two given hypotheses, when maximizing the difference between them, leads to a maximization of exceeding a given p-value . This relationship serves as the basis of type I and type II errors and confidence intervals .

Definition [ edit ] For statistical inference, the statistic about which we want to make inferences is y ∈ ∈ Y {\displaystyle y\in Y} , where the random vector Y {\displaystyle Y} is a function of an unknown parameter, θ θ {\displaystyle \theta } .

The parameter θ θ {\displaystyle \theta } , in turn, is partitioned into ( ψ ψ , λ λ {\displaystyle \psi ,\lambda } ), where ψ ψ {\displaystyle \psi } is the parameter of interest , and λ λ {\displaystyle \lambda } is the nuisance parameter . For concreteness, ψ ψ {\displaystyle \psi } might be the population mean, μ μ {\displaystyle \mu } , and the nuisance parameter λ λ {\displaystyle \lambda } the standard deviation of the population mean, σ σ {\displaystyle \sigma } .

[ 1 ] Thus, statistical inference is concerned with the expectation of random vector Y {\displaystyle Y} , E ( Y ) = E ( Y ; θ θ ) = ∫ ∫ y f Y ( y ; θ θ ) d y {\displaystyle E(Y)=E(Y;\theta )=\int yf_{Y}(y;\theta )dy} .

To construct areas of uncertainty in frequentist inference, a pivot is used which defines the area around ψ ψ {\displaystyle \psi } that can be used to provide an interval to estimate uncertainty. The pivot is a probability such that for a pivot, p {\displaystyle p} , which is a function, that p ( t , ψ ψ ) {\displaystyle p(t,\psi )} is strictly increasing in ψ ψ {\displaystyle \psi } , where t ∈ ∈ T {\displaystyle t\in T} is a random vector.

This allows that, for some 0 < c {\displaystyle c} < 1, we can define P { p ( T , ψ ψ ) ≤ ≤ p c ∗ ∗ } {\displaystyle P\{p(T,\psi )\leq p_{c}^{*}\}} , which is the probability that the pivot function is less than some well-defined value. This implies P { ψ ψ ≤ ≤ q ( T , c ) } = 1 − − c {\displaystyle P\{\psi \leq q(T,c)\}=1-c} , where q ( t , c ) {\displaystyle q(t,c)} is a 1 − − c {\displaystyle 1-c} upper limit for ψ ψ {\displaystyle \psi } .

Note that 1 − − c {\displaystyle 1-c} is a range of outcomes that define a one-sided limit for ψ ψ {\displaystyle \psi } , and that 1 − − 2 c {\displaystyle 1-2c} is a two-sided limit for ψ ψ {\displaystyle \psi } , when we want to estimate a range of outcomes where ψ ψ {\displaystyle \psi } may occur. This rigorously defines the confidence interval , which is the range of outcomes about which we can make statistical inferences.

Fisherian reduction and Neyman-Pearson operational criteria [ edit ] Two complementary concepts in frequentist inference are the Fisherian reduction and the Neyman-Pearson operational criteria. Together these concepts illustrate a way of constructing frequentist intervals that define the limits for ψ ψ {\displaystyle \psi } . The Fisherian reduction is a method of determining the interval within which the true value of ψ ψ {\displaystyle \psi } may lie, while the Neyman-Pearson operational criteria is a decision rule about making a priori probability assumptions.

The Fisherian reduction is defined as follows: Determine the likelihood function (this is usually just gathering the data); Reduce to a sufficient statistic S {\displaystyle S} of the same dimension as θ θ {\displaystyle \theta } ; Find the function of S {\displaystyle S} that has a distribution depending only on ψ ψ {\displaystyle \psi } ; Invert that distribution (this yields a cumulative distribution function or CDF) to obtain limits for ψ ψ {\displaystyle \psi } at an arbitrary set of probability levels; Use the conditional distribution of the data given S = s {\displaystyle S=s} informally or formally as to assess the adequacy of the formulation.

[ 2 ] Essentially, the Fisherian reduction is design to find where the sufficient statistic can be used to determine the range of outcomes where ψ ψ {\displaystyle \psi } may occur on a probability distribution that defines all the potential values of ψ ψ {\displaystyle \psi } . This is necessary to formulating confidence intervals, where we can find a range of outcomes over which ψ ψ {\displaystyle \psi } is likely to occur in the long-run.

The Neyman-Pearon operational criteria is an even more specific understanding of the range of outcomes where the relevant statistic, ψ ψ {\displaystyle \psi } , can be said to occur in the long run. The Neyman-Pearson operational criteria defines the likelihood of that range actually being adequate or of the range being inadequate. The Neyman-Pearson criteria defines the range of the probability distribution that, if ψ ψ {\displaystyle \psi } exists in this range, is still below the true population statistic. For example, if the distribution from the Fisherian reduction exceeds a threshold that we consider to be a priori implausible, then the Neyman-Pearson reduction's evaluation of that distribution can be used to infer where looking purely at the Fisherian reduction's distributions can give us inaccurate results. Thus, the Neyman-Pearson reduction is used to find the probability of type I and type II errors .

[ 3 ] As a point of reference, the complement to this in Bayesian statistics is the minimum Bayes risk criterion .

Because of the reliance of the Neyman-Pearson criteria on our ability to find a range of outcomes where ψ ψ {\displaystyle \psi } is likely to occur, the Neyman-Pearson approach is only possible where a Fisherian reduction can be achieved.

[ 4 ] Experimental design and methodology [ edit ] Frequentist inferences are associated with the application frequentist probability to experimental design and interpretation, and specifically with the view that any given experiment can be considered one of an infinite sequence of possible repetitions of the same experiment, each capable of producing statistically independent results.

[ 5 ] In this view, the frequentist inference approach to drawing conclusions from data is effectively to require that the correct conclusion should be drawn with a given (high) probability, among this notional set of repetitions.

However, exactly the same procedures can be developed under a subtly different formulation. This is one where a pre-experiment point of view is taken. It can be argued that the design of an experiment should include, before undertaking the experiment, decisions about exactly what steps will be taken to reach a conclusion from the data yet to be obtained. These steps can be specified by the scientist so that there is a high probability of reaching a correct decision where, in this case, the probability relates to a yet to occur set of random events and hence does not rely on the frequency interpretation of probability. This formulation has been discussed by Neyman, [ 6 ] among others. This is especially pertinent because the significance of a frequentist test can vary under model selection, a violation of the likelihood principle.

The statistical philosophy of frequentism [ edit ] Frequentism is the study of probability with the assumption that results occur with a given frequency over some period of time or with repeated sampling. As such, frequentist analysis must be formulated with consideration to the assumptions of the problem frequentism attempts to analyze. This requires looking into whether the question at hand is concerned with understanding variety of a statistic or locating the true value of a statistic.

The difference between these assumptions is critical for interpreting a hypothesis test .

There are broadly two camps of statistical inference, the epistemic approach and the epidemiological approach . The epistemic approach is the study of variability ; namely, how often do we expect a statistic to deviate from some observed value. The epidemiological approach is concerned with the study of uncertainty ; in this approach, the value of the statistic is fixed but our understanding of that statistic is incomplete.

[ 7 ] For concreteness, imagine trying to measure the stock market quote versus evaluating an asset's price. The stock market fluctuates so greatly that trying to find exactly where a stock price is going to be is not useful: the stock market is better understood using the epistemic approach, where we can try to quantify its fickle movements. Conversely, the price of an asset might not change that much from day to day: it is better to locate the true value of the asset rather than find a range of prices and thus the epidemiological approach is better. The difference between these approaches is non-trivial for the purposes of inference.

For the epistemic approach, we formulate the problem as if we want to attribute probability to a hypothesis. This can only be done with Bayesian statistics, where the interpretation of probability is straightforward because Bayesian statistics is conditional on the entire sample space, whereas frequentist testing is concerned with the whole experimental design. Frequentist statistics is conditioned not on solely the data but also on the experimental design .

[ 8 ] In frequentist statistics, the cutoff for understanding the frequency occurrence is derived from the family distribution used in the experiment design. For example, a binomial distribution and a negative binomial distribution can be used to analyze exactly the same data, but because their tail ends are different the frequentist analysis will realize different levels of statistical significance for the same data that assumes different probability distributions. This difference does not occur in Bayesian inference. For more, see the likelihood principle , which frequentist statistics inherently violates.

[ 9 ] For the epidemiological approach, the central idea behind frequentist statistics must be discussed. Frequentist statistics is designed so that, in the long-run , the frequency of a statistic may be understood, and in the long-run the range of the true mean of a statistic can be inferred. This leads to the Fisherian reduction and the Neyman-Pearson operational criteria,  discussed above. When we define the Fisherian reduction and the Neyman-Pearson operational criteria for any statistic, we are assessing, according to these authors, the likelihood that the true value of the statistic will occur within a given range of outcomes assuming a number of repetitions of our sampling method.

[ 8 ] This allows for inference where, in the long-run, we can define that the combined results of multiple frequentist inferences to mean that a 95% confidence interval literally means the true mean lies in the confidence interval 95% of the time, but not that the mean is in a particular confidence interval with 95% certainty. This is a popular misconception.

Very commonly the epistemic view and the epidemiological view are incorrectly regarded as interconvertible. First, the epistemic view is centered around Fisherian significance tests that are designed to provide inductive evidence against the null hypothesis, H 0 {\displaystyle H_{0}} , in a single experiment, and is defined by the Fisherian p-value. Conversely, the epidemiological view, conducted with Neyman-Pearson hypothesis testing, is designed to minimize the Type II false acceptance errors in the long-run by providing error minimizations that work in the long-run. The difference between the two is critical because the epistemic view stresses the conditions under which we might find one value to be statistically significant; meanwhile, the epidemiological view defines the conditions under which long-run results present valid results. These are extremely different inferences, because one-time, epistemic conclusions do not inform long-run errors, and long-run errors cannot be used to certify whether one-time experiments are sensical. The assumption of one-time experiments to long-run occurrences is a misattribution, and the assumption of long run trends to individuals experiments is an example of the ecological fallacy.

[ 10 ] Relationship with other approaches [ edit ] Main article: Statistical inference § Paradigms for inference Further information: Probability interpretations Frequentist inferences stand in contrast to other types of statistical inferences, such as Bayesian inferences and fiducial inferences . While the " Bayesian inference " is sometimes held to include the approach to inferences leading to optimal decisions , a more restricted view is taken here for simplicity.

Bayesian inference [ edit ] Main article: Bayesian inference § In frequentist statistics and decision theory Bayesian inference is based in Bayesian probability , which treats “probability” as equivalent with “certainty”, and thus that the essential difference between the frequentist inference and the Bayesian inference is the same as the difference between the two interpretations of what a “probability” means. However, where appropriate, Bayesian inferences (meaning in this case an application of Bayes' theorem ) are used by those employing frequency probability .

There are two major differences in the frequentist and Bayesian approaches to inference that are not included in the above consideration of the interpretation of probability: In a frequentist approach to inference, unknown parameters are typically considered as being fixed, rather than as being random variates . In contrast, a Bayesian approach allows probabilities to be associated with unknown parameters, where these probabilities can sometimes have a frequency probability interpretation as well as a Bayesian one . The Bayesian approach allows these probabilities to have an interpretation as representing the scientist's belief that given values of the parameter are true (see Bayesian probability - Personal probabilities and objective methods for constructing priors ).

The result of a Bayesian approach can be a probability distribution for what is known about the parameters given the results of the experiment or study. The result of a frequentist approach is either a decision from a significance test or a confidence interval .

See also [ edit ] Intuitive statistics German tank problem References [ edit ] ^ Cox (2006) , pp. 1–2.

^ Cox (2006) , pp. 24, 47.

^ "OpenStax CNX" .

cnx.org . Retrieved 2021-09-14 .

^ Cox (2006) , p. 24.

^ Everitt (2002) .

^ Jerzy (1937) , pp. 236, 333–380.

^ Romeijn, Jan-Willem (2017), "Philosophy of Statistics" , in Zalta, Edward N. (ed.), The Stanford Encyclopedia of Philosophy (Spring 2017 ed.), Metaphysics Research Lab, Stanford University , retrieved 2021-09-14 ^ a b Wagenmakers et al. (2008) .

^ Vidakovic, Brani.

"The Likelihood Principle" (PDF) .

^ Hubbard, R.; Bayarri, M.J. (2003).

"Confusion over measures of evidence (p's) versus errors (α's) in classical statistical testing" (PDF) .

The American Statistician .

57 (3): 171– 182.

doi : 10.1198/0003130031856 .

Bibliography [ edit ] Cox, D. R. (2006-08-01).

Principles of Statistical Inference . Cambridge University Press.

ISBN 0521685672 .

Everitt, B.S. (2002).

The Cambridge Dictionary of Statistics .

Cambridge University Press .

ISBN 0-521-81099-X .

Jerzy, Neyman (1937).

"Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability" .

Philosophical Transactions of the Royal Society of London A .

236 (767): 236, 333– 380.

Bibcode : 1937RSPTA.236..333N .

doi : 10.1098/rsta.1937.0005 .

JSTOR 91337 .

Wagenmakers, Eric-Jan; Lee, Michael; Lodewyckx, Tom; Iverson, Geoffrey J. (2008), Hoijtink, Herbert; Klugkist, Irene; Boelen, Paul A. (eds.), "Bayesian Versus Frequentist Inference", Bayesian Evaluation of Informative Hypotheses , Statistics for Social and Behavioral Sciences, New York, NY: Springer, pp.

181– 207, doi : 10.1007/978-0-387-09612-4_9 , ISBN 978-0-387-09612-4 v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐pnrbp
Cached time: 20250812021415
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.500 seconds
Real time usage: 0.819 seconds
Preprocessor visited node count: 2632/1000000
Revision size: 18087/2097152 bytes
Post‐expand include size: 200056/2097152 bytes
Template argument size: 11127/2097152 bytes
Highest expansion depth: 17/100
Expensive parser function count: 7/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 56597/5000000 bytes
Lua time usage: 0.291/10.000 seconds
Lua memory usage: 8642221/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  477.753      1 -total
 20.02%   95.658      1 Template:Reflist
 17.11%   81.745      1 Template:Statistics
 16.58%   79.207      1 Template:Navbox_with_collapsible_groups
 16.48%   78.744      1 Template:Statistics_topics_sidebar
 16.20%   77.407      1 Template:Sidebar
 15.43%   73.726      1 Template:Multiple_issues
 14.32%   68.423      2 Template:Cite_web
 11.65%   55.659      1 Template:Short_description
 10.01%   47.801     11 Template:Navbox Saved in parser cache with key enwiki:pcache:15537745:|#|:idhash:canonical and timestamp 20250812021415 and revision id 1303184525. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Frequentist_inference&oldid=1303184525 " Category : Statistical inference Hidden categories: Articles with short description Short description is different from Wikidata Articles needing additional references from April 2021 All articles needing additional references Wikipedia articles with style issues from December 2024 All articles with style issues Articles with multiple maintenance issues This page was last edited on 29 July 2025, at 14:34 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Frequentist inference 9 languages Add topic

