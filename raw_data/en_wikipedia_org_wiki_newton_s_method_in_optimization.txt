Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Newton's method 2 Geometric interpretation 3 Higher dimensions 4 Convergence 5 Computing the Newton direction 6 Some caveats 7 See also 8 Notes 9 References 10 External links Toggle the table of contents Newton's method in optimization 9 languages Català Español Français 日本語 Norsk nynorsk Polski Português Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Method for finding stationary points of a function A comparison of gradient descent (green) and Newton's method (red) for minimizing a function (with small step sizes). Newton's method uses curvature information (i.e. the second derivative) to take a more direct route.

In calculus , Newton's method (also called Newton–Raphson ) is an iterative method for finding the roots of a differentiable function f {\displaystyle f} , which are solutions to the equation f ( x ) = 0 {\displaystyle f(x)=0} . However, to optimize a twice-differentiable f {\displaystyle f} , our goal is to find the roots of f ′ {\displaystyle f'} . We can therefore use Newton's method on its derivative f ′ {\displaystyle f'} to find solutions to f ′ ( x ) = 0 {\displaystyle f'(x)=0} , also known as the critical points of f {\displaystyle f} . These solutions may be minima, maxima, or saddle points; see section "Several variables" in Critical point (mathematics) and also section "Geometric interpretation" in this article. This is relevant in optimization , which aims to find (global) minima of the function f {\displaystyle f} .

Newton's method [ edit ] The central problem of optimization is minimization of functions. Let us first consider the case of univariate functions, i.e., functions of a single real variable. We will later consider the more general and more practically useful multivariate case.

Given a twice differentiable function f : R → → R {\displaystyle f:\mathbb {R} \to \mathbb {R} } , we seek to solve the optimization problem min x ∈ ∈ R f ( x ) .

{\displaystyle \min _{x\in \mathbb {R} }f(x).} Newton's method attempts to solve this problem by constructing a sequence { x k } {\displaystyle \{x_{k}\}} from an initial guess (starting point) x 0 ∈ ∈ R {\displaystyle x_{0}\in \mathbb {R} } that converges towards a minimizer x ∗ ∗ {\displaystyle x_{*}} of f {\displaystyle f} by using a sequence of second-order Taylor approximations of f {\displaystyle f} around the iterates. The second-order Taylor expansion of f around x k {\displaystyle x_{k}} is f ( x k + t ) ≈ ≈ f ( x k ) + f ′ ( x k ) t + 1 2 f ″ ( x k ) t 2 .

{\displaystyle f(x_{k}+t)\approx f(x_{k})+f'(x_{k})t+{\frac {1}{2}}f''(x_{k})t^{2}.} The next iterate x k + 1 {\displaystyle x_{k+1}} is defined so as to minimize this quadratic approximation in t {\displaystyle t} , and setting x k + 1 = x k + t {\displaystyle x_{k+1}=x_{k}+t} . If the second derivative is positive, the quadratic approximation is a convex function of t {\displaystyle t} , and its minimum can be found by setting the derivative to zero. Since 0 = d d t ( f ( x k ) + f ′ ( x k ) t + 1 2 f ″ ( x k ) t 2 ) = f ′ ( x k ) + f ″ ( x k ) t , {\displaystyle \displaystyle 0={\frac {\rm {d}}{{\rm {d}}t}}\left(f(x_{k})+f'(x_{k})t+{\frac {1}{2}}f''(x_{k})t^{2}\right)=f'(x_{k})+f''(x_{k})t,} the minimum is achieved for t = − − f ′ ( x k ) f ″ ( x k ) .

{\displaystyle t=-{\frac {f'(x_{k})}{f''(x_{k})}}.} Putting everything together, Newton's method performs the iteration x k + 1 = x k + t = x k − − f ′ ( x k ) f ″ ( x k ) .

{\displaystyle x_{k+1}=x_{k}+t=x_{k}-{\frac {f'(x_{k})}{f''(x_{k})}}.} Geometric interpretation [ edit ] The geometric interpretation of Newton's method is that at each iteration, it amounts to the fitting of a parabola to the graph of f ( x ) {\displaystyle f(x)} at the trial value x k {\displaystyle x_{k}} , having the same slope and curvature as the graph at that point, and then proceeding to the maximum or minimum of that parabola (in higher dimensions, this may also be a saddle point ), see below. Note that if f {\displaystyle f} happens to be a quadratic function, then the exact extremum is found in one step.

Higher dimensions [ edit ] The above iterative scheme can be generalized to d > 1 {\displaystyle d>1} dimensions by replacing the derivative with the gradient (different authors use different notation for the gradient, including f ′ ( x ) = ∇ ∇ f ( x ) = g f ( x ) ∈ ∈ R d {\displaystyle f'(x)=\nabla f(x)=g_{f}(x)\in \mathbb {R} ^{d}} ), and the reciprocal of the second derivative with the inverse of the Hessian matrix (different authors use different notation for the Hessian, including f ″ ( x ) = ∇ ∇ 2 f ( x ) = H f ( x ) ∈ ∈ R d × × d {\displaystyle f''(x)=\nabla ^{2}f(x)=H_{f}(x)\in \mathbb {R} ^{d\times d}} ).  One thus obtains the iterative scheme x k + 1 = x k − − [ f ″ ( x k ) ] − − 1 f ′ ( x k ) , k ≥ ≥ 0.

{\displaystyle x_{k+1}=x_{k}-[f''(x_{k})]^{-1}f'(x_{k}),\qquad k\geq 0.} Often Newton's method is modified to include a small step size 0 < γ γ ≤ ≤ 1 {\displaystyle 0<\gamma \leq 1} instead of γ γ = 1 {\displaystyle \gamma =1} : x k + 1 = x k − − γ γ [ f ″ ( x k ) ] − − 1 f ′ ( x k ) .

{\displaystyle x_{k+1}=x_{k}-\gamma [f''(x_{k})]^{-1}f'(x_{k}).} This is often done to ensure that the Wolfe conditions , or much simpler and efficient Armijo's condition , are satisfied at each step of the method. For step sizes other than 1, the method is often referred to as the relaxed or damped Newton's method.

Convergence [ edit ] If f is a strongly convex function with Lipschitz Hessian, then provided that x 0 {\displaystyle x_{0}} is close enough to x ∗ ∗ = arg ⁡ ⁡ min f ( x ) {\displaystyle x_{*}=\arg \min f(x)} , the sequence x 0 , x 1 , x 2 , … … {\displaystyle x_{0},x_{1},x_{2},\dots } generated by Newton's method will converge to the (necessarily unique)  minimizer x ∗ ∗ {\displaystyle x_{*}} of f {\displaystyle f} quadratically fast.

[ 1 ] That is, ‖ ‖ x k + 1 − − x ∗ ∗ ‖ ‖ ≤ ≤ 1 2 ‖ ‖ x k − − x ∗ ∗ ‖ ‖ 2 , ∀ ∀ k ≥ ≥ 0.

{\displaystyle \|x_{k+1}-x_{*}\|\leq {\frac {1}{2}}\|x_{k}-x_{*}\|^{2},\qquad \forall k\geq 0.} Computing the Newton direction [ edit ] Finding the inverse of the Hessian in high dimensions to compute the Newton direction h = − − ( f ″ ( x k ) ) − − 1 f ′ ( x k ) {\displaystyle h=-(f''(x_{k}))^{-1}f'(x_{k})} can be an expensive operation. In such cases, instead of directly inverting the Hessian, it is better to calculate the vector h {\displaystyle h} as the solution to the system of linear equations [ f ″ ( x k ) ] h = − − f ′ ( x k ) {\displaystyle [f''(x_{k})]h=-f'(x_{k})} which may be solved by various factorizations or approximately (but to great accuracy) using iterative methods . Many of these methods are only applicable to certain types of equations, for example the Cholesky factorization and conjugate gradient will only work if f ″ ( x k ) {\displaystyle f''(x_{k})} is a positive definite matrix. While this may seem like a limitation, it is often a useful indicator of something gone wrong; for example if a minimization problem is being approached and f ″ ( x k ) {\displaystyle f''(x_{k})} is not positive definite, then the iterations are converging to a saddle point and not a minimum.

On the other hand, if a constrained optimization is done (for example, with Lagrange multipliers ), the problem may become one of saddle point finding, in which case the Hessian will be symmetric indefinite and the solution of x k + 1 {\displaystyle x_{k+1}} will need to be done with a method that will work for such, such as the L D L ⊤ ⊤ {\displaystyle LDL^{\top }} variant of Cholesky factorization or the conjugate residual method .

There also exist various quasi-Newton methods , where an approximation for the Hessian (or its inverse directly) is built up from changes in the gradient.

If the Hessian is close to a non- invertible matrix , the inverted Hessian can be numerically unstable and the solution may diverge. In this case, certain workarounds have been tried in the past, which have varied success with certain problems. One can,  for example, modify the Hessian by adding a correction matrix B k {\displaystyle B_{k}} so as to make f ″ ( x k ) + B k {\displaystyle f''(x_{k})+B_{k}} positive definite. One approach is to diagonalize the Hessian and choose B k {\displaystyle B_{k}} so that f ″ ( x k ) + B k {\displaystyle f''(x_{k})+B_{k}} has the same eigenvectors as the Hessian, but with each negative eigenvalue replaced by ϵ ϵ > 0 {\displaystyle \epsilon >0} .

An approach exploited in the Levenberg–Marquardt algorithm (which uses an approximate Hessian) is to add a scaled identity matrix to the Hessian, μ μ I {\displaystyle \mu I} , with the scale adjusted at every iteration as needed. For large μ μ {\displaystyle \mu } and small Hessian, the iterations will behave like gradient descent with step size 1 / μ μ {\displaystyle 1/\mu } . This results in slower but more reliable convergence where the Hessian doesn't provide useful information.

Some caveats [ edit ] Newton's method, in its original version, has several caveats: It does not work if the Hessian is not invertible. This is clear from the very definition of Newton's method, which requires taking the inverse of the Hessian.

It may not converge at all, but can enter a cycle having more than 1 point. See the Newton's method § Failure of the method to converge to the root .

It can converge to a saddle point instead of to a local minimum, see the section " Geometric interpretation " in this article.

The popular modifications of Newton's method, such as quasi-Newton methods or Levenberg-Marquardt algorithm mentioned above, also have caveats: For example, it is usually required that the cost function is (strongly) convex and the Hessian is globally bounded or Lipschitz continuous, for example this is mentioned in the section "Convergence" in this article. If one looks at the papers by Levenberg and Marquardt in the reference for Levenberg–Marquardt algorithm , which are the original sources for the mentioned method, one can see that there is basically no theoretical analysis in the paper by Levenberg, while the paper by Marquardt only analyses a local situation and does not prove a global convergence result.  One can compare with Backtracking line search method for Gradient descent, which has good theoretical guarantee under more general assumptions, and can be implemented and works well in practical large scale problems such as Deep Neural Networks.

See also [ edit ] Quasi-Newton method Gradient descent Gauss–Newton algorithm Levenberg–Marquardt algorithm Trust region Optimization Nelder–Mead method Self-concordant function - a function for which Newton's method has very good global convergence rate.

[ 2 ] : Sec.6.2 Notes [ edit ] ^ Nocedal, Jorge; Wright, Stephen J. (2006).

Numerical optimization (2nd ed.). New York: Springer. p. 44.

ISBN 0387303030 .

^ Nemirovsky and Ben-Tal (2023).

"Optimization III: Convex Optimization" (PDF) .

References [ edit ] Avriel, Mordecai (2003).

Nonlinear Programming: Analysis and Methods . Dover Publishing.

ISBN 0-486-43227-0 .

Bonnans, J. Frédéric; Gilbert, J. Charles; Lemaréchal, Claude ; Sagastizábal, Claudia A.

(2006).

Numerical optimization: Theoretical and practical aspects . Universitext (Second revised ed. of translation of 1997 French ed.). Berlin: Springer-Verlag.

doi : 10.1007/978-3-540-35447-5 .

ISBN 3-540-35445-X .

MR 2265882 .

Fletcher, Roger (1987).

Practical Methods of Optimization (2nd ed.). New York: John Wiley & Sons .

ISBN 978-0-471-91547-8 .

Givens, Geof H.; Hoeting, Jennifer A. (2013).

Computational Statistics . Hoboken, New Jersey: John Wiley & Sons. pp.

24– 58.

ISBN 978-0-470-53331-4 .

Nocedal, Jorge; Wright, Stephen J. (1999).

Numerical Optimization . Springer-Verlag.

ISBN 0-387-98793-2 .

Kovalev, Dmitry; Mishchenko, Konstantin; Richtárik, Peter (2019). "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates".

arXiv : 1912.01597 [ cs.LG ].

External links [ edit ] Korenblum, Daniel (Aug 29, 2015).

"Newton-Raphson visualization (1D)" .

Bl.ocks . ffe9653768cb80dfc0da.

v t e Sir Isaac Newton Publications Fluxions (1671) De Motu (1684) Principia (1687) Opticks (1704) Queries (1704) Arithmetica (1707) De Analysi (1711) Other writings Quaestiones (1661–1665) " standing on the shoulders of giants " (1675) Notes on the Jewish Temple (c. 1680) " General Scholium " (1713; " hypotheses non fingo " ) Ancient Kingdoms Amended (1728) Corruptions of Scripture (1754) Contributions Calculus fluxion Impact depth Inertia Newton disc Newton polygon Newton–Okounkov body Newton's reflector Newtonian telescope Newton scale Newton's metal Spectrum Structural coloration Newtonianism Bucket argument Newton's inequalities Newton's law of cooling Newton's law of universal gravitation post-Newtonian expansion parameterized gravitational constant Newton–Cartan theory Schrödinger–Newton equation Newton's laws of motion Kepler's laws Newtonian dynamics Newton's method in optimization Apollonius's problem truncated Newton method Gauss–Newton algorithm Newton's rings Newton's theorem about ovals Newton–Pepys problem Newtonian potential Newtonian fluid Classical mechanics Corpuscular theory of light Leibniz–Newton calculus controversy Newton's notation Rotating spheres Newton's cannonball Newton–Cotes formulas Newton's method generalized Gauss–Newton method Newton fractal Newton's identities Newton polynomial Newton's theorem of revolving orbits Newton–Euler equations Newton number kissing number problem Newton's quotient Parallelogram of force Newton–Puiseux theorem Absolute space and time Luminiferous aether Newtonian series table Personal life Woolsthorpe Manor (birthplace) Cranbury Park (home) Early life Later life Apple tree Religious views Occult studies Scientific Revolution Copernican Revolution Relations Catherine Barton (niece) John Conduitt (nephew-in-law) Isaac Barrow (professor) William Clarke (mentor) Benjamin Pulleyn (tutor) Roger Cotes (student) William Whiston (student) John Keill (disciple) William Stukeley (friend) William Jones (friend) Abraham de Moivre (friend) Depictions Newton by Blake (monotype) Newton by Paolozzi (sculpture) Isaac Newton Gargoyle Astronomers Monument Namesake Newton (unit) Newton's cradle Isaac Newton Institute Isaac Newton Medal Isaac Newton Telescope Isaac Newton Group of Telescopes XMM-Newton Sir Isaac Newton Sixth Form Statal Institute of Higher Education Isaac Newton Newton International Fellowship Categories Isaac Newton v t e Optimization : Algorithms , methods , and heuristics Unconstrained nonlinear Functions Golden-section search Powell's method Line search Nelder–Mead method Successive parabolic interpolation Gradients Convergence Trust region Wolfe conditions Quasi–Newton Berndt–Hall–Hall–Hausman Broyden–Fletcher–Goldfarb–Shanno and L-BFGS Davidon–Fletcher–Powell Symmetric rank-one (SR1) Other methods Conjugate gradient Gauss–Newton Gradient Mirror Levenberg–Marquardt Powell's dog leg method Truncated Newton Hessians Newton's method Optimization computes maxima and minima.

Constrained nonlinear General Barrier methods Penalty methods Differentiable Augmented Lagrangian methods Sequential quadratic programming Successive linear programming Convex optimization Convex minimization Cutting-plane method Reduced gradient (Frank–Wolfe) Subgradient method Linear and quadratic Interior point Affine scaling Ellipsoid algorithm of Khachiyan Projective algorithm of Karmarkar Basis- exchange Simplex algorithm of Dantzig Revised simplex algorithm Criss-cross algorithm Principal pivoting algorithm of Lemke Active-set method Combinatorial Paradigms Approximation algorithm Dynamic programming Greedy algorithm Integer programming Branch and bound / cut Graph algorithms Minimum spanning tree Borůvka Prim Kruskal Shortest path Bellman–Ford SPFA Dijkstra Floyd–Warshall Network flows Dinic Edmonds–Karp Ford–Fulkerson Push–relabel maximum flow Metaheuristics Evolutionary algorithm Hill climbing Local search Parallel metaheuristics Simulated annealing Spiral optimization algorithm Tabu search Software Retrieved from " https://en.wikipedia.org/w/index.php?title=Newton%27s_method_in_optimization&oldid=1296498583 " Category : Optimization algorithms and methods Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 20 June 2025, at 10:11 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Newton's method in optimization 9 languages Add topic

