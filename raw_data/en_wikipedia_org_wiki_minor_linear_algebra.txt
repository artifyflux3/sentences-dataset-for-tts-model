Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition and illustration Toggle Definition and illustration subsection 1.1 First minors 1.2 General definition 1.3 Complement 2 Applications of minors and cofactors Toggle Applications of minors and cofactors subsection 2.1 Cofactor expansion of the determinant 2.2 Inverse of a matrix 2.3 Other applications 3 Multilinear algebra approach 4 A remark about different notation 5 See also 6 References 7 External links Toggle the table of contents Minor (linear algebra) 31 languages العربية Azərbaycanca Беларуская Català Čeština Deutsch Ελληνικά Español Euskara فارسی Français Galego 한국어 Հայերեն Bahasa Indonesia Italiano עברית Қазақша Latviešu Nederlands 日本語 Polski Português Română Русский Slovenščina Svenska தமிழ் Українська Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Determinant of a subsection of a square matrix This article is about a concept in linear algebra. For the concept of "minor" in graph theory, see Graph minor .

In linear algebra , a minor of a matrix A is the determinant of some smaller square matrix generated from A by removing one or more of its rows and columns. Minors obtained by removing just one row and one column from square matrices ( first minors ) are required for calculating matrix cofactors , which are useful for computing both the determinant and inverse of square matrices.  The requirement that the square matrix be smaller than the original matrix is often omitted in the definition.

Definition and illustration [ edit ] First minors [ edit ] If A is a square matrix, then the minor of the entry in the i -th row and j -th column (also called the ( i , j ) minor , or a first minor [ 1 ] ) is the determinant of the submatrix formed by deleting the i -th row and j -th column. This number is often denoted M i , j . The ( i , j ) cofactor is obtained by multiplying the minor by (−1) i + j .

To illustrate these definitions, consider the following 3 × 3 matrix, [ 1 4 7 3 0 5 − − 1 9 11 ] {\displaystyle {\begin{bmatrix}1&4&7\\3&0&5\\-1&9&11\\\end{bmatrix}}} To compute the minor M 2,3 and the cofactor C 2,3 , we find the determinant of the above matrix with row 2 and column 3 removed.

M 2 , 3 = det [ 1 4 ◻ ◻ ◻ ◻ ◻ ◻ ◻ ◻ − − 1 9 ◻ ◻ ] = det [ 1 4 − − 1 9 ] = 9 − − ( − − 4 ) = 13 {\displaystyle M_{2,3}=\det {\begin{bmatrix}1&4&\Box \\\Box &\Box &\Box \\-1&9&\Box \\\end{bmatrix}}=\det {\begin{bmatrix}1&4\\-1&9\\\end{bmatrix}}=9-(-4)=13} So the cofactor of the (2,3) entry is C 2 , 3 = ( − − 1 ) 2 + 3 ( M 2 , 3 ) = − − 13.

{\displaystyle C_{2,3}=(-1)^{2+3}(M_{2,3})=-13.} General definition [ edit ] Let A be an m × n matrix and k an integer with 0 < k ≤ m , and k ≤ n . A k × k minor of A , also called minor determinant of order k of A or, if m = n , the ( n − k ) th minor determinant of A (the word "determinant" is often omitted, and the word "degree" is sometimes used instead of "order") is the determinant of a k × k matrix obtained from A by deleting m − k rows and n − k columns. Sometimes the term is used to refer to the k × k matrix obtained from A as above (by deleting m − k rows and n − k columns), but this matrix should be referred to as a (square) submatrix of A , leaving the term "minor" to refer to the determinant of this matrix. For a matrix A as above, there are a total of ( m k ) ⋅ ⋅ ( n k ) {\textstyle {m \choose k}\cdot {n \choose k}} minors of size k × k . The minor of order zero is often defined to be 1. For a square matrix, the zeroth minor is just the determinant of the matrix.

[ 2 ] [ 3 ] Let I = 1 ≤ ≤ i 1 < i 2 < ⋯ ⋯ < i k ≤ ≤ m , J = 1 ≤ ≤ j 1 < j 2 < ⋯ ⋯ < j k ≤ ≤ n , {\displaystyle {\begin{aligned}I&=1\leq i_{1}<i_{2}<\cdots <i_{k}\leq m,\\[2pt]J&=1\leq j_{1}<j_{2}<\cdots <j_{k}\leq n,\end{aligned}}} be ordered sequences (in natural order, as it is always assumed when talking about minors unless otherwise stated) of indexes. The minor det ( ( A i p , j q ) p , q = 1 , … … , k ) {\textstyle \det {\bigl (}(\mathbf {A} _{i_{p},j_{q}})_{p,q=1,\ldots ,k}{\bigr )}} corresponding to these choices of indexes is denoted det I , J A {\displaystyle \det _{I,J}A} or det A I , J {\displaystyle \det \mathbf {A} _{I,J}} or [ A ] I , J {\displaystyle [\mathbf {A} ]_{I,J}} or M I , J {\displaystyle M_{I,J}} or M i 1 , i 2 , … … , i k , j 1 , j 2 , … … , j k {\displaystyle M_{i_{1},i_{2},\ldots ,i_{k},j_{1},j_{2},\ldots ,j_{k}}} or M ( i ) , ( j ) {\displaystyle M_{(i),(j)}} (where the ( i ) denotes the sequence of indexes I , etc.), depending on the source. Also, there are two types of denotations in use in literature: by the minor associated to ordered sequences of indexes I and J , some authors [ 4 ] mean the determinant of the matrix that is formed as above, by taking the elements of the original matrix from the rows whose indexes are in I and columns whose indexes are in J , whereas some other authors mean by a minor associated to I and J the determinant of the matrix formed from the original matrix by deleting the rows in I and columns in J ; [ 2 ] which notation is used should always be checked. In this article, we use the inclusive definition of choosing the elements from rows of I and columns of J . The exceptional case is the case of the first minor or the ( i , j ) -minor described above; in that case, the exclusive meaning M i , j = det ( ( A p , q ) p ≠ ≠ i , q ≠ ≠ j ) {\textstyle M_{i,j}=\det {\bigl (}\left(\mathbf {A} _{p,q}\right)_{p\neq i,q\neq j}{\bigr )}} is standard everywhere in the literature and is used in this article also.

Complement [ edit ] The complement B ijk ..., pqr ...

of a minor M ijk ..., pqr ...

of a square matrix, A , is formed by the determinant of the matrix A from which all the rows ( ijk...

) and columns ( pqr...

) associated with M ijk ..., pqr ...

have been removed.  The complement of the first minor of an element a ij is merely that element.

[ 5 ] Applications of minors and cofactors [ edit ] Cofactor expansion of the determinant [ edit ] Main article: Laplace expansion The cofactors feature prominently in Laplace's formula for the expansion of determinants, which is a method of computing larger determinants in terms of smaller ones. Given an n × n matrix A = ( a ij ) , the determinant of A , denoted det( A ) , can be written as the sum of the cofactors of any row or column of the matrix multiplied by the entries that generated them. In other words, defining C i j = ( − − 1 ) i + j M i j {\displaystyle C_{ij}=(-1)^{i+j}M_{ij}} then the cofactor expansion along the j -th column gives: det ( A ) = a 1 j C 1 j + a 2 j C 2 j + a 3 j C 3 j + ⋯ ⋯ + a n j C n j = ∑ ∑ i = 1 n a i j C i j = ∑ ∑ i = 1 n a i j ( − − 1 ) i + j M i j {\displaystyle {\begin{aligned}\det(\mathbf {A} )&=a_{1j}C_{1j}+a_{2j}C_{2j}+a_{3j}C_{3j}+\cdots +a_{nj}C_{nj}\\[2pt]&=\sum _{i=1}^{n}a_{ij}C_{ij}\\[2pt]&=\sum _{i=1}^{n}a_{ij}(-1)^{i+j}M_{ij}\end{aligned}}} The cofactor expansion along the i -th row gives: det ( A ) = a i 1 C i 1 + a i 2 C i 2 + a i 3 C i 3 + ⋯ ⋯ + a i n C i n = ∑ ∑ j = 1 n a i j C i j = ∑ ∑ j = 1 n a i j ( − − 1 ) i + j M i j {\displaystyle {\begin{aligned}\det(\mathbf {A} )&=a_{i1}C_{i1}+a_{i2}C_{i2}+a_{i3}C_{i3}+\cdots +a_{in}C_{in}\\[2pt]&=\sum _{j=1}^{n}a_{ij}C_{ij}\\[2pt]&=\sum _{j=1}^{n}a_{ij}(-1)^{i+j}M_{ij}\end{aligned}}} Inverse of a matrix [ edit ] Main article: Invertible matrix One can write down the inverse of an invertible matrix by computing its cofactors by using Cramer's rule , as follows. The matrix formed by all of the cofactors of a square matrix A is called the cofactor matrix (also called the matrix of cofactors or, sometimes, comatrix ): C = [ C 11 C 12 ⋯ ⋯ C 1 n C 21 C 22 ⋯ ⋯ C 2 n ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ C n 1 C n 2 ⋯ ⋯ C n n ] {\displaystyle \mathbf {C} ={\begin{bmatrix}C_{11}&C_{12}&\cdots &C_{1n}\\C_{21}&C_{22}&\cdots &C_{2n}\\\vdots &\vdots &\ddots &\vdots \\C_{n1}&C_{n2}&\cdots &C_{nn}\end{bmatrix}}} Then the inverse of A is the transpose of the cofactor matrix times the reciprocal of the determinant of A : A − − 1 = 1 det ⁡ ⁡ ( A ) C T .

{\displaystyle \mathbf {A} ^{-1}={\frac {1}{\operatorname {det} (\mathbf {A} )}}\mathbf {C} ^{\mathsf {T}}.} The transpose of the cofactor matrix is called the adjugate matrix (also called the classical adjoint ) of A .

The above formula can be generalized as follows: Let I = 1 ≤ ≤ i 1 < i 2 < … … < i k ≤ ≤ n , J = 1 ≤ ≤ j 1 < j 2 < … … < j k ≤ ≤ n , {\displaystyle {\begin{aligned}I&=1\leq i_{1}<i_{2}<\ldots <i_{k}\leq n,\\[2pt]J&=1\leq j_{1}<j_{2}<\ldots <j_{k}\leq n,\end{aligned}}} be ordered sequences (in natural order) of indexes (here A is an n × n matrix). Then [ 6 ] [ A − − 1 ] I , J = ± ± [ A ] J ′ , I ′ det A , {\displaystyle [\mathbf {A} ^{-1}]_{I,J}=\pm {\frac {[\mathbf {A} ]_{J',I'}}{\det \mathbf {A} }},} where I′ , J′ denote the ordered sequences of indices (the indices are in natural order of magnitude, as above) complementary to I , J , so that every index 1, ..., n appears exactly once in either I or I' , but not in both (similarly for the J and J' ) and [ A ] I , J denotes the determinant of the submatrix of A formed by choosing the rows of the index set I and columns of index set J . Also, [ A ] I , J = det ( ( A i p , j q ) p , q = 1 , … … , k ) .

{\displaystyle [\mathbf {A} ]_{I,J}=\det {\bigl (}(A_{i_{p},j_{q}})_{p,q=1,\ldots ,k}{\bigr )}.} A simple proof can be given using wedge product. Indeed, [ A − − 1 ] I , J ( e 1 ∧ ∧ … … ∧ ∧ e n ) = ± ± ( A − − 1 e j 1 ) ∧ ∧ … … ∧ ∧ ( A − − 1 e j k ) ∧ ∧ e i 1 ′ ∧ ∧ … … ∧ ∧ e i n − − k ′ , {\displaystyle {\bigl [}\mathbf {A} ^{-1}{\bigr ]}_{I,J}(e_{1}\wedge \ldots \wedge e_{n})=\pm (\mathbf {A} ^{-1}e_{j_{1}})\wedge \ldots \wedge (\mathbf {A} ^{-1}e_{j_{k}})\wedge e_{i'_{1}}\wedge \ldots \wedge e_{i'_{n-k}},} where e 1 , … … , e n {\displaystyle e_{1},\ldots ,e_{n}} are the basis vectors. Acting by A on both sides, one gets [ A − − 1 ] I , J det A ( e 1 ∧ ∧ … … ∧ ∧ e n ) = ± ± ( e j 1 ) ∧ ∧ … … ∧ ∧ ( e j k ) ∧ ∧ ( A e i 1 ′ ) ∧ ∧ … … ∧ ∧ ( A e i n − − k ′ ) = ± ± [ A ] J ′ , I ′ ( e 1 ∧ ∧ … … ∧ ∧ e n ) .

{\displaystyle {\begin{aligned}&\ {\bigl [}\mathbf {A} ^{-1}{\bigr ]}_{I,J}\det \mathbf {A} (e_{1}\wedge \ldots \wedge e_{n})\\[2pt]=&\ \pm (e_{j_{1}})\wedge \ldots \wedge (e_{j_{k}})\wedge (\mathbf {A} e_{i'_{1}})\wedge \ldots \wedge (\mathbf {A} e_{i'_{n-k}})\\[2pt]=&\ \pm [\mathbf {A} ]_{J',I'}(e_{1}\wedge \ldots \wedge e_{n}).\end{aligned}}} The sign can be worked out to be ( − − 1 ) ( ∑ ∑ s = 1 k i s − − ∑ ∑ s = 1 k j s ) , {\displaystyle (-1)^{\left(\sum _{s=1}^{k}i_{s}-\sum _{s=1}^{k}j_{s}\right)},} so the sign is determined by the sums of elements in I and J .

Other applications [ edit ] Given an m × n matrix with real entries (or entries from any other field ) and rank r , then there exists at least one non-zero r × r minor, while all larger minors are zero.

We will use the following notation for minors: if A is an m × n matrix, I is a subset of {1, ..., m } with k elements, and J is a subset of {1, ..., n } with k elements, then we write [ A ] I , J for the k × k minor of A that corresponds to the rows with index in I and the columns with index in J .

If I = J , then [ A ] I , J is called a principal minor .

If the matrix that corresponds to a principal minor is a square upper-left submatrix of the larger matrix (i.e., it consists of matrix elements in rows and columns from 1 to k , also known as a leading principal submatrix), then the principal minor is called a leading principal minor (of order k ) or corner (principal) minor (of order k ) .

[ 3 ] For an n × n square matrix, there are n leading principal minors.

A basic minor of a matrix is the determinant of a square submatrix that is of maximal size with nonzero determinant.

[ 3 ] For Hermitian matrices , the leading principal minors can be used to test for positive definiteness and the principal minors can be used to test for positive semidefiniteness . See Sylvester's criterion for more details.

Both the formula for ordinary matrix multiplication and the Cauchy–Binet formula for the determinant of the product of two matrices are special cases of the following general statement about the minors of a product of two matrices.
Suppose that A is an m × n matrix, B is an n × p matrix, I is a subset of {1, ..., m } with k elements and J is a subset of {1, ..., p } with k elements. Then [ A B ] I , J = ∑ ∑ K [ A ] I , K [ B ] K , J {\displaystyle [\mathbf {AB} ]_{I,J}=\sum _{K}[\mathbf {A} ]_{I,K}[\mathbf {B} ]_{K,J}\,} where the sum extends over all subsets K of {1, ..., n } with k elements. This formula is a straightforward extension of the Cauchy–Binet formula.

Multilinear algebra approach [ edit ] A more systematic, algebraic treatment of minors is given in multilinear algebra , using the wedge product : the k -minors of a matrix are the entries in the k -th exterior power map.

If the columns of a matrix are wedged together k at a time, the k × k minors appear as the components of the resulting k -vectors. For example, the 2 × 2 minors of the matrix ( 1 4 3 − − 1 2 1 ) {\displaystyle {\begin{pmatrix}1&4\\3&\!\!-1\\2&1\\\end{pmatrix}}} are −13 (from the first two rows), −7 (from the first and last row), and 5 (from the last two rows). Now consider the wedge product ( e 1 + 3 e 2 + 2 e 3 ) ∧ ∧ ( 4 e 1 − − e 2 + e 3 ) {\displaystyle (\mathbf {e} _{1}+3\mathbf {e} _{2}+2\mathbf {e} _{3})\wedge (4\mathbf {e} _{1}-\mathbf {e} _{2}+\mathbf {e} _{3})} where the two expressions correspond to the two columns of our matrix. Using the properties of the wedge product, namely that it is bilinear and alternating , e i ∧ ∧ e i = 0 , {\displaystyle \mathbf {e} _{i}\wedge \mathbf {e} _{i}=0,} and antisymmetric , e i ∧ ∧ e j = − − e j ∧ ∧ e i , {\displaystyle \mathbf {e} _{i}\wedge \mathbf {e} _{j}=-\mathbf {e} _{j}\wedge \mathbf {e} _{i},} we can simplify this expression to − − 13 e 1 ∧ ∧ e 2 − − 7 e 1 ∧ ∧ e 3 + 5 e 2 ∧ ∧ e 3 {\displaystyle -13\mathbf {e} _{1}\wedge \mathbf {e} _{2}-7\mathbf {e} _{1}\wedge \mathbf {e} _{3}+5\mathbf {e} _{2}\wedge \mathbf {e} _{3}} where the coefficients agree with the minors computed earlier.

A remark about different notation [ edit ] In some books, instead of cofactor the term adjunct is used.

[ 7 ] Moreover, it is denoted as A ij and defined in the same way as cofactor: A i j = ( − − 1 ) i + j M i j {\displaystyle \mathbf {A} _{ij}=(-1)^{i+j}\mathbf {M} _{ij}} Using this notation the inverse matrix is written this way: M − − 1 = 1 det ( M ) [ A 11 A 21 ⋯ ⋯ A n 1 A 12 A 22 ⋯ ⋯ A n 2 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ A 1 n A 2 n ⋯ ⋯ A n n ] {\displaystyle \mathbf {M} ^{-1}={\frac {1}{\det(M)}}{\begin{bmatrix}A_{11}&A_{21}&\cdots &A_{n1}\\A_{12}&A_{22}&\cdots &A_{n2}\\\vdots &\vdots &\ddots &\vdots \\A_{1n}&A_{2n}&\cdots &A_{nn}\end{bmatrix}}} Keep in mind that adjunct is not adjugate or adjoint . In modern terminology, the "adjoint" of a matrix most often refers to the corresponding adjoint operator .

See also [ edit ] Submatrix Compound matrix References [ edit ] ^ Burnside, William Snow & Panton, Arthur William (1886) Theory of Equations: with an Introduction to the Theory of Binary Algebraic Form .

^ a b Elementary Matrix Algebra (Third edition), Franz E. Hohn, The Macmillan Company, 1973, ISBN 978-0-02-355950-1 ^ a b c "Minor".

Encyclopedia of Mathematics .

^ Linear Algebra and Geometry, Igor R. Shafarevich,  Alexey O. Remizov, Springer-Verlag Berlin Heidelberg, 2013, ISBN 978-3-642-30993-9 ^ Bertha Jeffreys, Methods of Mathematical Physics , p.135, Cambridge University Press, 1999 ISBN 0-521-66402-0 .

^ Viktor Vasil_evich Prasolov (13 June 1994).

Problems and Theorems in Linear Algebra . American Mathematical Soc. pp. 15–.

ISBN 978-0-8218-0236-6 .

^ Felix Gantmacher , Theory of matrices (1st ed., original language is Russian), Moscow: State Publishing House of technical and theoretical literature, 1953, p.491, External links [ edit ] MIT Linear Algebra Lecture on Cofactors at Google Video, from MIT OpenCourseWare Cofactor Expansion at PlanetMath .

"Minor" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category NewPP limit report
Parsed by mw‐web.codfw.main‐648944c8d8‐b8qp6
Cached time: 20250813182717
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.340 seconds
Real time usage: 0.481 seconds
Preprocessor visited node count: 4670/1000000
Revision size: 16660/2097152 bytes
Post‐expand include size: 38282/2097152 bytes
Template argument size: 6360/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 30954/5000000 bytes
Lua time usage: 0.177/10.000 seconds
Lua memory usage: 4360132/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  328.078      1 -total
 25.91%   85.017      1 Template:Reflist
 21.31%   69.921      1 Template:Linear_algebra
 20.87%   68.473      1 Template:Navbox
 18.11%   59.424      1 Template:Short_description
 17.96%   58.927     74 Template:Math
 14.49%   47.544      3 Template:Isbn
 11.74%   38.506      2 Template:Pagetype
  7.92%   25.991      2 Template:Cite_book
  6.81%   22.332     78 Template:Main_other Saved in parser cache with key enwiki:pcache:262107:|#|:idhash:canonical and timestamp 20250813182717 and revision id 1305720413. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Minor_(linear_algebra)&oldid=1305720413 " Categories : Matrix theory Determinants Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 13 August 2025, at 18:26 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Minor (linear algebra) 31 languages Add topic

