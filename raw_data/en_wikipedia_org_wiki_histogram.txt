Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Etymology 2 Examples 3 Mathematical definitions Toggle Mathematical definitions subsection 3.1 Cumulative histogram 3.2 Number of bins and width 3.2.1 Square-root choice 3.2.2 Sturges's formula 3.2.3 Rice rule 3.2.4 Doane's formula 3.2.5 Scott's normal reference rule 3.2.6 Terrell–Scott rule 3.2.7 Freedman–Diaconis rule 3.2.8 Minimizing cross-validation estimated squared error 3.2.9 Shimazaki and Shinomoto's choice 3.2.10 Variable bin widths 3.2.11 Remark 4 Applications 5 See also 6 References 7 Further reading 8 External links Toggle the table of contents Histogram 50 languages العربية Azərbaycanca Български Català Čeština Dansk Deutsch Eesti Ελληνικά Español Euskara فارسی Français Galego 한국어 हिन्दी Bahasa Indonesia Italiano עברית Қазақша Кыргызча Latviešu Magyar Македонски Bahasa Melayu Nederlands 日本語 Norsk bokmål Norsk nynorsk ਪੰਜਾਬੀ Piemontèis Polski Português Русский Shqip Sicilianu Simple English Slovenčina Slovenščina Српски / srpski Sunda Suomi Svenska Tagalog தமிழ் Türkçe Українська ייִדיש 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In this histogram, height is divided into bins of equal width (5 feet) Graphical representation of the distribution of numerical data For the histogram used in digital image processing, see Image histogram and Color histogram .

A histogram is a visual representation of the distribution of quantitative data. To construct a histogram, the first step is to "bin" (or "bucket") the range of values— divide the entire range of values into a series of intervals—and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) are adjacent and are typically (but not required to be) of equal size.

[ 1 ] Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation : estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x -axis are all 1, then a histogram is identical to a relative frequency plot.

Histograms are sometimes confused with bar charts . In a histogram, each bin is for a different range of values, so altogether the histogram illustrates the distribution of values. But in a bar chart, each bar is for a different category of observations (e.g., each bar might be for a different population), so altogether the bar chart can be used to compare different categories. Some authors recommend that bar charts always have gaps between the bars to clarify that they are not histograms.

[ 2 ] [ 3 ] Etymology [ edit ] The term "histogram" was first introduced by Karl Pearson , the founder of mathematical statistics , in lectures delivered in 1892 at University College London . Pearson's term is sometimes incorrectly said to combine the Greek root γραμμα (gramma) = "figure" or "drawing" with the root ἱστορία (historia) = "inquiry" or "history". Alternatively the root ἱστίον (histion) is also proposed, meaning "web" or "tissue" (as in histology , the study of biological tissue). Both of these etymologies are incorrect, and in fact Pearson, who knew Ancient Greek well, derived the term from a different if homophonous Greek root, ἱστός = "something set upright", "mast", referring to the vertical bars in the graph. Pearson's new term was embedded in a series of other analogous neologisms , such as "stigmogram" and "radiogram".

[ 4 ] Pearson himself noted in 1895 that although the term "histogram" was new, the type of graph it designates was "a common form of graphical representation".

[ 5 ] In fact the technique of using a bar graph to represent statistical measurements was devised by the Scottish economist , William Playfair , in his Commercial and political atlas (1786).

[ 4 ] Examples [ edit ] This is the data for the histogram to the right, using 500 items: Bin/Interval Count/Frequency −3.5 to −2.51 9 −2.5 to −1.51 32 −1.5 to −0.51 109 −0.5 to 0.49 180 0.5 to 1.49 132 1.5 to 2.49 34 2.5 to 3.49 4 The words used to describe the patterns in a histogram are: "symmetric", "skewed left" or "right", "unimodal", "bimodal" or "multimodal".

Symmetric, unimodal Skewed right Skewed left Bimodal Multimodal Symmetric It is a good idea to plot the data using several different bin widths to learn more about it. Here is an example on tips given in a restaurant.

Tips using a $1 bin width, skewed right, unimodal Tips using a 10c bin width, still skewed right, multimodal with modes at $ and 50c amounts, indicates rounding, also some outliers The U.S. Census Bureau found that there were 124 million people who work outside of their homes.

[ 6 ] Using their data on the time occupied by travel to work, the table below shows the absolute number of people who responded with travel times "at least 30 but less than 35 minutes" is higher than the numbers for the categories above and below it. This is likely due to people rounding their reported journey time.

[ citation needed ] The problem of reporting values as somewhat arbitrarily rounded numbers is a common phenomenon when collecting data from people.

[ citation needed ] Histogram of travel time (to work), US 2000 census. Area under the curve equals the total number of cases. This diagram uses Q/width from the table.

Data by absolute numbers Interval Width Quantity Quantity/width 0 5 4180 836 5 5 13687 2737 10 5 18618 3723 15 5 19634 3926 20 5 17981 3596 25 5 7190 1438 30 5 16369 3273 35 5 3212 642 40 5 4122 824 45 15 9200 613 60 30 6461 215 90 60 3435 57 This histogram shows the number of cases per unit interval as the height of each block, so that the area of each block is equal to the number of people in the survey who fall into its category. The area under the curve represents the total number of cases (124 million). This type of histogram shows absolute numbers, with Q in thousands.

Histogram of travel time (to work), US 2000 census. Area under the curve equals 1. This diagram uses Q/total/width (crowding) from the table. The height of a block represents crowding which is defined as  - percentage per horizontal unit.

Data by proportion Interval Width Quantity (Q) Q/total/width 0 5 4180 0.0067 5 5 13687 0.0221 10 5 18618 0.0300 15 5 19634 0.0316 20 5 17981 0.0290 25 5 7190 0.0116 30 5 16369 0.0264 35 5 3212 0.0052 40 5 4122 0.0066 45 15 9200 0.0049 60 30 6461 0.0017 90 60 3435 0.0005 This histogram differs from the first only in the vertical scale.  The area of each block is the fraction of the total that each category represents, and the total area of all the bars is equal to 1 (the fraction meaning "all"). The curve displayed is a simple density estimate . This version shows proportions, and is also known as a unit area histogram.

In other words, a histogram represents a frequency distribution by means of rectangles whose widths represent class intervals and whose areas are proportional to the corresponding frequencies: the height of each is the average frequency density for the interval. The intervals are placed together in order to show that the data represented by the histogram, while exclusive, is also contiguous. (E.g., in a histogram it is possible to have two connecting intervals of 10.5–20.5 and 20.5–33.5, but not two connecting intervals of 10.5–20.5 and 22.5–32.5.  Empty intervals are represented as empty and not skipped.) [ 7 ] Mathematical definitions [ edit ] An ordinary and a cumulative histogram of the same data. The data shown is a random sample of 10,000 points from a normal distribution with a mean of 0 and a standard deviation of 1.

The data used to construct a histogram are generated via a function m i that counts the number of observations that fall into each of the disjoint categories (known as bins ). Thus, if we let n be the total number of observations and k be the total number of bins, the histogram data m i meet the following conditions: n = ∑ ∑ i = 1 k m i .

{\displaystyle n=\sum _{i=1}^{k}{m_{i}}.} A histogram can be thought of as a simplistic kernel density estimation , which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently.

An alternative to kernel density estimation is the average shifted histogram, [ 8 ] which is fast to compute and gives a smooth curve estimate of the density without using kernels.

Cumulative histogram [ edit ] A cumulative histogram: a mapping that counts the cumulative number of observations in all of the bins up to the specified bin. That is, the cumulative histogram M i of a histogram m j can be defined as: M i = ∑ ∑ j = 1 i m j .

{\displaystyle M_{i}=\sum _{j=1}^{i}{m_{j}}.} Number of bins and width [ edit ] There is no "best" number of bins, and different bin sizes can reveal different features of the data.  Grouping data is at least as old as Graunt 's work in the 17th century, but no systematic guidelines were given [ 9 ] until Sturges 's work in 1926.

[ 10 ] Using wider bins where the density of the underlying data points is low reduces noise due to sampling randomness; using narrower bins where the density is high (so the signal drowns the noise) gives greater precision to the density estimation.  Thus varying the bin-width within a histogram can be beneficial.  Nonetheless, equal-width bins are widely used.

Some theoreticians have attempted to determine an optimal number of bins, but these methods generally make strong assumptions about the shape of the distribution.  Depending on the actual data distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.

[ 11 ] The number of bins k can be assigned directly or can be calculated from a suggested bin width h as: Histogram data represented with different bin widths k = ⌈ max x − − min x h ⌉ .

{\displaystyle k=\left\lceil {\frac {\max x-\min x}{h}}\right\rceil .} The braces indicate the ceiling function .

Square-root choice [ edit ] k = ⌈ ⌈ n ⌉ ⌉ {\displaystyle k=\lceil {\sqrt {n}}\rceil \,} which takes the square root of the number of data  points  in  the  sample and rounds to the next integer . This rule is suggested by a number of elementary statistics textbooks [ 12 ] and widely implemented in many software packages.

[ 13 ] Sturges's formula [ edit ] Sturges's rule [ 10 ] is derived from a binomial distribution and implicitly assumes an approximately normal distribution.

k = ⌈ ⌈ log 2 ⁡ ⁡ n ⌉ ⌉ + 1 , {\displaystyle k=\lceil \log _{2}n\rceil +1,\,} Sturges's formula implicitly bases bin sizes on the range of the data, and can perform poorly if n < 30 , because the number of bins will be small—less than seven—and unlikely to show trends in the data well. On the other extreme, Sturges's formula may overestimate bin width for very large datasets, resulting in oversmoothed histograms.

[ 14 ] It may also perform poorly if the data are not normally distributed.

When compared to Scott's rule and the Terrell-Scott rule, two other widely accepted formulas for histogram bins, the output of Sturges's formula is closest when n ≈ 100 .

[ 14 ] Rice rule [ edit ] k = ⌈ ⌈ 2 n 3 ⌉ ⌉ {\displaystyle k=\lceil 2{\sqrt[{3}]{n}}\rceil } The Rice rule [ 15 ] is presented as a simple alternative to Sturges's rule.

Doane's formula [ edit ] Doane's formula [ 16 ] is a modification of Sturges's formula which attempts to improve its performance with non-normal data.

k = 1 + log 2 ⁡ ⁡ ( n ) + log 2 ⁡ ⁡ ( 1 + | g 1 | σ σ g 1 ) {\displaystyle k=1+\log _{2}(n)+\log _{2}\left(1+{\frac {|g_{1}|}{\sigma _{g_{1}}}}\right)} where g 1 {\displaystyle g_{1}} is the estimated 3rd-moment- skewness of the distribution and σ σ g 1 = 6 ( n − − 2 ) ( n + 1 ) ( n + 3 ) {\displaystyle \sigma _{g_{1}}={\sqrt {\frac {6(n-2)}{(n+1)(n+3)}}}} Scott's normal reference rule [ edit ] Bin width h {\displaystyle h} is given by h = 3.49 σ σ ^ ^ n 3 , {\displaystyle h={\frac {3.49{\hat {\sigma }}}{\sqrt[{3}]{n}}},} where σ σ ^ ^ {\displaystyle {\hat {\sigma }}} is the sample standard deviation .

Scott's normal reference rule [ 17 ] is optimal for random samples of normally distributed data, in the sense that it minimizes the integrated mean squared error of the density estimate.

[ 9 ] This is the default rule used in Microsoft Excel.

[ 18 ] Terrell–Scott rule [ edit ] k = 2 n 3 {\displaystyle k={\sqrt[{3}]{2n}}} The Terrell–Scott rule [ 14 ] [ 19 ] is not a normal reference rule. It gives the minimum number of bins required for an asymptotically optimal histogram, where optimality is measured by the integrated mean squared error. The bound is derived by finding the 'smoothest' possible density, which turns out to be 3 4 ( 1 − − x 2 ) {\displaystyle {\frac {3}{4}}(1-x^{2})} . Any other density will require more bins, hence the above estimate is also referred to as the 'oversmoothed' rule. The similarity of the formulas and the fact that Terrell and Scott were at Rice University when the proposed it suggests that this is also the origin of the Rice rule.

Freedman–Diaconis rule [ edit ] The Freedman–Diaconis rule gives bin width h {\displaystyle h} as: [ 20 ] [ 9 ] h = 2 IQR ⁡ ⁡ ( x ) n 3 , {\displaystyle h=2{\frac {\operatorname {IQR} (x)}{\sqrt[{3}]{n}}},} which is based on the interquartile range , denoted by IQR. It replaces 3.5σ of Scott's rule with 2 IQR, which is less sensitive than the standard deviation to outliers in data.

Minimizing cross-validation estimated squared error [ edit ] This approach of minimizing integrated mean squared error from Scott's rule can be generalized beyond normal distributions, by using leave-one out cross validation: [ 21 ] [ 22 ] a r g m i n h J ^ ^ ( h ) = a r g m i n h ( 2 ( n − − 1 ) h − − n + 1 n 2 ( n − − 1 ) h ∑ ∑ k N k 2 ) {\displaystyle {\underset {h}{\operatorname {arg\,min} }}{\hat {J}}(h)={\underset {h}{\operatorname {arg\,min} }}\left({\frac {2}{(n-1)h}}-{\frac {n+1}{n^{2}(n-1)h}}\sum _{k}N_{k}^{2}\right)} Here, N k {\displaystyle N_{k}} is the number of datapoints in the k th bin, and choosing the value of h that minimizes J will minimize integrated mean squared error.

Shimazaki and Shinomoto's choice [ edit ] The choice is based on minimization of an estimated L 2 risk function [ 23 ] a r g m i n h 2 m ¯ ¯ − − v h 2 {\displaystyle {\underset {h}{\operatorname {arg\,min} }}{\frac {2{\bar {m}}-v}{h^{2}}}} where m ¯ ¯ {\displaystyle \textstyle {\bar {m}}} and v {\displaystyle \textstyle v} are mean and biased variance of a histogram with bin-width h {\displaystyle \textstyle h} , m ¯ ¯ = 1 k ∑ ∑ i = 1 k m i {\displaystyle \textstyle {\bar {m}}={\frac {1}{k}}\sum _{i=1}^{k}m_{i}} and v = 1 k ∑ ∑ i = 1 k ( m i − − m ¯ ¯ ) 2 {\displaystyle \textstyle v={\frac {1}{k}}\sum _{i=1}^{k}(m_{i}-{\bar {m}})^{2}} .

Variable bin widths [ edit ] Rather than choosing evenly spaced bins, for some applications it is preferable to vary the bin width. This avoids bins with low counts. A common case is to choose equiprobable bins , where the number of samples in each bin is expected to be approximately equal. The bins may be chosen according to some known distribution or may be chosen based on the data so that each bin has ≈ ≈ n / k {\displaystyle \approx n/k} samples. When plotting the histogram, the frequency density is used for the dependent axis. While all bins have approximately equal area, the heights of the histogram approximate the density distribution.

For equiprobable bins, the following rule for the number of bins is suggested: [ 24 ] k = 2 n 2 / 5 {\displaystyle k=2n^{2/5}} This choice of bins is motivated by maximizing the power of a Pearson chi-squared test testing whether the bins do contain equal numbers of samples. More specifically, for a given confidence interval α α {\displaystyle \alpha } it is recommended to choose between 1/2 and 1 times the following equation: [ 25 ] k = 4 ( 2 n 2 Φ Φ − − 1 ( α α ) ) 1 5 {\displaystyle k=4\left({\frac {2n^{2}}{\Phi ^{-1}(\alpha )}}\right)^{\frac {1}{5}}} Where Φ Φ − − 1 {\displaystyle \Phi ^{-1}} is the probit function. Following this rule for α α = 0.05 {\displaystyle \alpha =0.05} would give between 1.88 n 2 / 5 {\displaystyle 1.88n^{2/5}} and 3.77 n 2 / 5 {\displaystyle 3.77n^{2/5}} ; the coefficient of 2 is chosen as an easy-to-remember value from this broad optimum.

Remark [ edit ] A good reason why the number of bins should be proportional to n 3 {\displaystyle {\sqrt[{3}]{n}}} is the following: suppose that the data are obtained as n {\displaystyle n} independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally "rugged" as n {\displaystyle n} tends to infinity. If s {\displaystyle s} is the "width" of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order n h / s {\displaystyle nh/s} and the relative standard error is of order s / ( n h ) {\displaystyle {\sqrt {s/(nh)}}} . Compared to the next bin, the relative change of the frequency is of order h / s {\displaystyle h/s} provided that the derivative of the density is non-zero. These two are of the same order if h {\displaystyle h} is of order s / n 3 {\displaystyle s/{\sqrt[{3}]{n}}} , so that k {\displaystyle k} is of order n 3 {\displaystyle {\sqrt[{3}]{n}}} . This simple cubic root choice can also be applied to bins with non-constant widths.

[ citation needed ] Histogram and density function for a Gumbel distribution [ 26 ] Applications [ edit ] In hydrology the histogram and estimated density function of rainfall and river discharge data, analysed with a probability distribution , are used to gain insight in their behaviour and frequency of occurrence.

[ 27 ] An example is shown in the blue figure.

In many Digital image processing programs there is an histogram tool, which show you the distribution of the contrast / brightness of the pixels .

histogram of contrast See also [ edit ] Wikimedia Commons has media related to Histograms .

Mathematics portal Data and information visualization Data binning Density estimation Kernel density estimation , a smoother but more complex method of density estimation Entropy estimation Freedman–Diaconis rule Image histogram Pareto chart Seven basic tools of quality V-optimal histograms References [ edit ] ^ Howitt, D.; Cramer, D. (2008).

Introduction to Statistics in Psychology (Fourth ed.). Prentice Hall.

ISBN 978-0-13-205161-3 .

^ Naomi, Robbins.

"A Histogram is NOT a Bar Chart" .

Forbes . Retrieved 31 July 2018 .

^ M. Eileen Magnello (December 2006).

"Karl Pearson and the Origins of Modern Statistics: An Elastician becomes a Statistician" .

The New Zealand Journal for the History and Philosophy of Science and Technology . 1 volume.

OCLC 682200824 .

^ a b Daniel Riaño Rufilanchas (2017), "On the origin of Karl Pearson’s term 'histogram'" , Estadística Española vol. 59, no. 192, p. 29-35.

^ Pearson, K.

(1895).

"Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material" .

Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences .

186 : 343– 414.

Bibcode : 1895RSPTA.186..343P .

doi : 10.1098/rsta.1895.0010 .

^ US 2000 census .

^ Dean, S., & Illowsky, B. (2009, February 19). Descriptive Statistics: Histogram. Retrieved from the Connexions Web site: http://cnx.org/content/m16298/1.11/ ^ David W. Scott (December 2009).

"Averaged shifted histogram" .

Wiley Interdisciplinary Reviews: Computational Statistics .

2 (2): 160– 164.

doi : 10.1002/wics.54 .

S2CID 122986682 .

^ a b c Scott, David W. (1992).

Multivariate Density Estimation: Theory, Practice, and Visualization . New York: John Wiley.

^ a b Sturges, H. A. (1926). "The choice of a class interval".

Journal of the American Statistical Association .

21 (153): 65– 66.

doi : 10.1080/01621459.1926.10502161 .

JSTOR 2965501 .

^ e.g.

§ 5.6 "Density Estimation", W. N. Venables and B. D. Ripley, Modern Applied Statistics with S (2002), Springer, 4th edition.

ISBN 0-387-95457-0 .

^ Lohaka, H.O. (2007).

"Making a grouped-data frequency table: development and examination of the iteration algorithm" . Doctoral dissertation, Ohio University. p. 87.

^ "MathWorks: Histogram" .

^ a b c Scott, David W. (2009). "Sturges' rule".

WIREs Computational Statistics .

1 (3): 303– 306.

doi : 10.1002/wics.35 .

S2CID 197483064 .

^ Online Statistics Education: A Multimedia Course of Study ( http://onlinestatbook.com/ ). Project Leader: David M. Lane, Rice University (chapter 2 "Graphing Distributions", section "Histograms") ^ Doane DP (1976) Aesthetic frequency classification. American Statistician, 30: 181–183 ^ Scott, David W. (1979). "On optimal and data-based histograms".

Biometrika .

66 (3): 605– 610.

doi : 10.1093/biomet/66.3.605 .

^ "Excel:Create a histogram" .

^ Terrell, G.R. and Scott, D.W., 1985. Oversmoothed nonparametric density estimates. Journal of the American Statistical Association, 80(389), pp.209-214.

^ Freedman, David; Diaconis, P. (1981).

"On the histogram as a density estimator: L 2 theory" (PDF) .

Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete .

57 (4): 453– 476.

CiteSeerX 10.1.1.650.2473 .

doi : 10.1007/BF01025868 .

S2CID 14437088 .

^ Wasserman, Larry (2004).

All of Statistics . New York: Springer. p. 310.

ISBN 978-1-4419-2322-6 .

^ Stone, Charles J. (1984).

"An asymptotically optimal histogram selection rule" (PDF) .

Proceedings of the Berkeley conference in honor of Jerzy Neyman and Jack Kiefer .

^ Shimazaki, H.; Shinomoto, S. (2007). "A method for selecting the bin size of a time histogram".

Neural Computation .

19 (6): 1503– 1527.

CiteSeerX 10.1.1.304.6404 .

doi : 10.1162/neco.2007.19.6.1503 .

PMID 17444758 .

S2CID 7781236 .

^ Jack Prins; Don McCormack; Di Michelson; Karen Horrell.

"Chi-square goodness-of-fit test" .

NIST/SEMATECH e-Handbook of Statistical Methods . NIST/SEMATECH. p. 7.2.1.1 . Retrieved 29 March 2019 .

^ Moore, David (1986). "3". In D'Agostino, Ralph; Stephens, Michael (eds.).

Goodness-of-Fit Techniques . New York, NY, US: Marcel Dekker Inc. p. 70.

ISBN 0-8247-7487-6 .

^ A calculator for probability distributions and density functions ^ An illustration of histograms and probability density functions Further reading [ edit ] Lancaster, H.O.

An Introduction to Medical Statistics.

John Wiley and Sons. 1974.

ISBN 0-471-51250-8 External links [ edit ] Wikimedia Commons has media related to Histogram .

Look up histogram in Wiktionary, the free dictionary.

Exploring Histograms , an essay by Aran Lunzer and Amelia McNamara Journey To Work and Place Of Work (location of census document cited in example) Smooth histogram for signals and images from a few samples Histograms: Construction, Analysis and Understanding with external links and an application to particle Physics.

A Method for Selecting the Bin Size of a Histogram Histograms: Theory and Practice , some great illustrations of some of the Bin Width concepts derived above.

Matlab function to plot nice histograms Dynamic Histogram in MS Excel Histogram construction and manipulation using Java applets, and charts on SOCR Toolbox for constructing the best histograms Archived 2017-10-24 at the Wayback Machine v t e Seven basic tools of quality Cause-and-effect diagram Check sheet Control chart Histogram Pareto chart Scatter diagram Stratification Quality (business) v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Histogram&oldid=1291484517 " Categories : Statistical charts and diagrams Quality control tools Estimation of densities Nonparametric statistics Frequency distribution Hidden categories: CS1: long volume value Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from August 2010 Articles with unsourced statements from June 2011 Articles with unsourced statements from May 2024 Commons category link is on Wikidata Commons link from Wikidata Webarchive template wayback links Statistics articles needing expert attention This page was last edited on 21 May 2025, at 14:47 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Histogram 50 languages Add topic

