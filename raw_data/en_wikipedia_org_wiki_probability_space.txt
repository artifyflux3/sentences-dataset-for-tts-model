Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Introduction 2 Definition 3 Discrete case 4 General case 5 Non-atomic case 6 Complete probability space 7 Examples Toggle Examples subsection 7.1 Discrete examples 7.1.1 Example 1 7.1.2 Example 2 7.1.3 Example 3 7.2 Non-atomic examples 7.2.1 Example 4 7.2.2 Example 5 8 Related concepts Toggle Related concepts subsection 8.1 Probability distribution 8.2 Random variables 8.3 Defining the events in terms of the sample space 8.4 Conditional probability 8.5 Independence 8.6 Mutual exclusivity 9 See also 10 References 11 Bibliography 12 External links Toggle the table of contents Probability space 38 languages Afrikaans العربية Asturianu Azərbaycanca Беларуская Български Català Чӑвашла Čeština Cymraeg Deutsch Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Íslenska Italiano עברית ქართული Magyar 日本語 Norsk bokmål Piemontèis Polski Português Русский Simple English Српски / srpski Svenska Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Mathematical concept This article is about the mathematical concept. For the novel, see Probability Space (novel) .

Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e In probability theory , a probability space or a probability triple ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},P)} is a mathematical construct that provides a formal model of a random process or "experiment". For example, one can define a probability space which models the throwing of a die .

A probability space consists of three elements: [ 1 ] [ 2 ] A sample space , Ω Ω {\displaystyle \Omega } , which is the set of all possible outcomes of a random process under consideration.

An event space , F {\displaystyle {\mathcal {F}}} , which is a set of events , where an event is a subset of outcomes in the sample space.

A probability function , P {\displaystyle P} , which assigns, to each event in the event space, a probability , which is a number between 0 and 1 (inclusive).

In order to provide a model of probability, these elements must satisfy probability axioms .

In the example of the throw of a standard die, The sample space Ω Ω {\displaystyle \Omega } is typically the set { 1 , 2 , 3 , 4 , 5 , 6 } {\displaystyle \{1,2,3,4,5,6\}} where each element in the set is a label which represents the outcome of the die landing on that label. For example, 1 {\displaystyle 1} represents the outcome that the die lands on 1.

The event space F {\displaystyle {\mathcal {F}}} could be the set of all subsets of the sample space, which would then contain simple events such as { 5 } {\displaystyle \{5\}} ("the die lands on 5"), as well as complex events such as { 2 , 4 , 6 } {\displaystyle \{2,4,6\}} ("the die lands on an even number").

The probability function P {\displaystyle P} would then map each event to the number of outcomes in that event divided by 6 – so for example, { 5 } {\displaystyle \{5\}} would be mapped to 1 / 6 {\displaystyle 1/6} , and { 2 , 4 , 6 } {\displaystyle \{2,4,6\}} would be mapped to 3 / 6 = 1 / 2 {\displaystyle 3/6=1/2} .

When an experiment is conducted, it results in exactly one outcome ω ω {\displaystyle \omega } from the sample space Ω Ω {\displaystyle \Omega } . All the events in the event space F {\displaystyle {\mathcal {F}}} that contain the selected outcome ω ω {\displaystyle \omega } are said to "have occurred". The probability function P {\displaystyle P} must be so defined that if the experiment were repeated arbitrarily many times, the number of occurrences of each event as a fraction of the total number of experiments, will most likely tend towards the probability assigned to that event.

The Soviet mathematician Andrey Kolmogorov introduced the notion of a probability space and the axioms of probability in the 1930s. In modern probability theory, there are alternative approaches for axiomatization, such as the algebra of random variables .

Introduction [ edit ] Probability space for throwing a die twice in succession: The sample space Ω Ω {\displaystyle \Omega } consists of all 36 possible outcomes; three different events (colored polygons) are shown, with their respective probabilities (assuming a discrete uniform distribution ).

A probability space is a mathematical triplet ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},P)} that presents a model for a particular class of real-world situations. As with other models, its author ultimately defines which elements Ω Ω {\displaystyle \Omega } , F {\displaystyle {\mathcal {F}}} , and P {\displaystyle P} will contain.

The sample space Ω Ω {\displaystyle \Omega } is the set of all possible outcomes. An outcome is the result of a single execution of the model. Outcomes may be states of nature, possibilities, experimental results and the like. Every instance of the real-world situation (or run of the experiment) must produce exactly one outcome. If outcomes of different runs of an experiment differ in any way that matters, they are distinct outcomes. Which differences matter depends on the kind of analysis we want to do. This leads to different choices of sample space.

The σ-algebra F {\displaystyle {\mathcal {F}}} is a collection of all the events we would like to consider. This collection may or may not include each of the elementary events. Here, an "event" is a set of zero or more outcomes; that is, a subset of the sample space. An event is considered to have "happened" during an experiment when the outcome of the latter is an element of the event. Since the same outcome may be a member of many events, it is possible for many events to have happened given a single outcome. For example, when the trial consists of throwing two dice, the set of all outcomes with a sum of 7 pips may constitute an event, whereas outcomes with an odd number of pips may constitute another event. If the outcome is the element of the elementary event of two pips on the first die and five on the second, then both of the events, "7 pips" and "odd number of pips", are said to have happened.

The probability measure P {\displaystyle P} is a set function returning an event's probability . A probability is a real number between zero (impossible events have probability zero, though probability-zero events are not necessarily impossible) and one (the event happens almost surely , with almost total certainty). Thus P {\displaystyle P} is a function P : F → → [ 0 , 1 ] .

{\displaystyle P:{\mathcal {F}}\to [0,1].} The probability measure function must satisfy two simple requirements: First, the probability of a countable union of mutually exclusive events must be equal to the countable sum of the probabilities of each of these events. For example, the probability of the union of the mutually exclusive events Head {\displaystyle {\text{Head}}} and Tail {\displaystyle {\text{Tail}}} in the random experiment of one coin toss, P ( Head ∪ ∪ Tail ) {\displaystyle P({\text{Head}}\cup {\text{Tail}})} , is the sum of probability for Head {\displaystyle {\text{Head}}} and the probability for Tail {\displaystyle {\text{Tail}}} , P ( Head ) + P ( Tail ) {\displaystyle P({\text{Head}})+P({\text{Tail}})} . Second, the probability of the sample space Ω Ω {\displaystyle \Omega } must be equal to 1 (which accounts for the fact that, given an execution of the model, some outcome must occur). In the previous example the probability of the set of outcomes P ( { Head , Tail } ) {\displaystyle P(\{{\text{Head}},{\text{Tail}}\})} must be equal to one, because it is entirely certain that the outcome will be either Head {\displaystyle {\text{Head}}} or Tail {\displaystyle {\text{Tail}}} (the model neglects any other possibility) in a single coin toss.

Not every subset of the sample space Ω Ω {\displaystyle \Omega } must necessarily be considered an event: some of the subsets are simply not of interest, others cannot be "measured" . This is not so obvious in a case like a coin toss. In a different example, one could consider javelin throw lengths, where the events typically are intervals like "between 60 and 65 meters" and unions of such intervals, but not sets like the "irrational numbers between 60 and 65 meters".

Definition [ edit ] In short, a probability space is a measure space such that the measure of the whole space is equal to one.

The expanded definition is the following: a probability space is a triple ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},P)} consisting of: the sample space Ω Ω {\displaystyle \Omega } – an arbitrary non-empty set , the σ-algebra F ⊆ ⊆ 2 Ω Ω {\displaystyle {\mathcal {F}}\subseteq 2^{\Omega }} (also called σ-field) – a set of subsets of Ω Ω {\displaystyle \Omega } , called events , such that: F {\displaystyle {\mathcal {F}}} contains the sample space: Ω Ω ∈ ∈ F {\displaystyle \Omega \in {\mathcal {F}}} , F {\displaystyle {\mathcal {F}}} is closed under complements : if A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} , then also ( Ω Ω ∖ ∖ A ) ∈ ∈ F {\displaystyle (\Omega \setminus A)\in {\mathcal {F}}} , F {\displaystyle {\mathcal {F}}} is closed under countable unions : if A i ∈ ∈ F {\displaystyle A_{i}\in {\mathcal {F}}} for i = 1 , 2 , … … {\displaystyle i=1,2,\dots } , then also ( ⋃ ⋃ i = 1 ∞ ∞ A i ) ∈ ∈ F {\textstyle (\bigcup _{i=1}^{\infty }A_{i})\in {\mathcal {F}}} The corollary from the previous two properties and De Morgan's law is that F {\displaystyle {\mathcal {F}}} is also closed under countable intersections : if A i ∈ ∈ F {\displaystyle A_{i}\in {\mathcal {F}}} for i = 1 , 2 , … … {\displaystyle i=1,2,\dots } , then also ( ⋂ ⋂ i = 1 ∞ ∞ A i ) ∈ ∈ F {\textstyle (\bigcap _{i=1}^{\infty }A_{i})\in {\mathcal {F}}} the probability measure P : F → → [ 0 , 1 ] {\displaystyle P:{\mathcal {F}}\to [0,1]} – a function on F {\displaystyle {\mathcal {F}}} such that: P is countably additive (also called σ-additive): if { A i } i = 1 ∞ ∞ ⊆ ⊆ F {\displaystyle \{A_{i}\}_{i=1}^{\infty }\subseteq {\mathcal {F}}} is a countable collection of pairwise disjoint sets , then P ( ⋃ ⋃ i = 1 ∞ ∞ A i ) = ∑ ∑ i = 1 ∞ ∞ P ( A i ) , {\textstyle P(\bigcup _{i=1}^{\infty }A_{i})=\sum _{i=1}^{\infty }P(A_{i}),} the measure of the entire sample space is equal to one: P ( Ω Ω ) = 1 {\displaystyle P(\Omega )=1} .

Discrete case [ edit ] Discrete probability theory needs only at most countable sample spaces Ω Ω {\displaystyle \Omega } . Probabilities can be ascribed to points of Ω Ω {\displaystyle \Omega } by the probability mass function p : Ω Ω → → [ 0 , 1 ] {\displaystyle p:\Omega \to [0,1]} such that ∑ ∑ ω ω ∈ ∈ Ω Ω p ( ω ω ) = 1 {\textstyle \sum _{\omega \in \Omega }p(\omega )=1} . All subsets of Ω Ω {\displaystyle \Omega } can be treated as events (thus, F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} is the power set ). The probability measure takes the simple form P ( A ) = ∑ ∑ ω ω ∈ ∈ A p ( ω ω ) for all A ⊆ ⊆ Ω Ω .

{\displaystyle P(A)=\sum _{\omega \in A}p(\omega )\quad {\text{for all }}A\subseteq \Omega .} ⁎ The greatest σ-algebra F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} describes the complete information. In general, a σ-algebra F ⊆ ⊆ 2 Ω Ω {\displaystyle {\mathcal {F}}\subseteq 2^{\Omega }} corresponds to a finite or countable partition Ω Ω = B 1 ∪ ∪ B 2 ∪ ∪ … … {\displaystyle \Omega =B_{1}\cup B_{2}\cup \dots } , the general form of an event A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} being A = B k 1 ∪ ∪ B k 2 ∪ ∪ … … {\displaystyle A=B_{k_{1}}\cup B_{k_{2}}\cup \dots } . See also the examples.

The case p ( ω ω ) = 0 {\displaystyle p(\omega )=0} is permitted by the definition, but rarely used, since such ω ω {\displaystyle \omega } can safely be excluded from the sample space.

General case [ edit ] If Ω is uncountable , still, it may happen that P ( ω ) ≠ 0 for some ω ; such ω are called atoms . They are an at most countable (maybe empty ) set, whose probability is the sum of probabilities of all atoms. If this sum is equal to 1 then all other points can safely be excluded from the sample space, returning us to the discrete case. Otherwise, if the sum of probabilities of all atoms is between 0 and 1, then the probability space decomposes into a discrete (atomic) part (maybe empty) and a non-atomic part.

Non-atomic case [ edit ] If P ( ω ) = 0 for all ω ∈ Ω (in this case, Ω must be uncountable, because otherwise P(Ω) = 1 could not be satisfied), then equation ( ⁎ ) fails: the probability of a set is not necessarily the sum over the probabilities of its elements, as summation is only defined for countable numbers of elements. This makes the probability space theory much more technical. A formulation stronger than summation, measure theory is applicable. Initially the probabilities are ascribed to some "generator" sets (see the examples). Then a limiting procedure allows assigning probabilities to sets that are limits of sequences of generator sets, or limits of limits, and so on. All these sets are the σ-algebra F {\displaystyle {\mathcal {F}}} . For technical details see Carathéodory's extension theorem . Sets belonging to F {\displaystyle {\mathcal {F}}} are called measurable . In general they are much more complicated than generator sets, but much better than non-measurable sets .

Complete probability space [ edit ] A probability space ( Ω Ω , F , P ) {\displaystyle (\Omega ,\;{\mathcal {F}},\;P)} is said to be a complete probability space if for all B ∈ ∈ F {\displaystyle B\in {\mathcal {F}}} with P ( B ) = 0 {\displaystyle P(B)=0} and all A ⊂ ⊂ B {\displaystyle A\;\subset \;B} one has A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} . Often, the study of probability spaces is restricted to complete probability spaces.

Examples [ edit ] Discrete examples [ edit ] Example 1 [ edit ] If the experiment consists of just one flip of a fair coin , then the outcome is either heads or tails: Ω Ω = { H , T } {\displaystyle \Omega =\{{\text{H}},{\text{T}}\}} . The σ-algebra F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} contains 2 2 = 4 {\displaystyle 2^{2}=4} events, namely: { H } {\displaystyle \{{\text{H}}\}} ("heads"), { T } {\displaystyle \{{\text{T}}\}} ("tails"), { } {\displaystyle \{\}} ("neither heads nor tails"), and { H , T } {\displaystyle \{{\text{H}},{\text{T}}\}} ("either heads or tails"); in other words, F = { { } , { H } , { T } , { H , T } } {\displaystyle {\mathcal {F}}=\{\{\},\{{\text{H}}\},\{{\text{T}}\},\{{\text{H}},{\text{T}}\}\}} . There is a fifty percent chance of tossing heads and fifty percent for tails, so the probability measure in this example is P ( { } ) = 0 {\displaystyle P(\{\})=0} , P ( { H } ) = 0.5 {\displaystyle P(\{{\text{H}}\})=0.5} , P ( { T } ) = 0.5 {\displaystyle P(\{{\text{T}}\})=0.5} , P ( { H , T } ) = 1 {\displaystyle P(\{{\text{H}},{\text{T}}\})=1} .

Example 2 [ edit ] The fair coin is tossed three times. There are 8 possible outcomes: Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT} (here "HTH" for example means that first time the coin landed heads, the second time tails, and the last time heads again). The complete information is described by the σ-algebra F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} of 2 8 = 256 events, where each of the events is a subset of Ω.

Alice knows the outcome of the second toss only. Thus her incomplete information is described by the partition Ω = A 1 ⊔ A 2 = {HHH, HHT, THH, THT} ⊔ {HTH, HTT, TTH, TTT} , where ⊔ is the disjoint union , and the corresponding σ-algebra F Alice = { { } , A 1 , A 2 , Ω Ω } {\displaystyle {\mathcal {F}}_{\text{Alice}}=\{\{\},A_{1},A_{2},\Omega \}} . Bryan knows only the total number of tails. His partition contains four parts: Ω = B 0 ⊔ B 1 ⊔ B 2 ⊔ B 3 = {HHH} ⊔ {HHT, HTH, THH} ⊔ {TTH, THT, HTT} ⊔ {TTT} ; accordingly, his σ-algebra F Bryan {\displaystyle {\mathcal {F}}_{\text{Bryan}}} contains 2 4 = 16 events.

The two σ-algebras are incomparable : neither F Alice ⊆ ⊆ F Bryan {\displaystyle {\mathcal {F}}_{\text{Alice}}\subseteq {\mathcal {F}}_{\text{Bryan}}} nor F Bryan ⊆ ⊆ F Alice {\displaystyle {\mathcal {F}}_{\text{Bryan}}\subseteq {\mathcal {F}}_{\text{Alice}}} ; both are sub-σ-algebras of 2 Ω .

Example 3 [ edit ] If 100 voters are to be drawn randomly from among all voters in California and asked whom they will vote for governor, then the set of all sequences of 100 Californian voters would be the sample space Ω. We assume that sampling without replacement is used: only sequences of 100 different voters are allowed. For simplicity an ordered sample is considered, that is a sequence (Alice, Bryan) is different from (Bryan, Alice). We also take for granted that each potential voter knows exactly his/her future choice, that is he/she does not choose randomly.

Alice knows only whether or not Arnold Schwarzenegger has received at least 60 votes. Her incomplete information is described by the σ-algebra F Alice {\displaystyle {\mathcal {F}}_{\text{Alice}}} that contains: (1) the set of all sequences in Ω where at least 60 people vote for Schwarzenegger; (2) the set of all sequences where fewer than 60 vote for Schwarzenegger; (3) the whole sample space Ω; and (4) the empty set ∅.

Bryan knows the exact number of voters who are going to vote for Schwarzenegger. His incomplete information is described by the corresponding partition Ω = B 0 ⊔ B 1 ⊔ ⋯ ⊔ B 100 and the σ-algebra F Bryan {\displaystyle {\mathcal {F}}_{\text{Bryan}}} consists of 2 101 events.

In this case, Alice's σ-algebra is a subset of Bryan's: F Alice ⊂ ⊂ F Bryan {\displaystyle {\mathcal {F}}_{\text{Alice}}\subset {\mathcal {F}}_{\text{Bryan}}} . Bryan's σ-algebra is in turn a subset of the much larger "complete information" σ-algebra 2 Ω consisting of 2 n ( n −1)⋯( n −99) events, where n is the number of all potential voters in California.

Non-atomic examples [ edit ] Example 4 [ edit ] A number between 0 and 1 is chosen at random, uniformly. Here Ω = [0,1], F {\displaystyle {\mathcal {F}}} is the σ-algebra of Borel sets on Ω, and P is the Lebesgue measure on [0,1].

In this case, the open intervals of the form ( a , b ) , where 0 < a < b < 1 , could be taken as the generator sets. Each such set can be ascribed the probability of P (( a , b )) = ( b − a ) , which generates the Lebesgue measure on [0,1], and the Borel σ-algebra on Ω.

Example 5 [ edit ] A fair coin is tossed endlessly. Here one can take Ω = {0,1} ∞ , the set of all infinite sequences of numbers 0 and 1.

Cylinder sets {( x 1 , x 2 , ...) ∈ Ω : x 1 = a 1 , ..., x n = a n } may be used as the generator sets. Each such set describes an event in which the first n tosses have resulted in a fixed sequence ( a 1 , ..., a n ) , and the rest of the sequence may be arbitrary. Each such event can be naturally given the probability of 2 − n .

These two non-atomic examples are closely related: a sequence ( x 1 , x 2 , ...) ∈ {0,1} ∞ leads to the number 2 −1 x 1 + 2 −2 x 2 + ⋯ ∈ [0,1] . This is not a one-to-one correspondence between {0,1} ∞ and [0,1] however: it is an isomorphism modulo zero , which allows for treating the two probability spaces as two forms of the same probability space. In fact, all non-pathological non-atomic probability spaces are the same in this sense. They are so-called standard probability spaces . Basic applications of probability spaces are insensitive to standardness. However, non-discrete conditioning is easy and natural on standard probability spaces, otherwise it becomes obscure.

Related concepts [ edit ] Probability distribution [ edit ] Main article: Probability distribution Random variables [ edit ] Main article: Random variable A random variable X is a measurable function X : Ω → S from the sample space Ω to another measurable space S called the state space .

If A ⊂ S , the notation Pr( X ∈ A ) is a commonly used shorthand for P ( { ω ω ∈ ∈ Ω Ω : X ( ω ω ) ∈ ∈ A } ) {\displaystyle P(\{\omega \in \Omega :X(\omega )\in A\})} .

Defining the events in terms of the sample space [ edit ] If Ω is countable , we almost always define F {\displaystyle {\mathcal {F}}} as the power set of Ω, i.e.

F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} which is trivially a σ-algebra and the biggest one we can create using Ω. We can therefore omit F {\displaystyle {\mathcal {F}}} and just write (Ω,P) to define the probability space.

On the other hand, if Ω is uncountable and we use F = 2 Ω Ω {\displaystyle {\mathcal {F}}=2^{\Omega }} we get into trouble defining our probability measure P because F {\displaystyle {\mathcal {F}}} is too "large", i.e. there will often be sets to which it will be impossible to assign a unique measure. In this case, we have to use a smaller σ-algebra F {\displaystyle {\mathcal {F}}} , for example the Borel algebra of  Ω, which is the smallest σ-algebra that makes all open sets measurable.

Conditional probability [ edit ] Main article: Conditional probability Kolmogorov's definition of probability spaces gives rise to the natural concept of conditional probability.  Every set A with non-zero probability (that is, P ( A ) > 0 ) defines another probability measure P ( B ∣ ∣ A ) = P ( B ∩ ∩ A ) P ( A ) {\displaystyle P(B\mid A)={P(B\cap A) \over P(A)}} on the space. This is usually pronounced as the "probability of B given A ".

For any event A such that P ( A ) > 0 , the function Q defined by Q ( B ) = P ( B | A ) for all events B is itself a probability measure.

Independence [ edit ] Main article: Statistical independence Two events, A and B are said to be independent if P ( A ∩ B ) = P ( A ) P ( B ) .

Two random variables, X and Y , are said to be independent if any event defined in terms of X is independent of any event defined in terms of Y . Formally, they generate independent σ-algebras, where two σ-algebras G and H , which are subsets of F are said to be independent if any element of G is independent of any element of H .

Mutual exclusivity [ edit ] Main article: Mutual exclusivity Two events, A and B are said to be mutually exclusive or disjoint if the occurrence of one implies the non-occurrence of the other, i.e., their intersection is empty. This is a stronger condition than the probability of their intersection being zero.

If A and B are disjoint events, then P ( A ∪ B ) = P ( A ) + P ( B ) . This extends to a (finite or countably infinite) sequence of events. However, the probability of the union of an uncountable set of events is not the sum of their probabilities. For example, if Z is a normally distributed random variable, then P ( Z = x ) is 0 for any x , but P ( Z ∈ R ) = 1 .

The event A ∩ B is referred to as " A and B ", and the event A ∪ B as " A or B ".

See also [ edit ] Space (mathematics) Measure space Fuzzy measure theory Filtered probability space Talagrand's concentration inequality References [ edit ] ^ Loève, Michel. Probability Theory, Vol 1. New York: D. Van Nostrand Company, 1955.

^ Stroock, D. W. (1999). Probability theory: an analytic view. Cambridge University Press.

Bibliography [ edit ] Pierre Simon de Laplace (1812) Analytical Theory of Probability The first major treatise blending calculus with probability theory, originally in French: Théorie Analytique des Probabilités .

Andrei Nikolajevich Kolmogorov (1950) Foundations of the Theory of Probability The modern measure-theoretic foundation of probability theory; the original German version ( Grundbegriffe der Wahrscheinlichkeitrechnung ) appeared in 1933.

Harold Jeffreys (1939) The Theory of Probability An empiricist, Bayesian approach to the foundations of probability theory.

Edward Nelson (1987) Radically Elementary Probability Theory Foundations of probability theory based on nonstandard analysis. Downloadable.

http://www.math.princeton.edu/~nelson/books.html Patrick Billingsley : Probability and Measure , John Wiley and Sons,  New York, Toronto, London, 1979.

Henk Tijms (2004) Understanding Probability A lively introduction to probability theory for the beginner, Cambridge Univ. Press.

David Williams (1991) Probability with martingales An undergraduate introduction to measure-theoretic probability, Cambridge Univ. Press.

Gut, Allan (2005).

Probability: A Graduate Course . Springer.

ISBN 0-387-22833-0 .

External links [ edit ] Sazonov, V.V. (2001) [1994], "Probability space" , Encyclopedia of Mathematics , EMS Press Animation demonstrating probability space of dice Virtual Laboratories in Probability and Statistics (principal author Kyle Siegrist), especially, Probability Spaces Citizendium Complete probability space Weisstein, Eric W.

"Probability space" .

MathWorld .

v t e Measure theory Basic concepts Absolute continuity of measures Lebesgue integration L p spaces Measure Measure space Probability space Measurable space / function Sets Almost everywhere Atom Baire set Borel set equivalence relation Borel space Carathéodory's criterion Cylindrical σ-algebra Cylinder set 𝜆-system Essential range infimum/supremum Locally measurable π -system σ-algebra Non-measurable set Vitali set Null set Support Transverse measure Universally measurable Types of measures Atomic Baire Banach Besov Borel Brown Complex Complete Content ( Logarithmically ) Convex Decomposable Discrete Equivalent Finite Inner ( Quasi- ) Invariant Locally finite Maximising Metric outer Outer Perfect Pre-measure ( Sub- ) Probability Projection-valued Radon Random Regular Borel regular Inner regular Outer regular Saturated Set function σ-finite s-finite Signed Singular Spectral Strictly positive Tight Vector Particular measures Counting Dirac Euler Gaussian Haar Harmonic Hausdorff Intensity Lebesgue Infinite-dimensional Logarithmic Product Projections Pushforward Spherical measure Tangent Trivial Young Maps Measurable function Bochner Strongly Weakly Convergence: almost everywhere of measures in measure of random variables in distribution in probability Cylinder set measure Random: compact set element measure process variable vector Projection-valued measure Main results Carathéodory's extension theorem Convergence theorems Dominated Monotone Vitali Decomposition theorems Hahn Jordan Maharam's Egorov's Fatou's lemma Fubini's Fubini–Tonelli Hölder's inequality Minkowski inequality Radon–Nikodym Riesz–Markov–Kakutani representation theorem Other results Disintegration theorem Lifting theory Lebesgue's density theorem Lebesgue differentiation theorem Sard's theorem Vitali–Hahn–Saks theorem For Lebesgue measure Isoperimetric inequality Brunn–Minkowski theorem Milman's reverse Minkowski–Steiner formula Prékopa–Leindler inequality Vitale's random Brunn–Minkowski inequality Applications & related Convex analysis Descriptive set theory Probability theory Real analysis Spectral theory Authority control databases National Germany United States France BnF data Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐p4698
Cached time: 20250812001710
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.617 seconds
Real time usage: 0.944 seconds
Preprocessor visited node count: 3721/1000000
Revision size: 24465/2097152 bytes
Post‐expand include size: 58145/2097152 bytes
Template argument size: 4961/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 7/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 40110/5000000 bytes
Lua time usage: 0.309/10.000 seconds
Lua memory usage: 5768495/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  542.633      1 -total
 17.06%   92.575      1 Template:Cite_book
 15.91%   86.349      1 Template:Probability_fundamentals
 15.52%   84.241      1 Template:Sidebar
 15.32%   83.137      1 Template:Short_description
  9.33%   50.653      2 Template:Pagetype
  8.26%   44.829      1 Template:Authority_control
  8.06%   43.763      2 Template:Navbox
  7.75%   42.063      1 Template:Measure_theory
  7.49%   40.642      1 Template:NumBlk Saved in parser cache with key enwiki:pcache:43325:|#|:idhash:canonical and timestamp 20250812001710 and revision id 1275268943. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Probability_space&oldid=1275268943 " Categories : Experiment (probability theory) Space (mathematics) Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 12 February 2025, at 00:56 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Probability space 38 languages Add topic

