Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Characterization Toggle Characterization subsection 1.1 Probability density function 1.2 Moments 1.3 Mode or modes 2 Multivariate generalization Toggle Multivariate generalization subsection 2.1 Probability density function 2.2 Use in statistical analysis 2.3 Relationship with the Dirichlet distribution 3 See also 4 References 5 Further reading 6 External links Toggle the table of contents Logit-normal distribution 4 languages Català فارسی Français Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Logit-normal Probability density function Cumulative distribution function Notation P ( N ( μ μ , σ σ 2 ) ) {\displaystyle P({\mathcal {N}}(\mu ,\,\sigma ^{2}))} Parameters σ 2 > 0 — squared scale (real), μ ∈ R — location Support x ∈ (0, 1) PDF 1 σ σ 2 π π e − − ( logit ⁡ ⁡ ( x ) − − μ μ ) 2 2 σ σ 2 1 x ( 1 − − x ) {\displaystyle {\frac {1}{\sigma {\sqrt {2\pi }}}}\,e^{-{\frac {(\operatorname {logit} (x)-\mu )^{2}}{2\sigma ^{2}}}}{\frac {1}{x(1-x)}}} CDF 1 2 [ 1 + erf ⁡ ⁡ ( logit ⁡ ⁡ ( x ) − − μ μ 2 σ σ 2 ) ] {\displaystyle {\frac {1}{2}}{\Big [}1+\operatorname {erf} {\Big (}{\frac {\operatorname {logit} (x)-\mu }{\sqrt {2\sigma ^{2}}}}{\Big )}{\Big ]}} Mean no analytical solution Median P ( μ μ ) {\displaystyle P(\mu )\,} Mode no analytical solution Variance no analytical solution MGF no analytical solution In probability theory , a logit-normal distribution is a probability distribution of a random variable whose logit has a normal distribution .  If Y is a random variable with a normal distribution, and t is the standard logistic function , then X = t ( Y ) has a logit-normal distribution; likewise, if X is logit-normally distributed, then Y = logit ( X )= log ( X /(1- X )) is normally distributed.  It is also known as the logistic normal distribution , [ 1 ] which often refers to a multinomial logit version (e.g.

[ 2 ] [ 3 ] [ 4 ] ).

A variable might be modeled as logit-normal if it is a proportion, which is bounded by zero and one, and where values of zero and one never occur.

Characterization [ edit ] Probability density function [ edit ] The probability density function (PDF) of a logit-normal distribution, for 0 < x < 1, is: f X ( x ; μ μ , σ σ ) = 1 σ σ 2 π π 1 x ( 1 − − x ) e − − ( logit ⁡ ⁡ ( x ) − − μ μ ) 2 2 σ σ 2 {\displaystyle f_{X}(x;\mu ,\sigma )={\frac {1}{\sigma {\sqrt {2\pi }}}}\,{\frac {1}{x(1-x)}}\,e^{-{\frac {(\operatorname {logit} (x)-\mu )^{2}}{2\sigma ^{2}}}}} where μ and σ are the mean and standard deviation of the variable’s logit (by definition, the variable’s logit is normally distributed).

The density obtained by changing the sign of μ is symmetrical, in that it is equal to f(1-x;- μ , σ ), shifting the mode to the other side of 0.5 (the midpoint of the (0,1) interval).

Plot of the Logitnormal PDF for various combinations of μ (facets) and σ (colors) Moments [ edit ] The moments of the logit-normal distribution have no analytic solution. The moments can  be estimated by numerical integration , however numerical integration can be prohibitive when the values of μ μ , σ σ 2 {\textstyle \mu ,\sigma ^{2}} are such that the density function diverges to infinity at the end points zero and one. An alternative is to use the observation that the logit-normal is a transformation of a normal random variable. This allows us to approximate the n {\displaystyle n} -th moment via the following quasi Monte Carlo estimate E [ X n ] ≈ ≈ 1 K − − 1 ∑ ∑ i = 1 K − − 1 ( P ( Φ Φ μ μ , σ σ 2 − − 1 ( i / K ) ) ) n , {\displaystyle E[X^{n}]\approx {\frac {1}{K-1}}\sum _{i=1}^{K-1}\left(P\left(\Phi _{\mu ,\sigma ^{2}}^{-1}(i/K)\right)\right)^{n},} where P {\textstyle P} is the standard logistic function, and Φ Φ μ μ , σ σ 2 − − 1 {\textstyle \Phi _{\mu ,\sigma ^{2}}^{-1}} is the inverse cumulative distribution function of a normal distribution with mean and variance μ μ , σ σ 2 {\textstyle \mu ,\sigma ^{2}} . When n = 1 {\displaystyle n=1} , this corresponds to the mean.

Mode or modes [ edit ] When the derivative of the density equals 0 then the location of the mode x satisfies the following equation: logit ⁡ ⁡ ( x ) = σ σ 2 ( 2 x − − 1 ) + μ μ .

{\displaystyle \operatorname {logit} (x)=\sigma ^{2}(2x-1)+\mu .} For some values of the parameters there are two solutions, i.e. the distribution is bimodal .

Multivariate generalization [ edit ] The logistic normal distribution is a generalization of the logit–normal distribution to D-dimensional probability vectors by taking a logistic transformation of a multivariate normal distribution.

[ 1 ] [ 5 ] [ 6 ] Probability density function [ edit ] The probability density function is: f X ( x ; μ μ , Σ Σ ) = 1 ( 2 π π ) D − − 1 | Σ Σ | 1 2 1 ∏ ∏ i = 1 D x i e − − 1 2 { log ⁡ ⁡ ( x − − D x D ) − − μ μ } ⊤ ⊤ Σ Σ − − 1 { log ⁡ ⁡ ( x − − D x D ) − − μ μ } , x ∈ ∈ S D , {\displaystyle f_{X}(\mathbf {x} ;{\boldsymbol {\mu }},{\boldsymbol {\Sigma }})={\frac {1}{(2\pi )^{D-1}|{\boldsymbol {\Sigma }}|^{\frac {1}{2}}}}\,{\frac {1}{\prod \limits _{i=1}^{D}x_{i}}}\,e^{-{\frac {1}{2}}\left\{\log \left({\frac {\mathbf {x} _{-D}}{x_{D}}}\right)-{\boldsymbol {\mu }}\right\}^{\top }{\boldsymbol {\Sigma }}^{-1}\left\{\log \left({\frac {\mathbf {x} _{-D}}{x_{D}}}\right)-{\boldsymbol {\mu }}\right\}}\quad ,\quad \mathbf {x} \in {\mathcal {S}}^{D}\;\;,} where x − − D {\displaystyle \mathbf {x} _{-D}} denotes a vector of the first (D-1) components of x {\displaystyle \mathbf {x} } and S D {\displaystyle {\mathcal {S}}^{D}} denotes the simplex of D-dimensional probability vectors. This follows from applying the additive logistic transformation to map a multivariate normal random variable y ∼ ∼ N ( μ μ , Σ Σ ) , y ∈ ∈ R D − − 1 {\displaystyle \mathbf {y} \sim {\mathcal {N}}\left({\boldsymbol {\mu }},{\boldsymbol {\Sigma }}\right)\;,\;\mathbf {y} \in \mathbb {R} ^{D-1}} to the simplex: x = [ e y 1 1 + ∑ ∑ i = 1 D − − 1 e y i , … … , e y D − − 1 1 + ∑ ∑ i = 1 D − − 1 e y i , 1 1 + ∑ ∑ i = 1 D − − 1 e y i ] ⊤ ⊤ {\displaystyle \mathbf {x} =\left[{\frac {e^{y_{1}}}{1+\sum _{i=1}^{D-1}e^{y_{i}}}},\dots ,{\frac {e^{y_{D-1}}}{1+\sum _{i=1}^{D-1}e^{y_{i}}}},{\frac {1}{1+\sum _{i=1}^{D-1}e^{y_{i}}}}\right]^{\top }} Gaussian density functions and corresponding logistic normal density functions after logistic transformation.

The unique inverse mapping is given by: y = [ log ⁡ ⁡ ( x 1 x D ) , … … , log ⁡ ⁡ ( x D − − 1 x D ) ] ⊤ ⊤ {\displaystyle \mathbf {y} =\left[\log \left({\frac {x_{1}}{x_{D}}}\right),\dots ,\log \left({\frac {x_{D-1}}{x_{D}}}\right)\right]^{\top }} .

This is the case of a vector x which components sum up to one. In the case of x with sigmoidal elements, that is, when y = [ log ⁡ ⁡ ( x 1 1 − − x 1 ) , … … , log ⁡ ⁡ ( x D 1 − − x D ) ] ⊤ ⊤ {\displaystyle \mathbf {y} =\left[\log \left({\frac {x_{1}}{1-x_{1}}}\right),\dots ,\log \left({\frac {x_{D}}{1-x_{D}}}\right)\right]^{\top }} we have f X ( x ; μ μ , Σ Σ ) = 1 ( 2 π π ) D − − 1 | Σ Σ | 1 2 1 ∏ ∏ i = 1 D ( x i ( 1 − − x i ) ) e − − 1 2 { log ⁡ ⁡ ( x 1 − − x ) − − μ μ } ⊤ ⊤ Σ Σ − − 1 { log ⁡ ⁡ ( x 1 − − x ) − − μ μ } {\displaystyle f_{X}(\mathbf {x} ;{\boldsymbol {\mu }},{\boldsymbol {\Sigma }})={\frac {1}{(2\pi )^{D-1}|{\boldsymbol {\Sigma }}|^{\frac {1}{2}}}}\,{\frac {1}{\prod \limits _{i=1}^{D}\left(x_{i}(1-x_{i})\right)}}\,e^{-{\frac {1}{2}}\left\{\log \left({\frac {\mathbf {x} }{1-\mathbf {x} }}\right)-{\boldsymbol {\mu }}\right\}^{\top }{\boldsymbol {\Sigma }}^{-1}\left\{\log \left({\frac {\mathbf {x} }{1-\mathbf {x} }}\right)-{\boldsymbol {\mu }}\right\}}} where the log and the division in the argument are taken element-wise. This is because the Jacobian matrix of the transformation is diagonal with elements 1 x i ( 1 − − x i ) {\displaystyle {\frac {1}{x_{i}(1-x_{i})}}} .

Use in statistical analysis [ edit ] The logistic normal distribution is a more flexible alternative to the Dirichlet distribution in that it can capture correlations between components of probability vectors. It also has the potential to simplify statistical analyses of compositional data by allowing one to answer questions about log-ratios of the components of the data vectors. One is often interested in ratios rather than absolute component values.

The probability simplex is a bounded space, making standard techniques that are typically applied to vectors in R n {\displaystyle \mathbb {R} ^{n}} less meaningful. Statistician John Aitchison described the problem of spurious negative correlations when applying such methods directly to simplicial vectors.

[ 5 ] However, mapping compositional data in S D {\displaystyle {\mathcal {S}}^{D}} through the inverse of the additive logistic transformation yields real-valued data in R D − − 1 {\displaystyle \mathbb {R} ^{D-1}} . Standard techniques can be applied to this representation of the data. This approach justifies use of the logistic normal distribution, which can thus be regarded as the "Gaussian of the simplex".

Relationship with the Dirichlet distribution [ edit ] Logistic normal approximation to Dirichlet distribution The Dirichlet and logistic normal distributions are never exactly equal for any choice of parameters. However, Aitchison described a method for approximating a Dirichlet with a logistic normal such that their Kullback–Leibler divergence (KL) is minimized: K ( p , q ) = ∫ ∫ S D p ( x ∣ ∣ α α ) log ⁡ ⁡ ( p ( x ∣ ∣ α α ) q ( x ∣ ∣ μ μ , Σ Σ ) ) d x {\displaystyle K(p,q)=\int _{{\mathcal {S}}^{D}}p\left(\mathbf {x} \mid {\boldsymbol {\alpha }}\right)\log \left({\frac {p\left(\mathbf {x} \mid {\boldsymbol {\alpha }}\right)}{q\left(\mathbf {x} \mid {\boldsymbol {\mu }},{\boldsymbol {\Sigma }}\right)}}\right)\,d\mathbf {x} } This is minimized by: μ μ ∗ ∗ = E p [ log ⁡ ⁡ ( x − − D x D ) ] , Σ Σ ∗ ∗ = Var p [ log ⁡ ⁡ ( x − − D x D ) ] {\displaystyle {\boldsymbol {\mu }}^{*}=\mathbf {E} _{p}\left[\log \left({\frac {\mathbf {x} _{-D}}{x_{D}}}\right)\right]\quad ,\quad {\boldsymbol {\Sigma }}^{*}={\textbf {Var}}_{p}\left[\log \left({\frac {\mathbf {x} _{-D}}{x_{D}}}\right)\right]} Using moment properties of the Dirichlet distribution, the solution can be written in terms of the digamma ψ ψ {\displaystyle \psi } and trigamma ψ ψ ′ {\displaystyle \psi '} functions: μ μ i ∗ ∗ = ψ ψ ( α α i ) − − ψ ψ ( α α D ) , i = 1 , … … , D − − 1 {\displaystyle \mu _{i}^{*}=\psi \left(\alpha _{i}\right)-\psi \left(\alpha _{D}\right)\quad ,\quad i=1,\ldots ,D-1} Σ Σ i i ∗ ∗ = ψ ψ ′ ( α α i ) + ψ ψ ′ ( α α D ) , i = 1 , … … , D − − 1 {\displaystyle \Sigma _{ii}^{*}=\psi '\left(\alpha _{i}\right)+\psi '\left(\alpha _{D}\right)\quad ,\quad i=1,\ldots ,D-1} Σ Σ i j ∗ ∗ = ψ ψ ′ ( α α D ) , i ≠ ≠ j {\displaystyle \Sigma _{ij}^{*}=\psi '\left(\alpha _{D}\right)\quad ,\quad i\neq j} This approximation is particularly accurate for large α α {\displaystyle {\boldsymbol {\alpha }}} . In fact, one can show that for α α i → → ∞ ∞ , i = 1 , … … , D {\displaystyle \alpha _{i}\rightarrow \infty ,i=1,\ldots ,D} , we have that p ( x ∣ ∣ α α ) → → q ( x ∣ ∣ μ μ ∗ ∗ , Σ Σ ∗ ∗ ) {\displaystyle p\left(\mathbf {x} \mid {\boldsymbol {\alpha }}\right)\rightarrow q\left(\mathbf {x} \mid {\boldsymbol {\mu }}^{*},{\boldsymbol {\Sigma }}^{*}\right)} .

See also [ edit ] Beta distribution and Kumaraswamy distribution , other two-parameter distributions on a bounded interval with similar shapes References [ edit ] ^ a b Aitchison, J.; Shen, S. M. (1980). "Logistic-normal distributions: Some properties and uses".

Biometrika .

67 (2): 261.

doi : 10.2307/2335470 .

ISSN 0006-3444 .

JSTOR 2335470 .

^ Huang, Jonathan; Tomasz, Malisiewicz.

"Fitting a Hierarchical Logistical Normal Distribution" (PDF) .

^ Peter Hoff, 2003.

Link ^ "Log-normal and logistic-normal terminology - AI and Social Science – Brendan O'Connor" .

brenocon.com . Retrieved 18 April 2018 .

^ a b J. Atchison. "The Statistical Analysis of Compositional Data."  Monographs on Statistics and Applied Probability, Chapman and Hall, 1986.

Book ^ Hinde, John (2011). "Logistic Normal Distribution". In Lovric, Miodrag (ed.).

International Encyclopedia of Statistical Sciences . Springer. pp.

754– 755.

doi : 10.1007/978-3-642-04898-2_342 .

ISBN 978-3-642-04897-5 .

Further reading [ edit ] Frederic, P. & Lad, F. (2008) Two Moments of the Logitnormal Distribution.

Communications in Statistics-Simulation and Computation . 37: 1263-1269 Mead, R. (1965). "A Generalised Logit-Normal Distribution".

Biometrics .

21 (3): 721– 732.

doi : 10.2307/2528553 .

JSTOR 2528553 .

PMID 5858101 .

External links [ edit ] logitnorm package for R v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Logit-normal_distribution&oldid=1296557204 " Category : Continuous distributions Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 20 June 2025, at 18:24 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Logit-normal distribution 4 languages Add topic

