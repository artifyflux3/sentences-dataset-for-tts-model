Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Overview Toggle Overview subsection 1.1 Probabilistic classifiers 1.2 Number of important feature variables 2 Problem statement Toggle Problem statement subsection 2.1 Frequentist or Bayesian approach to pattern recognition 3 Uses 4 Algorithms Toggle Algorithms subsection 4.1 Classification methods (methods predicting categorical labels) 4.2 Clustering methods (methods for classifying and predicting categorical labels) 4.3 Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together) 4.4 General methods for predicting arbitrarily-structured (sets of) labels 4.5 Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations) 4.6 Real-valued sequence labeling methods (predicting sequences of real-valued labels) 4.7 Regression methods (predicting real-valued labels) 4.8 Sequence labeling methods (predicting sequences of categorical labels) 5 See also 6 References 7 Further reading 8 External links Toggle the table of contents Pattern recognition 33 languages العربية Azərbaycanca Català Deutsch Ελληνικά Español فارسی Français 한국어 Հայերեն Bahasa Indonesia Italiano עברית Jawa Қазақша മലയാളം Bahasa Melayu Nederlands 日本語 Norsk bokmål Polski Português Русский Српски / srpski Srpskohrvatski / српскохрватски Suomi Tagalog ไทย Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Automated recognition of patterns and regularities in data This article is about pattern recognition as a branch of engineering. For the cognitive process, see Pattern recognition (psychology) . For other uses, see Pattern recognition (disambiguation) .

This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Pattern recognition" – news · newspapers · books · scholar · JSTOR ( May 2019 ) ( Learn how and when to remove this message ) Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning Journals and conferences AAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning v t e Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess PR capabilities but their primary function is to distinguish and create emergent  patterns.   PR has applications in statistical data analysis , signal processing , image analysis , information retrieval , bioinformatics , data compression , computer graphics and machine learning . Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning , due to the increased availability of big data and a new abundance of processing power .

Pattern recognition systems are commonly trained from labeled "training" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns.

KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and signal processing into consideration. It originated in engineering , and the term is popular in the context of computer vision : a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition .

In machine learning , pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification , which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam"). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression , which assigns a real-valued output to each input; [ 1 ] sequence labeling , which assigns a class to each member of a sequence of values [ 2 ] (for example, part of speech tagging , which assigns a part of speech to each word in an input sentence); and parsing , which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.

[ 3 ] Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors .

Overview [ edit ] Further information on Combination Of Shifted FIlter REsponses: COSFIRE A modern definition of pattern recognition is: The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

[ 4 ] Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value.

Supervised learning assumes that a set of training data (the training set ) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with Occam's Razor , discussed below).

Unsupervised learning , on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.

[ 5 ] A combination of the two that has been explored is semi-supervised learning , which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). In cases of unsupervised learning, there may be no training data at all.

Sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. The unsupervised equivalent of classification is normally known as clustering , based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space ), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different. In community ecology , the term classification is used to refer to what is commonly known as "clustering".

The piece of input data for which an output value is generated is formally termed an instance . The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. These feature vectors can be seen as defining points in an appropriate multidimensional space , and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors. Features typically are either categorical (also known as nominal , i.e., consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting of one of a set of ordered items, e.g., "large", "medium" or "small"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together, and this is also the case for integer-valued and real-valued data. Many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).

Probabilistic classifiers [ edit ] Main article: Probabilistic classifier Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N -best labels with associated probabilities, for some value of N , instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification ), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms: They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory . Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.) Correspondingly, they can abstain when the confidence of choosing any particular output is too low.

Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation .

Number of important feature variables [ edit ] Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.

[ 6 ] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of n {\displaystyle n} features the powerset consisting of all 2 n − − 1 {\displaystyle 2^{n}-1} subsets of features need to be explored. The Branch-and-Bound algorithm [ 7 ] does reduce this complexity but is intractable for medium to large values of the number of available features n {\displaystyle n} Techniques to transform the raw feature vectors ( feature extraction ) are sometimes used prior to application of the pattern-matching algorithm.

Feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.

Problem statement [ edit ] The problem of pattern recognition can be stated as follows: Given an unknown function g : X → → Y {\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}} (the ground truth ) that maps input instances x ∈ ∈ X {\displaystyle {\boldsymbol {x}}\in {\mathcal {X}}} to output labels y ∈ ∈ Y {\displaystyle y\in {\mathcal {Y}}} , along with training data D = { ( x 1 , y 1 ) , … … , ( x n , y n ) } {\displaystyle \mathbf {D} =\{({\boldsymbol {x}}_{1},y_{1}),\dots ,({\boldsymbol {x}}_{n},y_{n})\}} assumed to represent accurate examples of the mapping, produce a function h : X → → Y {\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}} that approximates as closely as possible the correct mapping g {\displaystyle g} . (For example, if the problem is filtering spam, then x i {\displaystyle {\boldsymbol {x}}_{i}} is some representation of an email and y {\displaystyle y} is either "spam" or "non-spam"). In order for this to be a well-defined problem, "approximates as closely as possible" needs to be defined rigorously. In decision theory , this is defined by specifying a loss function or cost function that assigns a specific value to "loss" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of X {\displaystyle {\mathcal {X}}} . In practice, neither the distribution of X {\displaystyle {\mathcal {X}}} nor the ground truth function g : X → → Y {\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}} are known exactly, but can be computed only empirically by collecting a large number of samples of X {\displaystyle {\mathcal {X}}} and hand-labeling them using the correct value of Y {\displaystyle {\mathcal {Y}}} (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification , the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function h : X → → Y {\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}} labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness ) on a "typical" test set.

For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form p ( l a b e l | x , θ θ ) = f ( x ; θ θ ) {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})=f\left({\boldsymbol {x}};{\boldsymbol {\theta }}\right)} where the feature vector input is x {\displaystyle {\boldsymbol {x}}} , and the function f is typically parameterized by some parameters θ θ {\displaystyle {\boldsymbol {\theta }}} .

[ 8 ] In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability p ( x | l a b e l ) {\displaystyle p({{\boldsymbol {x}}|{\rm {label}}})} is instead estimated and combined with the prior probability p ( l a b e l | θ θ ) {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})} using Bayes' rule , as follows: p ( l a b e l | x , θ θ ) = p ( x | l a b e l , θ θ ) p ( l a b e l | θ θ ) ∑ ∑ L ∈ ∈ all labels p ( x | L ) p ( L | θ θ ) .

{\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\sum _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})}}.} When the labels are continuously distributed (e.g., in regression analysis ), the denominator involves integration rather than summation: p ( l a b e l | x , θ θ ) = p ( x | l a b e l , θ θ ) p ( l a b e l | θ θ ) ∫ ∫ L ∈ ∈ all labels p ( x | L ) p ( L | θ θ ) d ⁡ ⁡ L .

{\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\int _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})\operatorname {d} L}}.} The value of θ θ {\displaystyle {\boldsymbol {\theta }}} is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate ) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability p ( θ θ ) {\displaystyle p({\boldsymbol {\theta }})} on different values of θ θ {\displaystyle {\boldsymbol {\theta }}} . Mathematically: θ θ ∗ ∗ = arg ⁡ ⁡ max θ θ p ( θ θ | D ) {\displaystyle {\boldsymbol {\theta }}^{*}=\arg \max _{\boldsymbol {\theta }}p({\boldsymbol {\theta }}|\mathbf {D} )} where θ θ ∗ ∗ {\displaystyle {\boldsymbol {\theta }}^{*}} is the value used for θ θ {\displaystyle {\boldsymbol {\theta }}} in the subsequent evaluation procedure, and p ( θ θ | D ) {\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )} , the posterior probability of θ θ {\displaystyle {\boldsymbol {\theta }}} , is given by p ( θ θ | D ) = [ ∏ ∏ i = 1 n p ( y i | x i , θ θ ) ] p ( θ θ ) .

{\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )=\left[\prod _{i=1}^{n}p(y_{i}|{\boldsymbol {x}}_{i},{\boldsymbol {\theta }})\right]p({\boldsymbol {\theta }}).} In the Bayesian approach to this problem, instead of choosing a single parameter vector θ θ ∗ ∗ {\displaystyle {\boldsymbol {\theta }}^{*}} , the probability of a given label for a new instance x {\displaystyle {\boldsymbol {x}}} is computed by integrating over all possible values of θ θ {\displaystyle {\boldsymbol {\theta }}} , weighted according to the posterior probability: p ( l a b e l | x ) = ∫ ∫ p ( l a b e l | x , θ θ ) p ( θ θ | D ) d ⁡ ⁡ θ θ .

{\displaystyle p({\rm {label}}|{\boldsymbol {x}})=\int p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})p({\boldsymbol {\theta }}|\mathbf {D} )\operatorname {d} {\boldsymbol {\theta }}.} Frequentist or Bayesian approach to pattern recognition [ edit ] The first pattern classifier – the linear discriminant presented by Fisher – was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix . Also the probability of each class p ( l a b e l | θ θ ) {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})} is estimated from the collected dataset. Note that the usage of ' Bayes rule ' in a pattern classifier does not make the classification approach Bayesian.

Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the ' a priori ' and the ' a posteriori ' knowledge. Later Kant defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities p ( l a b e l | θ θ ) {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})} can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the Beta- ( conjugate prior ) and Dirichlet-distributions . The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.

Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.

Uses [ edit ] The face was automatically detected by special software.

Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings. Other typical applications of pattern recognition techniques are automatic speech recognition , speaker identification , classification of text into several categories (e.g., spam or non-spam email messages), the automatic recognition of handwriting on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.

[ 9 ] [ 10 ] The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.

[ 11 ] [ 12 ] Optical character recognition is an example of the application of a pattern classifier. The method of signing one's name was captured with stylus and overlay starting in 1990.

[ citation needed ] The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers.

[ citation needed ] Pattern recognition has many real-world applications in image processing. Some examples include: identification and authentication: e.g., license plate recognition , [ 13 ] fingerprint analysis, face detection /verification, [ 14 ] and voice-based authentication .

[ 15 ] medical diagnosis: e.g., screening for cervical cancer (Papnet), [ 16 ] breast tumors or heart sounds; defense: various navigation and guidance systems, target recognition systems, shape recognition technology etc.

mobility: advanced driver assistance systems , autonomous vehicle technology , etc.

[ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] In psychology, pattern recognition is used to make sense of and identify objects, and is closely related to perception. This explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways. The first concerns template matching and the second concerns feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long-term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. One observation is a capital E having three horizontal lines and one vertical line.

[ 22 ] Algorithms [ edit ] Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative .

Classification methods (methods predicting categorical labels) [ edit ] Main article: Statistical classification Parametric: [ 23 ] Linear discriminant analysis Quadratic discriminant analysis Maximum entropy classifier (aka logistic regression , multinomial logistic regression ): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.) Nonparametric: [ 24 ] Decision trees , decision lists Kernel estimation and K-nearest-neighbor algorithms Naive Bayes classifier Neural networks (multi-layer perceptrons) Perceptrons Support vector machines Gene expression programming Clustering methods (methods for classifying and predicting categorical labels) [ edit ] Main article: Cluster analysis Categorical mixture models Hierarchical clustering (agglomerative or divisive) K-means clustering Correlation clustering Kernel principal component analysis (Kernel PCA) Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together) [ edit ] Main article: Ensemble learning Boosting (meta-algorithm) Bootstrap aggregating ("bagging") Ensemble averaging Mixture of experts , hierarchical mixture of experts General methods for predicting arbitrarily-structured (sets of) labels [ edit ] Bayesian networks Markov random fields Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations) [ edit ] Unsupervised: Multilinear principal component analysis (MPCA) Real-valued sequence labeling methods (predicting sequences of real-valued labels) [ edit ] Main article: sequence labeling Kalman filters Particle filters Regression methods (predicting real-valued labels) [ edit ] Main article: Regression analysis Gaussian process regression (kriging) Linear regression and extensions Independent component analysis (ICA) Principal components analysis (PCA) Sequence labeling methods (predicting sequences of categorical labels) [ edit ] Conditional random fields (CRFs) Hidden Markov models (HMMs) Maximum entropy Markov models (MEMMs) Recurrent neural networks (RNNs) Dynamic time warping (DTW) This article may contain unverified or indiscriminate information in embedded lists .

Please help clean up the lists by removing items or incorporating them into the text of the article.

( May 2014 ) See also [ edit ] Adaptive resonance theory – Theory in neuropsychology Black box – System where only the inputs and outputs can be viewed, and not its implementation Cache language model Compound-term processing Computer-aided diagnosis – Type of diagnosis assisted by computers Contextual image classification Data mining – Process of extracting and discovering patterns in large data sets Deep learning – Branch of machine learning Grey box model – Mathematical data production model with limited structure Information theory – Scientific study of digital information List of datasets for machine learning research List of numerical-analysis software List of numerical libraries Neocognitron – Type of artificial neural network Perception – Interpretation of sensory information Perceptual learning – Process of learning better perception skills Predictive analytics – Statistical techniques analyzing facts to make predictions about unknown events Prior knowledge for pattern recognition Sequence mining – Data mining technique Pages displaying short descriptions of redirect targets Template matching – Technique in digital image processing References [ edit ] ^ Howard, W.R. (2007-02-20). "Pattern Recognition and Machine Learning".

Kybernetes .

36 (2): 275.

doi : 10.1108/03684920710743466 .

ISSN 0368-492X .

^ "Sequence Labeling" (PDF) .

utah.edu .

Archived (PDF) from the original on 2018-11-06 . Retrieved 2018-11-06 .

^ Ian., Chiswell (2007).

Mathematical logic, p. 34 . Oxford University Press.

ISBN 9780199215621 .

OCLC 799802313 .

^ Bishop, Christopher M. (2006).

Pattern Recognition and Machine Learning . Springer.

^ Carvalko, J.R., Preston K. (1972). "On Determining Optimum Simple Golay Marking Transforms for Binary Image Processing".

IEEE Transactions on Computers .

21 (12): 1430– 33.

doi : 10.1109/T-C.1972.223519 .

S2CID 21050445 .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) .

^ Isabelle Guyon Clopinet, André Elisseeff (2003).

An Introduction to Variable and Feature Selection . The Journal of Machine Learning Research, Vol. 3, 1157-1182.

Link Archived 2016-03-04 at the Wayback Machine ^ Iman Foroutan; Jack Sklansky (1987). "Feature Selection for Automatic Classification of Non-Gaussian Data".

IEEE Transactions on Systems, Man, and Cybernetics .

17 (2): 187– 198.

doi : 10.1109/TSMC.1987.4309029 .

S2CID 9871395 .

.

^ For linear discriminant analysis the parameter vector θ θ {\displaystyle {\boldsymbol {\theta }}} consists of the two mean vectors μ μ 1 {\displaystyle {\boldsymbol {\mu }}_{1}} and μ μ 2 {\displaystyle {\boldsymbol {\mu }}_{2}} and the common covariance matrix Σ Σ {\displaystyle {\boldsymbol {\Sigma }}} .

^ Milewski, Robert; Govindaraju, Venu (31 March 2008).

"Binarization and cleanup of handwritten text from carbon copy medical form images" .

Pattern Recognition .

41 (4): 1308– 1315.

Bibcode : 2008PatRe..41.1308M .

doi : 10.1016/j.patcog.2007.08.018 .

Archived from the original on 10 September 2020 . Retrieved 26 October 2011 .

^ Sarangi, Susanta; Sahidullah, Md; Saha, Goutam (September 2020). "Optimization of data-driven filterbank for automatic speaker verification".

Digital Signal Processing .

104 : 102795.

arXiv : 2007.10729 .

Bibcode : 2020DSP...10402795S .

doi : 10.1016/j.dsp.2020.102795 .

S2CID 220665533 .

^ Richard O. Duda , Peter E. Hart , David G. Stork (2001).

Pattern classification (2nd ed.). Wiley, New York.

ISBN 978-0-471-05669-0 .

Archived from the original on 2020-08-19 . Retrieved 2019-11-26 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ R. Brunelli, Template Matching Techniques in Computer Vision: Theory and Practice , Wiley, ISBN 978-0-470-51706-2 , 2009 ^ The Automatic Number Plate Recognition Tutorial Archived 2006-08-20 at the Wayback Machine http://anpr-tutorial.com/ ^ Neural Networks for Face Recognition Archived 2016-03-04 at the Wayback Machine Companion to Chapter 4 of the textbook Machine Learning.

^ Poddar, Arnab; Sahidullah, Md; Saha, Goutam (March 2018).

"Speaker Verification with Short Utterances: A Review of Challenges, Trends and Opportunities" .

IET Biometrics .

7 (2): 91– 101.

doi : 10.1049/iet-bmt.2017.0065 . Archived from the original on 2019-09-03 . Retrieved 2019-08-27 .

^ PAPNET For Cervical Screening Archived 2012-07-08 at archive.today ^ "Development of an Autonomous Vehicle Control Strategy Using a Single Camera and Deep Neural Networks (2018-01-0035 Technical Paper)- SAE Mobilus" .

saemobilus.sae.org . 3 April 2018.

doi : 10.4271/2018-01-0035 .

Archived from the original on 2019-09-06 . Retrieved 2019-09-06 .

^ Gerdes, J. Christian; Kegelman, John C.; Kapania, Nitin R.; Brown, Matthew; Spielberg, Nathan A. (2019-03-27).

"Neural network vehicle models for high-performance automated driving" .

Science Robotics .

4 (28): eaaw1975.

doi : 10.1126/scirobotics.aaw1975 .

ISSN 2470-9476 .

PMID 33137751 .

S2CID 89616974 .

^ Pickering, Chris (2017-08-15).

"How AI is paving the way for fully autonomous cars" .

The Engineer .

Archived from the original on 2019-09-06 . Retrieved 2019-09-06 .

^ Ray, Baishakhi; Jana, Suman; Pei, Kexin; Tian, Yuchi (2017-08-28). "DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars".

arXiv : 1708.08559 .

Bibcode : 2017arXiv170808559T .

{{ cite journal }} : Cite journal requires |journal= ( help ) ^ Sinha, P. K.; Hadjiiski, L. M.; Mutib, K. (1993-04-01). "Neural Networks in Autonomous Vehicle Control".

IFAC Proceedings Volumes . 1st IFAC International Workshop on Intelligent Autonomous Vehicles, Hampshire, UK, 18–21 April.

26 (1): 335– 340.

doi : 10.1016/S1474-6670(17)49322-0 .

ISSN 1474-6670 .

^ "A-level Psychology Attention Revision - Pattern recognition | S-cool, the revision website" . S-cool.co.uk.

Archived from the original on 2013-06-22 . Retrieved 2012-09-17 .

^ Assuming known distributional shape of feature distributions per class, such as the Gaussian shape.

^ No distributional assumption regarding shape of feature distributions per class.

Further reading [ edit ] Fukunaga, Keinosuke (1990).

Introduction to Statistical Pattern Recognition (2nd ed.). Boston: Academic Press.

ISBN 978-0-12-269851-4 .

Hornegger, Joachim; Paulus, Dietrich W. R. (1999).

Applied Pattern Recognition: A Practical Introduction to Image and Speech Processing in C++ (2nd ed.). San Francisco: Morgan Kaufmann Publishers.

ISBN 978-3-528-15558-2 .

Schuermann, Juergen (1996).

Pattern Classification: A Unified View of Statistical and Neural Approaches . New York: Wiley.

ISBN 978-0-471-13534-0 .

Godfried T. Toussaint, ed. (1988).

Computational Morphology . Amsterdam: North-Holland Publishing Company.

ISBN 9781483296722 .

Kulikowski, Casimir A.; Weiss, Sholom M. (1991).

Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems . San Francisco: Morgan Kaufmann Publishers.

ISBN 978-1-55860-065-2 .

Duda, Richard O.; Hart, Peter E.; Stork, David G. (2000).

Pattern Classification (2nd ed.). Wiley-Interscience.

ISBN 978-0471056690 .

Jain, Anil.K.; Duin, Robert.P.W.; Mao, Jianchang (2000). "Statistical pattern recognition: a review".

IEEE Transactions on Pattern Analysis and Machine Intelligence .

22 (1): 4– 37.

CiteSeerX 10.1.1.123.8151 .

doi : 10.1109/34.824819 .

S2CID 192934 .

An introductory tutorial to classifiers (introducing the basic terms, with numeric example) Kovalevsky, V. A. (1980).

Image Pattern Recognition . New York, NY: Springer New York.

ISBN 978-1-4612-6033-2 .

OCLC 852790446 .

External links [ edit ] The International Association for Pattern Recognition List of Pattern Recognition web sites Journal of Pattern Recognition Research Archived 2008-09-08 at the Wayback Machine Pattern Recognition Info Pattern Recognition (Journal of the Pattern Recognition Society) International Journal of Pattern Recognition and Artificial Intelligence Archived 2004-12-11 at the Wayback Machine International Journal of Applied Pattern Recognition Open Pattern Recognition Project , intended to be an open source platform for sharing algorithms of pattern recognition Improved Fast Pattern Matching Improved Fast Pattern Matching v t e Differentiable computing General Differentiable programming Information geometry Statistical manifold Automatic differentiation Neuromorphic computing Pattern recognition Ricci calculus Computational learning theory Inductive bias Hardware IPU TPU VPU Memristor SpiNNaker Software libraries TensorFlow PyTorch Keras scikit-learn Theano JAX Flux.jl MindSpore Portals Computer programming Technology Authority control databases National Germany United States Japan Czech Republic Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐645954cd9f‐8mwh4
Cached time: 20250815194117
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.842 seconds
Real time usage: 1.114 seconds
Preprocessor visited node count: 5638/1000000
Revision size: 35869/2097152 bytes
Post‐expand include size: 125589/2097152 bytes
Template argument size: 4823/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 14/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 116866/5000000 bytes
Lua time usage: 0.557/10.000 seconds
Lua memory usage: 20962729/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  893.512      1 -total
 25.63%  229.010      1 Template:Reflist
 25.09%  224.155     17 Template:Annotated_link
 17.02%  152.097     11 Template:Cite_journal
 10.07%   90.020      1 Template:Machine_learning
  9.76%   87.185      1 Template:Sidebar_with_collapsible_lists
  7.95%   71.041      1 Template:Short_description
  6.51%   58.203      1 Template:More_citations_needed
  6.30%   56.284      2 Template:Ambox
  6.09%   54.458     10 Template:Cite_book Saved in parser cache with key enwiki:pcache:126706:|#|:idhash:canonical and timestamp 20250815194117 and revision id 1296419396. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Pattern_recognition&oldid=1296419396 " Categories : Pattern recognition Machine learning Formal sciences Computational fields of study Hidden categories: CS1 maint: multiple names: authors list Webarchive template wayback links Webarchive template archiveis links CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Articles needing additional references from May 2019 All articles needing additional references All articles with unsourced statements Articles with unsourced statements from January 2011 Articles needing cleanup from May 2014 All pages needing cleanup Wikipedia list cleanup from May 2014 Pages displaying short descriptions of redirect targets via Module:Annotated link This page was last edited on 19 June 2025, at 22:19 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Pattern recognition 33 languages Add topic

