Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Probability mass function 1.2 Combinatorial identities 2 Properties Toggle Properties subsection 2.1 Working example 2.2 Symmetries 2.3 Order of draws 2.4 Tail bounds 3 Statistical Inference Toggle Statistical Inference subsection 3.1 Hypergeometric test 4 Related distributions Toggle Related distributions subsection 4.1 Multivariate hypergeometric distribution 4.1.1 Example 5 Occurrence and applications Toggle Occurrence and applications subsection 5.1 Application to auditing elections 5.2 Application to Texas hold'em poker 5.3 Application to Keno 6 See also 7 References Toggle References subsection 7.1 Citations 7.2 Sources 8 External links Toggle the table of contents Hypergeometric distribution 31 languages Беларуская Català Čeština Deutsch Ελληνικά Español Euskara فارسی Français Galego 한국어 Italiano עברית Magyar Македонски Nederlands 日本語 Norsk bokmål Norsk nynorsk Polski Português Русский Shqip Slovenščina Suomi Svenska Tagalog Türkçe Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Multivariate hypergeometric distribution ) Discrete probability distribution Not to be confused with Geometric distribution or Hypergeometric function .

Hypergeometric Probability mass function Cumulative distribution function Parameters N ∈ ∈ { 0 , 1 , 2 , … … } K ∈ ∈ { 0 , 1 , 2 , … … , N } n ∈ ∈ { 0 , 1 , 2 , … … , N } {\displaystyle {\begin{aligned}N&\in \left\{0,1,2,\dots \right\}\\K&\in \left\{0,1,2,\dots ,N\right\}\\n&\in \left\{0,1,2,\dots ,N\right\}\end{aligned}}\,} Support k ∈ ∈ { max ( 0 , n + K − − N ) , … … , min ( n , K ) } {\displaystyle \scriptstyle {k\,\in \,\{\max {(0,\,n+K-N)},\,\dots ,\,\min {(n,\,K)}\}}\,} PMF ( K k ) ( N − − K n − − k ) ( N n ) {\displaystyle {\frac {{\binom {K}{k}}{\binom {N-K}{n-k}}}{\binom {N}{n}}}} CDF 1 − − ( n k + 1 ) ( N − − n K − − k − − 1 ) ( N K ) 3 F 2 [ 1 , k + 1 − − K , k + 1 − − n k + 2 , N + k + 2 − − K − − n ; 1 ] , {\displaystyle 1-{{{n \choose {k+1}}{{N-n} \choose {K-k-1}}} \over {N \choose K}}\,_{3}F_{2}\!\!\left[{\begin{array}{c}1,\ k+1-K,\ k+1-n\\k+2,\ N+k+2-K-n\end{array}};1\right],} where p F q {\displaystyle \,_{p}F_{q}} is the generalized hypergeometric function Mean n K N {\displaystyle n{K \over N}} Mode ⌈ ( n + 1 ) ( K + 1 ) N + 2 ⌉ − − 1 , ⌊ ( n + 1 ) ( K + 1 ) N + 2 ⌋ {\displaystyle \left\lceil {\frac {(n+1)(K+1)}{N+2}}\right\rceil -1,\left\lfloor {\frac {(n+1)(K+1)}{N+2}}\right\rfloor } Variance n K N N − − K N N − − n N − − 1 {\displaystyle n{K \over N}{N-K \over N}{N-n \over N-1}} Skewness ( N − − 2 K ) ( N − − 1 ) 1 2 ( N − − 2 n ) [ n K ( N − − K ) ( N − − n ) ] 1 2 ( N − − 2 ) {\displaystyle {\frac {(N-2K)(N-1)^{\frac {1}{2}}(N-2n)}{[nK(N-K)(N-n)]^{\frac {1}{2}}(N-2)}}} Excess kurtosis 1 n K ( N − − K ) ( N − − n ) ( N − − 2 ) ( N − − 3 ) ⋅ ⋅ {\displaystyle \left.{\frac {1}{nK(N-K)(N-n)(N-2)(N-3)}}\cdot \right.} [ ( N − − 1 ) N 2 ( N ( N + 1 ) − − 6 K ( N − − K ) − − 6 n ( N − − n ) ) {\displaystyle {\big [}(N-1)N^{2}{\big (}N(N+1)-6K(N-K)-6n(N-n){\big )}} + 6 n K ( N − − K ) ( N − − n ) ( 5 N − − 6 ) ] {\displaystyle {}+6nK(N-K)(N-n)(5N-6){\big ]}} MGF ( N − − K n ) 2 F 1 ( − − n , − − K ; N − − K − − n + 1 ; e t ) ( N n ) {\displaystyle {\frac {{\binom {N-K}{n}}\,_{2}F_{1}(-n,-K\,;\,N-K-n+1\,;\,e^{t})}{\binom {N}{n}}}} CF ( N − − K n ) 2 F 1 ( − − n , − − K ; N − − K − − n + 1 ; e i t ) ( N n ) {\displaystyle {\frac {{\binom {N-K}{n}}\,_{2}F_{1}(-n,-K\,;\,N-K-n+1\,;\,e^{it})}{\binom {N}{n}}}} In probability theory and statistics , the hypergeometric distribution is a discrete probability distribution that describes the probability of k {\displaystyle k} successes (random draws for which the object drawn has a specified feature) in n {\displaystyle n} draws, without replacement, from a finite population of size N {\displaystyle N} that contains exactly K {\displaystyle K} objects with that feature, wherein each draw is either a success or a failure. In contrast, the binomial distribution describes the probability of k {\displaystyle k} successes in n {\displaystyle n} draws with replacement.

Definitions [ edit ] Probability mass function [ edit ] The following conditions characterize the hypergeometric distribution: The result of each draw (the elements of the population being sampled) can be classified into one of two mutually exclusive categories (e.g. Pass/Fail or  Employed/Unemployed).

The probability of a success changes on each draw, as each draw decreases the population ( sampling without replacement from a finite population).

A random variable X {\displaystyle X} follows the hypergeometric distribution if its probability mass function (pmf) is given by [ 1 ] p X ( k ) = Pr ( X = k ) = ( K k ) ( N − − K n − − k ) ( N n ) , {\displaystyle p_{X}(k)=\Pr(X=k)={\frac {{\binom {K}{k}}{\binom {N-K}{n-k}}}{\binom {N}{n}}},} where N {\displaystyle N} is the population size, K {\displaystyle K} is the number of success states in the population, n {\displaystyle n} is the number of draws (i.e. quantity drawn in each trial), k {\displaystyle k} is the number of observed successes, ( a b ) {\textstyle \textstyle {a \choose b}} is a binomial coefficient .

The pmf is positive when max ( 0 , n + K − − N ) ≤ ≤ k ≤ ≤ min ( K , n ) {\displaystyle \max(0,n+K-N)\leq k\leq \min(K,n)} .

A random variable distributed hypergeometrically with parameters N {\displaystyle N} , K {\displaystyle K} and n {\displaystyle n} is written X ∼ ∼ Hypergeometric ⁡ ⁡ ( N , K , n ) {\textstyle X\sim \operatorname {Hypergeometric} (N,K,n)} and has probability mass function p X ( k ) {\textstyle p_{X}(k)} above.

Combinatorial identities [ edit ] As required, we have ∑ ∑ 0 ≤ ≤ k ≤ ≤ min ( n , K ) ( K k ) ( N − − K n − − k ) ( N n ) = 1 , {\displaystyle \sum _{0\leq k\leq {\textrm {min}}(n,K)}{{K \choose k}{N-K \choose n-k} \over {N \choose n}}=1,} which essentially follows from Vandermonde's identity from combinatorics .

Also note that ( K k ) ( N − − K n − − k ) ( N n ) = ( n k ) ( N − − n K − − k ) ( N K ) ; {\displaystyle {{K \choose k}{N-K \choose n-k} \over {N \choose n}}={{{n \choose k}{{N-n} \choose {K-k}}} \over {N \choose K}};} This identity can be shown by expressing the binomial coefficients in terms of factorials and rearranging the latter. Additionally, it
follows from the symmetry of the problem, described in two different but interchangeable ways.

For example, consider two rounds of drawing without replacement. In the first round, K {\displaystyle K} out of N {\displaystyle N} neutral marbles are drawn from an urn without replacement and coloured green. Then the colored marbles are put back. In the second round, n {\displaystyle n} marbles are drawn without replacement and colored red. Then, the number of marbles with both colors on them (that is, the number of marbles that have been drawn twice) has the hypergeometric distribution. The symmetry in K {\displaystyle K} and n {\displaystyle n} stems from the fact that the two rounds are independent, and one could have started by drawing n {\displaystyle n} balls and colouring them red first.

Note that we are interested in the probability of k {\displaystyle k} successes in n {\displaystyle n} draws without replacement , since the probability of success on each trial is not the same, as the size of the remaining population changes as we remove each marble. Keep in mind not to confuse with the binomial distribution , which describes the probability of k {\displaystyle k} successes in n {\displaystyle n} draws with replacement.

Properties [ edit ] Working example [ edit ] The classical application of the hypergeometric distribution is sampling without replacement . Think of an urn with two colors of marbles , red and green. Define drawing a green marble as a success and drawing a red marble as a failure. Let N describe the number of all marbles in the urn (see contingency table below) and K describe the number of green marbles , then N − K corresponds to the number of red marbles . Now, standing next to the urn, you close your eyes and draw n marbles without replacement. Define X as a random variable whose outcome is k , the number of green marbles drawn in the experiment. This situation is illustrated by the following contingency table : drawn not drawn total green marbles k K − k K red marbles n − k N + k − n − K N − K total n N − n N Indeed, we are interested in calculating the probability of drawing k green marbles in n draws, given that there are K green marbles out of a total of N marbles. For this example, assume that there are 5 green and 45 red marbles in the urn. Standing next to the urn, you close your eyes and draw 10 marbles without replacement. What is the probability that exactly 4 of the 10 are green?

This problem is summarized by the following contingency table: drawn not drawn total green marbles k = 4 K − k = 1 K = 5 red marbles n − k = 6 N + k − n − K = 39 N − K = 45 total n = 10 N − n = 40 N = 50 To find the probability of drawing k green marbles in exactly n draws out of N total draws , we identify X as a hyper-geometric random variable to use the formula P ( X = k ) = f ( k ; N , K , n ) = ( K k ) ( N − − K n − − k ) ( N n ) .

{\displaystyle P(X=k)=f(k;N,K,n)={{{K \choose k}{{N-K} \choose {n-k}}} \over {N \choose n}}.} To intuitively explain the given formula, consider the two symmetric problems represented by the identity ( K k ) ( N − − K n − − k ) ( N n ) = ( n k ) ( N − − n K − − k ) ( N K ) {\displaystyle {{K \choose k}{N-K \choose n-k} \over {N \choose n}}={{{n \choose k}{{N-n} \choose {K-k}}} \over {N \choose K}}} left-hand side - drawing a total of only n marbles out of the urn. We want to find the probability of the outcome of drawing k green marbles out of K total green marbles, and drawing n-k red marbles out of N-K red marbles, in these n rounds.

right hand side - alternatively, drawing all N marbles out of the urn. We want to find the probability of the outcome of drawing k green marbles in n draws out of the total N draws, and K-k green marbles in the rest N-n draws.

Back to the calculations, we use the formula above to calculate the probability of drawing exactly k green marbles P ( X = 4 ) = f ( 4 ; 50 , 5 , 10 ) = ( 5 4 ) ( 45 6 ) ( 50 10 ) = 5 ⋅ ⋅ 8145060 10272278170 = 0.003964583 … … .

{\displaystyle P(X=4)=f(4;50,5,10)={{{5 \choose 4}{{45} \choose {6}}} \over {50 \choose 10}}={5\cdot 8145060 \over 10272278170}=0.003964583\dots .} Intuitively we would expect it to be even more unlikely that all 5 green marbles will be among the 10 drawn.

P ( X = 5 ) = f ( 5 ; 50 , 5 , 10 ) = ( 5 5 ) ( 45 5 ) ( 50 10 ) = 1 ⋅ ⋅ 1221759 10272278170 = 0.0001189375 … … , {\displaystyle P(X=5)=f(5;50,5,10)={{{5 \choose 5}{{45} \choose {5}}} \over {50 \choose 10}}={1\cdot 1221759 \over 10272278170}=0.0001189375\dots ,} As expected, the probability of drawing 5 green marbles is roughly 35 times less likely than that of drawing 4.

Symmetries [ edit ] Swapping the roles of green and red marbles: f ( k ; N , K , n ) = f ( n − − k ; N , N − − K , n ) {\displaystyle f(k;N,K,n)=f(n-k;N,N-K,n)} Swapping the roles of drawn and not drawn marbles: f ( k ; N , K , n ) = f ( K − − k ; N , K , N − − n ) {\displaystyle f(k;N,K,n)=f(K-k;N,K,N-n)} Swapping the roles of green and drawn marbles: f ( k ; N , K , n ) = f ( k ; N , n , K ) {\displaystyle f(k;N,K,n)=f(k;N,n,K)} These symmetries generate the dihedral group D 4 {\displaystyle D_{4}} .

Order of draws [ edit ] The probability of drawing any set of green and red marbles (the hypergeometric distribution) depends only on the  numbers of green and red marbles, not on the order in which they appear; i.e., it is an exchangeable distribution. As a result, the probability of drawing a green marble in the i th {\displaystyle i^{\text{th}}} draw is [ 2 ] P ( G i ) = K N .

{\displaystyle P(G_{i})={\frac {K}{N}}.} This is an ex ante probability—that is, it is based on not knowing the results of the previous draws.

Tail bounds [ edit ] Let X ∼ ∼ Hypergeometric ⁡ ⁡ ( N , K , n ) {\displaystyle X\sim \operatorname {Hypergeometric} (N,K,n)} and p = K / N {\displaystyle p=K/N} . Then for 0 < t < K / N {\displaystyle 0<t<K/N} we can derive the following bounds: [ 3 ] Pr [ X ≤ ≤ ( p − − t ) n ] ≤ ≤ e − − n D ( p − − t ∥ ∥ p ) ≤ ≤ e − − 2 t 2 n Pr [ X ≥ ≥ ( p + t ) n ] ≤ ≤ e − − n D ( p + t ∥ ∥ p ) ≤ ≤ e − − 2 t 2 n {\displaystyle {\begin{aligned}\Pr[X\leq (p-t)n]&\leq e^{-n{\text{D}}(p-t\parallel p)}\leq e^{-2t^{2}n}\\\Pr[X\geq (p+t)n]&\leq e^{-n{\text{D}}(p+t\parallel p)}\leq e^{-2t^{2}n}\\\end{aligned}}\!} where D ( a ∥ ∥ b ) = a log ⁡ ⁡ a b + ( 1 − − a ) log ⁡ ⁡ 1 − − a 1 − − b {\displaystyle D(a\parallel b)=a\log {\frac {a}{b}}+(1-a)\log {\frac {1-a}{1-b}}} is the Kullback–Leibler divergence and it is used that D ( a ∥ ∥ b ) ≥ ≥ 2 ( a − − b ) 2 {\displaystyle D(a\parallel b)\geq 2(a-b)^{2}} .

[ 4 ] Note : In order to derive the previous bounds, one has to start by observing that X = ∑ ∑ i = 1 n Y i n {\displaystyle X={\frac {\sum _{i=1}^{n}Y_{i}}{n}}} where Y i {\displaystyle Y_{i}} are dependent random variables with a specific distribution D {\displaystyle D} . Because most of the theorems about bounds in sum of random variables are concerned with independent sequences of them, one has to first create a sequence Z i {\displaystyle Z_{i}} of independent random variables with the same distribution D {\displaystyle D} and apply the theorems on X ′ = ∑ ∑ i = 1 n Z i n {\displaystyle X'={\frac {\sum _{i=1}^{n}Z_{i}}{n}}} . Then, it is proved from Hoeffding [ 3 ] that the results and bounds obtained via this process hold for X {\displaystyle X} as well.

If n is larger than N /2, it can be useful to apply symmetry to "invert" the bounds, which give you the following: [ 4 ] [ 5 ] Pr [ X ≤ ≤ ( p − − t ) n ] ≤ ≤ e − − ( N − − n ) D ( p + t n N − − n | | p ) ≤ ≤ e − − 2 t 2 n n N − − n Pr [ X ≥ ≥ ( p + t ) n ] ≤ ≤ e − − ( N − − n ) D ( p − − t n N − − n | | p ) ≤ ≤ e − − 2 t 2 n n N − − n {\displaystyle {\begin{aligned}\Pr[X\leq (p-t)n]&\leq e^{-(N-n){\text{D}}(p+{\tfrac {tn}{N-n}}||p)}\leq e^{-2t^{2}n{\tfrac {n}{N-n}}}\\\\\Pr[X\geq (p+t)n]&\leq e^{-(N-n){\text{D}}(p-{\tfrac {tn}{N-n}}||p)}\leq e^{-2t^{2}n{\tfrac {n}{N-n}}}\\\end{aligned}}\!} Statistical Inference [ edit ] Hypergeometric test [ edit ] See also: Fisher's noncentral hypergeometric distribution The hypergeometric test uses the hypergeometric distribution to measure the statistical significance of having drawn a sample consisting of a specific number of k {\displaystyle k} successes (out of n {\displaystyle n} total draws) from a population of size N {\displaystyle N} containing K {\displaystyle K} successes. In a test for over-representation of successes in the sample, the hypergeometric p-value is calculated as the probability of randomly drawing k {\displaystyle k} or more successes from the population in n {\displaystyle n} total draws. In a test for under-representation, the p-value is the probability of randomly drawing k {\displaystyle k} or fewer successes.

Biologist and statistician Ronald Fisher The test based on the hypergeometric distribution (hypergeometric test) is identical to the corresponding one-tailed version of Fisher's exact test .

[ 6 ] Reciprocally, the p-value of a two-sided Fisher's exact test can be calculated as the sum of two appropriate hypergeometric tests (for more information see [ 7 ] ).

The test is often used to identify which sub-populations are over- or under-represented in a sample. This test has a wide range of applications. For example, a marketing group could use the test to understand their customer base by testing a set of known customers for over-representation of various demographic subgroups (e.g., women, people under 30).

Related distributions [ edit ] Let X ∼ ∼ Hypergeometric ⁡ ⁡ ( N , K , n ) {\displaystyle X\sim \operatorname {Hypergeometric} (N,K,n)} and p = K / N {\displaystyle p=K/N} .

If n = 1 {\displaystyle n=1} then X {\displaystyle X} has a Bernoulli distribution with parameter p {\displaystyle p} .

Let Y {\displaystyle Y} have a binomial distribution with parameters n {\displaystyle n} and p {\displaystyle p} ; this models the number of successes in the analogous sampling problem with replacement.  If N {\displaystyle N} and K {\displaystyle K} are large compared to n {\displaystyle n} , and p {\displaystyle p} is not close to 0 or 1, then X {\displaystyle X} and Y {\displaystyle Y} have similar distributions, i.e., P ( X ≤ ≤ k ) ≈ ≈ P ( Y ≤ ≤ k ) {\displaystyle P(X\leq k)\approx P(Y\leq k)} .

If n {\displaystyle n} is large, N {\displaystyle N} and K {\displaystyle K} are large compared to n {\displaystyle n} , and p {\displaystyle p} is not close to 0 or 1, then P ( X ≤ ≤ k ) ≈ ≈ Φ Φ ( k − − n p n p ( 1 − − p ) ) {\displaystyle P(X\leq k)\approx \Phi \left({\frac {k-np}{\sqrt {np(1-p)}}}\right)} where Φ Φ {\displaystyle \Phi } is the standard normal distribution function If the probabilities of drawing a green or red marble are not equal (e.g. because green marbles are bigger/easier to grasp than red marbles) then X {\displaystyle X} has a noncentral hypergeometric distribution The beta-binomial distribution is a conjugate prior for the hypergeometric distribution.

The following table describes four distributions related to the number of successes in a sequence of draws: With replacements No replacements Given number of draws binomial distribution hypergeometric distribution Given number of failures negative binomial distribution negative hypergeometric distribution Multivariate hypergeometric distribution [ edit ] Multivariate hypergeometric distribution Parameters c ∈ ∈ N + = { 1 , 2 , … … } {\displaystyle c\in \mathbb {N} _{+}=\lbrace 1,2,\ldots \rbrace } ( K 1 , … … , K c ) ∈ ∈ N c {\displaystyle (K_{1},\ldots ,K_{c})\in \mathbb {N} ^{c}} N = ∑ ∑ i = 1 c K i {\displaystyle N=\sum _{i=1}^{c}K_{i}} n ∈ ∈ { 0 , … … , N } {\displaystyle n\in \lbrace 0,\ldots ,N\rbrace } Support { k ∈ ∈ ( Z 0 + ) c : ∀ ∀ i k i ≤ ≤ K i , ∑ ∑ i = 1 c k i = n } {\displaystyle \left\{\mathbf {k} \in \left(\mathbb {Z} _{0+}\right)^{c}\,:\,\forall i\ k_{i}\leq K_{i},\sum _{i=1}^{c}k_{i}=n\right\}} PMF ∏ ∏ i = 1 c ( K i k i ) ( N n ) {\displaystyle {\frac {\prod \limits _{i=1}^{c}{\binom {K_{i}}{k_{i}}}}{\binom {N}{n}}}} Mean E ⁡ ⁡ ( k i ) = n K i N {\displaystyle \operatorname {E} (k_{i})=n{\frac {K_{i}}{N}}} Variance Var ⁡ ⁡ ( k i ) = n N − − n N − − 1 K i N ( 1 − − K i N ) {\displaystyle \operatorname {Var} (k_{i})=n{\frac {N-n}{N-1}}\;{\frac {K_{i}}{N}}\left(1-{\frac {K_{i}}{N}}\right)} Cov ⁡ ⁡ ( k i , k j ) = − − n N − − n N − − 1 K i N K j N , i ≠ ≠ j {\displaystyle \operatorname {Cov} (k_{i},k_{j})=-n{\frac {N-n}{N-1}}\;{\frac {K_{i}}{N}}{\frac {K_{j}}{N}},i\neq j} Corr ⁡ ⁡ ( k i , k j ) = − − K i K j ( N − − K i ) ( N − − K j ) {\displaystyle \operatorname {Corr} (k_{i},k_{j})=-{\sqrt {\frac {K_{i}K_{j}}{\left(N-K_{i}\right)\left(N-K_{j}\right)}}}} The model of an urn with green and red marbles can be extended to the case where there are more than two colors of marbles. If there are K i marbles of color i in the urn and you take n marbles at random without replacement, then the number of marbles of each color in the sample ( k 1 , k 2 ,..., k c ) has the multivariate hypergeometric distribution: Pr ( X 1 = k 1 , … … , X c = k c ) = ∏ ∏ i = 1 c ( K i k i ) ( N n ) {\displaystyle \Pr(X_{1}=k_{1},\ldots ,X_{c}=k_{c})={\frac {\prod \limits _{i=1}^{c}{\binom {K_{i}}{k_{i}}}}{\binom {N}{n}}}} This has the same relationship to the multinomial distribution that the hypergeometric distribution has to the binomial distribution—the multinomial distribution is the "with-replacement" distribution and the multivariate hypergeometric is the "without-replacement" distribution.

The properties of this distribution are given in the adjacent table, [ 8 ] where c is the number of different colors and N = ∑ ∑ i = 1 c K i {\displaystyle N=\sum _{i=1}^{c}K_{i}} is the total number of marbles in the urn.

Example [ edit ] Suppose there are 5 black, 10 white, and 15 red marbles in an urn.  If six marbles are chosen without replacement, the probability that exactly two of each color are chosen is P ( 2 black , 2 white , 2 red ) = ( 5 2 ) ( 10 2 ) ( 15 2 ) ( 30 6 ) = 0.079575596816976 {\displaystyle P(2{\text{ black}},2{\text{ white}},2{\text{ red}})={{{5 \choose 2}{10 \choose 2}{15 \choose 2}} \over {30 \choose 6}}=0.079575596816976} Occurrence and applications [ edit ] Application to auditing elections [ edit ] Samples used for election audits and resulting chance of missing a problem Election audits typically test a sample of machine-counted precincts to see if recounts by hand or machine match the original counts. Mismatches result in either a report or a larger recount. The sampling rates are usually defined by law, not statistical design, so for a legally defined sample size n , what is the probability of missing a problem which is present in K precincts, such as a hack or bug? This is the probability that k = 0 .

Bugs are often obscure, and a hacker can minimize detection by affecting only a few precincts, which will still affect close elections, so a plausible scenario is for K to be on the order of 5% of N . Audits typically cover 1% to 10% of precincts (often 3%), [ 9 ] [ 10 ] [ 11 ] so they have a high chance of missing a problem. For example, if a problem is present in 5 of 100 precincts, a 3% sample has 86% probability that k = 0 so the problem would not be noticed, and only 14% probability of the problem appearing in the sample (positive k ): P ⁡ ⁡ { X = 0 } = [ ( Hack 0 ) ( N − − Hack n − − 0 ) ] [ ( N n ) ] = [ ( N − − Hack n ) ] [ ( N n ) ] = [ ( N − − Hack ) !

n !

( N − − Hack − − n ) !

] [ N !

n !

( N − − n ) !

] = [ ( N − − Hack ) !

( N − − Hack − − n ) !

] [ N !

( N − − n ) !

] = [ ( 100 − − 5 3 ) ] [ ( 100 3 ) ] = [ ( 100 − − 5 ) !

( 100 − − 5 − − 3 ) !

] [ 100 !

( 100 − − 3 ) !

] = [ 95 !

92 !

] [ 100 !

97 !

] = 95 × × 94 × × 93 100 × × 99 × × 98 = 86 % % {\displaystyle {\begin{aligned}\operatorname {\boldsymbol {\mathcal {P}}} \{\ X=0\ \}&={\frac {\ \left[\ {\binom {\text{Hack}}{0}}{\binom {N\ -\ {\text{Hack}}}{n\ -\ 0}}\ \right]\ }{\left[\ {\binom {N}{n}}\ \right]}}={\frac {\ \left[\ {\binom {N\ -\ {\text{Hack}}}{n}}\ \right]}{\ \left[\ {\binom {N}{n}}\ \right]\ }}={\frac {\ \left[\ {\frac {\ (N\ -\ {\text{Hack}})!\ }{n!(N\ -\ {\text{Hack}}-n)!}}\ \right]\ }{\left[\ {\frac {N!}{n!(N\ -\ n)!}}\ \right]}}={\frac {\ \left[\ {\frac {(N-{\text{Hack}})!}{(N\ -\ {\text{Hack}}\ -\ n)!}}\ \right]\ }{\left[\ {\frac {N!}{(N\ -\ n)!}}\ \right]}}\\[8pt]&={\frac {\ \left[\ {\binom {100-5}{3}}\ \right]\ }{\ \left[\ {\binom {100}{3}}\ \right]\ }}={\frac {\ \left[\ {\frac {(100-5)!}{(100-5-3)!}}\ \right]\ }{\left[\ {\frac {100!}{(100-3)!}}\ \right]}}={\frac {\ \left[\ {\frac {95!}{92!}}\ \right]\ }{\ \left[\ {\frac {100!}{97!}}\ \right]\ }}={\frac {\ 95\times 94\times 93\ }{100\times 99\times 98}}=86\%\end{aligned}}} The sample would need 45 precincts in order to have probability under 5% that k = 0 in the sample, and thus have probability over 95% of finding the problem: P ⁡ ⁡ { X = 0 } = [ ( 100 − − 5 45 ) ] [ ( 100 45 ) ] = [ 95 !

50 !

] [ 100 !

55 !

] = 95 × × 94 × × ⋯ ⋯ × × 51 100 × × 99 × × ⋯ ⋯ × × 56 = 55 × × 54 × × 53 × × 52 × × 51 100 × × 99 × × 98 × × 97 × × 96 = 4.6 % % .

{\displaystyle \operatorname {\boldsymbol {\mathcal {P}}} \{\ X=0\ \}={\frac {\ \left[\ {\binom {100-5}{45}}\ \right]\ }{\left[\ {\binom {100}{45}}\ \right]}}={\frac {\ \left[\ {\frac {95!}{50!}}\ \right]\ }{\left[\ {\frac {100!}{55!}}\ \right]}}={\frac {\ 95\times 94\times \cdots \times 51\ }{\ 100\times 99\times \cdots \times 56\ }}={\frac {\ 55\times 54\times 53\times 52\times 51\ }{\ 100\times 99\times 98\times 97\times 96\ }}=4.6\%~.} Application to Texas hold'em poker [ edit ] In hold'em poker players make the best hand they can combining the two cards in their hand with the 5 cards (community cards) eventually turned up on the table. The deck has 52 and there are 13 of each suit.
For this example assume a player has 2 clubs in the hand and there are 3 cards showing on the table, 2 of which are also clubs. The player would like to know the probability of one of the next 2 cards to be shown being a club to complete the flush .

(Note that the probability calculated in this example assumes no information is known about the cards in the other players' hands; however, experienced poker players may consider how the other players place their bets (check, call, raise, or fold) in considering the probability for each scenario. Strictly speaking, the approach to calculating success probabilities outlined here is accurate in a scenario where there is just one player at the table; in a multiplayer game this probability might be adjusted somewhat based on the betting play of the opponents.) There are 4 clubs showing so there are 9 clubs still unseen. There are 5 cards showing (2 in the hand and 3 on the table) so there are 52 − − 5 = 47 {\displaystyle 52-5=47} still unseen.

The probability that one of the next two cards turned is a club can be calculated using hypergeometric with k = 1 , n = 2 , K = 9 {\displaystyle k=1,n=2,K=9} and N = 47 {\displaystyle N=47} . (about 31.64%) The probability that both of the next two cards turned are clubs can be calculated using hypergeometric with k = 2 , n = 2 , K = 9 {\displaystyle k=2,n=2,K=9} and N = 47 {\displaystyle N=47} . (about 3.33%) The probability that neither of the next two cards turned are clubs can be calculated using hypergeometric with k = 0 , n = 2 , K = 9 {\displaystyle k=0,n=2,K=9} and N = 47 {\displaystyle N=47} . (about 65.03%) Application to Keno [ edit ] The hypergeometric distribution is indispensable for calculating Keno odds. In Keno, 20 balls are randomly drawn from a collection of 80 numbered balls in a container, rather like American Bingo . Prior to each draw, a player selects a certain number of spots by marking a paper form supplied for this purpose. For example, a player might play a 6-spot by marking 6 numbers, each from a range of 1 through 80 inclusive. Then (after all players have taken their forms to a cashier and been given a duplicate of their marked form, and paid their wager) 20 balls are drawn. Some of the balls drawn may match some or all of the balls selected by the player. Generally speaking, the more hits (balls drawn that match player numbers selected) the greater the payoff.

For example, if a customer bets ("plays") $1 for a 6-spot (not an uncommon example) and hits 4 out of the 6, the casino would pay out $4. Payouts can vary from one casino to the next, but $4 is a typical value here. The probability of this event is: P ( X = 4 ) = f ( 4 ; 80 , 6 , 20 ) = ( 6 4 ) ( 80 − − 6 20 − − 4 ) ( 80 20 ) ≈ ≈ 0.02853791 {\displaystyle P(X=4)=f(4;80,6,20)={{{6 \choose 4}{{80-6} \choose {20-4}}} \over {80 \choose 20}}\approx 0.02853791} Similarly, the chance for hitting 5 spots out of 6 selected is ( 6 5 ) ( 74 15 ) ( 80 20 ) ≈ ≈ 0.003095639 {\displaystyle {{{6 \choose 5}{{74} \choose {15}}} \over {80 \choose 20}}\approx 0.003095639} while a typical payout might be $88. The payout for hitting all 6 would be around $1500 (probability ≈ 0.000128985 or 7752-to-1). The only other nonzero payout might be $1 for hitting 3 numbers (i.e., you get your bet back), which has a probability near 0.129819548.

Taking the sum of products of payouts times corresponding probabilities we get an expected return of 0.70986492 or roughly 71% for a 6-spot, for a house advantage of 29%. Other spots-played have a similar expected return. This very poor return (for the player) is usually explained by the large overhead (floor space, equipment, personnel) required for the game.

See also [ edit ] Noncentral hypergeometric distributions Negative hypergeometric distribution Multinomial distribution Sampling (statistics) Generalized hypergeometric function Coupon collector's problem Geometric distribution Keno Lady tasting tea References [ edit ] Citations [ edit ] ^ Rice, John A. (2007).

Mathematical Statistics and Data Analysis (Third ed.). Duxbury Press. p. 42.

^ Pollard, David (Spring 2010).

"Symmetry" (PDF) .

Stat 330/600 course handouts . Yale University . Retrieved 2025-01-19 .

^ a b Hoeffding, Wassily (1963).

"Probability inequalities for sums of bounded random variables" (PDF) .

Journal of the American Statistical Association .

58 (301): 13– 30.

doi : 10.2307/2282952 .

JSTOR 2282952 .

.

^ a b "Another Tail of the Hypergeometric Distribution" .

wordpress.com . 8 December 2015 . Retrieved 19 March 2018 .

^ Serfling, Robert (1974). "Probability inequalities for the sum in sampling without replacement".

The Annals of Statistics .

2 (1): 39– 48.

doi : 10.1214/aos/1176342611 .

.

^ Rivals, I.; Personnaz, L.; Taing, L.; Potier, M.-C (2007).

"Enrichment or depletion of a GO category within a class of genes: which test?" .

Bioinformatics .

23 (4): 401– 407.

doi : 10.1093/bioinformatics/btl633 .

PMID 17182697 .

^ K. Preacher and N. Briggs.

"Calculation for Fisher's Exact Test: An interactive calculation tool for Fisher's exact probability test for 2 x 2 tables (interactive page)" .

^ Duan, X. G. (2021). "Better understanding of the multivariate hypergeometric distribution with implications in design-based survey sampling".

arXiv : 2101.00548 [ math.ST ].

^ Glazer, Amanda; Spertus, Jacob (10 February 2020). "Start spreading the news: New York's post-election audit has major flaws".

SSRN 3536011 .

^ "State audit laws" .

Verified Voting . 10 February 2017. Archived from the original on 4 January 2020 . Retrieved 2 April 2018 .

^ "Post-election audits" .

ncsl.org . National Conference of State Legislatures . Retrieved 2 April 2018 .

This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( August 2011 ) ( Learn how and when to remove this message ) Sources [ edit ] Berkopec, Aleš (2007).

"HyperQuick algorithm for discrete hypergeometric distribution" .

Journal of Discrete Algorithms .

5 (2): 341– 347.

doi : 10.1016/j.jda.2006.01.001 .

Skala, M. (2011). "Hypergeometric tail inequalities: ending the insanity".

arXiv : 1311.5939 [ math.PR ].

unpublished note External links [ edit ] The Hypergeometric Distribution and Binomial Approximation to a Hypergeometric Random Variable by Chris Boucher, Wolfram Demonstrations Project .

Weisstein, Eric W.

"Hypergeometric Distribution" .

MathWorld .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐4b2tr
Cached time: 20250817044143
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.640 seconds
Real time usage: 0.917 seconds
Preprocessor visited node count: 3126/1000000
Revision size: 29578/2097152 bytes
Post‐expand include size: 107053/2097152 bytes
Template argument size: 3706/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 66936/5000000 bytes
Lua time usage: 0.302/10.000 seconds
Lua memory usage: 5943241/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  506.807      1 -total
 29.99%  151.979      1 Template:Reflist
 18.77%   95.103      4 Template:Navbox
 18.50%   93.748      1 Template:ProbDistributions
 16.10%   81.586      1 Template:Short_description
 14.71%   74.546      1 Template:Cite_book
 10.73%   54.367      2 Template:Pagetype
 10.14%   51.367      1 Template:More_footnotes
  8.88%   44.998      1 Template:Ambox
  8.87%   44.969      2 Template:Infobox_probability_distribution Saved in parser cache with key enwiki:pcache:180841:|#|:idhash:canonical and timestamp 20250817044143 and revision id 1303267051. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Hypergeometric_distribution&oldid=1303267051#Multivariate_hypergeometric_distribution " Categories : Discrete distributions Factorial and binomial topics Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from August 2011 All articles lacking in-text citations This page was last edited on 30 July 2025, at 00:31 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Hypergeometric distribution 31 languages Add topic

