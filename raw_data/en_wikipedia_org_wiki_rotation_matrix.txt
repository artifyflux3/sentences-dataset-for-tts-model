Title: Rotation matrix

URL Source: https://en.wikipedia.org/wiki/Rotation_matrix

Published Time: 2004-07-25T08:16:53Z

Markdown Content:
Jump to content
Main menu
Search
Appearance
Donate
Create account
Log in
Personal tools
Toggle the table of contents
Rotation matrix
25 languages
Article
Talk
Read
Edit
View history
Tools
From Wikipedia, the free encyclopedia
	
This article needs editing to comply with Wikipedia's Manual of Style. In particular, it has problems with MOS:FORMULA - avoid mixing <math>...</math> and {{math}} in the same expression. Please help improve the content. (July 2025) (Learn how and when to remove this message)

In linear algebra, a rotation matrix is a transformation matrix that is used to perform a rotation in Euclidean space. For example, using the convention below, the matrix

𝑅
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]

rotates points in the xy plane counterclockwise through an angle θ about the origin of a two-dimensional Cartesian coordinate system. To perform the rotation on a plane point with standard coordinates v = (x, y), it should be written as a column vector, and multiplied by the matrix R:

𝑅
𝑣
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
[
𝑥


𝑦
]
=
[
𝑥
cos
⁡
𝜃
−
𝑦
sin
⁡
𝜃


𝑥
sin
⁡
𝜃
+
𝑦
cos
⁡
𝜃
]
.

If x and y are the coordinates of the endpoint of a vector with the length r and the angle 
𝜙
 with respect to the x-axis, so that 
𝑥
=
𝑟
cos
⁡
𝜙
 and 
𝑦
=
𝑟
sin
⁡
𝜙
, then the above equations become the trigonometric summation angle formulae:
𝑅
𝑣
=
𝑟
[
cos
⁡
𝜙
cos
⁡
𝜃
−
sin
⁡
𝜙
sin
⁡
𝜃


cos
⁡
𝜙
sin
⁡
𝜃
+
sin
⁡
𝜙
cos
⁡
𝜃
]
=
𝑟
[
cos
⁡
(
𝜙
+
𝜃
)


sin
⁡
(
𝜙
+
𝜃
)
]
.
Indeed, this is the trigonometric summation angle formulae in matrix form. One way to understand this is to say we have a vector at an angle 30° from the x-axis, and we wish to rotate that angle by a further 45°. We simply need to compute the vector endpoint coordinates at 75°.

The examples in this article apply to active rotations of vectors counterclockwise in a right-handed coordinate system (y counterclockwise from x) by pre-multiplication (the rotation matrix R applied on the left of the column vector v to be rotated). If any one of these is changed (such as rotating axes instead of vectors, a passive transformation), then the inverse of the example matrix should be used, which coincides with its transpose.

Since matrix multiplication has no effect on the zero vector (the coordinates of the origin), rotation matrices describe rotations about the origin. Rotation matrices provide an algebraic description of such rotations, and are used extensively for computations in geometry, physics, and computer graphics. In some literature, the term rotation is generalized to include improper rotations, characterized by orthogonal matrices with a determinant of −1 (instead of +1). An improper rotation combines a proper rotation with reflections (which invert orientation). In other cases, where reflections are not being considered, the label proper may be dropped. The latter convention is followed in this article.

Rotation matrices are square matrices, with real entries. More specifically, they can be characterized as orthogonal matrices with determinant 1; that is, a square matrix R is a rotation matrix if and only if RT = R−1 and det R = 1. The set of all orthogonal matrices of size n with determinant +1 is a representation of a group known as the special orthogonal group SO(n), one example of which is the rotation group SO(3). The set of all orthogonal matrices of size n with determinant +1 or −1 is a representation of the (general) orthogonal group O(n).

In two dimensions[edit]
A counterclockwise rotation of a vector through angle θ. The vector is initially aligned with the x-axis.

In two dimensions, the standard rotation matrix has the following form:

𝑅
(
𝜃
)
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
.

This rotates column vectors by means of the following matrix multiplication,

[
𝑥
′


𝑦
′
]
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
[
𝑥


𝑦
]
.

Thus, the new coordinates (x′, y′) of a point (x, y) after rotation are

𝑥
′
	
=
𝑥
cos
⁡
𝜃
−
𝑦
sin
⁡
𝜃


𝑦
′
	
=
𝑥
sin
⁡
𝜃
+
𝑦
cos
⁡
𝜃
.
Examples[edit]

For example, when the vector (initially aligned with the x-axis of the Cartesian coordinate system)

𝑥
^
=
[
1


0
]

is rotated by an angle θ, its new coordinates are

[
cos
⁡
𝜃


sin
⁡
𝜃
]
,

and when the vector (initially aligned with the y-axis of the coordinate system)

𝑦
^
=
[
0


1
]

is rotated by an angle θ, its new coordinates are

[
−
sin
⁡
𝜃


cos
⁡
𝜃
]
.
Direction[edit]

The direction of vector rotation is counterclockwise if θ is positive (e.g. 90°), and clockwise if θ is negative (e.g. −90°) for 
𝑅
(
𝜃
)
. Thus the clockwise rotation matrix is found as (by replacing θ with -θ and using the trigonometric symmetry of 
sin
⁡
(
−
𝜃
)
=
−
sin
⁡
(
𝜃
)
 and 
cos
⁡
(
−
𝜃
)
=
cos
⁡
(
𝜃
)
)

𝑅
(
−
𝜃
)
=
[
cos
⁡
𝜃
	
sin
⁡
𝜃


−
sin
⁡
𝜃
	
cos
⁡
𝜃
]
.

An alternative convention uses rotating axes (instead of rotating a vector),[1] and the above matrices also represent a rotation of the axes clockwise through an angle θ.

The two-dimensional case is the only non-trivial case where the rotation matrices group is commutative; it does not matter in which order rotations are multiply performed. For the 3-dimensional case, for example, a different order of multiple rotations gives a different result. (E.g., rotating a cell phone along z-axis then y-axis is not equal to rotations along the y-axis then z-axis.)

Non-standard orientation of the coordinate system[edit]
A rotation through angle θ with non-standard axes.

If a standard right-handed Cartesian coordinate system is used, with the x-axis to the right and the y-axis up, the rotation R(θ) is counterclockwise. If a left-handed Cartesian coordinate system is used, with x directed to the right but y directed down, R(θ) is clockwise. Such non-standard orientations are rarely used in mathematics but are common in 2D computer graphics, which often have the origin in the top left corner and the y-axis down the screen or page.[2]

See below for other alternative conventions which may change the sense of the rotation produced by a rotation matrix.

Common 2D rotations[edit]

Matrices

[
0
	
−
1


1
	
0
]
,
[
−
1
	
0


0
	
−
1
]
,
[
0
	
1


−
1
	
0
]

are 2D rotation matrices corresponding to counter-clockwise rotations of respective angles of 90°, 180°, and 270°.

Relationship with complex plane[edit]

The matrices of the shape
[
𝑥
	
−
𝑦


𝑦
	
𝑥
]
form a ring, since their set is closed under addition and multiplication. Since
[
0
	
−
1


1
	
0
]
2
 
=
 
[
−
1
	
0


0
	
−
1
]
 
=
−
𝐼
(where 
𝐼
 is the identity matrix), the map

[
𝑥
	
−
𝑦


𝑦
	
𝑥
]
=
𝑥
[
1
	
0


0
	
1
]
+
𝑦
[
0
	
−
1


1
	
0
]
↦
𝑥
+
𝑖
𝑦

(where 
[
0
	
−
1


1
	
0
]
 corresponds to 
𝑖
) is a ring isomorphism from this ring to the field of the complex numbers ⁠
𝐶
⁠ (incidentally, this shows that this ring is a field). Under this isomorphism, the rotation matrices 
[
cos
⁡
𝑡
	
−
sin
⁡
𝑡


sin
⁡
𝑡
	
cos
⁡
𝑡
]
=
cos
⁡
𝑡
[
1
	
0


0
	
1
]
+
sin
⁡
𝑡
[
0
	
−
1


1
	
0
]
correspond to the circle of the unit complex numbers, the complex numbers of modulus 1, since 
cos
⁡
𝑡
2
+
sin
⁡
𝑡
2
=
1
. As a result, the following equality holds,
𝑒
𝑖
𝑡
=
cos
⁡
𝑡
+
𝑖
sin
⁡
𝑡
=
cos
⁡
𝑡
[
1
	
0


0
	
1
]
+
sin
⁡
𝑡
[
0
	
−
1


1
	
0
]
=
(
cos
⁡
𝑡
	
−
sin
⁡
𝑡


sin
⁡
𝑡
	
cos
⁡
𝑡
)
where the first equality is Euler's formula, the matrix 
𝐼
=
[
1
	
0


0
	
1
]
corresponds to 1, and the matrix 
[
0
	
−
1


1
	
0
]
 corresponds to the imaginary unit 
𝑖
.

If one identifies 
𝑅
2
 with 
𝐶
 through the linear isomorphism 
(
𝑎
,
𝑏
)
↦
𝑎
+
𝑖
𝑏
, where 
(
𝑎
,
𝑏
)
∈
𝑅
2
 and 
𝑎
+
𝑖
𝑏
∈
𝐶
, the action of a matrix 
[
𝑥
	
−
𝑦


𝑦
	
𝑥
]
 on a vector 
(
𝑎
,
𝑏
)
 corresponds to multiplication on the complex number 
𝑎
+
𝑖
𝑏
 by x + iy. In other words, a vector rotation corresponds to multiplication on a complex number (corresponding to the vector being rotated) by a complex number of modulus 1 (corresponding to the rotation matrix).

In three dimensions[edit]
See also: Rotation formalisms in three dimensions
Basic 3D rotations[edit]

A basic 3D rotation (also called elemental rotation) is a rotation about one of the axes of a coordinate system. The following three basic rotation matrices rotate vectors by an angle θ about the x-, y-, or z-axis, in three dimensions, using the right-hand rule—which codifies their alternating signs.[3] Notice that the right-hand rule only works when multiplying 
𝑅
⋅
𝑥
→
. (The same matrices can also represent a clockwise rotation of the axes keeping the vectors unchanged.[nb 1])




𝑅
𝑥
(
𝜃
)
	
=
[
1
	
0
	
0


0
	
cos
⁡
𝜃
	
−
sin
⁡
𝜃


0
	
sin
⁡
𝜃
	
cos
⁡
𝜃
]


𝑅
𝑦
(
𝜃
)
	
=
[
cos
⁡
𝜃
	
0
	
sin
⁡
𝜃


0
	
1
	
0


−
sin
⁡
𝜃
	
0
	
cos
⁡
𝜃
]


𝑅
𝑧
(
𝜃
)
	
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃
	
0


sin
⁡
𝜃
	
cos
⁡
𝜃
	
0


0
	
0
	
1
]

For column vectors, each of these basic vector rotations appears counterclockwise when the axis about which they occur points toward the observer, the coordinate system is right-handed, and the angle θ is positive. Rz, for instance, would rotate toward the y-axis a vector aligned with the x-axis, as can easily be checked by operating with Rz on the vector (1,0,0):

𝑅
𝑧
(
90
∘
)
[
1


0


0
]
=
[
cos
⁡
90
∘
	
−
sin
⁡
90
∘
	
0


sin
⁡
90
∘
	
cos
⁡
90
∘
	
0


0
	
0
	
1
]
[
1


0


0
]
=
[
0
	
−
1
	
0


1
	
0
	
0


0
	
0
	
1
]
[
1


0


0
]
=
[
0


1


0
]

This is similar to the rotation produced by the above-mentioned two-dimensional rotation matrix. See below for alternative conventions which may apparently or actually invert the sense of the rotation produced by these matrices.

General 3D rotations[edit]

Other 3D rotation matrices can be obtained from these three using matrix multiplication. For example, the product

𝑅
=
𝑅
𝑧
(
𝛼
)
𝑅
𝑦
(
𝛽
)
𝑅
𝑥
(
𝛾
)
	
=
[
cos
⁡
𝛼
	
−
sin
⁡
𝛼
	
0


sin
⁡
𝛼
	
cos
⁡
𝛼
	
0


0
	
0
	
1
]
yaw
[
cos
⁡
𝛽
	
0
	
sin
⁡
𝛽


0
	
1
	
0


−
sin
⁡
𝛽
	
0
	
cos
⁡
𝛽
]
pitch
[
1
	
0
	
0


0
	
cos
⁡
𝛾
	
−
sin
⁡
𝛾


0
	
sin
⁡
𝛾
	
cos
⁡
𝛾
]
roll

	
=
[
cos
⁡
𝛼
cos
⁡
𝛽
	
cos
⁡
𝛼
sin
⁡
𝛽
sin
⁡
𝛾
−
sin
⁡
𝛼
cos
⁡
𝛾
	
cos
⁡
𝛼
sin
⁡
𝛽
cos
⁡
𝛾
+
sin
⁡
𝛼
sin
⁡
𝛾


sin
⁡
𝛼
cos
⁡
𝛽
	
sin
⁡
𝛼
sin
⁡
𝛽
sin
⁡
𝛾
+
cos
⁡
𝛼
cos
⁡
𝛾
	
sin
⁡
𝛼
sin
⁡
𝛽
cos
⁡
𝛾
−
cos
⁡
𝛼
sin
⁡
𝛾


−
sin
⁡
𝛽
	
cos
⁡
𝛽
sin
⁡
𝛾
	
cos
⁡
𝛽
cos
⁡
𝛾
]

represents a rotation whose yaw, pitch, and roll angles are α, β and γ, respectively. More formally, it is an intrinsic rotation whose Tait–Bryan angles are α, β, γ, about axes z, y, x, respectively. Similarly, the product



𝑅
=
𝑅
𝑥
(
𝛾
)
𝑅
𝑦
(
𝛽
)
𝑅
𝑧
(
𝛼
)
	
=
[
cos
⁡
𝛾
	
−
sin
⁡
𝛾
	
0


sin
⁡
𝛾
	
cos
⁡
𝛾
	
0


0
	
0
	
1
]
roll
[
cos
⁡
𝛽
	
0
	
sin
⁡
𝛽


0
	
1
	
0


−
sin
⁡
𝛽
	
0
	
cos
⁡
𝛽
]
pitch
[
1
	
0
	
0


0
	
cos
⁡
𝛼
	
−
sin
⁡
𝛼


0
	
sin
⁡
𝛼
	
cos
⁡
𝛼
]
yaw

	
=
[
cos
⁡
𝛽
cos
⁡
𝛾
	
sin
⁡
𝛼
sin
⁡
𝛽
cos
⁡
𝛾
−
cos
⁡
𝛼
sin
⁡
𝛾
	
cos
⁡
𝛼
sin
⁡
𝛽
cos
⁡
𝛾
+
sin
⁡
𝛼
sin
⁡
𝛾


cos
⁡
𝛽
sin
⁡
𝛾
	
sin
⁡
𝛼
sin
⁡
𝛽
sin
⁡
𝛾
+
cos
⁡
𝛼
cos
⁡
𝛾
	
cos
⁡
𝛼
sin
⁡
𝛽
sin
⁡
𝛾
−
sin
⁡
𝛼
cos
⁡
𝛾


−
sin
⁡
𝛽
	
sin
⁡
𝛼
cos
⁡
𝛽
	
cos
⁡
𝛼
cos
⁡
𝛽
]

represents an extrinsic rotation whose (improper) Euler angles are α, β, γ, about axes x, y, z.

These matrices produce the desired effect only if they are used to premultiply column vectors, and (since in general matrix multiplication is not commutative) only if they are applied in the specified order (see Ambiguities for more details). The order of rotation operations is from right to left; the matrix adjacent to the column vector is the first to be applied, and then the one to the left.[4]

Conversion from rotation matrix to axis–angle[edit]

Every rotation in three dimensions is defined by its axis (a vector along this axis is unchanged by the rotation), and its angle — the amount of rotation about that axis (Euler rotation theorem).

There are several methods to compute the axis and angle from a rotation matrix (see also axis–angle representation). Here, we only describe the method based on the computation of the eigenvectors and eigenvalues of the rotation matrix. It is also possible to use the trace of the rotation matrix.

Determining the axis[edit]
A rotation R around axis u can be decomposed using 3 endomorphisms P, (I − P), and Q (click to enlarge).

Given a 3 × 3 rotation matrix R, a vector u parallel to the rotation axis must satisfy

𝑅
𝑢
=
𝑢
,

since the rotation of u around the rotation axis must result in u. The equation above may be solved for u which is unique up to a scalar factor unless R is the identity matrix I.

Further, the equation may be rewritten

𝑅
𝑢
=
𝐼
𝑢
⟹
(
𝑅
−
𝐼
)
𝑢
=
0
,

which shows that u lies in the null space of R − I.

Viewed in another way, u is an eigenvector of R corresponding to the eigenvalue λ = 1. Every rotation matrix must have this eigenvalue, the other two eigenvalues being complex conjugates of each other. It follows that a general rotation matrix in three dimensions has, up to a multiplicative constant, only one real eigenvector.

One way to determine the rotation axis is by showing that:

0
	
=
𝑅
𝑇
0
+
0

	
=
𝑅
𝑇
(
𝑅
−
𝐼
)
𝑢
+
(
𝑅
−
𝐼
)
𝑢

	
=
(
𝑅
𝑇
𝑅
−
𝑅
𝑇
+
𝑅
−
𝐼
)
𝑢

	
=
(
𝐼
−
𝑅
𝑇
+
𝑅
−
𝐼
)
𝑢

	
=
(
𝑅
−
𝑅
𝑇
)
𝑢

Since (R − RT) is a skew-symmetric matrix, we can choose u such that

[
𝑢
]
×
=
(
𝑅
−
𝑅
𝑇
)
.

The matrix–vector product becomes a cross product of a vector with itself, ensuring that the result is zero:

(
𝑅
−
𝑅
𝑇
)
𝑢
=
[
𝑢
]
×
𝑢
=
𝑢
×
𝑢
=
0

Therefore, if

𝑅
=
[
𝑎
	
𝑏
	
𝑐


𝑑
	
𝑒
	
𝑓


𝑔
	
ℎ
	
𝑖
]
,

then

𝑢
=
[
ℎ
−
𝑓


𝑐
−
𝑔


𝑑
−
𝑏
]
.

The magnitude of u computed this way is ‖u‖ = 2 sin θ, where θ is the angle of rotation.

This does not work if R is symmetric. Above, if R − RT is zero, then all subsequent steps are invalid. In this case, the angle of rotation is 0° or 180° and any nonzero column of I + R is an eigenvector of R with eigenvalue 1 because R(I + R) = R + R2 = R + RRT = I + R.[5]

Determining the angle[edit]

To find the angle of a rotation, once the axis of the rotation is known, select a vector v perpendicular to the axis. Then the angle of the rotation is the angle between v and Rv.

A more direct method, however, is to simply calculate the trace: the sum of the diagonal elements of the rotation matrix. Care should be taken to select the right sign for the angle θ to match the chosen axis:

tr
⁡
(
𝑅
)
=
1
+
2
cos
⁡
𝜃
,

from which follows that the angle's absolute value is

|
𝜃
|
=
arccos
⁡
(
tr
⁡
(
𝑅
)
−
1
2
)
.

For the rotation axis 
𝑛
=
(
𝑛
1
,
𝑛
2
,
𝑛
3
)
, you can get the correct angle[6] from

{
cos
⁡
𝜃
	
=
	
tr
⁡
(
𝑅
)
−
1
2


sin
⁡
𝜃
	
=
	
−
tr
⁡
(
𝐾
𝑛
𝑅
)
2

where

𝐾
𝑛
=
[
0
	
−
𝑛
3
	
𝑛
2


𝑛
3
	
0
	
−
𝑛
1


−
𝑛
2
	
𝑛
1
	
0
]

Rotation matrix from axis and angle[edit]

The matrix of a proper rotation R by angle θ around the axis u = (ux, uy, uz), a unit vector with u2
x + u2
y + u2
z = 1, is given by:[7] [8] [9] [10]

𝑅
=
[
𝑢
𝑥
2
(
1
−
cos
⁡
𝜃
)
+
cos
⁡
𝜃
	
𝑢
𝑥
𝑢
𝑦
(
1
−
cos
⁡
𝜃
)
−
𝑢
𝑧
sin
⁡
𝜃
	
𝑢
𝑥
𝑢
𝑧
(
1
−
cos
⁡
𝜃
)
+
𝑢
𝑦
sin
⁡
𝜃


𝑢
𝑥
𝑢
𝑦
(
1
−
cos
⁡
𝜃
)
+
𝑢
𝑧
sin
⁡
𝜃
	
𝑢
𝑦
2
(
1
−
cos
⁡
𝜃
)
+
cos
⁡
𝜃
	
𝑢
𝑦
𝑢
𝑧
(
1
−
cos
⁡
𝜃
)
−
𝑢
𝑥
sin
⁡
𝜃


𝑢
𝑥
𝑢
𝑧
(
1
−
cos
⁡
𝜃
)
−
𝑢
𝑦
sin
⁡
𝜃
	
𝑢
𝑦
𝑢
𝑧
(
1
−
cos
⁡
𝜃
)
+
𝑢
𝑥
sin
⁡
𝜃
	
𝑢
𝑧
2
(
1
−
cos
⁡
𝜃
)
+
cos
⁡
𝜃
]
.

A derivation of this matrix from first principles can be found in section 9.2 here.[11] The basic idea to derive this matrix is dividing the problem into few known simple steps.

First rotate the given axis and the point such that the axis lies in one of the coordinate planes (xy, yz or zx)
Then rotate the given axis and the point such that the axis is aligned with one of the two coordinate axes for that particular coordinate plane (x, y or z)
Use one of the fundamental rotation matrices to rotate the point depending on the coordinate axis with which the rotation axis is aligned.
Reverse rotate the axis-point pair such that it attains the final configuration as that was in step 2 (Undoing step 2)
Reverse rotate the axis-point pair which was done in step 1 (undoing step 1)

This can be written more concisely as [12]

𝑅
=
(
cos
⁡
𝜃
)
𝐼
+
(
sin
⁡
𝜃
)
[
𝑢
]
×
+
(
1
−
cos
⁡
𝜃
)
(
𝑢
⊗
𝑢
)
,

where [u]× is the cross product matrix of u; the expression u ⊗ u is the outer product, and I is the identity matrix. Alternatively, the matrix entries are:

𝑅
𝑗
𝑘
=
{
cos
2
⁡
𝜃
2
+
sin
2
⁡
𝜃
2
(
2
𝑢
𝑗
2
−
1
)
,
	
if 
𝑗
=
𝑘


2
𝑢
𝑗
𝑢
𝑘
sin
2
⁡
𝜃
2
−
𝜀
𝑗
𝑘
𝑙
𝑢
𝑙
sin
⁡
𝜃
,
	
if 
𝑗
≠
𝑘

where εjkl is the Levi-Civita symbol with ε123 = 1. This is a matrix form of Rodrigues' rotation formula, (or the equivalent, differently parametrized Euler–Rodrigues formula) with[nb 2]

𝑢
⊗
𝑢
=
𝑢
𝑢
𝑇
=
[
𝑢
𝑥
2
	
𝑢
𝑥
𝑢
𝑦
	
𝑢
𝑥
𝑢
𝑧


𝑢
𝑥
𝑢
𝑦
	
𝑢
𝑦
2
	
𝑢
𝑦
𝑢
𝑧


𝑢
𝑥
𝑢
𝑧
	
𝑢
𝑦
𝑢
𝑧
	
𝑢
𝑧
2
]
,
[
𝑢
]
×
=
[
0
	
−
𝑢
𝑧
	
𝑢
𝑦


𝑢
𝑧
	
0
	
−
𝑢
𝑥


−
𝑢
𝑦
	
𝑢
𝑥
	
0
]
.

In 
𝑅
3
 the rotation of a vector x around the axis u by an angle θ can be written as:

𝑅
𝑢
(
𝜃
)
𝑥
=
𝑢
(
𝑢
⋅
𝑥
)
+
cos
⁡
(
𝜃
)
(
𝑢
×
𝑥
)
×
𝑢
+
sin
⁡
(
𝜃
)
(
𝑢
×
𝑥
)

or equivalently:

𝑅
𝑢
(
𝜃
)
𝑥
=
𝑥
cos
⁡
(
𝜃
)
+
𝑢
(
𝑥
⋅
𝑢
)
(
1
−
cos
⁡
(
𝜃
)
)
−
𝑥
×
𝑢
sin
⁡
𝜃

This can also be written in tensor notation as:[13]

(
𝑅
𝑢
(
𝜃
)
𝑥
)
𝑖
=
(
𝑅
𝑢
(
𝜃
)
)
𝑖
𝑗
𝑥
𝑗
with
(
𝑅
𝑢
(
𝜃
)
)
𝑖
𝑗
=
𝛿
𝑖
𝑗
cos
⁡
(
𝜃
)
+
𝑢
𝑖
𝑢
𝑗
(
1
−
cos
⁡
(
𝜃
)
)
−
sin
⁡
𝜃
𝜀
𝑖
𝑗
𝑘
𝑢
𝑘

If the 3D space is right-handed and θ > 0, this rotation will be counterclockwise when u points towards the observer (Right-hand rule). Explicitly, with 
(
𝛼
,
𝛽
,
𝑢
)
 a right-handed orthonormal basis,

𝑅
𝑢
(
𝜃
)
𝛼
=
cos
⁡
(
𝜃
)
𝛼
+
sin
⁡
(
𝜃
)
𝛽
,
𝑅
𝑢
(
𝜃
)
𝛽
=
−
sin
⁡
(
𝜃
)
𝛼
+
cos
⁡
(
𝜃
)
𝛽
,
𝑅
𝑢
(
𝜃
)
𝑢
=
𝑢
.

Note the striking merely apparent differences to the equivalent Lie-algebraic formulation below.

Properties[edit]

For any n-dimensional rotation matrix R acting on 
𝑅
𝑛
,

𝑅
𝑇
=
𝑅
−
1
 (The rotation is an orthogonal matrix)

It follows that:

det
𝑅
=
±
1

A rotation is termed proper if det R = 1, and improper (or a roto-reflection) if det R = –1. For even dimensions n = 2k, the n eigenvalues λ of a proper rotation occur as pairs of complex conjugates which are roots of unity: λ = e±iθj for j = 1, ..., k, which is real only for λ = ±1. Therefore, there may be no vectors fixed by the rotation (λ = 1), and thus no axis of rotation. Any fixed eigenvectors occur in pairs, and the axis of rotation is an even-dimensional subspace.

For odd dimensions n = 2k + 1, a proper rotation R will have an odd number of eigenvalues, with at least one λ = 1 and the axis of rotation will be an odd dimensional subspace. Proof:

det
(
𝑅
−
𝐼
)
	
=
det
(
𝑅
𝑇
)
det
(
𝑅
−
𝐼
)
=
det
(
𝑅
𝑇
𝑅
−
𝑅
𝑇
)
=
det
(
𝐼
−
𝑅
𝑇
)

	
=
det
(
𝐼
−
𝑅
)
=
(
−
1
)
𝑛
det
(
𝑅
−
𝐼
)
=
−
det
(
𝑅
−
𝐼
)
.

Here I is the identity matrix, and we use det(RT) = det(R) = 1, as well as (−1)n = −1 since n is odd. Therefore, det(R – I) = 0, meaning there is a nonzero vector v with (R – I)v = 0, that is Rv = v, a fixed eigenvector. There may also be pairs of fixed eigenvectors in the even-dimensional subspace orthogonal to v, so the total dimension of fixed eigenvectors is odd.

For example, in 2-space n = 2, a rotation by angle θ has eigenvalues λ = eiθ and λ = e−iθ, so there is no axis of rotation except when θ = 0, the case of the null rotation. In 3-space n = 3, the axis of a non-null proper rotation is always a unique line, and a rotation around this axis by angle θ has eigenvalues λ = 1, eiθ, e−iθ. In 4-space n = 4, the four eigenvalues are of the form e±iθ, e±iφ. The null rotation has θ = φ = 0. The case of θ = 0, φ ≠ 0 is called a simple rotation, with two unit eigenvalues forming an axis plane, and a two-dimensional rotation orthogonal to the axis plane. Otherwise, there is no axis plane. The case of θ = φ is called an isoclinic rotation, having eigenvalues e±iθ repeated twice, so every vector is rotated through an angle θ.

The trace of a rotation matrix is equal to the sum of its eigenvalues. For n = 2, a rotation by angle θ has trace 2 cos θ. For n = 3, a rotation around any axis by angle θ has trace 1 + 2 cos θ. For n = 4, and the trace is 2(cos θ + cos φ), which becomes 4 cos θ for an isoclinic rotation.

Examples[edit]
The 2 × 2 rotation matrix
𝑄
=
[
0
	
1


−
1
	
0
]
corresponds to a 90° planar rotation clockwise about the origin.
The transpose of the 2 × 2 matrix
𝑀
=
[
0.936
	
0.352


0.352
	
−
0.936
]
is its inverse, but since its determinant is −1, this is not a proper rotation matrix; it is a reflection across the line 11y = 2x.
The 3 × 3 rotation matrix
𝑄
=
[
1
	
0
	
0


0
	
3
2
	
1
2


0
	
−
1
2
	
3
2
]
=
[
1
	
0
	
0


0
	
cos
⁡
30
∘
	
sin
⁡
30
∘


0
	
−
sin
⁡
30
∘
	
cos
⁡
30
∘
]
corresponds to a −30° rotation around the x-axis in three-dimensional space.
The 3 × 3 rotation matrix
𝑄
=
[
0.36
	
0.48
	
−
0.80


−
0.80
	
0.60
	
0.00


0.48
	
0.64
	
0.60
]
corresponds to a rotation of approximately −74° around the axis (−⁠
1
/
2
⁠,1,1) in three-dimensional space.
The 3 × 3 permutation matrix
𝑃
=
[
0
	
0
	
1


1
	
0
	
0


0
	
1
	
0
]
is a rotation matrix, as is the matrix of any even permutation, and rotates through 120° about the axis x = y = z.




	
The 3 × 3 matrix
𝑀
=
[
3
	
−
4
	
1


5
	
3
	
−
7


−
9
	
2
	
6
]
has determinant +1, but is not orthogonal (its transpose is not its inverse), so it is not a rotation matrix.
The 4 × 3 matrix
𝑀
=
[
0.5
	
−
0.1
	
0.7


0.1
	
0.5
	
−
0.5


−
0.7
	
0.5
	
0.5


−
0.5
	
−
0.7
	
−
0.1
]
is not square, and so cannot be a rotation matrix; yet MTM yields a 3 × 3 identity matrix (the columns are orthonormal).
The 4 × 4 matrix
𝑄
=
−
𝐼
=
[
−
1
	
0
	
0
	
0


0
	
−
1
	
0
	
0


0
	
0
	
−
1
	
0


0
	
0
	
0
	
−
1
]
describes an isoclinic rotation in four dimensions, a rotation through equal angles (180°) through two orthogonal planes.
The 5 × 5 rotation matrix
𝑄
=
[
0
	
−
1
	
0
	
0
	
0


1
	
0
	
0
	
0
	
0


0
	
0
	
−
1
	
0
	
0


0
	
0
	
0
	
−
1
	
0


0
	
0
	
0
	
0
	
1
]
rotates vectors in the plane of the first two coordinate axes 90°, rotates vectors in the plane of the next two axes 180°, and leaves the last coordinate axis unmoved.

Geometry[edit]

In Euclidean geometry, a rotation is an example of an isometry, a transformation that moves points without changing the distances between them. Rotations are distinguished from other isometries by two additional properties: they leave (at least) one point fixed, and they leave "handedness" unchanged. In contrast, a translation moves every point, a reflection exchanges left- and right-handed ordering, a glide reflection does both, and an improper rotation combines a change in handedness with a normal rotation.

If a fixed point is taken as the origin of a Cartesian coordinate system, then every point can be given coordinates as a displacement from the origin. Thus one may work with the vector space of displacements instead of the points themselves. Now suppose (p1, ..., pn) are the coordinates of the vector p from the origin O to point P. Choose an orthonormal basis for our coordinates; then the squared distance to P, by Pythagoras, is

𝑑
2
(
𝑂
,
𝑃
)
=
‖
𝑝
‖
2
=
∑
𝑟
=
1
𝑛
𝑝
𝑟
2

which can be computed using the matrix multiplication

‖
𝑝
‖
2
=
[
𝑝
1
⋯
𝑝
𝑛
]
[
𝑝
1


⋮


𝑝
𝑛
]
=
𝑝
𝑇
𝑝
.

A geometric rotation transforms lines to lines, and preserves ratios of distances between points. From these properties it can be shown that a rotation is a linear transformation of the vectors, and thus can be written in matrix form, Qp. The fact that a rotation preserves, not just ratios, but distances themselves, is stated as

𝑝
𝑇
𝑝
=
(
𝑄
𝑝
)
𝑇
(
𝑄
𝑝
)
,

or

𝑝
𝑇
𝐼
𝑝
	
=
(
𝑝
𝑇
𝑄
𝑇
)
(
𝑄
𝑝
)

	
=
𝑝
𝑇
(
𝑄
𝑇
𝑄
)
𝑝
.

Because this equation holds for all vectors, p, one concludes that every rotation matrix, Q, satisfies the orthogonality condition,

𝑄
𝑇
𝑄
=
𝐼
.

Rotations preserve handedness because they cannot change the ordering of the axes, which implies the special matrix condition,

det
𝑄
=
+
1.

Equally important, it can be shown that any matrix satisfying these two conditions acts as a rotation.

Multiplication[edit]

The inverse of a rotation matrix is its transpose, which is also a rotation matrix:

(
𝑄
𝑇
)
𝑇
(
𝑄
𝑇
)
	
=
𝑄
𝑄
𝑇
=
𝐼


det
𝑄
𝑇
	
=
det
𝑄
=
+
1.

The product of two rotation matrices is a rotation matrix:

(
𝑄
1
𝑄
2
)
𝑇
(
𝑄
1
𝑄
2
)
	
=
𝑄
2
𝑇
(
𝑄
1
𝑇
𝑄
1
)
𝑄
2
=
𝐼


det
(
𝑄
1
𝑄
2
)
	
=
(
det
𝑄
1
)
(
det
𝑄
2
)
=
+
1.

For n > 2, multiplication of n × n rotation matrices is generally not commutative.

𝑄
1
	
=
[
0
	
−
1
	
0


1
	
0
	
0


0
	
0
	
1
]
	
𝑄
2
	
=
[
0
	
0
	
1


0
	
1
	
0


−
1
	
0
	
0
]


𝑄
1
𝑄
2
	
=
[
0
	
−
1
	
0


0
	
0
	
1


−
1
	
0
	
0
]
	
𝑄
2
𝑄
1
	
=
[
0
	
0
	
1


1
	
0
	
0


0
	
1
	
0
]
.

Noting that any identity matrix is a rotation matrix, and that matrix multiplication is associative, we may summarize all these properties by saying that the n × n rotation matrices form a group, which for n > 2 is non-abelian, called a special orthogonal group, and denoted by SO(n), SO(n,R), SOn, or SOn(R), the group of n × n rotation matrices is isomorphic to the group of rotations in an n-dimensional space. This means that multiplication of rotation matrices corresponds to composition of rotations, applied in left-to-right order of their corresponding matrices.

Ambiguities[edit]
Alias and alibi rotations

The interpretation of a rotation matrix can be subject to many ambiguities.

In most cases the effect of the ambiguity is equivalent to the effect of a rotation matrix inversion (for these orthogonal matrices equivalently matrix transpose).

Alias or alibi (passive or active) transformation
The coordinates of a point P may change due to either a rotation of the coordinate system CS (alias), or a rotation of the point P (alibi). In the latter case, the rotation of P also produces a rotation of the vector v representing P. In other words, either P and v are fixed while CS rotates (alias), or CS is fixed while P and v rotate (alibi). Any given rotation can be legitimately described both ways, as vectors and coordinate systems actually rotate with respect to each other, about the same axis but in opposite directions. Throughout this article, we chose the alibi approach to describe rotations. For instance,
𝑅
(
𝜃
)
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
represents a counterclockwise rotation of a vector v by an angle θ, or a rotation of CS by the same angle but in the opposite direction (i.e. clockwise). Alibi and alias transformations are also known as active and passive transformations, respectively.
Pre-multiplication or post-multiplication
The same point P can be represented either by a column vector v or a row vector w. Rotation matrices can either pre-multiply column vectors (Rv), or post-multiply row vectors (wR). However, Rv produces a rotation in the opposite direction with respect to wR. Throughout this article, rotations produced on column vectors are described by means of a pre-multiplication. To obtain exactly the same rotation (i.e. the same final coordinates of point P), the equivalent row vector must be post-multiplied by the transpose of R (i.e. wRT).
Right- or left-handed coordinates
The matrix and the vector can be represented with respect to a right-handed or left-handed coordinate system. Throughout the article, we assumed a right-handed orientation, unless otherwise specified.
Vectors or forms
The vector space has a dual space of linear forms, and the matrix can act on either vectors or forms.
Decompositions[edit]
Independent planes[edit]

Consider the 3 × 3 rotation matrix

𝑄
=
[
0.36
	
0.48
	
−
0.80


−
0.80
	
0.60
	
0.00


0.48
	
0.64
	
0.60
]
.

If Q acts in a certain direction, v, purely as a scaling by a factor λ, then we have

𝑄
𝑣
=
𝜆
𝑣
,

so that

0
=
(
𝜆
𝐼
−
𝑄
)
𝑣
.

Thus λ is a root of the characteristic polynomial for Q,

0
	
=
det
(
𝜆
𝐼
−
𝑄
)

	
=
𝜆
3
−
39
25
𝜆
2
+
39
25
𝜆
−
1

	
=
(
𝜆
−
1
)
(
𝜆
2
−
14
25
𝜆
+
1
)
.

Two features are noteworthy. First, one of the roots (or eigenvalues) is 1, which tells us that some direction is unaffected by the matrix. For rotations in three dimensions, this is the axis of the rotation (a concept that has no meaning in any other dimension). Second, the other two roots are a pair of complex conjugates, whose product is 1 (the constant term of the quadratic), and whose sum is 2 cos θ (the negated linear term). This factorization is of interest for 3 × 3 rotation matrices because the same thing occurs for all of them. (As special cases, for a null rotation the "complex conjugates" are both 1, and for a 180° rotation they are both −1.) Furthermore, a similar factorization holds for any n × n rotation matrix. If the dimension, n, is odd, there will be a "dangling" eigenvalue of 1; and for any dimension the rest of the polynomial factors into quadratic terms like the one here (with the two special cases noted). We are guaranteed that the characteristic polynomial will have degree n and thus n eigenvalues. And since a rotation matrix commutes with its transpose, it is a normal matrix, so can be diagonalized. We conclude that every rotation matrix, when expressed in a suitable coordinate system, partitions into independent rotations of two-dimensional subspaces, at most ⁠
n
/
2
⁠ of them.

The sum of the entries on the main diagonal of a matrix is called the trace; it does not change if we reorient the coordinate system, and always equals the sum of the eigenvalues. This has the convenient implication for 2 × 2 and 3 × 3 rotation matrices that the trace reveals the angle of rotation, θ, in the two-dimensional space (or subspace). For a 2 × 2 matrix the trace is 2 cos θ, and for a 3 × 3 matrix it is 1 + 2 cos θ. In the three-dimensional case, the subspace consists of all vectors perpendicular to the rotation axis (the invariant direction, with eigenvalue 1). Thus we can extract from any 3 × 3 rotation matrix a rotation axis and an angle, and these completely determine the rotation.

Sequential angles[edit]

The constraints on a 2 × 2 rotation matrix imply that it must have the form

𝑄
=
[
𝑎
	
−
𝑏


𝑏
	
𝑎
]

with a2 + b2 = 1. Therefore, we may set a = cos θ and b = sin θ, for some angle θ. To solve for θ it is not enough to look at a alone or b alone; we must consider both together to place the angle in the correct quadrant, using a two-argument arctangent function.

Now consider the first column of a 3 × 3 rotation matrix,

[
𝑎


𝑏


𝑐
]
.

Although a2 + b2 will probably not equal 1, but some value r2 < 1, we can use a slight variation of the previous computation to find a so-called Givens rotation that transforms the column to

[
𝑟


0


𝑐
]
,

zeroing b. This acts on the subspace spanned by the x- and y-axes. We can then repeat the process for the xz-subspace to zero c. Acting on the full matrix, these two rotations produce the schematic form

𝑄
𝑥
𝑧
𝑄
𝑥
𝑦
𝑄
=
[
1
	
0
	
0


0
	
∗
	
∗


0
	
∗
	
∗
]
.

Shifting attention to the second column, a Givens rotation of the yz-subspace can now zero the z value. This brings the full matrix to the form

𝑄
𝑦
𝑧
𝑄
𝑥
𝑧
𝑄
𝑥
𝑦
𝑄
=
[
1
	
0
	
0


0
	
1
	
0


0
	
0
	
1
]
,

which is an identity matrix. Thus we have decomposed Q as

𝑄
=
𝑄
𝑥
𝑦
−
1
𝑄
𝑥
𝑧
−
1
𝑄
𝑦
𝑧
−
1
.

An n × n rotation matrix will have (n − 1) + (n − 2) + ⋯ + 2 + 1, or

∑
𝑘
=
1
𝑛
−
1
𝑘
=
1
2
𝑛
(
𝑛
−
1
)

entries below the diagonal to zero. We can zero them by extending the same idea of stepping through the columns with a series of rotations in a fixed sequence of planes. We conclude that the set of n × n rotation matrices, each of which has n2 entries, can be parameterized by ⁠
1
/
2
⁠n(n − 1) angles.

xzxw	xzyw	xyxw	xyzw
yxyw	yxzw	yzyw	yzxw
zyzw	zyxw	zxzw	zxyw
xzxb	yzxb	xyxb	zyxb
yxyb	zxyb	yzyb	xzyb
zyzb	xyzb	zxzb	yxzb

In three dimensions this restates in matrix form an observation made by Euler, so mathematicians call the ordered sequence of three angles Euler angles. However, the situation is somewhat more complicated than we have so far indicated. Despite the small dimension, we actually have considerable freedom in the sequence of axis pairs we use; and we also have some freedom in the choice of angles. Thus we find many different conventions employed when three-dimensional rotations are parameterized for physics, or medicine, or chemistry, or other disciplines. When we include the option of world axes or body axes, 24 different sequences are possible. And while some disciplines call any sequence Euler angles, others give different names (Cardano, Tait–Bryan, roll-pitch-yaw) to different sequences.

One reason for the large number of options is that, as noted previously, rotations in three dimensions (and higher) do not commute. If we reverse a given sequence of rotations, we get a different outcome. This also implies that we cannot compose two rotations by adding their corresponding angles. Thus Euler angles are not vectors, despite a similarity in appearance as a triplet of numbers.

Nested dimensions[edit]

A 3 × 3 rotation matrix such as

𝑄
3
×
3
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃
	
0


sin
⁡
𝜃
	
cos
⁡
𝜃
	
0


0
	
0
	
1
]

suggests a 2 × 2 rotation matrix,

𝑄
2
×
2
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃


sin
⁡
𝜃
	
cos
⁡
𝜃
]
,

is embedded in the upper left corner:

𝑄
3
×
3
=
[
𝑄
2
×
2
	
0


0
𝑇
	
1
]
.

This is no illusion; not just one, but many, copies of n-dimensional rotations are found within (n + 1)-dimensional rotations, as subgroups. Each embedding leaves one direction fixed, which in the case of 3 × 3 matrices is the rotation axis. For example, we have

𝑄
𝑥
(
𝜃
)
	
=
[
1
	
0
	
0


0
	
cos
⁡
𝜃
	
−
sin
⁡
𝜃


0
	
sin
⁡
𝜃
	
cos
⁡
𝜃
]
,


𝑄
𝑦
(
𝜃
)
	
=
[
cos
⁡
𝜃
	
0
	
sin
⁡
𝜃


0
	
1
	
0


−
sin
⁡
𝜃
	
0
	
cos
⁡
𝜃
]
,


𝑄
𝑧
(
𝜃
)
	
=
[
cos
⁡
𝜃
	
−
sin
⁡
𝜃
	
0


sin
⁡
𝜃
	
cos
⁡
𝜃
	
0


0
	
0
	
1
]
,

fixing the x-axis, the y-axis, and the z-axis, respectively. The rotation axis need not be a coordinate axis; if u = (x,y,z) is a unit vector in the desired direction, then

𝑄
𝑢
(
𝜃
)
	
=
[
0
	
−
𝑧
	
𝑦


𝑧
	
0
	
−
𝑥


−
𝑦
	
𝑥
	
0
]
sin
⁡
𝜃
+
(
𝐼
−
𝑢
𝑢
𝑇
)
cos
⁡
𝜃
+
𝑢
𝑢
𝑇

	
=
[
(
1
−
𝑥
2
)
𝑐
𝜃
+
𝑥
2
	
−
𝑧
𝑠
𝜃
−
𝑥
𝑦
𝑐
𝜃
+
𝑥
𝑦
	
𝑦
𝑠
𝜃
−
𝑥
𝑧
𝑐
𝜃
+
𝑥
𝑧


𝑧
𝑠
𝜃
−
𝑥
𝑦
𝑐
𝜃
+
𝑥
𝑦
	
(
1
−
𝑦
2
)
𝑐
𝜃
+
𝑦
2
	
−
𝑥
𝑠
𝜃
−
𝑦
𝑧
𝑐
𝜃
+
𝑦
𝑧


−
𝑦
𝑠
𝜃
−
𝑥
𝑧
𝑐
𝜃
+
𝑥
𝑧
	
𝑥
𝑠
𝜃
−
𝑦
𝑧
𝑐
𝜃
+
𝑦
𝑧
	
(
1
−
𝑧
2
)
𝑐
𝜃
+
𝑧
2
]

	
=
[
𝑥
2
(
1
−
𝑐
𝜃
)
+
𝑐
𝜃
	
𝑥
𝑦
(
1
−
𝑐
𝜃
)
−
𝑧
𝑠
𝜃
	
𝑥
𝑧
(
1
−
𝑐
𝜃
)
+
𝑦
𝑠
𝜃


𝑥
𝑦
(
1
−
𝑐
𝜃
)
+
𝑧
𝑠
𝜃
	
𝑦
2
(
1
−
𝑐
𝜃
)
+
𝑐
𝜃
	
𝑦
𝑧
(
1
−
𝑐
𝜃
)
−
𝑥
𝑠
𝜃


𝑥
𝑧
(
1
−
𝑐
𝜃
)
−
𝑦
𝑠
𝜃
	
𝑦
𝑧
(
1
−
𝑐
𝜃
)
+
𝑥
𝑠
𝜃
	
𝑧
2
(
1
−
𝑐
𝜃
)
+
𝑐
𝜃
]
,

where cθ = cos θ, sθ = sin θ, is a rotation by angle θ leaving axis u fixed.

A direction in (n + 1)-dimensional space will be a unit magnitude vector, which we may consider a point on a generalized sphere, Sn. Thus it is natural to describe the rotation group SO(n + 1) as combining SO(n) and Sn. A suitable formalism is the fiber bundle,

𝑆
𝑂
(
𝑛
)
↪
𝑆
𝑂
(
𝑛
+
1
)
→
𝑆
𝑛
,

where for every direction in the base space, Sn, the fiber over it in the total space, SO(n + 1), is a copy of the fiber space, SO(n), namely the rotations that keep that direction fixed.

Thus we can build an n × n rotation matrix by starting with a 2 × 2 matrix, aiming its fixed axis on S2 (the ordinary sphere in three-dimensional space), aiming the resulting rotation on S3, and so on up through Sn−1. A point on Sn can be selected using n numbers, so we again have ⁠
1
/
2
⁠n(n − 1) numbers to describe any n × n rotation matrix.

In fact, we can view the sequential angle decomposition, discussed previously, as reversing this process. The composition of n − 1 Givens rotations brings the first column (and row) to (1, 0, ..., 0), so that the remainder of the matrix is a rotation matrix of dimension one less, embedded so as to leave (1, 0, ..., 0) fixed.

Skew parameters via Cayley's formula[edit]
Main articles: Cayley transform and Skew-symmetric matrix

When an n × n rotation matrix Q, does not include a −1 eigenvalue, thus none of the planar rotations which it comprises are 180° rotations, then Q + I is an invertible matrix. Most rotation matrices fit this description, and for them it can be shown that (Q − I)(Q + I)−1 is a skew-symmetric matrix, A. Thus AT = −A; and since the diagonal is necessarily zero, and since the upper triangle determines the lower one, A contains ⁠
1
/
2
⁠n(n − 1) independent numbers.

Conveniently, I − A is invertible whenever A is skew-symmetric; thus we can recover the original matrix using the Cayley transform,

𝐴
↦
(
𝐼
+
𝐴
)
(
𝐼
−
𝐴
)
−
1
,

which maps any skew-symmetric matrix A to a rotation matrix. In fact, aside from the noted exceptions, we can produce any rotation matrix in this way. Although in practical applications we can hardly afford to ignore 180° rotations, the Cayley transform is still a potentially useful tool, giving a parameterization of most rotation matrices without trigonometric functions.

In three dimensions, for example, we have (Cayley 1846)

	
[
0
	
−
𝑧
	
𝑦


𝑧
	
0
	
−
𝑥


−
𝑦
	
𝑥
	
0
]
↦


1
1
+
𝑥
2
+
𝑦
2
+
𝑧
2
	
[
1
+
𝑥
2
−
𝑦
2
−
𝑧
2
	
2
𝑥
𝑦
−
2
𝑧
	
2
𝑦
+
2
𝑥
𝑧


2
𝑥
𝑦
+
2
𝑧
	
1
−
𝑥
2
+
𝑦
2
−
𝑧
2
	
2
𝑦
𝑧
−
2
𝑥


2
𝑥
𝑧
−
2
𝑦
	
2
𝑥
+
2
𝑦
𝑧
	
1
−
𝑥
2
−
𝑦
2
+
𝑧
2
]
.

If we condense the skew entries into a vector, (x,y,z), then we produce a 90° rotation around the x-axis for (1, 0, 0), around the y-axis for (0, 1, 0), and around the z-axis for (0, 0, 1). The 180° rotations are just out of reach; for, in the limit as x → ∞, (x, 0, 0) does approach a 180° rotation around the x axis, and similarly for other directions.

Decomposition into shears[edit]

For the 2D case, a rotation matrix can be decomposed into three shear matrices (Paeth 1986):

𝑅
(
𝜃
)
	
=
[
1
	
−
tan
⁡
𝜃
2


0
	
1
]
[
1
	
0


sin
⁡
𝜃
	
1
]
[
1
	
−
tan
⁡
𝜃
2


0
	
1
]

This is useful, for instance, in computer graphics, since shears can be implemented with fewer multiplication instructions than rotating a bitmap directly. On modern computers, this may not matter, but it can be relevant for very old or low-end microprocessors.

A rotation can also be written as two shears and a squeeze mapping (an area preserving scaling) (Daubechies & Sweldens 1998):

𝑅
(
𝜃
)
	
=
[
1
	
0


tan
⁡
𝜃
	
1
]
[
1
	
−
sin
⁡
𝜃
cos
⁡
𝜃


0
	
1
]
[
cos
⁡
𝜃
	
0


0
	
1
cos
⁡
𝜃
]
Group theory[edit]

Below follow some basic facts about the role of the collection of all rotation matrices of a fixed dimension (here mostly 3) in mathematics and particularly in physics where rotational symmetry is a requirement of every truly fundamental law (due to the assumption of isotropy of space), and where the same symmetry, when present, is a simplifying property of many problems of less fundamental nature. Examples abound in classical mechanics and quantum mechanics. Knowledge of the part of the solutions pertaining to this symmetry applies (with qualifications) to all such problems and it can be factored out of a specific problem at hand, thus reducing its complexity. A prime example – in mathematics and physics – would be the theory of spherical harmonics. Their role in the group theory of the rotation groups is that of being a representation space for the entire set of finite-dimensional irreducible representations of the rotation group SO(3). For this topic, see Rotation group SO(3) § Spherical harmonics.

The main articles listed in each subsection are referred to for more detail.

Lie group[edit]
Main articles: Special orthogonal group and Rotation group SO(3)

The n × n rotation matrices for each n form a group, the special orthogonal group, SO(n). This algebraic structure is coupled with a topological structure inherited from 
GL
𝑛
⁡
(
𝑅
)
 in such a way that the operations of multiplication and taking the inverse are analytic functions of the matrix entries. Thus SO(n) is for each n a Lie group. It is compact and connected, but not simply connected. It is also a semi-simple group, in fact a simple group with the exception SO(4).[14] The relevance of this is that all theorems and all machinery from the theory of analytic manifolds (analytic manifolds are in particular smooth manifolds) apply and the well-developed representation theory of compact semi-simple groups is ready for use.

Lie algebra[edit]
Main article: Rotation group SO(3) § Lie algebra

The Lie algebra so(n) of SO(n) is given by

𝑠
𝑜
(
𝑛
)
=
𝑜
(
𝑛
)
=
{
𝑋
∈
𝑀
𝑛
(
𝑅
)
∣
𝑋
=
−
𝑋
𝑇
}
,

and is the space of skew-symmetric matrices of dimension n, see classical group, where o(n) is the Lie algebra of O(n), the orthogonal group. For reference, the most common basis for so(3) is

𝐿
𝑥
=
[
0
	
0
	
0


0
	
0
	
−
1


0
	
1
	
0
]
,
𝐿
𝑦
=
[
0
	
0
	
1


0
	
0
	
0


−
1
	
0
	
0
]
,
𝐿
𝑧
=
[
0
	
−
1
	
0


1
	
0
	
0


0
	
0
	
0
]
.
Exponential map[edit]
Main articles: Rotation group SO(3) § Exponential map, and Matrix exponential

Connecting the Lie algebra to the Lie group is the exponential map, which is defined using the standard matrix exponential series for eA[15] For any skew-symmetric matrix A, exp(A) is always a rotation matrix.[nb 3]

An important practical example is the 3 × 3 case. In rotation group SO(3), it is shown that one can identify every A ∈ so(3) with an Euler vector ω = θu, where u = (x, y, z) is a unit magnitude vector.

By the properties of the identification 
𝑠
𝑢
(
2
)
≅
𝑅
3
, u is in the null space of A. Thus, u is left invariant by exp(A) and is hence a rotation axis.

According to Rodrigues' rotation formula on matrix form, one obtains,

exp
⁡
(
𝐴
)
	
=
exp
⁡
(
𝜃
(
𝑢
⋅
𝐿
)
)

	
=
exp
⁡
(
[
0
	
−
𝑧
𝜃
	
𝑦
𝜃


𝑧
𝜃
	
0
	
−
𝑥
𝜃


−
𝑦
𝜃
	
𝑥
𝜃
	
0
]
)

	
=
𝐼
+
sin
⁡
𝜃
 
𝑢
⋅
𝐿
+
(
1
−
cos
⁡
𝜃
)
(
𝑢
⋅
𝐿
)
2
,

where

𝑢
⋅
𝐿
=
[
0
	
−
𝑧
	
𝑦


𝑧
	
0
	
−
𝑥


−
𝑦
	
𝑥
	
0
]
.

This is the matrix for a rotation around axis u by the angle θ. For full detail, see exponential map SO(3).

Baker–Campbell–Hausdorff formula[edit]
Main articles: Baker–Campbell–Hausdorff formula and Rotation group SO(3) § Baker–Campbell–Hausdorff formula

The BCH formula provides an explicit expression for Z = log(eXeY) in terms of a series expansion of nested commutators of X and Y.[16] This general expansion unfolds as[nb 4]

𝑍
=
𝐶
(
𝑋
,
𝑌
)
=
𝑋
+
𝑌
+
1
2
[
𝑋
,
𝑌
]
+
1
12
[
𝑋
,
[
𝑋
,
𝑌
]
]
−
1
12
[
𝑌
,
[
𝑋
,
𝑌
]
]
+
⋯
.

In the 3 × 3 case, the general infinite expansion has a compact form,[17]

𝑍
=
𝛼
𝑋
+
𝛽
𝑌
+
𝛾
[
𝑋
,
𝑌
]
,

for suitable trigonometric function coefficients, detailed in the Baker–Campbell–Hausdorff formula for SO(3).

As a group identity, the above holds for all faithful representations, including the doublet (spinor representation), which is simpler. The same explicit formula thus follows straightforwardly through Pauli matrices; see the 2 × 2 derivation for SU(2). For the general n × n case, one might use Ref.[18]

Spin group[edit]
Main articles: Spin group and Rotation group SO(3) § Connection between SO(3) and SU(2)

The Lie group of n × n rotation matrices, SO(n), is not simply connected, so Lie theory tells us it is a homomorphic image of a universal covering group. Often the covering group, which in this case is called the spin group denoted by Spin(n), is simpler and more natural to work with.[19]

In the case of planar rotations, SO(2) is topologically a circle, S1. Its universal covering group, Spin(2), is isomorphic to the real line, R, under addition. Whenever angles of arbitrary magnitude are used one is taking advantage of the convenience of the universal cover. Every 2 × 2 rotation matrix is produced by a countable infinity of angles, separated by integer multiples of 2π. Correspondingly, the fundamental group of SO(2) is isomorphic to the integers, Z.

In the case of spatial rotations, SO(3) is topologically equivalent to three-dimensional real projective space, RP3. Its universal covering group, Spin(3), is isomorphic to the 3-sphere, S3. Every 3 × 3 rotation matrix is produced by two opposite points on the sphere. Correspondingly, the fundamental group of SO(3) is isomorphic to the two-element group, Z2.

We can also describe Spin(3) as isomorphic to quaternions of unit norm under multiplication, or to certain 4 × 4 real matrices, or to 2 × 2 complex special unitary matrices, namely SU(2). The covering maps for the first and the last case are given by

𝐻
⊃
{
𝑞
∈
𝐻
:
‖
𝑞
‖
=
1
}
∋
𝑤
+
𝑖
𝑥
+
𝑗
𝑦
+
𝑘
𝑧
↦
[
1
−
2
𝑦
2
−
2
𝑧
2
	
2
𝑥
𝑦
−
2
𝑧
𝑤
	
2
𝑥
𝑧
+
2
𝑦
𝑤


2
𝑥
𝑦
+
2
𝑧
𝑤
	
1
−
2
𝑥
2
−
2
𝑧
2
	
2
𝑦
𝑧
−
2
𝑥
𝑤


2
𝑥
𝑧
−
2
𝑦
𝑤
	
2
𝑦
𝑧
+
2
𝑥
𝑤
	
1
−
2
𝑥
2
−
2
𝑦
2
]
∈
S
O
(
3
)
,

and

S
U
(
2
)
∋
[
𝛼
	
𝛽


−
𝛽
¯
	
𝛼
¯
]
↦
[
1
2
(
𝛼
2
−
𝛽
2
+
𝛼
2
¯
−
𝛽
2
¯
)
	
𝑖
2
(
−
𝛼
2
−
𝛽
2
+
𝛼
2
¯
+
𝛽
2
¯
)
	
−
𝛼
𝛽
−
𝛼
¯
𝛽
¯


𝑖
2
(
𝛼
2
−
𝛽
2
−
𝛼
2
¯
+
𝛽
2
¯
)
	
𝑖
2
(
𝛼
2
+
𝛽
2
+
𝛼
2
¯
+
𝛽
2
¯
)
	
−
𝑖
(
+
𝛼
𝛽
−
𝛼
¯
𝛽
¯
)


𝛼
𝛽
¯
+
𝛼
¯
𝛽
	
𝑖
(
−
𝛼
𝛽
¯
+
𝛼
¯
𝛽
)
	
𝛼
𝛼
¯
−
𝛽
𝛽
¯
]
∈
S
O
(
3
)
.

For a detailed account of the SU(2)-covering and the quaternionic covering, see spin group SO(3).

Many features of these cases are the same for higher dimensions. The coverings are all two-to-one, with SO(n), n > 2, having fundamental group Z2. The natural setting for these groups is within a Clifford algebra. One type of action of the rotations is produced by a kind of "sandwich", denoted by qvq∗. More importantly in applications to physics, the corresponding spin representation of the Lie algebra sits inside the Clifford algebra. It can be exponentiated in the usual way to give rise to a 2-valued representation, also known as projective representation of the rotation group. This is the case with SO(3) and SU(2), where the 2-valued representation can be viewed as an "inverse" of the covering map. By properties of covering maps, the inverse can be chosen ono-to-one as a local section, but not globally.

Infinitesimal rotations[edit]
Main article: Infinitesimal rotation matrix

The matrices in the Lie algebra are not themselves rotations; the skew-symmetric matrices are derivatives, proportional differences of rotations. An actual "differential rotation", or infinitesimal rotation matrix has the form

𝐼
+
𝐴
𝑑
𝜃
,

where dθ is vanishingly small and A ∈ so(n), for instance with A = Lx,

𝑑
𝐿
𝑥
=
[
1
	
0
	
0


0
	
1
	
−
𝑑
𝜃


0
	
𝑑
𝜃
	
1
]
.

The computation rules are as usual except that infinitesimals of second order are routinely dropped. With these rules, these matrices do not satisfy all the same properties as ordinary finite rotation matrices under the usual treatment of infinitesimals.[20] It turns out that the order in which infinitesimal rotations are applied is irrelevant. To see this exemplified, consult infinitesimal rotations SO(3).

Conversions[edit]
See also: Rotation formalisms in three dimensions § Conversion formulae between formalisms

We have seen the existence of several decompositions that apply in any dimension, namely independent planes, sequential angles, and nested dimensions. In all these cases we can either decompose a matrix or construct one. We have also given special attention to 3 × 3 rotation matrices, and these warrant further attention, in both directions (Stuelpnagel 1964).

Quaternion[edit]
Main article: Quaternions and spatial rotation

Given the unit quaternion q = w + xi + yj + zk, the equivalent pre-multiplied (to be used with column vectors) 3 × 3 rotation matrix is [21]

𝑄
=
[
1
−
2
𝑦
2
−
2
𝑧
2
	
2
𝑥
𝑦
−
2
𝑧
𝑤
	
2
𝑥
𝑧
+
2
𝑦
𝑤


2
𝑥
𝑦
+
2
𝑧
𝑤
	
1
−
2
𝑥
2
−
2
𝑧
2
	
2
𝑦
𝑧
−
2
𝑥
𝑤


2
𝑥
𝑧
−
2
𝑦
𝑤
	
2
𝑦
𝑧
+
2
𝑥
𝑤
	
1
−
2
𝑥
2
−
2
𝑦
2
]
.

Now every quaternion component appears multiplied by two in a term of degree two, and if all such terms are zero what is left is an identity matrix. This leads to an efficient, robust conversion from any quaternion – whether unit or non-unit – to a 3 × 3 rotation matrix. Given:

𝑛
	
=
𝑤
×
𝑤
+
𝑥
×
𝑥
+
𝑦
×
𝑦
+
𝑧
×
𝑧


𝑠
	
=
{
0
	
if 
𝑛
=
0


2
𝑛
	
otherwise

we can calculate

𝑄
=
[
1
−
𝑠
(
𝑦
𝑦
+
𝑧
𝑧
)
	
𝑠
(
𝑥
𝑦
−
𝑤
𝑧
)
	
𝑠
(
𝑥
𝑧
+
𝑤
𝑦
)


𝑠
(
𝑥
𝑦
+
𝑤
𝑧
)
	
1
−
𝑠
(
𝑥
𝑥
+
𝑧
𝑧
)
	
𝑠
(
𝑦
𝑧
−
𝑤
𝑥
)


𝑠
(
𝑥
𝑧
−
𝑤
𝑦
)
	
𝑠
(
𝑦
𝑧
+
𝑤
𝑥
)
	
1
−
𝑠
(
𝑥
𝑥
+
𝑦
𝑦
)
]

Freed from the demand for a unit quaternion, we find that nonzero quaternions act as homogeneous coordinates for 3 × 3 rotation matrices. The Cayley transform, discussed earlier, is obtained by scaling the quaternion so that its w component is 1. For a 180° rotation around any axis, w will be zero, which explains the Cayley limitation.

The sum of the entries along the main diagonal (the trace), plus one, equals 4 − 4(x2 + y2 + z2), which is 4w2. Thus we can write the trace itself as 2w2 + 2w2 − 1; and from the previous version of the matrix we see that the diagonal entries themselves have the same form: 2x2 + 2w2 − 1, 2y2 + 2w2 − 1, and 2z2 + 2w2 − 1. So we can easily compare the magnitudes of all four quaternion components using the matrix diagonal. We can, in fact, obtain all four magnitudes using sums and square roots, and choose consistent signs using the skew-symmetric part of the off-diagonal entries:

𝑡
	
=
tr
⁡
𝑄
=
𝑄
𝑥
𝑥
+
𝑄
𝑦
𝑦
+
𝑄
𝑧
𝑧
(
the trace of 
𝑄
)


𝑟
	
=
1
+
𝑡


𝑤
	
=
1
2
𝑟


𝑥
	
=
sgn
⁡
(
𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧
)
|
1
2
1
+
𝑄
𝑥
𝑥
−
𝑄
𝑦
𝑦
−
𝑄
𝑧
𝑧
|


𝑦
	
=
sgn
⁡
(
𝑄
𝑥
𝑧
−
𝑄
𝑧
𝑥
)
|
1
2
1
−
𝑄
𝑥
𝑥
+
𝑄
𝑦
𝑦
−
𝑄
𝑧
𝑧
|


𝑧
	
=
sgn
⁡
(
𝑄
𝑦
𝑥
−
𝑄
𝑥
𝑦
)
|
1
2
1
−
𝑄
𝑥
𝑥
−
𝑄
𝑦
𝑦
+
𝑄
𝑧
𝑧
|

Alternatively, use a single square root and division

𝑡
	
=
tr
⁡
𝑄
=
𝑄
𝑥
𝑥
+
𝑄
𝑦
𝑦
+
𝑄
𝑧
𝑧


𝑟
	
=
1
+
𝑡


𝑠
	
=
1
2
𝑟


𝑤
	
=
1
2
𝑟


𝑥
	
=
(
𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧
)
𝑠


𝑦
	
=
(
𝑄
𝑥
𝑧
−
𝑄
𝑧
𝑥
)
𝑠


𝑧
	
=
(
𝑄
𝑦
𝑥
−
𝑄
𝑥
𝑦
)
𝑠

This is numerically stable so long as the trace, t, is not negative; otherwise, we risk dividing by (nearly) zero. In that case, suppose Qxx is the largest diagonal entry, so x will have the largest magnitude (the other cases are derived by cyclic permutation); then the following is safe.

𝑟
	
=
1
+
𝑄
𝑥
𝑥
−
𝑄
𝑦
𝑦
−
𝑄
𝑧
𝑧


𝑠
	
=
1
2
𝑟


𝑤
	
=
(
𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧
)
𝑠


𝑥
	
=
1
2
𝑟


𝑦
	
=
(
𝑄
𝑥
𝑦
+
𝑄
𝑦
𝑥
)
𝑠


𝑧
	
=
(
𝑄
𝑧
𝑥
+
𝑄
𝑥
𝑧
)
𝑠

If the matrix contains significant error, such as accumulated numerical error, we may construct a symmetric 4 × 4 matrix,

𝐾
=
1
3
[
𝑄
𝑥
𝑥
−
𝑄
𝑦
𝑦
−
𝑄
𝑧
𝑧
	
𝑄
𝑦
𝑥
+
𝑄
𝑥
𝑦
	
𝑄
𝑧
𝑥
+
𝑄
𝑥
𝑧
	
𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧


𝑄
𝑦
𝑥
+
𝑄
𝑥
𝑦
	
𝑄
𝑦
𝑦
−
𝑄
𝑥
𝑥
−
𝑄
𝑧
𝑧
	
𝑄
𝑧
𝑦
+
𝑄
𝑦
𝑧
	
𝑄
𝑥
𝑧
−
𝑄
𝑧
𝑥


𝑄
𝑧
𝑥
+
𝑄
𝑥
𝑧
	
𝑄
𝑧
𝑦
+
𝑄
𝑦
𝑧
	
𝑄
𝑧
𝑧
−
𝑄
𝑥
𝑥
−
𝑄
𝑦
𝑦
	
𝑄
𝑦
𝑥
−
𝑄
𝑥
𝑦


𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧
	
𝑄
𝑥
𝑧
−
𝑄
𝑧
𝑥
	
𝑄
𝑦
𝑥
−
𝑄
𝑥
𝑦
	
𝑄
𝑥
𝑥
+
𝑄
𝑦
𝑦
+
𝑄
𝑧
𝑧
]
,

and find the eigenvector, (x, y, z, w), of its largest magnitude eigenvalue. (If Q is truly a rotation matrix, that value will be 1.) The quaternion so obtained will correspond to the rotation matrix closest to the given matrix (Bar-Itzhack 2000) (Note: formulation of the cited article is post-multiplied, works with row vectors).

Polar decomposition[edit]

If the n × n matrix M is nonsingular, its columns are linearly independent vectors; thus the Gram–Schmidt process can adjust them to be an orthonormal basis. Stated in terms of numerical linear algebra, we convert M to an orthogonal matrix, Q, using QR decomposition. However, we often prefer a Q closest to M, which this method does not accomplish. For that, the tool we want is the polar decomposition (Fan & Hoffman 1955; Higham 1989).

To measure closeness, we may use any matrix norm invariant under orthogonal transformations. A convenient choice is the Frobenius norm, ‖Q − M‖F, squared, which is the sum of the squares of the element differences. Writing this in terms of the trace, Tr, our goal is,

Find Q minimizing Tr( (Q − M)T(Q − M) ), subject to QTQ = I.

Though written in matrix terms, the objective function is just a quadratic polynomial. We can minimize it in the usual way, by finding where its derivative is zero. For a 3 × 3 matrix, the orthogonality constraint implies six scalar equalities that the entries of Q must satisfy. To incorporate the constraint(s), we may employ a standard technique, Lagrange multipliers, assembled as a symmetric matrix, Y. Thus our method is:

Differentiate Tr( (Q − M)T(Q − M) + (QTQ − I)Y ) with respect to (the entries of) Q, and equate to zero.

Consider a 2 × 2 example. Including constraints, we seek to minimize

	
(
𝑄
𝑥
𝑥
−
𝑀
𝑥
𝑥
)
2
+
(
𝑄
𝑥
𝑦
−
𝑀
𝑥
𝑦
)
2
+
(
𝑄
𝑦
𝑥
−
𝑀
𝑦
𝑥
)
2
+
(
𝑄
𝑦
𝑦
−
𝑀
𝑦
𝑦
)
2

	
+
(
𝑄
𝑥
𝑥
2
+
𝑄
𝑦
𝑥
2
−
1
)
𝑌
𝑥
𝑥
+
(
𝑄
𝑥
𝑦
2
+
𝑄
𝑦
𝑦
2
−
1
)
𝑌
𝑦
𝑦
+
2
(
𝑄
𝑥
𝑥
𝑄
𝑥
𝑦
+
𝑄
𝑦
𝑥
𝑄
𝑦
𝑦
)
𝑌
𝑥
𝑦
.

Taking the derivative with respect to Qxx, Qxy, Qyx, Qyy in turn, we assemble a matrix.

2
[
𝑄
𝑥
𝑥
−
𝑀
𝑥
𝑥
+
𝑄
𝑥
𝑥
𝑌
𝑥
𝑥
+
𝑄
𝑥
𝑦
𝑌
𝑥
𝑦
	
𝑄
𝑥
𝑦
−
𝑀
𝑥
𝑦
+
𝑄
𝑥
𝑥
𝑌
𝑥
𝑦
+
𝑄
𝑥
𝑦
𝑌
𝑦
𝑦


𝑄
𝑦
𝑥
−
𝑀
𝑦
𝑥
+
𝑄
𝑦
𝑥
𝑌
𝑥
𝑥
+
𝑄
𝑦
𝑦
𝑌
𝑥
𝑦
	
𝑄
𝑦
𝑦
−
𝑀
𝑦
𝑦
+
𝑄
𝑦
𝑥
𝑌
𝑥
𝑦
+
𝑄
𝑦
𝑦
𝑌
𝑦
𝑦
]

In general, we obtain the equation

0
=
2
(
𝑄
−
𝑀
)
+
2
𝑄
𝑌
,

so that

𝑀
=
𝑄
(
𝐼
+
𝑌
)
=
𝑄
𝑆
,

where Q is orthogonal and S is symmetric. To ensure a minimum, the Y matrix (and hence S) must be positive definite. Linear algebra calls QS the polar decomposition of M, with S the positive square root of S2 = MTM.

𝑆
2
=
(
𝑄
𝑇
𝑀
)
𝑇
(
𝑄
𝑇
𝑀
)
=
𝑀
𝑇
𝑄
𝑄
𝑇
𝑀
=
𝑀
𝑇
𝑀

When M is non-singular, the Q and S factors of the polar decomposition are uniquely determined. However, the determinant of S is positive because S is positive definite, so Q inherits the sign of the determinant of M. That is, Q is only guaranteed to be orthogonal, not a rotation matrix. This is unavoidable; an M with negative determinant has no uniquely defined closest rotation matrix.

Axis and angle[edit]
Main article: Axis–angle representation

To efficiently construct a rotation matrix Q from an angle θ and a unit axis u, we can take advantage of symmetry and skew-symmetry within the entries. If x, y, and z are the components of the unit vector representing the axis, and

𝑐
	
=
cos
⁡
𝜃


𝑠
	
=
sin
⁡
𝜃


𝐶
	
=
1
−
𝑐

then

𝑄
(
𝜃
)
=
[
𝑥
𝑥
𝐶
+
𝑐
	
𝑥
𝑦
𝐶
−
𝑧
𝑠
	
𝑥
𝑧
𝐶
+
𝑦
𝑠


𝑦
𝑥
𝐶
+
𝑧
𝑠
	
𝑦
𝑦
𝐶
+
𝑐
	
𝑦
𝑧
𝐶
−
𝑥
𝑠


𝑧
𝑥
𝐶
−
𝑦
𝑠
	
𝑧
𝑦
𝐶
+
𝑥
𝑠
	
𝑧
𝑧
𝐶
+
𝑐
]

Determining an axis and angle, like determining a quaternion, is only possible up to the sign; that is, (u, θ) and (−u, −θ) correspond to the same rotation matrix, just like q and −q. Additionally, axis–angle extraction presents additional difficulties. The angle can be restricted to be from 0° to 180°, but angles are formally ambiguous by multiples of 360°. When the angle is zero, the axis is undefined. When the angle is 180°, the matrix becomes symmetric, which has implications in extracting the axis. Near multiples of 180°, care is needed to avoid numerical problems: in extracting the angle, a two-argument arctangent with atan2(sin θ, cos θ) equal to θ avoids the insensitivity of arccos; and in computing the axis magnitude in order to force unit magnitude, a brute-force approach can lose accuracy through underflow (Moler & Morrison 1983).

A partial approach is as follows:

𝑥
	
=
𝑄
𝑧
𝑦
−
𝑄
𝑦
𝑧


𝑦
	
=
𝑄
𝑥
𝑧
−
𝑄
𝑧
𝑥


𝑧
	
=
𝑄
𝑦
𝑥
−
𝑄
𝑥
𝑦


𝑟
	
=
𝑥
2
+
𝑦
2
+
𝑧
2


𝑡
	
=
𝑄
𝑥
𝑥
+
𝑄
𝑦
𝑦
+
𝑄
𝑧
𝑧


𝜃
	
=
atan2
⁡
(
𝑟
,
𝑡
−
1
)

The x-, y-, and z-components of the axis would then be divided by r. A fully robust approach will use a different algorithm when t, the trace of the matrix Q, is negative, as with quaternion extraction. When r is zero because the angle is zero, an axis must be provided from some source other than the matrix.

Euler angles[edit]

Complexity of conversion escalates with Euler angles (used here in the broad sense). The first difficulty is to establish which of the twenty-four variations of Cartesian axis order we will use. Suppose the three angles are θ1, θ2, θ3; physics and chemistry may interpret these as

𝑄
(
𝜃
1
,
𝜃
2
,
𝜃
3
)
=
𝑄
𝑧
(
𝜃
1
)
𝑄
𝑦
(
𝜃
2
)
𝑄
𝑧
(
𝜃
3
)
,

while aircraft dynamics may use

𝑄
(
𝜃
1
,
𝜃
2
,
𝜃
3
)
=
𝑄
𝑧
(
𝜃
3
)
𝑄
𝑦
(
𝜃
2
)
𝑄
𝑥
(
𝜃
1
)
.

One systematic approach begins with choosing the rightmost axis. Among all permutations of (x,y,z), only two place that axis first; one is an even permutation and the other odd. Choosing parity thus establishes the middle axis. That leaves two choices for the left-most axis, either duplicating the first or not. These three choices gives us 3 × 2 × 2 = 12 variations; we double that to 24 by choosing static or rotating axes.

This is enough to construct a matrix from angles, but triples differing in many ways can give the same rotation matrix. For example, suppose we use the zyz convention above; then we have the following equivalent pairs:

(90°,	45°,	−105°)	≡	(−270°,	−315°,	255°)	multiples of 360°
(72°,	0°,	0°)	≡	(40°,	0°,	32°)	singular alignment
(45°,	60°,	−30°)	≡	(−135°,	−60°,	150°)	bistable flip

Angles for any order can be found using a concise common routine (Herter & Lott 1993; Shoemake 1994).

The problem of singular alignment, the mathematical analog of physical gimbal lock, occurs when the middle rotation aligns the axes of the first and last rotations. It afflicts every axis order at either even or odd multiples of 90°. These singularities are not characteristic of the rotation matrix as such, and only occur with the usage of Euler angles.

The singularities are avoided when considering and manipulating the rotation matrix as orthonormal row vectors (in 3D applications often named the right-vector, up-vector and out-vector) instead of as angles. The singularities are also avoided when working with quaternions.

Vector to vector formulation[edit]

In some instances it is interesting to describe a rotation by specifying how a vector is mapped into another through the shortest path (smallest angle). In 
𝑅
3
 this completely describes the associated rotation matrix. In general, given x, y ∈ 
𝑆
n, the matrix

𝑅
:=
𝐼
+
𝑦
𝑥
𝑇
−
𝑥
𝑦
𝑇
+
1
1
+
⟨
𝑥
,
𝑦
⟩
(
𝑦
𝑥
𝑇
−
𝑥
𝑦
𝑇
)
2

belongs to SO(n + 1) and maps x to y.[22]

Voigt notation[edit]

In materials science, the four-dimensional stiffness and compliance tensors are often simplified to a two-dimensional matrix using Voigt notation. When applying a rotational transform through angle 
𝜃
 in this notation, the rotation matrix is given by[23]

𝑇
=
[
cos
2
⁡
𝜃
	
sin
2
⁡
𝜃
	
2
sin
⁡
𝜃
cos
⁡
𝜃


sin
2
⁡
𝜃
	
cos
2
⁡
𝜃
	
2
sin
⁡
𝜃
cos
⁡
𝜃


−
sin
⁡
𝜃
cos
⁡
𝜃
	
sin
⁡
𝜃
cos
⁡
𝜃
	
cos
2
⁡
𝜃
−
sin
2
⁡
𝜃
]
.

This is particularly useful in composite laminate design, where plies are often rotated by a certain angle to bring the properties of the laminate closer to isotropic.

Uniform random rotation matrices[edit]

We sometimes need to generate a uniformly distributed random rotation matrix. It seems intuitively clear in two dimensions that this means the rotation angle is uniformly distributed between 0 and 2π. That intuition is correct, but does not carry over to higher dimensions. For example, if we decompose 3 × 3 rotation matrices in axis–angle form, the angle should not be uniformly distributed; the probability that (the magnitude of) the angle is at most θ should be ⁠
1
/
π
⁠(θ − sin θ), for 0 ≤ θ ≤ π.

Since SO(n) is a connected and locally compact Lie group, we have a simple standard criterion for uniformity, namely that the distribution be unchanged when composed with any arbitrary rotation (a Lie group "translation"). This definition corresponds to what is called Haar measure. León, Massé & Rivest (2006) show how to use the Cayley transform to generate and test matrices according to this criterion.

We can also generate a uniform distribution in any dimension using the subgroup algorithm of Diaconis & Shahshahani (1987). This recursively exploits the nested dimensions group structure of SO(n), as follows. Generate a uniform angle and construct a 2 × 2 rotation matrix. To step from n to n + 1, generate a vector v uniformly distributed on the n-sphere Sn, embed the n × n matrix in the next larger size with last column (0, ..., 0, 1), and rotate the larger matrix so the last column becomes v.

As usual, we have special alternatives for the 3 × 3 case. Each of these methods begins with three independent random scalars uniformly distributed on the unit interval. Arvo (1992) takes advantage of the odd dimension to change a Householder reflection to a rotation by negation, and uses that to aim the axis of a uniform planar rotation.

Another method uses unit quaternions. Multiplication of rotation matrices is homomorphic to multiplication of quaternions, and multiplication by a unit quaternion rotates the unit sphere. Since the homomorphism is a local isometry, we immediately conclude that to produce a uniform distribution on SO(3) we may use a uniform distribution on S3. In practice: create a four-element vector where each element is a sampling of a normal distribution. Normalize its length and you have a uniformly sampled random unit quaternion which represents a uniformly sampled random rotation. Note that the aforementioned only applies to rotations in dimension 3. For a generalised idea of quaternions, one should look into Rotors.

Euler angles can also be used, though not with each angle uniformly distributed (Murnaghan 1962; Miles 1965).

For the axis–angle form, the axis is uniformly distributed over the unit sphere of directions, S2, while the angle has the nonuniform distribution over [0,π] noted previously (Miles 1965).

See also[edit]
Euler–Rodrigues formula
Euler's rotation theorem
Rodrigues' rotation formula
Plane of rotation
Axis–angle representation
Rotation group SO(3)
Rotation formalisms in three dimensions
Rotation operator (vector space)
Transformation matrix
Yaw-pitch-roll system
Kabsch algorithm
Isometry
Rigid transformation
Rotations in 4-dimensional Euclidean space
Trigonometric Identities
Versor
Remarks[edit]
^ Note that if instead of rotating vectors, it is the reference frame that is being rotated, the signs on the sin θ terms will be reversed. If reference frame A is rotated anti-clockwise about the origin through an angle θ to create reference frame B, then Rx (with the signs flipped) will transform a vector described in reference frame A coordinates to reference frame B coordinates. Coordinate frame transformations in aerospace, robotics, and other fields are often performed using this interpretation of the rotation matrix.
^ Note that
𝑢
⊗
𝑢
=
(
[
𝑢
]
×
)
2
+
𝐼
so that, in Rodrigues' notation, equivalently,
𝑅
=
𝐼
+
(
sin
⁡
𝜃
)
[
𝑢
]
×
+
(
1
−
cos
⁡
𝜃
)
(
[
𝑢
]
×
)
2
.
^ Note that this exponential map of skew-symmetric matrices to rotation matrices is quite different from the Cayley transform discussed earlier, differing to the third order,
𝑒
2
𝐴
−
𝐼
+
𝐴
𝐼
−
𝐴
=
−
2
3
𝐴
3
+
O
(
𝐴
4
)
.
Conversely, a skew-symmetric matrix A specifying a rotation matrix through the Cayley map specifies the same rotation matrix through the map exp(2 artanh A).
^ For a detailed derivation, see Derivative of the exponential map. Issues of convergence of this series to the right element of the Lie algebra are here swept under the carpet. Convergence is guaranteed when ‖X‖ + ‖Y‖ < log 2 and ‖Z‖ < log 2. If these conditions are not fulfilled, the series may still converge. A solution always exists since exp is onto[clarification needed] in the cases under consideration.
Notes[edit]
^ Swokowski, Earl (1979). Calculus with Analytic Geometry (Second ed.). Boston: Prindle, Weber, and Schmidt. ISBN 0-87150-268-2.
^ W3C recommendation (2003). "Scalable Vector Graphics – the initial coordinate system".
^ Weisstein, Eric W. "Rotation Matrix". mathworld.wolfram.com. Retrieved 2025-07-15.
^ "Rotation Matrices" (PDF). Retrieved 30 November 2021.
^ Palais, Bob; Palais, Richard (2007-12-20). "Euler's fixed point theorem: The axis of a rotation". Journal of Fixed Point Theory and Applications. 2 (2): 215–220. doi:10.1007/s11784-007-0042-5. ISSN 1661-7738. MR 2372984.
^ Kuo Kan, Liang (6 October 2018). "Efficient conversion from rotating matrix to rotation axis and angle by extending Rodrigues' formula". arXiv:1810.02999 [cs.CG].
^ Taylor, Camillo J.; Kriegman, David J. (1994). "Minimization on the Lie Group SO(3) and Related Manifolds" (PDF). Technical Report No. 9405. Yale University.
^ Balakrishnan, V. (1999). "How is a vector rotated?". Resonance. 4 (10): 61–68. doi:10.1007/BF02834260.
^ Morawiec, Adam (2004). Orientations and Rotations. Springer. doi:10.1007/978-3-662-09156-2. ISBN 978-3-642-07386-1.
^ Palazzolo, A. (1976). "Formalism for the rotation matrix of rotations about an arbitrary axis". Am. J. Phys. 44 (1): 63–67. Bibcode:1976AmJPh..44...63P. doi:10.1119/1.10140.
^ Cole, Ian R. (January 2015). Modelling CPV (thesis). Loughborough University. hdl:2134/18050.
^ Mathews, Jon (1976). "Coordinate-free rotation formalism". Am. J. Phys. 44 (12): 121. Bibcode:1976AmJPh..44.1210M. doi:10.1119/1.10264.
^ Koehler, T. R.; Trickey, S. B. (1978). "Euler vectors and rotations about an arbitrary axis". Am. J. Phys. 46 (6): 650. Bibcode:1978AmJPh..46..650K. doi:10.1119/1.11223.
^ Baker (2003); Fulton & Harris (1991)
^ (Wedderburn 1934, §8.02)
^ Hall 2004, Ch. 3; Varadarajan 1984, §2.15
^ (Engø 2001)
^ Curtright, T L; Fairlie, D B; Zachos, C K (2014). "A compact formula for rotations as spin matrix polynomials". SIGMA. 10: 084. arXiv:1402.3541. Bibcode:2014SIGMA..10..084C. doi:10.3842/SIGMA.2014.084. S2CID 18776942.
^ Baker 2003, Ch. 5; Fulton & Harris 1991, pp. 299–315
^ (Goldstein, Poole & Safko 2002, §4.8)
^ Shoemake, Ken (1985). "Animating rotation with quaternion curves". Computer Graphics: SIGGRAPH '85 Conference Proceedings. SIGGRAPH '85, 22–26 July 1985, San Francisco. Vol. 19. Association for Computing Machinery. pp. 245–254. doi:10.1145/325334.325242. ISBN 0897911660.
^ Cid, Jose Ángel; Tojo, F. Adrián F. (2018). "A Lipschitz condition along a transversal foliation implies local uniqueness for ODEs". Electronic Journal of Qualitative Theory of Differential Equations. 13 (13): 1–14. arXiv:1801.01724. doi:10.14232/ejqtde.2018.1.13.
^ Clyne, T. W., & Hull, D. (2019). Tensor Analysis of Anisotropic Materials and the Elastic Deformation of Laminae. In An Introduction to Composite Materials (pp. 43–66). chapter, Cambridge: Cambridge University Press.
References[edit]
Arvo, James (1992), "Fast random rotation matrices", in David Kirk (ed.), Graphics Gems III, San Diego: Academic Press Professional, pp. 117–120, Bibcode:1992grge.book.....K, ISBN 978-0-12-409671-4
Baker, Andrew (2003), Matrix Groups: An Introduction to Lie Group Theory, Springer, ISBN 978-1-85233-470-3
Bar-Itzhack, Itzhack Y. (Nov–Dec 2000), "New method for extracting the quaternion from a rotation matrix", Journal of Guidance, Control and Dynamics, 23 (6): 1085–1087, Bibcode:2000JGCD...23.1085B, doi:10.2514/2.4654, ISSN 0731-5090
Björck, Åke; Bowie, Clazett (June 1971), "An iterative algorithm for computing the best estimate of an orthogonal matrix", SIAM Journal on Numerical Analysis, 8 (2): 358–364, Bibcode:1971SJNA....8..358B, doi:10.1137/0708036, ISSN 0036-1429
Cayley, Arthur (1846), "Sur quelques propriétés des déterminants gauches", Journal für die reine und angewandte Mathematik, 1846 (32): 119–123, doi:10.1515/crll.1846.32.119, ISSN 0075-4102, S2CID 199546746; reprinted as article 52 in Cayley, Arthur (1889), The collected mathematical papers of Arthur Cayley, vol. I (1841–1853), Cambridge University Press, pp. 332–336
Diaconis, Persi; Shahshahani, Mehrdad (1987), "The subgroup algorithm for generating uniform random variables" (PDF), Probability in the Engineering and Informational Sciences, 1: 15–32, doi:10.1017/S0269964800000255, ISSN 0269-9648, S2CID 122752374, archived from the original (PDF) on 21 January 2022
Engø, Kenth (June 2001), "On the BCH-formula in so(3)", BIT Numerical Mathematics, 41 (3): 629–632, doi:10.1023/A:1021979515229, ISSN 0006-3835, S2CID 126053191
Fan, Ky; Hoffman, Alan J. (February 1955), "Some metric inequalities in the space of matrices", Proceedings of the American Mathematical Society, 6 (1): 111–116, doi:10.2307/2032662, ISSN 0002-9939, JSTOR 2032662
Fulton, William; Harris, Joe (1991), Representation Theory: A First Course, Graduate Texts in Mathematics, vol. 129, New York, Berlin, Heidelberg: Springer, ISBN 978-0-387-97495-8, MR 1153249
Goldstein, Herbert; Poole, Charles P.; Safko, John L. (2002), Classical Mechanics (third ed.), Addison Wesley, ISBN 978-0-201-65702-9
Hall, Brian C. (2004), Lie Groups, Lie Algebras, and Representations: An Elementary Introduction, Springer, ISBN 978-0-387-40122-5 (GTM 222)
Herter, Thomas; Lott, Klaus (September–October 1993), "Algorithms for decomposing 3-D orthogonal matrices into primitive rotations", Computers & Graphics, 17 (5): 517–527, doi:10.1016/0097-8493(93)90003-R, ISSN 0097-8493
Higham, Nicholas J. (October 1, 1989), "Matrix nearness problems and applications", in Gover, Michael J. C.; Barnett, Stephen (eds.), Applications of Matrix Theory, Oxford University Press, pp. 1–27, ISBN 978-0-19-853625-3
León, Carlos A.; Massé, Jean-Claude; Rivest, Louis-Paul (February 2006), "A statistical model for random rotations", Journal of Multivariate Analysis, 97 (2): 412–430, doi:10.1016/j.jmva.2005.03.009, ISSN 0047-259X
Miles, Roger E. (December 1965), "On random rotations in R3", Biometrika, 52 (3/4): 636–639, doi:10.2307/2333716, ISSN 0006-3444, JSTOR 2333716
Moler, Cleve; Morrison, Donald (1983), "Replacing square roots by pythagorean sums", IBM Journal of Research and Development, 27 (6): 577–581, doi:10.1147/rd.276.0577, ISSN 0018-8646
Murnaghan, Francis D. (1950), "The element of volume of the rotation group", Proceedings of the National Academy of Sciences, 36 (11): 670–672, Bibcode:1950PNAS...36..670M, doi:10.1073/pnas.36.11.670, ISSN 0027-8424, PMC 1063502, PMID 16589056
Murnaghan, Francis D. (1962), The Unitary and Rotation Groups, Lectures on applied mathematics, Washington: Spartan Books
Cayley, Arthur (1889), The collected mathematical papers of Arthur Cayley, vol. I (1841–1853), Cambridge University Press, pp. 332–336
Paeth, Alan W. (1986), "A Fast Algorithm for General Raster Rotation" (PDF), Proceedings, Graphics Interface '86: 77–81
Daubechies, Ingrid; Sweldens, Wim (1998), "Factoring wavelet transforms into lifting steps" (PDF), Journal of Fourier Analysis and Applications, 4 (3): 247–269, Bibcode:1998JFAA....4..247D, doi:10.1007/BF02476026, S2CID 195242970
Pique, Michael E. (1990), "Rotation Tools", in Andrew S. Glassner (ed.), Graphics Gems, San Diego: Academic Press Professional, pp. 465–469, ISBN 978-0-12-286166-6
Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, Brian P. (2007), "Section 21.5.2. Picking a Random Rotation Matrix", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8, archived from the original on 2011-08-11, retrieved 2011-08-18
Shepperd, Stanley W. (May–June 1978), "Quaternion from rotation matrix", Journal of Guidance and Control, 1 (3): 223–224, doi:10.2514/3.55767b
Shoemake, Ken (1994), "Euler angle conversion", in Paul Heckbert (ed.), Graphics Gems IV, San Diego: Academic Press Professional, pp. 222–229, ISBN 978-0-12-336155-4
Stuelpnagel, John (October 1964), "On the parameterization of the three-dimensional rotation group", SIAM Review, 6 (4): 422–430, Bibcode:1964SIAMR...6..422S, doi:10.1137/1006093, ISSN 0036-1445, S2CID 13990266 (Also NASA-CR-53568.)
Varadarajan, Veeravalli S. (1984), Lie Groups, Lie Algebras, and Their Representation, Springer, ISBN 978-0-387-90969-1 (GTM 102)
Wedderburn, Joseph H. M. (1934), Lectures on Matrices, AMS, ISBN 978-0-8218-3204-2
External links[edit]
"Rotation", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Rotation matrices at Mathworld
Math Awareness Month 2000 interactive demo (requires Java)
Rotation Matrices at MathPages
(in Italian) A parametrization of SOn(R) by generalized Euler Angles
Rotation about any point
hide
vte
Matrix classes

Explicitly constrained entries	
AlternantAnti-diagonalAnti-HermitianAnti-symmetricArrowheadBandBidiagonalBisymmetricBlock-diagonalBlockBlock tridiagonalBooleanCauchyCentrosymmetricConferenceComplex HadamardCopositiveDiagonally dominantDiagonalDiscrete Fourier TransformElementaryEquivalentFrobeniusGeneralized permutationHadamardHankelHermitianHessenbergHollowIntegerLogicalMatrix unitMetzlerMooreNonnegativePentadiagonalPermutationPersymmetricPolynomialQuaternionicSignatureSkew-HermitianSkew-symmetricSkylineSparseSylvesterSymmetricToeplitzTriangularTridiagonalVandermondeWalshZ

Constant	
ExchangeHilbertIdentityLehmerOf onesPascalPauliRedhefferShiftZero

Conditions on eigenvalues or eigenvectors	
CompanionConvergentDefectiveDefiniteDiagonalizableHurwitz-stablePositive-definiteStieltjes

Satisfying conditions on products or inverses	
CongruentIdempotent or ProjectionInvertibleInvolutoryNilpotentNormalOrthogonalUnimodularUnipotentUnitaryTotally unimodularWeighing

With specific applications	
AdjugateAlternating signAugmentedBézoutCarlemanCartanCirculantCofactorCommutationConfusionCoxeterDistanceDuplication and eliminationEuclidean distanceFundamental (linear differential equation)GeneratorGramHessianHouseholderJacobianMomentPayoffPickRandomRotationRouth-HurwitzSeifertShearSimilaritySymplecticTotally positiveTransformation

Used in statistics	
CenteringCorrelationCovarianceDesignDoubly stochasticFisher informationHatPrecisionStochasticTransition

Used in graph theory	
AdjacencyBiadjacencyDegreeEdmondsIncidenceLaplacianSeidel adjacencyTutte

Used in science and engineering	
Cabibbo–Kobayashi–MaskawaDensityFundamental (computer vision)Fuzzy associativeGammaGell-MannHamiltonianIrregularOverlapSState transitionSubstitutionZ (chemistry)

Related terms	
Jordan normal formLinear independenceMatrix exponentialMatrix representation of conic sectionsPerfect matrixPseudoinverseRow echelon formWronskian


 Mathematics portalList of matricesCategory:Matrices (mathematics)
Categories: Transformation (function)Matrices (mathematics)Mathematical physics
This page was last edited on 30 July 2025, at 18:56 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Code of Conduct
Developers
Statistics
Cookie statement
Mobile view
