Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Introduction 2 Normal distribution Toggle Normal distribution subsection 2.1 Known mean, known variance 2.2 Estimation of parameters 2.2.1 Unknown mean, known variance 2.2.2 Known mean, unknown variance 2.2.3 Unknown mean, unknown variance 3 Non-parametric methods Toggle Non-parametric methods subsection 3.1 Conformal Prediction 4 Contrast with other intervals Toggle Contrast with other intervals subsection 4.1 Contrast with confidence intervals 5 In regression analysis 6 Bayesian statistics 7 Applications 8 See also 9 Notes 10 References 11 Further reading Toggle the table of contents Prediction interval 13 languages Deutsch Español Français 한국어 Italiano 日本語 Norsk bokmål Polski Simple English Sunda Türkçe 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Estimate of an interval in which future observations will fall Not to be confused with Prediction error .

This article has multiple issues.

Please help improve it or discuss these issues on the talk page .

( Learn how and when to remove these messages ) This article possibly contains original research .

Please improve it by verifying the claims made and adding inline citations . Statements consisting only of original research should be removed.

( May 2021 ) ( Learn how and when to remove this message ) This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( May 2021 ) ( Learn how and when to remove this message ) ( Learn how and when to remove this message ) In statistical inference , specifically predictive inference , a prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis .

A simple example is given by a six-sided die with face values ranging from 1 to 6. The confidence interval for the estimated expected value of the face value will be around 3.5 and will become narrower with a larger sample size. However, the prediction interval for the next roll will approximately range from 1 to 6, even with any number of samples seen so far.

Prediction intervals are used in both frequentist statistics and Bayesian statistics : a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed.

Introduction [ edit ] If one makes the parametric assumption that the underlying distribution is a normal distribution , and has a sample set { X 1 , ..., X n }, then confidence intervals and credible intervals may be used to estimate the population mean μ and population standard deviation σ of the underlying population, while prediction intervals may be used to estimate the value of the next sample variable, X n +1 .

Alternatively, in Bayesian terms , a prediction interval can be described as a credible interval for the variable itself, rather than for a parameter of the distribution thereof.

The concept of prediction intervals need not be restricted to inference about a single future sample value but can be extended to more complicated cases. For example, in the context of river flooding where analyses are often based on annual values of the largest flow within the year, there may be interest in making inferences about the largest flood likely to be experienced within the next 50 years.

Since prediction intervals are only concerned with past and future observations, rather than unobservable population parameters, they are advocated as a better method than confidence intervals by some statisticians, such as Seymour Geisser , [ citation needed ] following the focus on observables by Bruno de Finetti .

[ citation needed ] Normal distribution [ edit ] Given a sample from a normal distribution , whose parameters are unknown, it is possible to give prediction intervals in the frequentist sense, i.e., an interval [ a , b ] based on statistics of the sample such that on repeated experiments, X n +1 falls in the interval the desired percentage of the time; one may call these "predictive confidence intervals ".

[ 1 ] A general technique of frequentist prediction intervals is to find and compute a pivotal quantity of the observables X 1 , ..., X n , X n +1 – meaning a function of observables and parameters whose probability distribution does not depend on the parameters – that can be inverted to give a probability of the future observation X n +1 falling in some interval computed in terms of the observed values so far, X 1 , … … , X n .

{\displaystyle X_{1},\dots ,X_{n}.} Such a pivotal quantity, depending only on observables, is called an ancillary statistic .

[ 2 ] The usual method of constructing pivotal quantities is to take the difference of two variables that depend on location, so that location cancels out, and then take the ratio of two variables that depend on scale, so that scale cancels out.
The most familiar pivotal quantity is the Student's t-statistic , which can be derived by this method and is used in the sequel.

Known mean, known variance [ edit ] See also: 68–95–99.7 rule A prediction interval [ ℓ , u ] for a future observation X in a normal distribution N ( μ , σ 2 ) with known mean and variance may be calculated from γ γ = P ( ℓ ℓ < X < u ) = P ( ℓ ℓ − − μ μ σ σ < X − − μ μ σ σ < u − − μ μ σ σ ) = P ( ℓ ℓ − − μ μ σ σ < Z < u − − μ μ σ σ ) , {\displaystyle \gamma =P(\ell <X<u)=P\left({\frac {\ell -\mu }{\sigma }}<{\frac {X-\mu }{\sigma }}<{\frac {u-\mu }{\sigma }}\right)=P\left({\frac {\ell -\mu }{\sigma }}<Z<{\frac {u-\mu }{\sigma }}\right),} where Z = X − − μ μ σ σ {\displaystyle Z={\frac {X-\mu }{\sigma }}} , the standard score of X , is distributed as  standard normal.

Hence ℓ ℓ − − μ μ σ σ = − − z , u − − μ μ σ σ = z , {\displaystyle {\frac {\ell -\mu }{\sigma }}=-z,\quad {\frac {u-\mu }{\sigma }}=z,} or ℓ ℓ = μ μ − − z σ σ , u = μ μ + z σ σ , {\displaystyle \ell =\mu -z\sigma ,\quad u=\mu +z\sigma ,} with z the quantile in the standard normal distribution for which: γ γ = P ( − − z < Z < z ) .

{\displaystyle \gamma =P(-z<Z<z).} or equivalently; 1 2 ( 1 − − γ γ ) = P ( Z > z ) .

{\displaystyle {\tfrac {1}{2}}(1-\gamma )=P(Z>z).} Prediction interval z 75% 1.15 [ 3 ] 90% 1.64 [ 3 ] 95% 1.96 [ 3 ] 99% 2.58 [ 3 ] Prediction interval (on the y-axis ) given from z (the quantile of the standard score , on the x-axis ). The y-axis is logarithmically compressed (but the values on it are not modified).

The prediction interval is conventionally written  as: [ μ μ − − z σ σ , μ μ + z σ σ ] .

{\displaystyle \left[\mu -z\sigma ,\ \mu +z\sigma \right].} For example, to calculate the 95% prediction interval for a normal distribution with a mean ( μ ) of 5 and a standard deviation ( σ ) of 1, then z is approximately 2. Therefore, the lower limit of the prediction interval is approximately 5 ‒ (2⋅1) = 3, and the upper limit is approximately 5 + (2⋅1) = 7, thus giving a prediction interval of approximately 3 to 7.

Diagram showing the cumulative distribution function for the normal distribution with mean ( μ ) 0 and variance ( σ 2 ) 1. In addition to the quantile function , the prediction interval for any standard score can be calculated by (1 − (1 − Φ μ , σ 2 (standard score))⋅2). For example, a standard score of x = 1.96 gives Φ μ , σ 2 (1.96) = 0.9750 corresponding to a prediction interval of (1 − (1 − 0.9750)⋅2) = 0.9500 = 95%.

Estimation of parameters [ edit ] For a distribution with unknown parameters, a direct approach to prediction is to estimate the parameters and then use the associated quantile function – for example, one could use the sample mean X ¯ ¯ {\displaystyle {\overline {X}}} as estimate for μ and the sample variance s 2 as an estimate for σ 2 . There are two natural choices for s 2 here – dividing by ( n − − 1 ) {\displaystyle (n-1)} yields an unbiased estimate, while dividing by n yields the maximum likelihood estimator , and either might be used. One then uses the quantile function with these estimated parameters Φ Φ X ¯ ¯ , s 2 − − 1 {\displaystyle \Phi _{{\overline {X}},s^{2}}^{-1}} to give a prediction interval.

This approach is usable, but the resulting interval will not have the repeated sampling interpretation [ 4 ] – it is not a predictive confidence interval.

For the sequel, use the sample mean: X ¯ ¯ = ( X 1 + ⋯ ⋯ + X n ) / n {\displaystyle {\overline {X}}=(X_{1}+\cdots +X_{n})/n} and the (unbiased) sample variance: s 2 = 1 n − − 1 ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle s^{2}={1 \over n-1}\sum _{i=1}^{n}(X_{i}-{\overline {X}})^{2}} Unknown mean, known variance [ edit ] Given [ 5 ] a normal distribution with unknown mean μ but known variance σ σ 2 {\displaystyle \sigma ^{2}} , the sample mean X ¯ ¯ {\displaystyle {\overline {X}}} of the observations X 1 , … … , X n {\displaystyle X_{1},\dots ,X_{n}} has distribution N ( μ μ , σ σ 2 / n ) , {\displaystyle N(\mu ,\sigma ^{2}/n),} while the future observation X n + 1 {\displaystyle X_{n+1}} has distribution N ( μ μ , σ σ 2 ) .

{\displaystyle N(\mu ,\sigma ^{2}).} Taking the difference of these cancels the μ and yields a normal distribution of variance σ σ 2 + ( σ σ 2 / n ) , {\displaystyle \sigma ^{2}+(\sigma ^{2}/n),} thus X n + 1 − − X ¯ ¯ σ σ 2 + ( σ σ 2 / n ) ∼ ∼ N ( 0 , 1 ) .

{\displaystyle {\frac {X_{n+1}-{\overline {X}}}{\sqrt {\sigma ^{2}+(\sigma ^{2}/n)}}}\sim N(0,1).} Solving for X n + 1 {\displaystyle X_{n+1}} gives the prediction distribution N ( X ¯ ¯ , σ σ 2 + ( σ σ 2 / n ) ) , {\displaystyle N({\overline {X}},\sigma ^{2}+(\sigma ^{2}/n)),} from which one can compute intervals as before. This is a predictive confidence interval in the sense that if one uses a quantile range of 100 p %, then on repeated applications of this computation, the future observation X n + 1 {\displaystyle X_{n+1}} will fall in the predicted interval 100 p % of the time.

Notice that this prediction distribution is more conservative than using the estimated mean X ¯ ¯ {\displaystyle {\overline {X}}} and known variance σ σ 2 {\displaystyle \sigma ^{2}} , as this uses compound variance σ σ 2 + ( σ σ 2 / n ) {\displaystyle \sigma ^{2}+(\sigma ^{2}/n)} , hence yields slightly wider intervals. This is necessary for the desired confidence interval property to hold.

Known mean, unknown variance [ edit ] Conversely, given a normal distribution with known mean μ but unknown variance σ σ 2 {\displaystyle \sigma ^{2}} , 
the sample variance s 2 {\displaystyle s^{2}} of the observations X 1 , … … , X n {\displaystyle X_{1},\dots ,X_{n}} has, up to scale, a χ χ n − − 1 2 {\displaystyle \chi _{n-1}^{2}} distribution ; more precisely: ( n − − 1 ) s 2 σ σ 2 ∼ ∼ χ χ n − − 1 2 .

{\displaystyle {\frac {(n-1)s^{2}}{\sigma ^{2}}}\sim \chi _{n-1}^{2}.} On the other hand, the future observation X n + 1 {\displaystyle X_{n+1}} has distribution N ( μ μ , σ σ 2 ) .

{\displaystyle N(\mu ,\sigma ^{2}).} Taking the ratio of the future observation residual X n + 1 − − μ μ {\displaystyle X_{n+1}-\mu } and the sample standard deviation s cancels the σ, yielding a Student's t-distribution with n – 1 degrees of freedom (see its derivation ): X n + 1 − − μ μ s ∼ ∼ T n − − 1 .

{\displaystyle {\frac {X_{n+1}-\mu }{s}}\sim T_{n-1}.} Solving for X n + 1 {\displaystyle X_{n+1}} gives the prediction distribution μ μ ± ± s T n − − 1 , {\displaystyle \mu \pm sT_{n-1},} from which one can compute intervals as before.

Notice that this prediction distribution is more conservative than using a normal distribution with the estimated standard deviation s {\displaystyle s} and known mean μ , as it uses the t-distribution instead of the normal distribution, hence yields wider intervals. This is necessary for the desired confidence interval property to hold.

Unknown mean, unknown variance [ edit ] Combining the above for a normal distribution N ( μ μ , σ σ 2 ) {\displaystyle N(\mu ,\sigma ^{2})} with both μ and σ 2 unknown yields the following ancillary statistic: [ 6 ] X n + 1 − − X ¯ ¯ s 1 + 1 / n ∼ ∼ T n − − 1 {\displaystyle {\frac {X_{n+1}-{\overline {X}}}{s{\sqrt {1+1/n}}}}\sim T_{n-1}} This simple combination is possible because the sample mean and sample variance of the normal distribution are independent statistics; this is only true for the normal distribution, and in fact characterizes the normal distribution.

Solving for X n + 1 {\displaystyle X_{n+1}} yields the prediction distribution X ¯ ¯ + s 1 + 1 / n ⋅ ⋅ T n − − 1 {\displaystyle {\overline {X}}+s{\sqrt {1+1/n}}\cdot T_{n-1}} The probability of X n + 1 {\displaystyle X_{n+1}} falling in a given interval is then: Pr ( X ¯ ¯ − − T n − − 1 , a s 1 + ( 1 / n ) ≤ ≤ X n + 1 ≤ ≤ X ¯ ¯ + T n − − 1 , a s 1 + ( 1 / n ) ) = p {\displaystyle \Pr \left({\overline {X}}-T_{n-1,a}s{\sqrt {1+(1/n)}}\leq X_{n+1}\leq {\overline {X}}+T_{n-1,a}s{\sqrt {1+(1/n)}}\,\right)=p} where T n-1,a is the 100((1 − p )/2) th percentile of Student's t-distribution with n − 1 degrees of freedom.  Therefore, the numbers X ¯ ¯ ± ± T n − − 1 , a s 1 + ( 1 / n ) {\displaystyle {\overline {X}}\pm T_{n-1,a}s{\sqrt {1+(1/n)}}} are the endpoints of a 100(1 − p )% prediction interval for X n + 1 {\displaystyle X_{n+1}} .

Non-parametric methods [ edit ] One can compute prediction intervals without any assumptions on the population, i.e. in a non-parametric way.

The residual bootstrap method can be used for constructing non-parametric prediction intervals.

Conformal Prediction [ edit ] Main article: conformal prediction In general the conformal prediction method is more general.
Let us look at the special case of using the minimum and maximum as boundaries for a prediction interval:
If one has a sample of identical random variables { X 1 , ..., X n }, then the probability that the next observation X n +1 will be the largest is 1/( n + 1), since all observations have equal probability of being the maximum. In the same way, the probability that X n +1 will be the smallest is 1/( n + 1). The other ( n − 1)/( n + 1) of the time, X n +1 falls between the sample maximum and sample minimum of the sample { X 1 , ..., X n }. Thus, denoting the sample maximum and minimum by M and m, this yields an ( n − 1)/( n + 1) prediction interval of [ m , M ].

[ citation needed ] Notice that while this gives the probability that a future observation will fall in a range, it does not give any estimate as to where in a segment it will fall – notably, if it falls outside the range of observed values, it may be far outside the range. See extreme value theory for further discussion. Formally, this applies not just to sampling from a population, but to any exchangeable sequence of random variables, not necessarily independent or identically distributed .

Contrast with other intervals [ edit ] Main article: Interval estimation See also: Tolerance interval and Quantile regression Contrast with confidence intervals [ edit ] Main article: Confidence interval In the formula for the predictive confidence interval no mention is made of the unobservable parameters μ and σ of population mean and standard deviation – the observed sample statistics X ¯ ¯ n {\displaystyle {\overline {X}}_{n}} and S n {\displaystyle S_{n}} of sample mean and standard deviation are used, and what is estimated is the outcome of future samples.

When considering prediction intervals, rather than using sample statistics as estimators of population parameters and applying confidence intervals to these estimates, one considers "the next sample" X n + 1 {\displaystyle X_{n+1}} as itself a statistic, and computes its sampling distribution .

In parameter confidence intervals, one estimates population parameters; if one wishes to interpret this as prediction of the next sample, one models "the next sample" as a draw from this estimated population, using the (estimated) population distribution. By contrast, in predictive confidence intervals, one uses the sampling distribution of (a statistic of) a sample of n or n + 1 observations from such a population, and the population distribution is not directly used, though the assumption about its form (though not the values of its parameters) is used in computing the sampling distribution.

In regression analysis [ edit ] Further information: Regression analysis § Prediction (interpolation and extrapolation) , and Mean and predicted outcome A common application of prediction intervals is to regression analysis .
Suppose the data is being modeled by a straight line ( simple linear regression ): y i = α α + β β x i + ε ε i {\displaystyle y_{i}=\alpha +\beta x_{i}+\varepsilon _{i}\,} where y i {\displaystyle y_{i}} is the response variable , x i {\displaystyle x_{i}} is the explanatory variable , ε i is a random error term, and α α {\displaystyle \alpha } and β β {\displaystyle \beta } are parameters.

Given estimates α α ^ ^ {\displaystyle {\hat {\alpha }}} and β β ^ ^ {\displaystyle {\hat {\beta }}} for the parameters, such as from a ordinary least squares , the predicted response value y d for a given explanatory value x d is y ^ ^ d = α α ^ ^ + β β ^ ^ x d , {\displaystyle {\hat {y}}_{d}={\hat {\alpha }}+{\hat {\beta }}x_{d},} (the point on the regression line), while the actual response would be y d = α α + β β x d + ε ε d .

{\displaystyle y_{d}=\alpha +\beta x_{d}+\varepsilon _{d}.\,} The point estimate y ^ ^ d {\displaystyle {\hat {y}}_{d}} is called the mean response , and is an estimate of the expected value of y d , E ( y ∣ ∣ x d ) .

{\displaystyle E(y\mid x_{d}).} A prediction interval instead gives an interval in which one expects y d to fall; this is not necessary if the actual parameters α and β are known (together with the error term ε i ), but if one is estimating from a sample , then one may use the standard error of the estimates for the intercept and slope ( α α ^ ^ {\displaystyle {\hat {\alpha }}} and β β ^ ^ {\displaystyle {\hat {\beta }}} ), as well as their correlation, to compute a prediction interval.

In regression, Faraway (2002 , p. 39) makes a distinction between intervals for predictions of the mean response vs. for predictions of observed response—affecting essentially the inclusion or not of the unity term within the square root in the expansion factors above ; for details, see Faraway (2002) .

Bayesian statistics [ edit ] See also: Posterior predictive distribution Seymour Geisser , a proponent of predictive inference, gives predictive applications of Bayesian statistics .

[ 7 ] In Bayesian statistics, one can compute (Bayesian) prediction intervals from the posterior probability of the random variable, as a credible interval . In theoretical work, credible intervals are not often calculated for the prediction of future events, but for inference of parameters – i.e., credible intervals of a parameter, not for the outcomes of the variable itself. However, particularly where applications are concerned with possible extreme values of yet to be observed cases, credible intervals for such values can be of practical importance.

Applications [ edit ] Prediction intervals are commonly used as definitions of reference ranges , such as reference ranges for blood tests to give an idea of whether a blood test is normal or not. For this purpose, the most commonly used prediction interval is the 95% prediction interval, and a reference range based on it can be called a standard reference range .

See also [ edit ] Extrapolation Posterior probability Prediction Prediction band Seymour Geisser Statistical model validation Trend estimation Notes [ edit ] ^ Geisser (1993 , p.

6 ): Chapter 2: Non-Bayesian predictive approaches ^ Geisser (1993 , p.

7 ) ^ a b c d Table A2 in Sterne & Kirkwood (2003 , p. 472) ^ Geisser (1993 , pp.

8–9 ) ^ Geisser (1993 , p.

7– ) ^ Geisser (1993 , Example 2.2, p. 9–10 ) ^ Geisser (1993) References [ edit ] Faraway, Julian J. (2002), Practical Regression and Anova using R (PDF) Geisser, Seymour (1993), Predictive Inference , CRC Press Sterne, Jonathan; Kirkwood, Betty R. (2003), Essential Medical Statistics , Blackwell Science , ISBN 0-86542-871-9 Further reading [ edit ] Chatfield, C. (1993). "Calculating Interval Forecasts".

Journal of Business & Economic Statistics .

11 (2): 121– 135.

doi : 10.2307/1391361 .

JSTOR 1391361 .

Lawless, J. F.; Fredette, M. (2005).

"Frequentist prediction intervals and predictive distributions" .

Biometrika .

92 (3): 529– 542.

doi : 10.1093/biomet/92.3.529 .

Meade, N.; Islam, T. (1995). "Prediction Intervals for Growth Curve Forecasts".

Journal of Forecasting .

14 (5): 413– 430.

doi : 10.1002/for.3980140502 .

ISO 16269-8 Standard Interpretation of Data, Part 8, Determination of Prediction Intervals v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐fxg6v
Cached time: 20250812020036
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.600 seconds
Real time usage: 0.828 seconds
Preprocessor visited node count: 3209/1000000
Revision size: 21238/2097152 bytes
Post‐expand include size: 188024/2097152 bytes
Template argument size: 11029/2097152 bytes
Highest expansion depth: 18/100
Expensive parser function count: 16/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 40745/5000000 bytes
Lua time usage: 0.324/10.000 seconds
Lua memory usage: 7567991/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  544.829      1 -total
 26.47%  144.205      1 Template:Statistics
 25.83%  140.736      1 Template:Navbox_with_collapsible_groups
 16.67%   90.821      3 Template:Citation
 12.70%   69.198      1 Template:Multiple_issues
 12.03%   65.542      1 Template:Short_description
 11.00%   59.947     11 Template:Navbox
  7.60%   41.382      2 Template:Pagetype
  7.13%   38.860      2 Template:Ambox
  7.06%   38.444      9 Template:Harvtxt Saved in parser cache with key enwiki:pcache:536062:|#|:idhash:canonical and timestamp 20250812020036 and revision id 1286934091. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Prediction_interval&oldid=1286934091 " Categories : Statistical forecasting Regression analysis Statistical intervals Hidden categories: Articles with short description Short description matches Wikidata Articles that may contain original research from May 2021 All articles that may contain original research Articles lacking in-text citations from May 2021 All articles lacking in-text citations Articles with multiple maintenance issues All articles with unsourced statements Articles with unsourced statements from August 2009 Articles with unsourced statements from January 2025 This page was last edited on 22 April 2025, at 22:39 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Prediction interval 13 languages Add topic

