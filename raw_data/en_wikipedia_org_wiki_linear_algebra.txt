Title: Linear algebra

URL Source: https://en.wikipedia.org/wiki/Linear_algebra

Published Time: 2001-03-06T08:00:01Z

Markdown Content:
**Linear algebra** is the branch of [mathematics](https://en.wikipedia.org/wiki/Mathematics "Mathematics") concerning [linear equations](https://en.wikipedia.org/wiki/Linear_equation "Linear equation") such as

![Image 1: {\displaystyle a_{1}x_{1}+\cdots +a_{n}x_{n}=b,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4f0f2986d54c01f3bccf464d266dfac923c80f3)
[linear maps](https://en.wikipedia.org/wiki/Linear_map "Linear map") such as

![Image 2: {\displaystyle (x_{1},\ldots ,x_{n})\mapsto a_{1}x_{1}+\cdots +a_{n}x_{n},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/705c4efbe2eac03a9a36065cd900df1932f9f7d6)
and their representations in [vector spaces](https://en.wikipedia.org/wiki/Vector_space "Vector space") and through [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)").[[1]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-1)[[2]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-2)[[3]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-3)

[![Image 3](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Linear_subspaces_with_shading.svg/250px-Linear_subspaces_with_shading.svg.png)](https://en.wikipedia.org/wiki/File:Linear_subspaces_with_shading.svg)

In three-dimensional [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space"), these three planes represent solutions to linear equations, and their intersection represents the set of common solutions: in this case, a unique point. The blue line is the common solution to two of these equations. 

Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of [geometry](https://en.wikipedia.org/wiki/Geometry "Geometry"), including for defining basic objects such as [lines](https://en.wikipedia.org/wiki/Line_(geometry) "Line (geometry)"), [planes](https://en.wikipedia.org/wiki/Plane_(geometry) "Plane (geometry)") and [rotations](https://en.wikipedia.org/wiki/Rotation_(mathematics) "Rotation (mathematics)"). Also, [functional analysis](https://en.wikipedia.org/wiki/Functional_analysis "Functional analysis"), a branch of [mathematical analysis](https://en.wikipedia.org/wiki/Mathematical_analysis "Mathematical analysis"), may be viewed as the application of linear algebra to [function spaces](https://en.wikipedia.org/wiki/Space_of_functions "Space of functions").

Linear algebra is also used in most sciences and fields of [engineering](https://en.wikipedia.org/wiki/Engineering "Engineering") because it allows [modeling](https://en.wikipedia.org/wiki/Mathematical_model "Mathematical model") many natural phenomena, and computing efficiently with such models. For [nonlinear systems](https://en.wikipedia.org/wiki/Nonlinear_system "Nonlinear system"), which cannot be modeled with linear algebra, it is often used for dealing with [first-order approximations](https://en.wikipedia.org/wiki/First-order_approximation "First-order approximation"), using the fact that the [differential](https://en.wikipedia.org/wiki/Differential_(mathematics) "Differential (mathematics)") of a [multivariate function](https://en.wikipedia.org/wiki/Multivariate_function "Multivariate function") at a point is the linear map that best approximates the function near that point.

The procedure (using counting rods) for solving simultaneous linear equations now called [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") appears in the ancient Chinese mathematical text [Chapter Eight: _Rectangular Arrays_](https://en.wikipedia.org/wiki/Rod_calculus#System_of_linear_equations "Rod calculus") of _[The Nine Chapters on the Mathematical Art](https://en.wikipedia.org/wiki/The\_Nine\_Chapters\_on\_the\_Mathematical\_Art "The Nine Chapters on the Mathematical Art")_. Its use is illustrated in eighteen problems, with two to five equations.[[4]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-4)

[Systems of linear equations](https://en.wikipedia.org/wiki/Systems_of_linear_equations "Systems of linear equations") arose in Europe with the introduction in 1637 by [René Descartes](https://en.wikipedia.org/wiki/Ren%C3%A9_Descartes "René Descartes") of [coordinates](https://en.wikipedia.org/wiki/Coordinates "Coordinates") in [geometry](https://en.wikipedia.org/wiki/Geometry "Geometry"). In fact, in this new geometry, now called [Cartesian geometry](https://en.wikipedia.org/wiki/Cartesian_geometry "Cartesian geometry"), lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.

The first systematic methods for solving linear systems used [determinants](https://en.wikipedia.org/wiki/Determinant "Determinant") and were first considered by [Leibniz](https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz "Gottfried Wilhelm Leibniz") in 1693. In 1750, [Gabriel Cramer](https://en.wikipedia.org/wiki/Gabriel_Cramer "Gabriel Cramer") used them for giving explicit solutions of linear systems, now called [Cramer's rule](https://en.wikipedia.org/wiki/Cramer%27s_rule "Cramer's rule"). Later, [Gauss](https://en.wikipedia.org/wiki/Gauss "Gauss") further described the method of elimination, which was initially listed as an advancement in [geodesy](https://en.wikipedia.org/wiki/Geodesy "Geodesy").[[5]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Vitulli,_Marie-5)

In 1844 [Hermann Grassmann](https://en.wikipedia.org/wiki/Hermann_Grassmann "Hermann Grassmann") published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, [James Joseph Sylvester](https://en.wikipedia.org/wiki/James_Joseph_Sylvester "James Joseph Sylvester") introduced the term _matrix_, which is Latin for _womb_.

Linear algebra grew with ideas noted in the [complex plane](https://en.wikipedia.org/wiki/Complex_plane "Complex plane"). For instance, two numbers w and z in ![Image 4: {\displaystyle \mathbb {C} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/f9add4085095b9b6d28d045fd9c92c2c09f549a7) have a difference _w_ – _z_, and the line segments _wz_ and 0(_w_ − _z_) are of the same length and direction. The segments are [equipollent](https://en.wikipedia.org/wiki/Equipollence_(geometry) "Equipollence (geometry)"). The four-dimensional system ![Image 5: {\displaystyle \mathbb {H} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/e050965453c42bcc6bd544546703c836bdafeac9) of [quaternions](https://en.wikipedia.org/wiki/Quaternion "Quaternion") was discovered by [W.R. Hamilton](https://en.wikipedia.org/wiki/William_Rowan_Hamilton "William Rowan Hamilton") in 1843.[[6]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-6) The term _vector_ was introduced as **v** = _x_**i** + _y_**j** + _z_**k** representing a point in space. The quaternion difference _p_ – _q_ also produces a segment equipollent to _pq_. Other [hypercomplex number](https://en.wikipedia.org/wiki/Hypercomplex_number "Hypercomplex number") systems also used the idea of a linear space with a [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra) "Basis (linear algebra)").

[Arthur Cayley](https://en.wikipedia.org/wiki/Arthur_Cayley "Arthur Cayley") introduced [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication") and the [inverse matrix](https://en.wikipedia.org/wiki/Inverse_matrix "Inverse matrix") in 1856, making possible the [general linear group](https://en.wikipedia.org/wiki/General_linear_group "General linear group"). The mechanism of [group representation](https://en.wikipedia.org/wiki/Group_representation "Group representation") became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[[5]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Vitulli,_Marie-5)

[Benjamin Peirce](https://en.wikipedia.org/wiki/Benjamin_Peirce "Benjamin Peirce") published his _Linear Associative Algebra_ (1872), and his son [Charles Sanders Peirce](https://en.wikipedia.org/wiki/Charles_Sanders_Peirce "Charles Sanders Peirce") extended the work later.[[7]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-7)

The [telegraph](https://en.wikipedia.org/wiki/Telegraph "Telegraph") required an explanatory system, and the 1873 publication by [James Clerk Maxwell](https://en.wikipedia.org/wiki/James_Clerk_Maxwell "James Clerk Maxwell") of _[A Treatise on Electricity and Magnetism](https://en.wikipedia.org/wiki/A\_Treatise\_on\_Electricity\_and\_Magnetism "A Treatise on Electricity and Magnetism")_ instituted a [field theory](https://en.wikipedia.org/wiki/Field_theory_(physics) "Field theory (physics)") of forces and required [differential geometry](https://en.wikipedia.org/wiki/Differential_geometry "Differential geometry") for expression. Linear algebra is flat differential geometry and serves in tangent spaces to [manifolds](https://en.wikipedia.org/wiki/Manifold "Manifold"). Electromagnetic symmetries of spacetime are expressed by the [Lorentz transformations](https://en.wikipedia.org/wiki/Lorentz_transformation "Lorentz transformation"), and much of the history of linear algebra is the [history of Lorentz transformations](https://en.wikipedia.org/wiki/History_of_Lorentz_transformations "History of Lorentz transformations").

The first modern and more precise definition of a vector space was introduced by [Peano](https://en.wikipedia.org/wiki/Peano "Peano") in 1888;[[5]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Vitulli,_Marie-5) by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century when many ideas and methods of previous centuries were generalized as [abstract algebra](https://en.wikipedia.org/wiki/Abstract_algebra "Abstract algebra"). The development of computers led to increased research in efficient [algorithms](https://en.wikipedia.org/wiki/Algorithm "Algorithm") for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modeling and simulations.[[5]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Vitulli,_Marie-5)

Until the 19th century, linear algebra was introduced through [systems of linear equations](https://en.wikipedia.org/wiki/Systems_of_linear_equations "Systems of linear equations") and [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)"). In modern mathematics, the presentation through _vector spaces_ is generally preferred, since it is more [synthetic](https://en.wikipedia.org/wiki/Synthetic_geometry "Synthetic geometry"), more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.

A vector space over a [field](https://en.wikipedia.org/wiki/Field_(mathematics) "Field (mathematics)")_F_ (often the field of the [real numbers](https://en.wikipedia.org/wiki/Real_number "Real number") or of the [complex numbers](https://en.wikipedia.org/wiki/Complex_number "Complex number")) is a [set](https://en.wikipedia.org/wiki/Set_(mathematics) "Set (mathematics)")_V_ equipped with two [binary operations](https://en.wikipedia.org/wiki/Binary_operation "Binary operation"). [Elements](https://en.wikipedia.org/wiki/Element_(mathematics) "Element (mathematics)") of _V_ are called _vectors_, and elements of _F_ are called _scalars_. The first operation, _[vector addition](https://en.wikipedia.org/wiki/Vector\_addition "Vector addition")_, takes any two vectors **v** and **w** and outputs a third vector **v** + **w**. The second operation, _[scalar multiplication](https://en.wikipedia.org/wiki/Scalar\_multiplication "Scalar multiplication")_, takes any scalar _a_ and any vector **v** and outputs a new vector _a_**v**. The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, **u**, **v** and **w** are arbitrary elements of _V_, and _a_ and _b_ are arbitrary scalars in the field _F_.)[[8]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-8)

**Axiom****Signification**
[Associativity](https://en.wikipedia.org/wiki/Associativity "Associativity") of addition**u** + (**v** + **w**) = (**u** + **v**) + **w**
[Commutativity](https://en.wikipedia.org/wiki/Commutativity "Commutativity") of addition**u** + **v** = **v** + **u**
[Identity element](https://en.wikipedia.org/wiki/Identity_element "Identity element") of addition There exists an element **0** in _V_, called the _[zero vector](https://en.wikipedia.org/wiki/Zero\_vector "Zero vector")_ (or simply _zero_), such that **v** + **0** = **v** for all **v** in _V_.
[Inverse elements](https://en.wikipedia.org/wiki/Inverse_element "Inverse element") of addition For every **v** in _V_, there exists an element −**v** in _V_, called the _[additive inverse](https://en.wikipedia.org/wiki/Additive\_inverse "Additive inverse")_ of **v**, such that **v** + (−**v**) = **0**
[Distributivity](https://en.wikipedia.org/wiki/Distributivity "Distributivity") of scalar multiplication with respect to vector addition _a_(**u** + **v**) = _a_**u** + _a_**v**
Distributivity of scalar multiplication with respect to field addition(_a_ + _b_)**v** = _a_**v** + _b_**v**
Compatibility of scalar multiplication with field multiplication _a_(_b_**v**) = (_ab_)**v**[[a]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-9)
Identity element of scalar multiplication 1**v** = **v**, where 1 denotes the [multiplicative identity](https://en.wikipedia.org/wiki/Multiplicative_identity "Multiplicative identity") of F.

The first four axioms mean that _V_ is an [abelian group](https://en.wikipedia.org/wiki/Abelian_group "Abelian group") under addition.

The elements of a specific vector space may have various natures; for example, they could be [tuples](https://en.wikipedia.org/wiki/Tuple "Tuple"), [sequences](https://en.wikipedia.org/wiki/Sequence "Sequence"), [functions](https://en.wikipedia.org/wiki/Function_(mathematics) "Function (mathematics)"), [polynomials](https://en.wikipedia.org/wiki/Polynomial_ring "Polynomial ring"), or a [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)"). Linear algebra is concerned with the properties of such objects that are common to all vector spaces.

**Linear maps** are [mappings](https://en.wikipedia.org/wiki/Map_(mathematics) "Map (mathematics)") between vector spaces that preserve the vector-space structure. Given two vector spaces _V_ and _W_ over a field F, a linear map (also called, in some contexts, linear transformation or linear mapping) is a [map](https://en.wikipedia.org/wiki/Map_(mathematics) "Map (mathematics)")

![Image 6: {\displaystyle T:V\to W}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c59f606d4e06de82ae3016ab89884480356b3d6)
that is compatible with addition and scalar multiplication, that is

![Image 7: {\displaystyle T(\mathbf {u} +\mathbf {v} )=T(\mathbf {u} )+T(\mathbf {v} ),\quad T(a\mathbf {v} )=aT(\mathbf {v} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/44b0a2c4d6ed55530894b9476f29aec608137744)
for any vectors **u**,**v** in _V_ and scalar _a_ in F.

An equivalent condition is that for any vectors **u**, **v** in _V_ and scalars _a_, _b_ in F, one has

![Image 8: {\displaystyle T(a\mathbf {u} +b\mathbf {v} )=aT(\mathbf {u} )+bT(\mathbf {v} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7cbf4fdd887414aebddb79520d6933b59bdec0ee).
When _V_ = _W_ are the same vector space, a linear map _T_: _V_ → _V_ is also known as a _linear operator_ on V.

A [bijective](https://en.wikipedia.org/wiki/Bijective "Bijective") linear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is an [isomorphism](https://en.wikipedia.org/wiki/Isomorphism "Isomorphism"). Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its [range](https://en.wikipedia.org/wiki/Range_of_a_function "Range of a function") (or image) and the set of elements that are mapped to the zero vector, called the [kernel](https://en.wikipedia.org/wiki/Kernel_(linear_operator) "Kernel (linear operator)") of the map. All these questions can be solved by using [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") or some variant of this [algorithm](https://en.wikipedia.org/wiki/Algorithm "Algorithm").

### Subspaces, span, and basis

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=4 "Edit section: Subspaces, span, and basis")]

The study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called [linear subspaces](https://en.wikipedia.org/wiki/Linear_subspace "Linear subspace"). More precisely, a linear subspace of a vector space V over a field F is a [subset](https://en.wikipedia.org/wiki/Subset "Subset")W of V such that **u** + **v** and _a_**u** are in W, for every **u**, **v** in W, and every a in F. (These conditions suffice for implying that W is a vector space.)

For example, given a linear map _T_: _V_ → _W_, the [image](https://en.wikipedia.org/wiki/Image_(function) "Image (function)")_T_(_V_) of V, and the [inverse image](https://en.wikipedia.org/wiki/Inverse_image "Inverse image")_T_−1(**0**) of **0** (called [kernel](https://en.wikipedia.org/wiki/Kernel_(linear_algebra) "Kernel (linear algebra)") or null space), are linear subspaces of W and V, respectively.

Another important way of forming a subspace is to consider [linear combinations](https://en.wikipedia.org/wiki/Linear_combination "Linear combination") of a set S of vectors: the set of all sums

![Image 9: {\displaystyle a_{1}\mathbf {v} _{1}+a_{2}\mathbf {v} _{2}+\cdots +a_{k}\mathbf {v} _{k},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6879901e55d5c805997cd7e9c44581e9bf717274)
where **v**1, **v**2, ..., **v**_k_ are in S, and _a_ 1, _a_ 2, ..., _a_ _k_ are in F form a linear subspace called the [span](https://en.wikipedia.org/wiki/Linear_span "Linear span") of S. The span of S is also the intersection of all linear subspaces containing S. In other words, it is the smallest (for the inclusion relation) linear subspace containing S.

A set of vectors is [linearly independent](https://en.wikipedia.org/wiki/Linearly_independent "Linearly independent") if none is in the span of the others. Equivalently, a set S of vectors is linearly independent if the only way to express the zero vector as a linear combination of elements of S is to take zero for every coefficient a i.

A set of vectors that spans a vector space is called a [spanning set](https://en.wikipedia.org/wiki/Spanning_set "Spanning set") or [generating set](https://en.wikipedia.org/wiki/Generating_set "Generating set"). If a spanning set S is _linearly dependent_ (that is not linearly independent), then some element **w** of S is in the span of the other elements of S, and the span would remain the same if one were to remove **w** from S. One may continue to remove elements of S until getting a _linearly independent spanning set_. Such a linearly independent set that spans a vector space V is called a [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra) "Basis (linear algebra)") of _V_. The importance of bases lies in the fact that they are simultaneously minimal-generating sets and maximal independent sets. More precisely, if S is a linearly independent set, and T is a spanning set such that _S_ ⊆ _T_, then there is a basis B such that _S_ ⊆ _B_ ⊆ _T_.

Any two bases of a vector space _V_ have the same [cardinality](https://en.wikipedia.org/wiki/Cardinality "Cardinality"), which is called the [dimension](https://en.wikipedia.org/wiki/Dimension_(vector_space) "Dimension (vector space)") of _V_; this is the [dimension theorem for vector spaces](https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces "Dimension theorem for vector spaces"). Moreover, two vector spaces over the same field F are [isomorphic](https://en.wikipedia.org/wiki/Isomorphic "Isomorphic") if and only if they have the same dimension.[[9]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-10)

If any basis of _V_ (and therefore every basis) has a finite number of elements, _V_ is a _finite-dimensional vector space_. If _U_ is a subspace of _V_, then dim _U_ ≤ dim _V_. In the case where _V_ is finite-dimensional, the equality of the dimensions implies _U_ = _V_.

If _U_ 1 and _U_ 2 are subspaces of _V_, then

![Image 10: {\displaystyle \dim(U_{1}+U_{2})=\dim U_{1}+\dim U_{2}-\dim(U_{1}\cap U_{2}),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b2f0694e66688bc88f285b1ae33a68d046cb3df0)
where _U_ 1 + _U_ 2 denotes the span of _U_ 1 ∪ _U_ 2.[[10]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-11)

Matrices allow explicit manipulation of finite-dimensional vector spaces and [linear maps](https://en.wikipedia.org/wiki/Linear_map "Linear map"). Their theory is thus an essential part of linear algebra.

Let V be a finite-dimensional vector space over a field _F_, and (**v**1, **v**2, ..., **v**_m_) be a basis of _V_ (thus m is the dimension of _V_). By definition of a basis, the map

![Image 11: {\displaystyle {\begin{aligned}(a_{1},\ldots ,a_{m})&\mapsto a_{1}\mathbf {v} _{1}+\cdots a_{m}\mathbf {v} _{m}\\F^{m}&\to V\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/46ee9985b8cbfdc38e8485c759129f7f0f953db8)
is a [bijection](https://en.wikipedia.org/wiki/Bijection "Bijection") from _F m_, the set of the [sequences](https://en.wikipedia.org/wiki/Sequence_(mathematics) "Sequence (mathematics)") of m elements of F, onto V. This is an [isomorphism](https://en.wikipedia.org/wiki/Isomorphism "Isomorphism") of vector spaces, if _F m_ is equipped with its standard structure of vector space, where vector addition and scalar multiplication are done component by component.

This isomorphism allows representing a vector by its [inverse image](https://en.wikipedia.org/wiki/Inverse_image "Inverse image") under this isomorphism, that is by the [coordinate vector](https://en.wikipedia.org/wiki/Coordinate_vector "Coordinate vector")(_a_ 1, ..., _a m_) or by the [column matrix](https://en.wikipedia.org/wiki/Column_matrix "Column matrix")

![Image 12: {\displaystyle {\begin{bmatrix}a_{1}\\\vdots \\a_{m}\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/70a031c92741d94ef93fa6613a3f993940c41943)
If W is another finite dimensional vector space (possibly the same), with a basis (**w**1, ..., **w**_n_), a linear map f from W to V is well defined by its values on the basis elements, that is (_f_(**w**1), ..., _f_(**w**_n_)). Thus, f is well represented by the list of the corresponding column matrices. That is, if

![Image 13: {\displaystyle f(w_{j})=a_{1,j}v_{1}+\cdots +a_{m,j}v_{m},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/866d92a12b845daa665a5d1ea59a0052137b1a37)
for _j_ = 1, ..., _n_, then f is represented by the matrix

![Image 14: {\displaystyle {\begin{bmatrix}a_{1,1}&\cdots &a_{1,n}\\\vdots &\ddots &\vdots \\a_{m,1}&\cdots &a_{m,n}\end{bmatrix}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5384246603a7c16c37ce4c5a430c50d590f8e0c4)
with m rows and n columns.

[Matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication") is defined in such a way that the product of two matrices is the matrix of the [composition](https://en.wikipedia.org/wiki/Function_composition "Function composition") of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing the same concepts.

Two matrices that encode the same linear transformation in different bases are called [similar](https://en.wikipedia.org/wiki/Similar_(linear_algebra) "Similar (linear algebra)"). It can be proved that two matrices are similar if and only if one can transform one into the other by [elementary row and column operations](https://en.wikipedia.org/wiki/Elementary_matrix "Elementary matrix"). For a matrix representing a linear map from W to V, the row operations correspond to change of bases in V and the column operations correspond to change of bases in W. Every matrix is similar to an [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix "Identity matrix") possibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map from W to V, there are bases such that a part of the basis of W is mapped bijectively on a part of the basis of V, and that the remaining basis elements of W, if any, are mapped to zero. [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") is the basic algorithm for finding these elementary operations, and proving these results.

A finite set of linear equations in a finite set of variables, for example, _x_ 1, _x_ 2, ..., _x n_, or _x_, _y_, ..., _z_ is called a **system of linear equations** or a **linear system**.[[11]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-12)[[12]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-13)[[13]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-14)[[14]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-15)[[15]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-16)

Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory have been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.

For example, let

be a linear system.

To such a system, one may associate its matrix

![Image 15: {\displaystyle M=\left[{\begin{array}{rrr}2&1&-1\\-3&-1&2\\-2&1&2\end{array}}\right].}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fc5140ca13be8f555186b8d57b6223a4ef2e1b0b)
and its right member vector

![Image 16: {\displaystyle \mathbf {v} ={\begin{bmatrix}8\\-11\\-3\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0a40dba1d9901273a39c9ba1bae9eb4c0cada2ef)
Let T be the linear transformation associated with the matrix M. A solution of the system (**[S](https://en.wikipedia.org/wiki/Linear_algebra#math_S)**) is a vector

![Image 17: {\displaystyle \mathbf {X} ={\begin{bmatrix}x\\y\\z\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e1ff89f5145cb10c907b9d8bf628579b19734c0f)
such that

![Image 18: {\displaystyle T(\mathbf {X} )=\mathbf {v} ,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5adab660fcaac2d62105be89cae62af17c0900a9)
that is an element of the [preimage](https://en.wikipedia.org/wiki/Preimage "Preimage") of v by T.

Let (**[S′](https://en.wikipedia.org/wiki/Linear_algebra#math_S%E2%80%B2)**) be the associated [homogeneous system](https://en.wikipedia.org/wiki/Homogeneous_system_of_linear_equations "Homogeneous system of linear equations"), where the right-hand sides of the equations are put to zero:

The solutions of (**[S′](https://en.wikipedia.org/wiki/Linear_algebra#math_S%E2%80%B2)**) are exactly the elements of the [kernel](https://en.wikipedia.org/wiki/Kernel_(linear_algebra) "Kernel (linear algebra)") of T or, equivalently, M.

The [Gaussian-elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") consists of performing [elementary row operations](https://en.wikipedia.org/wiki/Elementary_row_operation "Elementary row operation") on the [augmented matrix](https://en.wikipedia.org/wiki/Augmented_matrix "Augmented matrix")

![Image 19: {\displaystyle \left[\!{\begin{array}{c|c}M&\mathbf {v} \end{array}}\!\right]=\left[{\begin{array}{rrr|r}2&1&-1&8\\-3&-1&2&-11\\-2&1&2&-3\end{array}}\right]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6a94bf6a98576cd520287535af3a6587376f9be8)
for putting it in [reduced row echelon form](https://en.wikipedia.org/wiki/Reduced_row_echelon_form "Reduced row echelon form"). These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is

![Image 20: {\displaystyle \left[\!{\begin{array}{c|c}M&\mathbf {v} \end{array}}\!\right]=\left[{\begin{array}{rrr|r}1&0&0&2\\0&1&0&3\\0&0&1&-1\end{array}}\right],}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a6a99163495ae1cf328208d89cadcdf23397fba0)
showing that the system (**[S](https://en.wikipedia.org/wiki/Linear_algebra#math_S)**) has the unique solution

![Image 21: {\displaystyle {\begin{aligned}x&=2\\y&=3\\z&=-1.\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9995c77c5af33f793e99b15e577480243041c6c5)
It follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the [ranks](https://en.wikipedia.org/wiki/Rank_of_a_matrix "Rank of a matrix"), [kernels](https://en.wikipedia.org/wiki/Kernel_(linear_algebra) "Kernel (linear algebra)"), [matrix inverses](https://en.wikipedia.org/wiki/Matrix_inverse "Matrix inverse").

Endomorphisms and square matrices
---------------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=7 "Edit section: Endomorphisms and square matrices")]

A linear [endomorphism](https://en.wikipedia.org/wiki/Endomorphism "Endomorphism") is a linear map that maps a vector space V to itself. If V has a basis of n elements, such an endomorphism is represented by a square matrix of size n.

Concerning general linear maps, linear endomorphisms, and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including [geometric transformations](https://en.wikipedia.org/wiki/Geometric_transformation "Geometric transformation"), [coordinate changes](https://en.wikipedia.org/wiki/Coordinate_change "Coordinate change"), [quadratic forms](https://en.wikipedia.org/wiki/Quadratic_form "Quadratic form"), and many other parts of mathematics.

The _determinant_ of a square matrix A is defined to be[[16]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-17)

![Image 22: {\displaystyle \sum _{\sigma \in S_{n}}(-1)^{\sigma }a_{1\sigma (1)}\cdots a_{n\sigma (n)},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ad2255d7cf0d7ad8e05b318ec9b67b136a13d796)
where _S n_ is the [group of all permutations](https://en.wikipedia.org/wiki/Symmetric_group "Symmetric group") of n elements, σ is a permutation, and (−1)_σ_ the [parity](https://en.wikipedia.org/wiki/Parity_of_a_permutation "Parity of a permutation") of the permutation. A matrix is [invertible](https://en.wikipedia.org/wiki/Invertible_matrix "Invertible matrix") if and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field).

[Cramer's rule](https://en.wikipedia.org/wiki/Cramer%27s_rule "Cramer's rule") is a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression "Closed-form expression"), in terms of determinants, of the solution of a [system of n linear equations in n unknowns](https://en.wikipedia.org/wiki/System_of_linear_equations "System of linear equations"). Cramer's rule is useful for reasoning about the solution, but, except for _n_ = 2 or 3, it is rarely used for computing a solution, since [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") is a faster algorithm.

The _determinant of an endomorphism_ is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense since this determinant is independent of the choice of the basis.

### Eigenvalues and eigenvectors

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=9 "Edit section: Eigenvalues and eigenvectors")]

If f is a linear endomorphism of a vector space V over a field F, an _eigenvector_ of f is a nonzero vector v of V such that _f_(_v_) = _av_ for some scalar a in F. This scalar a is an _eigenvalue_ of f.

If the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix z; the equation defining eigenvectors and eigenvalues becomes

![Image 23: {\displaystyle Mz=az.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9256a6bf21c144488d5efa785da63950abd46958)
Using the [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix "Identity matrix")I, whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten

![Image 24: {\displaystyle (M-aI)z=0.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/22d42e0d9e0b62bc4e78cfd102b21e4a7217ac6c)
As z is supposed to be nonzero, this means that _M_ – _aI_ is a [singular matrix](https://en.wikipedia.org/wiki/Singular_matrix "Singular matrix"), and thus that its determinant det (_M_ − _aI_) equals zero. The eigenvalues are thus the [roots](https://en.wikipedia.org/wiki/Root_of_a_function "Root of a function") of the [polynomial](https://en.wikipedia.org/wiki/Polynomial "Polynomial")

![Image 25: {\displaystyle \det(xI-M).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/29c19c2b35265004add12e16a97d849ba94764d0)
If V is of dimension n, this is a [monic polynomial](https://en.wikipedia.org/wiki/Monic_polynomial "Monic polynomial") of degree n, called the [characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial "Characteristic polynomial") of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.

If a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix "Diagonal matrix") such that the entries on the [main diagonal](https://en.wikipedia.org/wiki/Main_diagonal "Main diagonal") are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to be [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix "Diagonalizable matrix"). More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after [extending](https://en.wikipedia.org/wiki/Field_extension "Field extension") the field of scalars. In this extended sense, if the characteristic polynomial is [square-free](https://en.wikipedia.org/wiki/Square-free_polynomial "Square-free polynomial"), then the matrix is diagonalizable.

A [symmetric matrix](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix") is always diagonalizable. There are non-diagonalizable matrices, the simplest being

![Image 26: {\displaystyle {\begin{bmatrix}0&1\\0&0\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb69343e096235ec162a5f08f9a59c684687c041)
(it cannot be diagonalizable since its square is the [zero matrix](https://en.wikipedia.org/wiki/Zero_matrix "Zero matrix"), and the square of a nonzero diagonal matrix is never zero).

When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The [Frobenius normal form](https://en.wikipedia.org/wiki/Frobenius_normal_form "Frobenius normal form") does not need to extend the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The [Jordan normal form](https://en.wikipedia.org/wiki/Jordan_normal_form "Jordan normal form") requires to extension of the field of scalar for containing all eigenvalues and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.

A [linear form](https://en.wikipedia.org/wiki/Linear_form "Linear form") is a linear map from a vector space V over a field F to the field of scalars F, viewed as a vector space over itself. Equipped by [pointwise](https://en.wikipedia.org/wiki/Pointwise "Pointwise") addition and multiplication by a scalar, the linear forms form a vector space, called the **dual space** of V, and usually denoted V*[[17]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-18) or V′.[[18]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-19)[[19]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-20)

If **v**1, ..., **v**_n_ is a basis of V (this implies that V is finite-dimensional), then one can define, for _i_ = 1, ..., _n_, a linear map _v i_* such that _v i_*(**v**_i_) = 1 and _v i_*(**v**_j_) = 0 if _j_ ≠ _i_. These linear maps form a basis of _V_*, called the [dual basis](https://en.wikipedia.org/wiki/Dual_basis "Dual basis") of **v**1, ..., **v**_n_. (If V is not finite-dimensional, the _v i_* may be defined similarly; they are linearly independent, but do not form a basis.)

For **v** in V, the map

![Image 27: {\displaystyle f\to f(\mathbf {v} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5bfba9654a09f3f65a2a150432f9e27a9c0fbdb4)
is a linear form on V*. This defines the [canonical linear map](https://en.wikipedia.org/wiki/Canonical_map "Canonical map") from V into (_V_*)*, the dual of V*, called the **[double dual](https://en.wikipedia.org/wiki/Double_dual "Double dual")** or **[bidual](https://en.wikipedia.org/wiki/Bidual "Bidual")** of V. This canonical map is an [isomorphism](https://en.wikipedia.org/wiki/Isomorphism "Isomorphism") if V is finite-dimensional, and this allows identifying V with its bidual. (In the infinite-dimensional case, the canonical map is injective, but not surjective.)

There is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the [bra–ket notation](https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation "Bra–ket notation")

![Image 28: {\displaystyle \langle f,\mathbf {x} \rangle }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e11176c78659642becbfb2d3d65abe2e4a34d39)
for denoting _f_(**x**).

Let

![Image 29: {\displaystyle f:V\to W}](https://wikimedia.org/api/rest_v1/media/math/render/svg/574dffa1c85efaef6b6ef553ebd8ad9cf7f87fd6)
be a linear map. For every linear form h on W, the [composite function](https://en.wikipedia.org/wiki/Composite_function "Composite function")_h_ ∘ _f_ is a linear form on V. This defines a linear map

![Image 30: {\displaystyle f^{*}:W^{*}\to V^{*}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d73927ebc5f7b531d76d7fa92c13722386219041)
between the dual spaces, which is called the **dual** or the **transpose** of f.

If V and W are finite-dimensional, and M is the matrix of f in terms of some ordered bases, then the matrix of f* over the dual bases is the [transpose](https://en.wikipedia.org/wiki/Transpose "Transpose")_M_ T of M, obtained by exchanging rows and columns.

If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in [bra–ket notation](https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation "Bra–ket notation") by

![Image 31: {\displaystyle \langle h^{\mathsf {T}},M\mathbf {v} \rangle =\langle h^{\mathsf {T}}M,\mathbf {v} \rangle .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/982967567f1a808c805fa4e7ce60a71fe9084498)
To highlight this symmetry, the two members of this equality are sometimes written

![Image 32: {\displaystyle \langle h^{\mathsf {T}}\mid M\mid \mathbf {v} \rangle .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/037596ed4087a884c3e4b0227a07ce7b1085cebe)

### Inner-product spaces

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=12 "Edit section: Inner-product spaces")]

Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an [inner product](https://en.wikipedia.org/wiki/Inner_product "Inner product"). The inner product is an example of a [bilinear form](https://en.wikipedia.org/wiki/Bilinear_form "Bilinear form"), and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an _inner product_ is a map.

![Image 33: {\displaystyle \langle \cdot ,\cdot \rangle :V\times V\to F}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c7871e0b592252e518983a32886d33692d7b4614)
that satisfies the following three [axioms](https://en.wikipedia.org/wiki/Axiom "Axiom") for all vectors **u**, **v**, **w** in _V_ and all scalars _a_ in _F_:[[20]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Jain-21)[[21]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Prugovec%CC%86ki-22)

*   [Conjugate](https://en.wikipedia.org/wiki/Complex_conjugate "Complex conjugate") symmetry: ![Image 34: {\displaystyle \langle \mathbf {u} ,\mathbf {v} \rangle ={\overline {\langle \mathbf {v} ,\mathbf {u} \rangle }}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/80733e7e51ef049170d29ce6f4e85fd47feee80f)

In ![Image 35: {\displaystyle \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/786849c765da7a84dbc3cce43e96aad58a5868dc), it is symmetric.with equality only for **v** = 0.
We can define the length of a vector **v** in _V_ by

![Image 36: {\displaystyle \|\mathbf {v} \|^{2}=\langle \mathbf {v} ,\mathbf {v} \rangle ,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b1d3513b25d826a8f6c02269655edf6517c29c67)
and we can prove the [Cauchy–Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality "Cauchy–Schwarz inequality"):

![Image 37: {\displaystyle |\langle \mathbf {u} ,\mathbf {v} \rangle |\leq \|\mathbf {u} \|\cdot \|\mathbf {v} \|.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8c1449df451ce2c043c47b58132bb97d9d91da9f)
In particular, the quantity

![Image 38: {\displaystyle {\frac {|\langle \mathbf {u} ,\mathbf {v} \rangle |}{\|\mathbf {u} \|\cdot \|\mathbf {v} \|}}\leq 1,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6851acc32facee4a5b99a4645e55da2d33fe4fcd)
and so we can call this quantity the cosine of the angle between the two vectors.

Two vectors are orthogonal if ⟨**u**, **v**⟩ = 0. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the [Gram–Schmidt](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt "Gram–Schmidt") procedure. Orthonormal bases are particularly easy to deal with, since if **v** = _a_ 1**v**1 + ⋯ + _a n_**v**_n_, then

![Image 39: {\displaystyle a_{i}=\langle \mathbf {v} ,\mathbf {v} _{i}\rangle .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/79e9249c7723dc4eff39ecf5dc91c1e5f2989c04)
The inner product facilitates the construction of many useful concepts. For instance, given a transform _T_, we can define its [Hermitian conjugate](https://en.wikipedia.org/wiki/Hermitian_conjugate "Hermitian conjugate")_T*_ as the linear transform satisfying

![Image 40: {\displaystyle \langle T\mathbf {u} ,\mathbf {v} \rangle =\langle \mathbf {u} ,T^{*}\mathbf {v} \rangle .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fb316df0527a9010dffd4beecdee839037111c9c)
If _T_ satisfies _TT*_ = _T*T_, we call _T_[normal](https://en.wikipedia.org/wiki/Normal_matrix "Normal matrix"). It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span _V_.

Relationship with geometry
--------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=13 "Edit section: Relationship with geometry")]

There is a strong relationship between linear algebra and [geometry](https://en.wikipedia.org/wiki/Geometry "Geometry"), which started with the introduction by [René Descartes](https://en.wikipedia.org/wiki/Ren%C3%A9_Descartes "René Descartes"), in 1637, of [Cartesian coordinates](https://en.wikipedia.org/wiki/Cartesian_coordinates "Cartesian coordinates"). In this new (at that time) geometry, now called [Cartesian geometry](https://en.wikipedia.org/wiki/Cartesian_geometry "Cartesian geometry"), points are represented by [Cartesian coordinates](https://en.wikipedia.org/wiki/Cartesian_coordinates "Cartesian coordinates"), which are sequences of three real numbers (in the case of the usual [three-dimensional space](https://en.wikipedia.org/wiki/Three-dimensional_space "Three-dimensional space")). The basic objects of geometry, which are [lines](https://en.wikipedia.org/wiki/Line_(geometry) "Line (geometry)") and [planes](https://en.wikipedia.org/wiki/Plane_(geometry) "Plane (geometry)") are represented by linear equations. Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra.

Most [geometric transformation](https://en.wikipedia.org/wiki/Geometric_transformation "Geometric transformation"), such as [translations](https://en.wikipedia.org/wiki/Translation_(geometry) "Translation (geometry)"), [rotations](https://en.wikipedia.org/wiki/Rotation "Rotation"), [reflections](https://en.wikipedia.org/wiki/Reflection_(mathematics) "Reflection (mathematics)"), [rigid motions](https://en.wikipedia.org/wiki/Rigid_motion "Rigid motion"), [isometries](https://en.wikipedia.org/wiki/Isometry "Isometry"), and [projections](https://en.wikipedia.org/wiki/Projection_(mathematics) "Projection (mathematics)") transform lines into lines. It follows that they can be defined, specified, and studied in terms of linear maps. This is also the case of [homographies](https://en.wikipedia.org/wiki/Homography "Homography") and [Möbius transformations](https://en.wikipedia.org/wiki/M%C3%B6bius_transformation "Möbius transformation") when considered as transformations of a [projective space](https://en.wikipedia.org/wiki/Projective_space "Projective space").

Until the end of the 19th century, geometric spaces were defined by [axioms](https://en.wikipedia.org/wiki/Axiom "Axiom") relating points, lines, and planes ([synthetic geometry](https://en.wikipedia.org/wiki/Synthetic_geometry "Synthetic geometry")). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, [Projective space](https://en.wikipedia.org/wiki/Projective_space "Projective space") and [Affine space](https://en.wikipedia.org/wiki/Affine_space "Affine space")). It has been shown that the two approaches are essentially equivalent.[[22]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-23) In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including [finite fields](https://en.wikipedia.org/wiki/Finite_field "Finite field").

Presently, most textbooks introduce geometric spaces from linear algebra, and geometry is often presented, at the elementary level, as a subfield of linear algebra.

Usage and applications
----------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=14 "Edit section: Usage and applications")]

Linear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.

### Functional analysis

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=15 "Edit section: Functional analysis")]

[Functional analysis](https://en.wikipedia.org/wiki/Functional_analysis "Functional analysis") studies [function spaces](https://en.wikipedia.org/wiki/Function_space "Function space"). These are vector spaces with additional structure, such as [Hilbert spaces](https://en.wikipedia.org/wiki/Hilbert_space "Hilbert space"). Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, [quantum mechanics](https://en.wikipedia.org/wiki/Quantum_mechanics "Quantum mechanics") ([wave functions](https://en.wikipedia.org/wiki/Wave_function "Wave function")) and [Fourier analysis](https://en.wikipedia.org/wiki/Fourier_analysis "Fourier analysis") ([orthogonal basis](https://en.wikipedia.org/wiki/Orthogonal_basis "Orthogonal basis")).

### Scientific computation

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=16 "Edit section: Scientific computation")]

Nearly all [scientific computations](https://en.wikipedia.org/wiki/Scientific_computation "Scientific computation") involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms "Basic Linear Algebra Subprograms") and [LAPACK](https://en.wikipedia.org/wiki/LAPACK "LAPACK") are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, to adapt them to the specificities of the computer ([cache](https://en.wikipedia.org/wiki/Cache_(computing) "Cache (computing)") size, number of available [cores](https://en.wikipedia.org/wiki/Multi-core_processor "Multi-core processor"),...).

Since the 1960s there have been processors with specialized instructions[[23]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-24) for optimizing the operations of linear algebra, optional array processors[[24]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-25) under the control of a conventional processor, supercomputers[[25]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-26)[[26]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-Star100HW-27)[[27]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-cray1hw-28) designed for array processing and conventional processors augmented[[28]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-29) with vector registers.

Some contemporary [processors](https://en.wikipedia.org/wiki/Processor_(computing) "Processor (computing)"), typically [graphics processing units](https://en.wikipedia.org/wiki/Graphics_processing_units "Graphics processing units") (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.[[29]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-30)

### Geometry of ambient space

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=17 "Edit section: Geometry of ambient space")]

The [modeling](https://en.wikipedia.org/wiki/Mathematical_model "Mathematical model") of [ambient space](https://en.wikipedia.org/wiki/Ambient_space "Ambient space") is based on [geometry](https://en.wikipedia.org/wiki/Geometry "Geometry"). Sciences concerned with this space use geometry widely. This is the case with [mechanics](https://en.wikipedia.org/wiki/Mechanics "Mechanics") and [robotics](https://en.wikipedia.org/wiki/Robotics "Robotics"), for describing [rigid body dynamics](https://en.wikipedia.org/wiki/Rigid_body_dynamics "Rigid body dynamics"); [geodesy](https://en.wikipedia.org/wiki/Geodesy "Geodesy") for describing [Earth shape](https://en.wikipedia.org/wiki/Earth_shape "Earth shape"); [perspectivity](https://en.wikipedia.org/wiki/Perspectivity "Perspectivity"), [computer vision](https://en.wikipedia.org/wiki/Computer_vision "Computer vision"), and [computer graphics](https://en.wikipedia.org/wiki/Computer_graphics "Computer graphics"), for describing the relationship between a scene and its plane representation; and many other scientific domains.

In all these applications, [synthetic geometry](https://en.wikipedia.org/wiki/Synthetic_geometry "Synthetic geometry") is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with [coordinates](https://en.wikipedia.org/wiki/Coordinates "Coordinates"). This requires the heavy use of linear algebra.

### Study of complex systems

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=18 "Edit section: Study of complex systems")]

Most physical phenomena are modeled by [partial differential equations](https://en.wikipedia.org/wiki/Partial_differential_equation "Partial differential equation"). To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting [cells](https://en.wikipedia.org/wiki/Discretization "Discretization"). For [linear systems](https://en.wikipedia.org/wiki/Linear_system "Linear system") this interaction involves [linear functions](https://en.wikipedia.org/wiki/Linear_function "Linear function"). For [nonlinear systems](https://en.wikipedia.org/wiki/Nonlinear_systems "Nonlinear systems"), this interaction is often approximated by linear functions.[[b]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-31)This is called a linear model or first-order approximation. Linear models are frequently used for complex nonlinear real-world systems because they make [parametrization](https://en.wikipedia.org/wiki/Parametrization_(geometry) "Parametrization (geometry)") more manageable.[[30]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-32) In both cases, very large matrices are generally involved. [Weather forecasting](https://en.wikipedia.org/wiki/Weather_forecasting "Weather forecasting") (or more specifically, [parametrization for atmospheric modeling](https://en.wikipedia.org/wiki/Parametrization_(atmospheric_modeling) "Parametrization (atmospheric modeling)")) is a typical example of a real-world application, where the whole Earth [atmosphere](https://en.wikipedia.org/wiki/Atmosphere "Atmosphere") is divided into cells of, say, 100 km of width and 100 km of height.

### Fluid mechanics, fluid dynamics, and thermal energy systems

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=19 "Edit section: Fluid mechanics, fluid dynamics, and thermal energy systems")]

[[31]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-33)[[32]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-34)[[33]](https://en.wikipedia.org/wiki/Linear_algebra#cite_note-35)

Linear algebra, a branch of mathematics dealing with [vector spaces](https://en.wikipedia.org/wiki/Vector_spaces "Vector spaces") and [linear mappings](https://en.wikipedia.org/wiki/Linear_mapping "Linear mapping") between these spaces, plays a critical role in various engineering disciplines, including [fluid mechanics](https://en.wikipedia.org/wiki/Fluid_mechanics "Fluid mechanics"), [fluid dynamics](https://en.wikipedia.org/wiki/Fluid_dynamics "Fluid dynamics"), and [thermal energy](https://en.wikipedia.org/wiki/Thermal_energy "Thermal energy") systems. Its application in these fields is multifaceted and indispensable for solving complex problems.

In [fluid mechanics](https://en.wikipedia.org/wiki/Fluid_mechanics "Fluid mechanics"), linear algebra is integral to understanding and solving problems related to the behavior of fluids. It assists in the modeling and simulation of fluid flow, providing essential tools for the analysis of [fluid dynamics](https://en.wikipedia.org/wiki/Fluid_dynamics "Fluid dynamics") problems. For instance, linear algebraic techniques are used to solve systems of [differential equations](https://en.wikipedia.org/wiki/Differential_equations "Differential equations") that describe fluid motion. These equations, often complex and [non-linear](https://en.wikipedia.org/wiki/Non-linear "Non-linear"), can be linearized using linear algebra methods, allowing for simpler solutions and analyses.

In the field of fluid dynamics, linear algebra finds its application in [computational fluid dynamics](https://en.wikipedia.org/wiki/Computational_fluid_dynamics "Computational fluid dynamics") (CFD), a branch that uses [numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis "Numerical analysis") and [data structures](https://en.wikipedia.org/wiki/Data_structure "Data structure") to solve and analyze problems involving fluid flows. CFD relies heavily on linear algebra for the computation of fluid flow and [heat transfer](https://en.wikipedia.org/wiki/Heat_transfer "Heat transfer") in various applications. For example, the [Navier–Stokes equations](https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations "Navier–Stokes equations"), fundamental in [fluid dynamics](https://en.wikipedia.org/wiki/Fluid_dynamics "Fluid dynamics"), are often solved using techniques derived from linear algebra. This includes the use of [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)") and [vectors](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics) "Vector (mathematics and physics)") to represent and manipulate fluid flow fields.

Furthermore, linear algebra plays a crucial role in [thermal energy](https://en.wikipedia.org/wiki/Thermal_energy "Thermal energy") systems, particularly in [power systems](https://en.wikipedia.org/wiki/Power_systems "Power systems") analysis. It is used to model and optimize the generation, [transmission](https://en.wikipedia.org/wiki/Electric_power_transmission "Electric power transmission"), and [distribution](https://en.wikipedia.org/wiki/Electric_power_distribution "Electric power distribution") of electric power. Linear algebraic concepts such as matrix operations and [eigenvalue](https://en.wikipedia.org/wiki/Eigenvalue "Eigenvalue") problems are employed to enhance the efficiency, reliability, and economic performance of [power systems](https://en.wikipedia.org/wiki/Power_systems "Power systems"). The application of linear algebra in this context is vital for the design and operation of modern [power systems](https://en.wikipedia.org/wiki/Power_systems "Power systems"), including [renewable energy](https://en.wikipedia.org/wiki/Renewable_energy "Renewable energy") sources and [smart grids](https://en.wikipedia.org/wiki/Smart_grid "Smart grid").

Overall, the application of linear algebra in [fluid mechanics](https://en.wikipedia.org/wiki/Fluid_mechanics "Fluid mechanics"), [fluid dynamics](https://en.wikipedia.org/wiki/Fluid_dynamics "Fluid dynamics"), and [thermal energy](https://en.wikipedia.org/wiki/Thermal_energy "Thermal energy") systems is an example of the profound interconnection between [mathematics](https://en.wikipedia.org/wiki/Mathematics "Mathematics") and [engineering](https://en.wikipedia.org/wiki/Engineering "Engineering"). It provides engineers with the necessary tools to model, analyze, and solve complex problems in these domains, leading to advancements in technology and industry.

Extensions and generalizations
------------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=20 "Edit section: Extensions and generalizations")]

This section presents several related topics that do not appear generally in elementary textbooks on linear algebra but are commonly considered, in advanced mathematics, as parts of linear algebra.

The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a [ring](https://en.wikipedia.org/wiki/Ring_(mathematics) "Ring (mathematics)")R, and this gives the structure called a **module** over R, or R-module.

The concepts of linear independence, span, basis, and linear maps (also called [module homomorphisms](https://en.wikipedia.org/wiki/Module_homomorphism "Module homomorphism")) are defined for modules exactly as for vector spaces, with the essential difference that, if R is not a field, there are modules that do not have any basis. The modules that have a basis are the [free modules](https://en.wikipedia.org/wiki/Free_module "Free module"), and those that are spanned by a finite set are the [finitely generated modules](https://en.wikipedia.org/wiki/Finitely_generated_module "Finitely generated module"). Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that [determinants](https://en.wikipedia.org/wiki/Determinant "Determinant") exist only if the ring is [commutative](https://en.wikipedia.org/wiki/Commutative_ring "Commutative ring"), and that a square matrix over a commutative ring is [invertible](https://en.wikipedia.org/wiki/Invertible_matrix "Invertible matrix") only if its determinant has a [multiplicative inverse](https://en.wikipedia.org/wiki/Multiplicative_inverse "Multiplicative inverse") in the ring.

Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a [cokernel](https://en.wikipedia.org/wiki/Cokernel "Cokernel") of a homomorphism of free modules.

Modules over the integers can be identified with [abelian groups](https://en.wikipedia.org/wiki/Abelian_group "Abelian group"), since the multiplication by an integer may be identified as a repeated addition. Most of the theory of abelian groups may be extended to modules over a [principal ideal domain](https://en.wikipedia.org/wiki/Principal_ideal_domain "Principal ideal domain"). In particular, over a principal ideal domain, every submodule of a free module is free, and the [fundamental theorem of finitely generated abelian groups](https://en.wikipedia.org/wiki/Fundamental_theorem_of_finitely_generated_abelian_groups "Fundamental theorem of finitely generated abelian groups") may be extended straightforwardly to finitely generated modules over a principal ring.

There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a [computational complexity](https://en.wikipedia.org/wiki/Computational_complexity "Computational complexity") that is much higher than similar algorithms over a field. For more details, see [Linear equation over a ring](https://en.wikipedia.org/wiki/Linear_equation_over_a_ring "Linear equation over a ring").

### Multilinear algebra and tensors

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=22 "Edit section: Multilinear algebra and tensors")]

In [multilinear algebra](https://en.wikipedia.org/wiki/Multilinear_algebra "Multilinear algebra"), one considers multivariable linear transformations, that is, mappings that are linear in each of several different variables. This line of inquiry naturally leads to the idea of the [dual space](https://en.wikipedia.org/wiki/Dual_space "Dual space"), the vector space _V*_ consisting of linear maps _f_: _V_ → _F_ where _F_ is the field of scalars. Multilinear maps _T_: _V n_ → _F_ can be described via [tensor products](https://en.wikipedia.org/wiki/Tensor_product "Tensor product") of elements of _V*_.

If, in addition to vector addition and scalar multiplication, there is a bilinear vector product _V_ × _V_ → _V_, the vector space is called an [algebra](https://en.wikipedia.org/wiki/Algebra_over_a_field "Algebra over a field"); for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).

### Topological vector spaces

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=23 "Edit section: Topological vector spaces")]

Vector spaces that are not finite-dimensional often require additional structure to be tractable. A [normed vector space](https://en.wikipedia.org/wiki/Normed_vector_space "Normed vector space") is a vector space along with a function called a [norm](https://en.wikipedia.org/wiki/Norm_(mathematics) "Norm (mathematics)"), which measures the "size" of elements. The norm induces a [metric](https://en.wikipedia.org/wiki/Metric_(mathematics) "Metric (mathematics)"), which measures the distance between elements, and induces a [topology](https://en.wikipedia.org/wiki/Topological_space "Topological space"), which allows for a definition of continuous maps. The metric also allows for a definition of [limits](https://en.wikipedia.org/wiki/Limit_(mathematics) "Limit (mathematics)") and [completeness](https://en.wikipedia.org/wiki/Complete_metric_space "Complete metric space") – a normed vector space that is complete is known as a [Banach space](https://en.wikipedia.org/wiki/Banach_space "Banach space"). A complete metric space along with the additional structure of an [inner product](https://en.wikipedia.org/wiki/Inner_product_space "Inner product space") (a conjugate symmetric [sesquilinear form](https://en.wikipedia.org/wiki/Sesquilinear_form "Sesquilinear form")) is known as a [Hilbert space](https://en.wikipedia.org/wiki/Hilbert_space "Hilbert space"), which is in some sense a particularly well-behaved Banach space. [Functional analysis](https://en.wikipedia.org/wiki/Functional_analysis "Functional analysis") applies the methods of linear algebra alongside those of [mathematical analysis](https://en.wikipedia.org/wiki/Mathematical_analysis "Mathematical analysis") to study various function spaces; the central objects of study in functional analysis are [L p spaces](https://en.wikipedia.org/wiki/Lp_space "Lp space"), which are Banach spaces, and especially the _L_ 2 space of square-integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods.

*   [Fundamental matrix (computer vision)](https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision) "Fundamental matrix (computer vision)")
*   [Geometric algebra](https://en.wikipedia.org/wiki/Geometric_algebra "Geometric algebra")
*   [Linear programming](https://en.wikipedia.org/wiki/Linear_programming "Linear programming")
*   [Linear regression](https://en.wikipedia.org/wiki/Linear_regression "Linear regression"), a statistical estimation method
*   [Numerical linear algebra](https://en.wikipedia.org/wiki/Numerical_linear_algebra "Numerical linear algebra")
*   [Outline of linear algebra](https://en.wikipedia.org/wiki/Outline_of_linear_algebra "Outline of linear algebra")
*   [Transformation matrix](https://en.wikipedia.org/wiki/Transformation_matrix "Transformation matrix")

1.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-9)**This axiom is not asserting the associativity of an operation, since there are two operations in question, scalar multiplication _b_**v**; and field multiplication: _ab_.
2.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-31)**This may have the consequence that some physically interesting solutions are omitted.

1.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-1)**Banerjee, Sudipto; Roy, Anindya (2014). _Linear Algebra and Matrix Analysis for Statistics_. Texts in Statistical Science (1st ed.). Chapman and Hall/CRC. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1420095388](https://en.wikipedia.org/wiki/Special:BookSources/978-1420095388 "Special:BookSources/978-1420095388").
2.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-2)**Strang, Gilbert (July 19, 2005). _Linear Algebra and Its Applications_ (4th ed.). Brooks Cole. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-03-010567-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-03-010567-8 "Special:BookSources/978-0-03-010567-8").
3.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-3)**Weisstein, Eric. ["Linear Algebra"](http://mathworld.wolfram.com/LinearAlgebra.html). _[MathWorld](https://en.wikipedia.org/wiki/MathWorld "MathWorld")_. Wolfram. Retrieved 16 April 2012.
4.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-4)**Hart, Roger (2010). [_The Chinese Roots of Linear Algebra_](https://books.google.com/books?id=zLPm3xE2qWgC). [JHU Press](https://en.wikipedia.org/wiki/JHU_Press "JHU Press"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[9780801899584](https://en.wikipedia.org/wiki/Special:BookSources/9780801899584 "Special:BookSources/9780801899584").
5.   ^ [_**a**_](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Vitulli,_Marie_5-0)[_**b**_](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Vitulli,_Marie_5-1)[_**c**_](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Vitulli,_Marie_5-2)[_**d**_](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Vitulli,_Marie_5-3)[Vitulli, Marie](https://en.wikipedia.org/wiki/Marie_A._Vitulli "Marie A. Vitulli"). ["A Brief History of Linear Algebra and Matrix Theory"](https://web.archive.org/web/20120910034016/http://darkwing.uoregon.edu/~vitulli/441.sp04/LinAlgHistory.html). _Department of Mathematics_. University of Oregon. Archived from [the original](http://darkwing.uoregon.edu/~vitulli/441.sp04/LinAlgHistory.html) on 2012-09-10. Retrieved 2014-07-08.
6.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-6)**Koecher, M., Remmert, R. (1991). Hamilton’s Quaternions. In: Numbers. Graduate Texts in Mathematics, vol 123. Springer, New York, NY. [https://doi.org/10.1007/978-1-4612-1005-4_10](https://doi.org/10.1007/978-1-4612-1005-4_10)
7.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-7)**[Benjamin Peirce](https://en.wikipedia.org/wiki/Benjamin_Peirce "Benjamin Peirce") (1872) _Linear Associative Algebra_, lithograph, new edition with corrections, notes, and an added 1875 paper by Peirce, plus notes by his son [Charles Sanders Peirce](https://en.wikipedia.org/wiki/Charles_Sanders_Peirce "Charles Sanders Peirce"), published in the _American Journal of Mathematics_ v. 4, 1881, Johns Hopkins University, pp.221–226, _Google_[Eprint](https://books.google.com/books?id=LQgPAAAAIAAJ&pg=PA221) and as an extract, D. Van Nostrand, 1882, _Google_[Eprint](https://archive.org/details/bub_gb_De0GAAAAYAAJ).
8.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-8)**[Roman (2005](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFRoman2005), ch. 1, p. 27)
9.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-10)**[Axler (2015)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFAxler2015) p. 82, §3.59
10.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-11)**[Axler (2015)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFAxler2015) p. 23, §1.45
11.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-12)**[Anton (1987](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFAnton1987), p.2)
12.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-13)**[Beauregard & Fraleigh (1973](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFBeauregardFraleigh1973), p.65)
13.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-14)**[Burden & Faires (1993](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFBurdenFaires1993), p.324)
14.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-15)**[Golub & Van Loan (1996](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFGolubVan_Loan1996), p.87)
15.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-16)**[Harper (1976](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFHarper1976), p.57)
16.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-17)**[Katznelson & Katznelson (2008)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFKatznelsonKatznelson2008) pp. 76–77, § 4.4.1–4.4.6
17.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-18)**[Katznelson & Katznelson (2008)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFKatznelsonKatznelson2008) p. 37 §2.1.3
18.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-19)**[Halmos (1974)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFHalmos1974) p. 20, §13
19.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-20)**[Axler (2015)](https://en.wikipedia.org/wiki/Linear_algebra#CITEREFAxler2015) p. 101, §3.94
20.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Jain_21-0)**P. K. Jain, Khalil Ahmad (1995). ["5.1 Definitions and basic properties of inner product spaces and Hilbert spaces"](https://books.google.com/books?id=yZ68h97pnAkC&pg=PA203). _Functional analysis_ (2nd ed.). New Age International. p.203. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[81-224-0801-X](https://en.wikipedia.org/wiki/Special:BookSources/81-224-0801-X "Special:BookSources/81-224-0801-X").
21.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Prugovec%CC%86ki_22-0)**Eduard Prugovec̆ki (1981). ["Definition 2.1"](https://books.google.com/books?id=GxmQxn2PF3IC&pg=PA18). _Quantum mechanics in Hilbert space_ (2nd ed.). Academic Press. pp.18 _ff_. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-12-566060-X](https://en.wikipedia.org/wiki/Special:BookSources/0-12-566060-X "Special:BookSources/0-12-566060-X").
22.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-23)**[Emil Artin](https://en.wikipedia.org/wiki/Emil_Artin "Emil Artin") (1957) _[Geometric Algebra](https://en.wikipedia.org/wiki/Geometric\_Algebra\_(book) "Geometric Algebra (book)")_[Interscience Publishers](https://en.wikipedia.org/wiki/Interscience_Publishers "Interscience Publishers")
23.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-24)**_IBM System/36O Model 40 - Sum of Products Instruction-RPQ W12561 - Special Systems Feature_. [IBM](https://en.wikipedia.org/wiki/IBM "IBM"). L22-6902.
24.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-25)**_IBM System/360 Custom Feature Description: 2938 Array Processor Model 1, - RPQ W24563; Model 2, RPQ 815188_. [IBM](https://en.wikipedia.org/wiki/IBM "IBM"). A24-3519.
25.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-26)**Barnes, George; Brown, Richard; Kato, Maso; Kuck, David; Slotnick, Daniel; Stokes, Richard (August 1968). ["The ILLIAC IV Computer"](http://gordonbell.azurewebsites.net/cgb%20files/computer%20structures%20readings%20and%20examples%201971.pdf)(PDF). _[IEEE Transactions on Computers](https://en.wikipedia.org/wiki/IEEE\_Transactions\_on\_Computers "IEEE Transactions on Computers")_. **C.17** (8): 746–757. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/tc.1968.229158](https://doi.org/10.1109%2Ftc.1968.229158). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0018-9340](https://search.worldcat.org/issn/0018-9340). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[206617237](https://api.semanticscholar.org/CorpusID:206617237). Retrieved October 31, 2024.
26.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-Star100HW_27-0)**[_Star-100 - Hardware Reference Manual_](http://bitsavers.trailing-edge.com/pdf/cdc/cyber/cyber_200/60256000_STAR-100hw_Dec75.pdf)(PDF). Revision 9. [Control Data Corporation](https://en.wikipedia.org/wiki/Control_Data_Corporation "Control Data Corporation"). December 15, 1975. 60256000. Retrieved October 31, 2024.
27.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-cray1hw_28-0)**[_Cray-1 - Computer System - Hardware Reference Manual_](http://bitsavers.trailing-edge.com/pdf/cray/CRAY-1/2240004C_CRAY-1_Hardware_Reference_Nov77.pdf)(PDF). Rev. C. [Cray Research, Inc.](https://en.wikipedia.org/wiki/Cray_Research,_Inc. "Cray Research, Inc.") November 4, 1977. 2240004. Retrieved October 31, 2024.
28.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-29)**[_IBM Enterprise Systems Architecture/370 and System/370 Vector Operations_](http://bitsavers.org/pdf/ibm/370/vectorFacility/SA22-7125-3_Vector_Operations_Aug88.pdf)(PDF) (Fourth ed.). [IBM](https://en.wikipedia.org/wiki/IBM "IBM"). August 1988. SA22-7125-3. Retrieved October 31, 2024.
29.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-30)**["GPU Performance Background User's Guide"](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html). _NVIDIA Docs_. Retrieved 2024-10-29.
30.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-32)**Savov, Ivan (2017). _No Bullshit Guide to Linear Algebra_. MinireferenceCo. pp.150–155. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[9780992001025](https://en.wikipedia.org/wiki/Special:BookSources/9780992001025 "Special:BookSources/9780992001025").
31.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-33)**["Special Topics in Mathematics with Applications: Linear Algebra and the Calculus of Variations | Mechanical Engineering"](https://ocw.mit.edu/courses/2-035-special-topics-in-mathematics-with-applications-linear-algebra-and-the-calculus-of-variations-spring-2007/). _MIT OpenCourseWare_.
32.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-34)**["Energy and power systems"](https://engineering.ucdenver.edu/electrical-engineering/research/energy-and-power-systems). _engineering.ucdenver.edu_.
33.   **[^](https://en.wikipedia.org/wiki/Linear_algebra#cite_ref-35)**["ME Undergraduate Curriculum | FAMU-FSU"](https://eng.famu.fsu.edu/me/undergraduate-curriculum#:~:text=MAS+3105+Linear+Algebra+(3),and+eigenvectors,+linear+transformations,+applications)). _eng.famu.fsu.edu_.

General and cited sources
-------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=27 "Edit section: General and cited sources")]

*   Anton, Howard (1987), _Elementary Linear Algebra_ (5th ed.), New York: [Wiley](https://en.wikipedia.org/wiki/John_Wiley_%26_Sons "John Wiley & Sons"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-471-84819-0](https://en.wikipedia.org/wiki/Special:BookSources/0-471-84819-0 "Special:BookSources/0-471-84819-0")
*   [Axler, Sheldon](https://en.wikipedia.org/wiki/Sheldon_Axler "Sheldon Axler") (2024), _Linear Algebra Done Right_, [Undergraduate Texts in Mathematics](https://en.wikipedia.org/wiki/Undergraduate_Texts_in_Mathematics "Undergraduate Texts in Mathematics") (4th ed.), [Springer Publishing](https://en.wikipedia.org/wiki/Springer_Publishing "Springer Publishing"), [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-3-031-41026-0](https://doi.org/10.1007%2F978-3-031-41026-0), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-031-41026-0](https://en.wikipedia.org/wiki/Special:BookSources/978-3-031-41026-0 "Special:BookSources/978-3-031-41026-0"), [MR](https://en.wikipedia.org/wiki/MR_(identifier) "MR (identifier)")[3308468](https://mathscinet.ams.org/mathscinet-getitem?mr=3308468)
*   Beauregard, Raymond A.; Fraleigh, John B. (1973), [_A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields_](https://archive.org/details/firstcourseinlin0000beau), Boston: [Houghton Mifflin Company](https://en.wikipedia.org/wiki/Houghton_Mifflin_Company "Houghton Mifflin Company"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-395-14017-X](https://en.wikipedia.org/wiki/Special:BookSources/0-395-14017-X "Special:BookSources/0-395-14017-X")
*   Burden, Richard L.; Faires, J. Douglas (1993), [_Numerical Analysis_](https://archive.org/details/numericalanalysi00burd) (5th ed.), Boston: [Prindle, Weber and Schmidt](https://en.wikipedia.org/w/index.php?title=Prindle,_Weber_and_Schmidt&action=edit&redlink=1 "Prindle, Weber and Schmidt (page does not exist)"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-534-93219-3](https://en.wikipedia.org/wiki/Special:BookSources/0-534-93219-3 "Special:BookSources/0-534-93219-3")
*   [Golub, Gene H.](https://en.wikipedia.org/wiki/Gene_H._Golub "Gene H. Golub"); [Van Loan, Charles F.](https://en.wikipedia.org/wiki/Charles_F._Van_Loan "Charles F. Van Loan") (1996), _Matrix Computations_, Johns Hopkins Studies in Mathematical Sciences (3rd ed.), Baltimore: [Johns Hopkins University Press](https://en.wikipedia.org/wiki/Johns_Hopkins_University_Press "Johns Hopkins University Press"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8018-5414-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8018-5414-9 "Special:BookSources/978-0-8018-5414-9")
*   [Halmos, Paul Richard](https://en.wikipedia.org/wiki/Paul_Halmos "Paul Halmos") (1974), _Finite-Dimensional Vector Spaces_, [Undergraduate Texts in Mathematics](https://en.wikipedia.org/wiki/Undergraduate_Texts_in_Mathematics "Undergraduate Texts in Mathematics") (1958 2nd ed.), [Springer Publishing](https://en.wikipedia.org/wiki/Springer_Publishing "Springer Publishing"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-387-90093-4](https://en.wikipedia.org/wiki/Special:BookSources/0-387-90093-4 "Special:BookSources/0-387-90093-4"), [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[1251216](https://search.worldcat.org/oclc/1251216)
*   Harper, Charlie (1976), _Introduction to Mathematical Physics_, New Jersey: [Prentice-Hall](https://en.wikipedia.org/wiki/Prentice-Hall "Prentice-Hall"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-13-487538-9](https://en.wikipedia.org/wiki/Special:BookSources/0-13-487538-9 "Special:BookSources/0-13-487538-9")
*   [Katznelson, Yitzhak](https://en.wikipedia.org/wiki/Yitzhak_Katznelson "Yitzhak Katznelson"); Katznelson, Yonatan R. (2008), _A (Terse) Introduction to Linear Algebra_, [American Mathematical Society](https://en.wikipedia.org/wiki/American_Mathematical_Society "American Mathematical Society"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8218-4419-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-4419-9 "Special:BookSources/978-0-8218-4419-9")
*   [Roman, Steven](https://en.wikipedia.org/wiki/Steven_Roman "Steven Roman") (March 22, 2005), _Advanced Linear Algebra_, [Graduate Texts in Mathematics](https://en.wikipedia.org/wiki/Graduate_Texts_in_Mathematics "Graduate Texts in Mathematics") (2nd ed.), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-24766-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-24766-3 "Special:BookSources/978-0-387-24766-3")

*   Fearnley-Sander, Desmond, "[Hermann Grassmann and the Creation of Linear Algebra](https://www.jstor.org/stable/pdf/2320145.pdf)", American Mathematical Monthly **86** (1979), pp.809–817.
*   [Grassmann, Hermann](https://en.wikipedia.org/wiki/Hermann_Grassmann "Hermann Grassmann") (1844), _Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert_, Leipzig: O. Wigand

### Introductory textbooks

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=30 "Edit section: Introductory textbooks")]

*   Anton, Howard (2005), _Elementary Linear Algebra (Applications Version)_ (9th ed.), Wiley International
*   Banerjee, Sudipto; Roy, Anindya (2014), _Linear Algebra and Matrix Analysis for Statistics_, Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1420095388](https://en.wikipedia.org/wiki/Special:BookSources/978-1420095388 "Special:BookSources/978-1420095388")
*   Bretscher, Otto (2004), _Linear Algebra with Applications_ (3rd ed.), Prentice Hall, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-13-145334-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-145334-0 "Special:BookSources/978-0-13-145334-0")
*   Farin, Gerald; [Hansford, Dianne](https://en.wikipedia.org/wiki/Dianne_Hansford "Dianne Hansford") (2004), _Practical Linear Algebra: A Geometry Toolbox_, AK Peters, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-56881-234-2](https://en.wikipedia.org/wiki/Special:BookSources/978-1-56881-234-2 "Special:BookSources/978-1-56881-234-2")
*   [Hefferon, Jim](https://en.wikipedia.org/wiki/Jim_Hefferon "Jim Hefferon") (2020). [_Linear Algebra_](https://hefferon.net/linearalgebra/) (4th ed.). [Ann Arbor, Michigan](https://en.wikipedia.org/wiki/Ann_Arbor,_Michigan "Ann Arbor, Michigan"): Orthogonal Publishing. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-944325-11-4](https://en.wikipedia.org/wiki/Special:BookSources/978-1-944325-11-4 "Special:BookSources/978-1-944325-11-4"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[1178900366](https://search.worldcat.org/oclc/1178900366). [OL](https://en.wikipedia.org/wiki/OL_(identifier) "OL (identifier)")[30872051M](https://openlibrary.org/books/OL30872051M).
*   Kolman, Bernard; Hill, David R. (2007), _Elementary Linear Algebra with Applications_ (9th ed.), Prentice Hall, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-13-229654-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-229654-0 "Special:BookSources/978-0-13-229654-0")
*   Lay, David C. (2005), _Linear Algebra and Its Applications_ (3rd ed.), Addison Wesley, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-321-28713-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-321-28713-7 "Special:BookSources/978-0-321-28713-7")
*   Leon, Steven J. (2006), [_Linear Algebra With Applications_](https://archive.org/details/linearalgebrawit00leon) (7th ed.), Pearson Prentice Hall, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-13-185785-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-185785-8 "Special:BookSources/978-0-13-185785-8")
*   Murty, Katta G. (2014) _[Computational and Algorithmic Linear Algebra and n-Dimensional Geometry](http://www.worldscientific.com/worldscibooks/10.1142/8261)_, World Scientific Publishing, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-981-4366-62-5](https://en.wikipedia.org/wiki/Special:BookSources/978-981-4366-62-5 "Special:BookSources/978-981-4366-62-5"). _[Chapter 1: Systems of Simultaneous Linear Equations](http://www.worldscientific.com/doi/suppl/10.1142/8261/suppl\_file/8261\_chap01.pdf)_
*   Noble, B. & Daniel, J.W. (2nd Ed. 1977) _[[1]](https://www.pearson.com/us/higher-education/program/Noble-Applied-Linear-Algebra-3rd-Edition/PGM17768.html)_, Pearson Higher Education, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0130413437](https://en.wikipedia.org/wiki/Special:BookSources/978-0130413437 "Special:BookSources/978-0130413437").
*   Poole, David (2010), _Linear Algebra: A Modern Introduction_ (3rd ed.), Cengage– Brooks/Cole, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-538-73545-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-538-73545-2 "Special:BookSources/978-0-538-73545-2")
*   Ricardo, Henry (2010), _A Modern Introduction To Linear Algebra_ (1st ed.), CRC Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4398-0040-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4398-0040-9 "Special:BookSources/978-1-4398-0040-9")
*   Sadun, Lorenzo (2008), _Applied Linear Algebra: the decoupling principle_ (2nd ed.), AMS, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8218-4441-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-4441-0 "Special:BookSources/978-0-8218-4441-0")
*   [Strang, Gilbert](https://en.wikipedia.org/wiki/Gilbert_Strang "Gilbert Strang") (2016), _Introduction to Linear Algebra_ (5th ed.), Wellesley-Cambridge Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-09802327-7-6](https://en.wikipedia.org/wiki/Special:BookSources/978-09802327-7-6 "Special:BookSources/978-09802327-7-6")
*   The Manga Guide to Linear Algebra (2012), by [Shin Takahashi](https://en.wikipedia.org/wiki/Shin_Takahashi "Shin Takahashi"), Iroha Inoue and Trend-Pro Co., Ltd., [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-59327-413-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59327-413-9 "Special:BookSources/978-1-59327-413-9")

*   Bhatia, Rajendra (November 15, 1996), _Matrix Analysis_, [Graduate Texts in Mathematics](https://en.wikipedia.org/wiki/Graduate_Texts_in_Mathematics "Graduate Texts in Mathematics"), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-94846-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-94846-1 "Special:BookSources/978-0-387-94846-1")
*   [Demmel, James W.](https://en.wikipedia.org/wiki/James_Demmel "James Demmel") (August 1, 1997), _Applied Numerical Linear Algebra_, SIAM, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-89871-389-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-89871-389-3 "Special:BookSources/978-0-89871-389-3")
*   [Dym, Harry](https://en.wikipedia.org/wiki/Harry_Dym "Harry Dym") (2007), _Linear Algebra in Action_, AMS, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8218-3813-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-3813-6 "Special:BookSources/978-0-8218-3813-6")
*   [Gantmacher, Felix R.](https://en.wikipedia.org/wiki/Felix_Gantmacher "Felix Gantmacher") (2005), _Applications of the Theory of Matrices_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-44554-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-44554-0 "Special:BookSources/978-0-486-44554-0")
*   Gantmacher, Felix R. (1990), _Matrix Theory Vol. 1_ (2nd ed.), American Mathematical Society, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8218-1376-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-1376-8 "Special:BookSources/978-0-8218-1376-8")
*   Gantmacher, Felix R. (2000), _Matrix Theory Vol. 2_ (2nd ed.), American Mathematical Society, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8218-2664-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-2664-5 "Special:BookSources/978-0-8218-2664-5")
*   [Gelfand, Israel M.](https://en.wikipedia.org/wiki/Israel_Gelfand "Israel Gelfand") (1989), _Lectures on Linear Algebra_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-66082-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-66082-0 "Special:BookSources/978-0-486-66082-0")
*   Glazman, I. M.; Ljubic, Ju. I. (2006), _Finite-Dimensional Linear Analysis_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-45332-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-45332-3 "Special:BookSources/978-0-486-45332-3")
*   Golan, Johnathan S. (January 2007), _The Linear Algebra a Beginning Graduate Student Ought to Know_ (2nd ed.), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4020-5494-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4020-5494-5 "Special:BookSources/978-1-4020-5494-5")
*   Golan, Johnathan S. (August 1995), _Foundations of Linear Algebra_, Kluwer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-7923-3614-3](https://en.wikipedia.org/wiki/Special:BookSources/0-7923-3614-3 "Special:BookSources/0-7923-3614-3")
*   Greub, Werner H. (October 16, 1981), _Linear Algebra_, Graduate Texts in Mathematics (4th ed.), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8018-5414-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8018-5414-9 "Special:BookSources/978-0-8018-5414-9")
*   Hoffman, Kenneth; [Kunze, Ray](https://en.wikipedia.org/wiki/Ray_Kunze "Ray Kunze") (1971), _Linear algebra_ (2nd ed.), Englewood Cliffs, N.J.: Prentice-Hall, Inc., [MR](https://en.wikipedia.org/wiki/MR_(identifier) "MR (identifier)")[0276251](https://mathscinet.ams.org/mathscinet-getitem?mr=0276251)
*   [Halmos, Paul R.](https://en.wikipedia.org/wiki/Paul_Halmos "Paul Halmos") (August 20, 1993), _Finite-Dimensional Vector Spaces_, [Undergraduate Texts in Mathematics](https://en.wikipedia.org/wiki/Undergraduate_Texts_in_Mathematics "Undergraduate Texts in Mathematics"), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-90093-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-90093-3 "Special:BookSources/978-0-387-90093-3")
*   Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (September 7, 2018), _Linear Algebra_ (5th ed.), Pearson, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-13-486024-4](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-486024-4 "Special:BookSources/978-0-13-486024-4")
*   [Horn, Roger A.](https://en.wikipedia.org/wiki/Roger_Horn "Roger Horn"); [Johnson, Charles R.](https://en.wikipedia.org/wiki/Charles_Royal_Johnson "Charles Royal Johnson") (February 23, 1990), _Matrix Analysis_, Cambridge University Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-521-38632-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-38632-6 "Special:BookSources/978-0-521-38632-6")
*   Horn, Roger A.; Johnson, Charles R. (June 24, 1994), _Topics in Matrix Analysis_, Cambridge University Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-521-46713-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-46713-1 "Special:BookSources/978-0-521-46713-1")
*   [Lang, Serge](https://en.wikipedia.org/wiki/Serge_Lang "Serge Lang") (March 9, 2004), _Linear Algebra_, Undergraduate Texts in Mathematics (3rd ed.), Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-96412-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-96412-6 "Special:BookSources/978-0-387-96412-6")
*   [Marcus, Marvin](https://en.wikipedia.org/wiki/Marvin_Marcus "Marvin Marcus"); [Minc, Henryk](https://en.wikipedia.org/wiki/Henryk_Minc "Henryk Minc") (2010), _A Survey of Matrix Theory and Matrix Inequalities_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-67102-4](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-67102-4 "Special:BookSources/978-0-486-67102-4")
*   Meyer, Carl D. (February 15, 2001), [_Matrix Analysis and Applied Linear Algebra_](https://web.archive.org/web/20091031193126/http://matrixanalysis.com/DownloadChapters.html), Society for Industrial and Applied Mathematics (SIAM), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-89871-454-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-89871-454-8 "Special:BookSources/978-0-89871-454-8"), archived from [the original](http://www.matrixanalysis.com/DownloadChapters.html) on October 31, 2009
*   [Mirsky, L.](https://en.wikipedia.org/wiki/Leon_Mirsky "Leon Mirsky") (1990), _An Introduction to Linear Algebra_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-66434-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-66434-7 "Special:BookSources/978-0-486-66434-7")
*   [Shafarevich, I. R.](https://en.wikipedia.org/wiki/Igor_Shafarevich "Igor Shafarevich"); Remizov, A. O (2012), [_Linear Algebra and Geometry_](https://www.springer.com/mathematics/algebra/book/978-3-642-30993-9), [Springer](https://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media "Springer Science+Business Media"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-642-30993-9](https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-30993-9 "Special:BookSources/978-3-642-30993-9")
*   [Shilov, Georgi E.](https://en.wikipedia.org/wiki/Georgiy_Shilov "Georgiy Shilov") (June 1, 1977), _Linear algebra_, Dover Publications, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-63518-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-63518-7 "Special:BookSources/978-0-486-63518-7")
*   Shores, Thomas S. (December 6, 2006), _Applied Linear Algebra and Matrix Analysis_, Undergraduate Texts in Mathematics, Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-33194-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-33194-2 "Special:BookSources/978-0-387-33194-2")
*   Smith, Larry (May 28, 1998), _Linear Algebra_, Undergraduate Texts in Mathematics, Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-98455-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-98455-1 "Special:BookSources/978-0-387-98455-1")
*   [Trefethen, Lloyd N.](https://en.wikipedia.org/wiki/Lloyd_N._Trefethen "Lloyd N. Trefethen"); Bau, David (1997), _Numerical Linear Algebra_, SIAM, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-898-71361-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-898-71361-9 "Special:BookSources/978-0-898-71361-9")

### Study guides and outlines

[[edit](https://en.wikipedia.org/w/index.php?title=Linear_algebra&action=edit&section=32 "Edit section: Study guides and outlines")]

*   Leduc, Steven A. (May 1, 1996), _Linear Algebra (Cliffs Quick Review)_, Cliffs Notes, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8220-5331-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8220-5331-6 "Special:BookSources/978-0-8220-5331-6")
*   Lipschutz, Seymour; Lipson, Marc (December 6, 2000), _Schaum's Outline of Linear Algebra_ (3rd ed.), McGraw-Hill, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-07-136200-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-136200-9 "Special:BookSources/978-0-07-136200-9")
*   Lipschutz, Seymour (January 1, 1989), _3,000 Solved Problems in Linear Algebra_, McGraw–Hill, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-07-038023-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-038023-3 "Special:BookSources/978-0-07-038023-3")
*   McMahon, David (October 28, 2005), _Linear Algebra Demystified_, McGraw–Hill Professional, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-07-146579-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-146579-3 "Special:BookSources/978-0-07-146579-3")
*   Zhang, Fuzhen (April 7, 2009), _Linear Algebra: Challenging Problems for Students_, The Johns Hopkins University Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8018-9125-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8018-9125-0 "Special:BookSources/978-0-8018-9125-0")

*   [MIT Linear Algebra Video Lectures](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/), a series of 34 recorded lectures by Professor [Gilbert Strang](https://en.wikipedia.org/wiki/Gilbert_Strang "Gilbert Strang") (Spring 2010)
*   [International Linear Algebra Society](https://www.math.technion.ac.il/iic/)
*   ["Linear algebra"](https://www.encyclopediaofmath.org/index.php?title=Linear_algebra), _[Encyclopedia of Mathematics](https://en.wikipedia.org/wiki/Encyclopedia\_of\_Mathematics "Encyclopedia of Mathematics")_, [EMS Press](https://en.wikipedia.org/wiki/European_Mathematical_Society "European Mathematical Society"), 2001 [1994]
*   [Linear Algebra](https://mathworld.wolfram.com/topics/LinearAlgebra.html) on [MathWorld](https://en.wikipedia.org/wiki/MathWorld "MathWorld")
*   [Matrix and Linear Algebra Terms](http://www.economics.soton.ac.uk/staff/aldrich/matrices.htm) on [Earliest Known Uses of Some of the Words of Mathematics](http://jeff560.tripod.com/mathword.html)
*   [Earliest Uses of Symbols for Matrices and Vectors](http://jeff560.tripod.com/matrices.html) on [Earliest Uses of Various Mathematical Symbols](http://jeff560.tripod.com/mathsym.html)
*   [Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab), a video presentation from [3Blue1Brown](https://en.wikipedia.org/wiki/3Blue1Brown "3Blue1Brown") of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view

*   Beezer, Robert A. (2009) [2004]. [_A First Course in Linear Algebra_](http://linear.ups.edu/). [Gainesville, Florida](https://en.wikipedia.org/wiki/Gainesville,_Florida "Gainesville, Florida"): [University Press of Florida](https://en.wikipedia.org/wiki/University_Press_of_Florida "University Press of Florida"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[9781616100049](https://en.wikipedia.org/wiki/Special:BookSources/9781616100049 "Special:BookSources/9781616100049").
*   Connell, Edwin H. (2004) [1999]. [_Elements of Abstract and Linear Algebra_](https://www.math.miami.edu/~ec/book/). [University of Miami](https://en.wikipedia.org/wiki/University_of_Miami "University of Miami"), [Coral Gables, Florida](https://en.wikipedia.org/wiki/Coral_Gables,_Florida "Coral Gables, Florida"): Self-published.
*   [Hefferon, Jim](https://en.wikipedia.org/wiki/Jim_Hefferon "Jim Hefferon") (2020). [_Linear Algebra_](https://hefferon.net/linearalgebra/) (4th ed.). [Ann Arbor, Michigan](https://en.wikipedia.org/wiki/Ann_Arbor,_Michigan "Ann Arbor, Michigan"): Orthogonal Publishing. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-944325-11-4](https://en.wikipedia.org/wiki/Special:BookSources/978-1-944325-11-4 "Special:BookSources/978-1-944325-11-4"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[1178900366](https://search.worldcat.org/oclc/1178900366). [OL](https://en.wikipedia.org/wiki/OL_(identifier) "OL (identifier)")[30872051M](https://openlibrary.org/books/OL30872051M).
*   [Margalit, Dan](https://en.wikipedia.org/wiki/Dan_Margalit_(mathematician) "Dan Margalit (mathematician)"); Rabinoff, Joseph (2019). [_Interactive Linear Algebra_](https://textbooks.math.gatech.edu/ila/). [Georgia Institute of Technology](https://en.wikipedia.org/wiki/Georgia_Institute_of_Technology "Georgia Institute of Technology"), [Atlanta, Georgia](https://en.wikipedia.org/wiki/Atlanta,_Georgia "Atlanta, Georgia"): Self-published.
*   Matthews, Keith R. (2013) [1991]. [_Elementary Linear Algebra_](http://www.numbertheory.org/book/). [University of Queensland](https://en.wikipedia.org/wiki/University_of_Queensland "University of Queensland"), [Brisbane, Australia](https://en.wikipedia.org/wiki/Brisbane,_Australia "Brisbane, Australia"): Self-published.
*   Mikaelian, Vahagn H. (2020) [2017]. [_Linear Algebra: Theory and Algorithms_](https://www.researchgate.net/publication/318066716). [Yerevan, Armenia](https://en.wikipedia.org/wiki/Yerevan,_Armenia "Yerevan, Armenia"): Self-published – via [ResearchGate](https://en.wikipedia.org/wiki/ResearchGate "ResearchGate").
*   Sharipov, Ruslan, _[Course of linear algebra and multidimensional geometry](https://arxiv.org/abs/math.HO/0405323)_
*   [Treil, Sergei](https://en.wikipedia.org/wiki/Sergei_Treil "Sergei Treil"), _[Linear Algebra Done Wrong](https://www.math.brown.edu/~treil/papers/LADW/LADW.html)_
