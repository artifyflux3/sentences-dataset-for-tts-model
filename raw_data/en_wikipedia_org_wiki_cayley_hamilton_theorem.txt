Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Examples Toggle Examples subsection 1.1 1 × 1 matrices 1.2 2 × 2 matrices 2 Applications Toggle Applications subsection 2.1 Determinant and inverse matrix 2.2 n -th power of matrix 2.3 Matrix functions 2.4 Algebraic number theory 3 Proofs Toggle Proofs subsection 3.1 Preliminaries 3.1.1 Adjugate matrices 3.2 A direct algebraic proof 3.3 A proof using polynomials with matrix coefficients 3.4 A synthesis of the first two proofs 3.5 A proof using matrices of endomorphisms 3.6 A bogus "proof": p ( A ) = det( AI n − A ) = det( A − A ) = 0 3.7 Proofs using methods of abstract algebra 3.8 A combinatorial proof 4 Abstraction and generalizations 5 See also 6 Remarks 7 Notes 8 References 9 External links Toggle the table of contents Cayley–Hamilton theorem 28 languages العربية বাংলা Bosanski Català Čeština Deutsch Ελληνικά Español فارسی Français 한국어 Hrvatski Italiano עברית Magyar Nederlands 日本語 Polski Português Română Русский Slovenčina Српски / srpski Srpskohrvatski / српскохрватски Svenska Українська اردو 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia A square matrix satisfies its own characteristic equation Arthur Cayley , F.R.S.

(1821–1895) is widely regarded as Britain's leading pure mathematician of the 19th century. Cayley in 1848 went to Dublin to attend lectures on quaternions by Hamilton, their discoverer. Later Cayley impressed him by being the second to publish work on them.

[ 1 ] Cayley stated the theorem for matrices of dimension 3 or less, and published a proof for the two-dimensional case.

William Rowan Hamilton (1805–1865), Irish physicist, astronomer, and mathematician, first foreign member of the American National Academy of Sciences . While maintaining an opposing position about how geometry should be studied, Hamilton always remained on the best terms with Cayley.

[ 1 ] Hamilton proved that for a linear function of quaternions there exists a certain equation, depending on the linear function, that is satisfied by the linear function itself.

[ 2 ] [ 3 ] [ 4 ] In linear algebra , the Cayley–Hamilton theorem (named after the mathematicians Arthur Cayley and William Rowan Hamilton ) states that every square matrix over a commutative ring (such as the real or complex numbers or the integers ) satisfies its own characteristic equation .

The characteristic polynomial of an n × × n {\displaystyle n\times n} matrix A is defined as [ 5 ] p A ( λ λ ) = det ( λ λ I n − − A ) {\displaystyle p_{A}(\lambda )=\det(\lambda I_{n}-A)} , where det is the determinant operation, λ is a variable scalar element of the base ring , and I n is the n × × n {\displaystyle n\times n} identity matrix . Since each entry of the matrix ( λ λ I n − − A ) {\displaystyle (\lambda I_{n}-A)} is either constant or linear in λ , the determinant of ( λ λ I n − − A ) {\displaystyle (\lambda I_{n}-A)} is a degree - n monic polynomial in λ , so it can be written as p A ( λ λ ) = λ λ n + c n − − 1 λ λ n − − 1 + ⋯ ⋯ + c 1 λ λ + c 0 .

{\displaystyle p_{A}(\lambda )=\lambda ^{n}+c_{n-1}\lambda ^{n-1}+\cdots +c_{1}\lambda +c_{0}.} By replacing the scalar variable λ with the matrix A , one can define an analogous matrix  polynomial expression, p A ( A ) = A n + c n − − 1 A n − − 1 + ⋯ ⋯ + c 1 A + c 0 I n .

{\displaystyle p_{A}(A)=A^{n}+c_{n-1}A^{n-1}+\cdots +c_{1}A+c_{0}I_{n}.} (Here, A {\displaystyle A} is the given matrix—not a variable, unlike λ λ {\displaystyle \lambda } —so p A ( A ) {\displaystyle p_{A}(A)} is a constant rather than a function.)
The Cayley–Hamilton theorem states that this polynomial expression is equal to the zero matrix , which is to say that p A ( A ) = 0 ; {\displaystyle p_{A}(A)=0;} that is, the characteristic polynomial p A {\displaystyle p_{A}} is an annihilating polynomial for A .

{\displaystyle A.} One use for the Cayley–Hamilton theorem is that it  allows A n to be expressed as a linear combination of the lower matrix powers of A : A n = − − c n − − 1 A n − − 1 − − ⋯ ⋯ − − c 1 A − − c 0 I n .

{\displaystyle A^{n}=-c_{n-1}A^{n-1}-\cdots -c_{1}A-c_{0}I_{n}.} When the ring is a field , the Cayley–Hamilton theorem is equivalent to the statement that the minimal polynomial of a square matrix divides its characteristic polynomial.

A special case of the theorem was first proved by Hamilton in 1853 [ 6 ] in terms of inverses of linear functions of quaternions .

[ 2 ] [ 3 ] [ 4 ] This corresponds to the special case of certain 4 × × 4 {\displaystyle 4\times 4} real or 2 × × 2 {\displaystyle 2\times 2} complex matrices. Cayley in 1858 stated the result for 3 × × 3 {\displaystyle 3\times 3} and smaller matrices, but only published a proof for the 2 × × 2 {\displaystyle 2\times 2} case.

[ 7 ] [ 8 ] As for n × × n {\displaystyle n\times n} matrices, Cayley stated “..., I have not thought it necessary to undertake the labor of a formal proof of the theorem in the general case of a matrix of any degree”. The general case was first proved by Ferdinand Frobenius in 1878.

[ 9 ] Examples [ edit ] 1 × 1 matrices [ edit ] For a 1 × × 1 {\displaystyle 1\times 1} matrix A = ( a ) , the characteristic polynomial is given by p ( λ ) = λ − a , and so p ( A ) = ( a ) − a (1) = 0 is trivial.

2 × 2 matrices [ edit ] As a concrete example, let A = ( 1 2 3 4 ) .

{\displaystyle A={\begin{pmatrix}1&2\\3&4\end{pmatrix}}.} Its characteristic polynomial is given by p ( λ λ ) = det ( λ λ I 2 − − A ) = det ( λ λ − − 1 − − 2 − − 3 λ λ − − 4 ) = ( λ λ − − 1 ) ( λ λ − − 4 ) − − ( − − 2 ) ( − − 3 ) = λ λ 2 − − 5 λ λ − − 2.

{\displaystyle {\begin{aligned}p(\lambda )&=\det(\lambda I_{2}-A)=\det \!{\begin{pmatrix}\lambda -1&-2\\-3&\lambda -4\end{pmatrix}}\\&=(\lambda -1)(\lambda -4)-(-2)(-3)=\lambda ^{2}-5\lambda -2.\end{aligned}}} The Cayley–Hamilton theorem claims that, if we define p ( X ) = X 2 − − 5 X − − 2 I 2 , {\displaystyle p(X)=X^{2}-5X-2I_{2},} then p ( A ) = A 2 − − 5 A − − 2 I 2 = ( 0 0 0 0 ) .

{\displaystyle p(A)=A^{2}-5A-2I_{2}={\begin{pmatrix}0&0\\0&0\\\end{pmatrix}}.} We can verify by computation that indeed, A 2 − − 5 A − − 2 I 2 = ( 7 10 15 22 ) − − ( 5 10 15 20 ) − − ( 2 0 0 2 ) = ( 0 0 0 0 ) .

{\displaystyle A^{2}-5A-2I_{2}={\begin{pmatrix}7&10\\15&22\\\end{pmatrix}}-{\begin{pmatrix}5&10\\15&20\\\end{pmatrix}}-{\begin{pmatrix}2&0\\0&2\\\end{pmatrix}}={\begin{pmatrix}0&0\\0&0\\\end{pmatrix}}.} For a generic 2 × × 2 {\displaystyle 2\times 2} matrix, A = ( a b c d ) , {\displaystyle A={\begin{pmatrix}a&b\\c&d\\\end{pmatrix}},} the characteristic polynomial is given by p ( λ ) = λ 2 − ( a + d ) λ + ( ad − bc ) , so the Cayley–Hamilton theorem states that p ( A ) = A 2 − − ( a + d ) A + ( a d − − b c ) I 2 = ( 0 0 0 0 ) ; {\displaystyle p(A)=A^{2}-(a+d)A+(ad-bc)I_{2}={\begin{pmatrix}0&0\\0&0\end{pmatrix}};} which is indeed always the case, evident by working out the entries of A 2 .

Proof A 2 − − ( a + d ) A + ( a d − − b c ) I 2 = ( a 2 + b c a b + b d a c + c d b c + d 2 ) − − ( a ( a + d ) b ( a + d ) c ( a + d ) d ( a + d ) ) + ( a d − − b c ) I 2 = ( b c − − a d 0 0 b c − − a d ) + ( a d − − b c ) I 2 = ( 0 0 0 0 ) {\displaystyle {\begin{aligned}&{}A^{2}-(a+d)A+(ad-bc)I_{2}\\[1ex]&={\begin{pmatrix}a^{2}+bc&ab+bd\\ac+cd&bc+d^{2}\\\end{pmatrix}}-{\begin{pmatrix}a(a+d)&b(a+d)\\c(a+d)&d(a+d)\end{pmatrix}}+(ad-bc)I_{2}\\[1ex]&={\begin{pmatrix}bc-ad&0\\0&bc-ad\\\end{pmatrix}}+(ad-bc)I_{2}\\[1ex]&={\begin{pmatrix}0&0\\0&0\end{pmatrix}}\end{aligned}}} Applications [ edit ] Determinant and inverse matrix [ edit ] See also: Determinant § Relation to eigenvalues and trace , and Characteristic polynomial § Properties For a general n × × n {\displaystyle n\times n} invertible matrix A , i.e., one with nonzero determinant, A −1 can thus be written as an ( n − − 1 ) {\displaystyle (n-1)} order polynomial expression in A : As indicated, the Cayley–Hamilton theorem amounts to the identity p ( A ) = A n + c n − − 1 A n − − 1 + ⋯ ⋯ + c 1 A + ( − − 1 ) n det ( A ) I n = 0.

{\displaystyle p(A)=A^{n}+c_{n-1}A^{n-1}+\cdots +c_{1}A+(-1)^{n}\det(A)I_{n}=0.} The coefficients c i are given by the elementary symmetric polynomials of the eigenvalues of A . Using Newton identities , the elementary symmetric polynomials can in turn be expressed in terms of power sum symmetric polynomials of the eigenvalues: s k = ∑ ∑ i = 1 n λ λ i k = tr ⁡ ⁡ ( A k ) , {\displaystyle s_{k}=\sum _{i=1}^{n}\lambda _{i}^{k}=\operatorname {tr} (A^{k}),} where tr( A k ) is the trace of the matrix A k . Thus, we can express c i in terms of the trace of powers of A .

In general, the formula for the coefficients c i is given in terms of complete exponential Bell polynomials as [ nb 1 ] c n − − k = ( − − 1 ) k k !

B k ( s 1 , − − 1 !

s 2 , 2 !

s 3 , … … , ( − − 1 ) k − − 1 ( k − − 1 ) !

s k ) .

{\displaystyle c_{n-k}={\frac {(-1)^{k}}{k!}}B_{k}(s_{1},-1!s_{2},2!s_{3},\ldots ,(-1)^{k-1}(k-1)!s_{k}).} In particular, the determinant of A equals (−1) n c 0 . Thus, the determinant can be written as the trace identity : det ( A ) = 1 n !

B n ( s 1 , − − 1 !

s 2 , 2 !

s 3 , … … , ( − − 1 ) n − − 1 ( n − − 1 ) !

s n ) .

{\displaystyle \det(A)={\frac {1}{n!}}B_{n}(s_{1},-1!s_{2},2!s_{3},\ldots ,(-1)^{n-1}(n-1)!s_{n}).} Likewise, the characteristic polynomial can be written as − − ( − − 1 ) n det ( A ) I n = A ( A n − − 1 + c n − − 1 A n − − 2 + ⋯ ⋯ + c 1 I n ) , {\displaystyle -(-1)^{n}\det(A)I_{n}=A(A^{n-1}+c_{n-1}A^{n-2}+\cdots +c_{1}I_{n}),} and, by multiplying both sides by A −1 (note −(−1) n = (−1) n −1 ), one is led to an expression for the inverse of A as a trace identity, A − − 1 = ( − − 1 ) n − − 1 det A ( A n − − 1 + c n − − 1 A n − − 2 + ⋯ ⋯ + c 1 I n ) , = 1 det A ∑ ∑ k = 0 n − − 1 ( − − 1 ) n + k − − 1 A n − − k − − 1 k !

B k ( s 1 , − − 1 !

s 2 , 2 !

s 3 , … … , ( − − 1 ) k − − 1 ( k − − 1 ) !

s k ) .

{\displaystyle {\begin{aligned}A^{-1}&={\frac {(-1)^{n-1}}{\det A}}(A^{n-1}+c_{n-1}A^{n-2}+\cdots +c_{1}I_{n}),\\[5pt]&={\frac {1}{\det A}}\sum _{k=0}^{n-1}(-1)^{n+k-1}{\frac {A^{n-k-1}}{k!}}B_{k}(s_{1},-1!s_{2},2!s_{3},\ldots ,(-1)^{k-1}(k-1)!s_{k}).\end{aligned}}} Another method for obtaining these coefficients c k for a general n × × n {\displaystyle n\times n} matrix, provided no root be zero, relies on the following alternative expression for the determinant , p ( λ λ ) = det ( λ λ I n − − A ) = λ λ n exp ⁡ ⁡ ( tr ⁡ ⁡ ( log ⁡ ⁡ ( I n − − A / λ λ ) ) ) .

{\displaystyle p(\lambda )=\det(\lambda I_{n}-A)=\lambda ^{n}\exp(\operatorname {tr} (\log(I_{n}-A/\lambda ))).} Hence, by virtue of the Mercator series , p ( λ λ ) = λ λ n exp ⁡ ⁡ ( − − tr ⁡ ⁡ ∑ ∑ m = 1 ∞ ∞ ( A λ λ ) m m ) , {\displaystyle p(\lambda )=\lambda ^{n}\exp \left(-\operatorname {tr} \sum _{m=1}^{\infty }{({A \over \lambda })^{m} \over m}\right),} where the exponential only needs be expanded to order λ − n , since p ( λ ) is of order n , the net negative powers of λ automatically vanishing by the C–H theorem. (Again, this requires a ring containing the rational numbers .) Differentiation of this expression with respect to λ allows one to express the coefficients of the characteristic polynomial for general n as determinants of m × m matrices, [ nb 2 ] c n − − m = ( − − 1 ) m m !

| tr ⁡ ⁡ A m − − 1 0 ⋯ ⋯ tr ⁡ ⁡ A 2 tr ⁡ ⁡ A m − − 2 ⋯ ⋯ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ tr ⁡ ⁡ A m − − 1 tr ⁡ ⁡ A m − − 2 ⋯ ⋯ ⋯ ⋯ 1 tr ⁡ ⁡ A m tr ⁡ ⁡ A m − − 1 ⋯ ⋯ ⋯ ⋯ tr ⁡ ⁡ A | .

{\displaystyle c_{n-m}={\frac {(-1)^{m}}{m!}}{\begin{vmatrix}\operatorname {tr} A&m-1&0&\cdots \\\operatorname {tr} A^{2}&\operatorname {tr} A&m-2&\cdots \\\vdots &\vdots &&&\vdots \\\operatorname {tr} A^{m-1}&\operatorname {tr} A^{m-2}&\cdots &\cdots &1\\\operatorname {tr} A^{m}&\operatorname {tr} A^{m-1}&\cdots &\cdots &\operatorname {tr} A\end{vmatrix}}~.} Examples For instance, the first few Bell polynomials are B 0 = 1, B 1 ( x 1 ) = x 1 , B 2 ( x 1 , x 2 ) = x 2 1 + x 2 , and B 3 ( x 1 , x 2 , x 3 ) = x 3 1 + 3 x 1 x 2 + x 3 .

Using these to specify the coefficients c i of the characteristic polynomial of a 2 × × 2 {\displaystyle 2\times 2} matrix yields c 2 = B 0 = 1 , c 1 = − − 1 1 !

B 1 ( s 1 ) = − − s 1 = − − tr ⁡ ⁡ ( A ) , c 0 = 1 2 !

B 2 ( s 1 , − − 1 !

s 2 ) = 1 2 ( s 1 2 − − s 2 ) = 1 2 ( ( tr ⁡ ⁡ ( A ) ) 2 − − tr ⁡ ⁡ ( A 2 ) ) .

{\displaystyle {\begin{aligned}c_{2}=B_{0}=1,\\[4pt]c_{1}={\frac {-1}{1!}}B_{1}(s_{1})=-s_{1}=-\operatorname {tr} (A),\\[4pt]c_{0}={\frac {1}{2!}}B_{2}(s_{1},-1!s_{2})={\frac {1}{2}}(s_{1}^{2}-s_{2})={\frac {1}{2}}((\operatorname {tr} (A))^{2}-\operatorname {tr} (A^{2})).\end{aligned}}} The coefficient c 0 gives the determinant of the 2 × × 2 {\displaystyle 2\times 2} matrix, c 1 minus its trace, while its inverse is given by A − − 1 = − − 1 det A ( A + c 1 I 2 ) = − − 2 ( A − − tr ⁡ ⁡ ( A ) I 2 ) ( tr ⁡ ⁡ ( A ) ) 2 − − tr ⁡ ⁡ ( A 2 ) .

{\displaystyle A^{-1}={\frac {-1}{\det A}}(A+c_{1}I_{2})={\frac {-2(A-\operatorname {tr} (A)I_{2})}{(\operatorname {tr} (A))^{2}-\operatorname {tr} (A^{2})}}.} It is apparent from the general formula for c n − k , expressed in terms of Bell polynomials, that the expressions − − tr ⁡ ⁡ ( A ) and 1 2 ( tr ⁡ ⁡ ( A ) 2 − − tr ⁡ ⁡ ( A 2 ) ) {\displaystyle -\operatorname {tr} (A)\quad {\text{and}}\quad {\tfrac {1}{2}}(\operatorname {tr} (A)^{2}-\operatorname {tr} (A^{2}))} always give the coefficients c n −1 of λ n −1 and c n −2 of λ n −2 in the characteristic polynomial of any n × × n {\displaystyle n\times n} matrix, respectively. So, for a 3 × × 3 {\displaystyle 3\times 3} matrix A , the statement of the Cayley–Hamilton theorem can also be written as A 3 − − ( tr ⁡ ⁡ A ) A 2 + 1 2 ( ( tr ⁡ ⁡ A ) 2 − − tr ⁡ ⁡ ( A 2 ) ) A − − det ( A ) I 3 = O , {\displaystyle A^{3}-(\operatorname {tr} A)A^{2}+{\frac {1}{2}}\left((\operatorname {tr} A)^{2}-\operatorname {tr} (A^{2})\right)A-\det(A)I_{3}=O,} where the right-hand side designates a 3 × × 3 {\displaystyle 3\times 3} matrix with all entries reduced to zero. Likewise, this determinant in the n = 3 case, is now det ( A ) = 1 3 !

B 3 ( s 1 , − − 1 !

s 2 , 2 !

s 3 ) = 1 6 ( s 1 3 + 3 s 1 ( − − s 2 ) + 2 s 3 ) = 1 6 [ ( tr ⁡ ⁡ A ) 3 − − 3 tr ⁡ ⁡ ( A 2 ) ( tr ⁡ ⁡ A ) + 2 tr ⁡ ⁡ ( A 3 ) ] .

{\displaystyle {\begin{aligned}\det(A)&={\frac {1}{3!}}B_{3}(s_{1},-1!s_{2},2!s_{3})={\frac {1}{6}}(s_{1}^{3}+3s_{1}(-s_{2})+2s_{3})\\[5pt]&={\frac {1}{6}}\left[(\operatorname {tr} A)^{3}-3\operatorname {tr} (A^{2})(\operatorname {tr} A)+2\operatorname {tr} (A^{3})\right].\end{aligned}}} This expression gives the negative of coefficient c n −3 of λ n −3 in the general case, as seen below.

Similarly, one can write for a 4 × × 4 {\displaystyle 4\times 4} matrix A , A 4 − − ( tr ⁡ ⁡ A ) A 3 + 1 2 [ ( tr ⁡ ⁡ A ) 2 − − tr ⁡ ⁡ ( A 2 ) ] A 2 − − 1 6 [ ( tr ⁡ ⁡ A ) 3 − − 3 tr ⁡ ⁡ ( A 2 ) ( tr ⁡ ⁡ A ) + 2 tr ⁡ ⁡ ( A 3 ) ] A + det ( A ) I 4 = O , {\displaystyle A^{4}-(\operatorname {tr} A)A^{3}+{\tfrac {1}{2}}\left[(\operatorname {tr} A)^{2}-\operatorname {tr} (A^{2})\right]A^{2}-{\tfrac {1}{6}}\left[(\operatorname {tr} A)^{3}-3\operatorname {tr} (A^{2})(\operatorname {tr} A)+2\operatorname {tr} (A^{3})\right]A+\det(A)I_{4}=O,} where, now, the determinant is c n −4 , 1 24 [ ( tr ⁡ ⁡ A ) 4 − − 6 tr ⁡ ⁡ ( A 2 ) ( tr ⁡ ⁡ A ) 2 + 3 ( tr ⁡ ⁡ ( A 2 ) ) 2 + 8 tr ⁡ ⁡ ( A 3 ) tr ⁡ ⁡ ( A ) − − 6 tr ⁡ ⁡ ( A 4 ) ] , {\displaystyle {\tfrac {1}{24}}\!\left[(\operatorname {tr} A)^{4}-6\operatorname {tr} (A^{2})(\operatorname {tr} A)^{2}+3\left(\operatorname {tr} (A^{2})\right)^{2}+8\operatorname {tr} (A^{3})\operatorname {tr} (A)-6\operatorname {tr} (A^{4})\right],} and so on for larger matrices. The increasingly complex expressions for the coefficients c k is deducible from Newton's identities or the Faddeev–LeVerrier algorithm .

n -th power of matrix [ edit ] The Cayley–Hamilton theorem always provides a relationship between the powers of A (though not always the simplest one), which allows one to simplify expressions involving such powers, and evaluate them without having to compute the power A n or any higher powers of A .

As an example, for A = ( 1 2 3 4 ) {\displaystyle A={\begin{pmatrix}1&2\\3&4\end{pmatrix}}} the theorem gives A 2 = 5 A + 2 I 2 .

{\displaystyle A^{2}=5A+2I_{2}\,.} Then, to calculate A 4 , observe A 3 = ( 5 A + 2 I 2 ) A = 5 A 2 + 2 A = 5 ( 5 A + 2 I 2 ) + 2 A = 27 A + 10 I 2 , A 4 = A 3 A = ( 27 A + 10 I 2 ) A = 27 A 2 + 10 A = 27 ( 5 A + 2 I 2 ) + 10 A = 145 A + 54 I 2 .

{\displaystyle {\begin{aligned}A^{3}&=(5A+2I_{2})A=5A^{2}+2A=5(5A+2I_{2})+2A=27A+10I_{2},\\[1ex]A^{4}&=A^{3}A=(27A+10I_{2})A=27A^{2}+10A=27(5A+2I_{2})+10A=145A+54I_{2}\,.\end{aligned}}} Likewise, A − − 1 = 1 2 ( A − − 5 I 2 ) .

A − − 2 = A − − 1 A − − 1 = 1 4 ( A 2 − − 10 A + 25 I 2 ) = 1 4 ( ( 5 A + 2 I 2 ) − − 10 A + 25 I 2 ) = 1 4 ( − − 5 A + 27 I 2 ) .

{\displaystyle {\begin{aligned}A^{-1}&={\frac {1}{2}}\left(A-5I_{2}\right)~.\\[1ex]A^{-2}&=A^{-1}A^{-1}={\frac {1}{4}}\left(A^{2}-10A+25I_{2}\right)={\frac {1}{4}}\left((5A+2I_{2})-10A+25I_{2}\right)={\frac {1}{4}}\left(-5A+27I_{2}\right)~.\end{aligned}}} Notice that we have been able to write the matrix power as the sum of two terms. In fact, matrix power of any order k can be written as a matrix polynomial of degree at most n − 1 , where n is the size of a square matrix. This is an instance where Cayley–Hamilton theorem can be used to express a matrix function, which we will discuss below systematically.

Matrix functions [ edit ] Given an analytic function f ( x ) = ∑ ∑ k = 0 ∞ ∞ a k x k {\displaystyle f(x)=\sum _{k=0}^{\infty }a_{k}x^{k}} and the characteristic polynomial p ( x ) of degree n of an n × n matrix A , the function can be expressed using long division as f ( x ) = q ( x ) p ( x ) + r ( x ) , {\displaystyle f(x)=q(x)p(x)+r(x),} where q ( x ) is some quotient polynomial and r ( x ) is a remainder polynomial such that 0 ≤ deg r ( x ) < n .

By the Cayley–Hamilton theorem, replacing x by the matrix A gives p ( A ) = 0 , so one has f ( A ) = r ( A ) .

{\displaystyle f(A)=r(A).} Thus, the analytic function of the matrix A can be expressed as a matrix polynomial of degree less than n .

Let the remainder polynomial be r ( x ) = c 0 + c 1 x + ⋯ ⋯ + c n − − 1 x n − − 1 .

{\displaystyle r(x)=c_{0}+c_{1}x+\cdots +c_{n-1}x^{n-1}.} Since p ( λ ) = 0 , evaluating the function f ( x ) at the n eigenvalues of A yields f ( λ λ i ) = r ( λ λ i ) = c 0 + c 1 λ λ i + ⋯ ⋯ + c n − − 1 λ λ i n − − 1 , for i = 1 , 2 , .

.

.

, n .

{\displaystyle f(\lambda _{i})=r(\lambda _{i})=c_{0}+c_{1}\lambda _{i}+\cdots +c_{n-1}\lambda _{i}^{n-1},\qquad {\text{for }}i=1,2,...,n.} This amounts to a system of n linear equations , which can be solved to determine the coefficients c i . Thus, one has f ( A ) = ∑ ∑ k = 0 n − − 1 c k A k .

{\displaystyle f(A)=\sum _{k=0}^{n-1}c_{k}A^{k}.} When the eigenvalues are repeated, that is λ i = λ j for some i ≠ j , two or more equations are identical; and hence the linear equations cannot be solved uniquely. For such cases, for an eigenvalue λ with multiplicity m , the first m − 1 derivatives of p ( x ) vanish at the eigenvalue. This leads to the extra m − 1 linearly independent solutions d k f ( x ) d x k | x = λ λ = d k r ( x ) d x k | x = λ λ for k = 1 , 2 , … … , m − − 1 , {\displaystyle \left.{\frac {\mathrm {d} ^{k}f(x)}{\mathrm {d} x^{k}}}\right|_{x=\lambda }=\left.{\frac {\mathrm {d} ^{k}r(x)}{\mathrm {d} x^{k}}}\right|_{x=\lambda }\qquad {\text{for }}k=1,2,\ldots ,m-1,} which, combined with others, yield the required n equations to solve for c i .

Finding a polynomial that passes through the points ( λ i , f ( λ i )) is essentially an interpolation problem , and can be solved using Lagrange or Newton interpolation techniques, leading to Sylvester's formula .

For example, suppose the task is to find the polynomial representation of f ( A ) = e A t w h e r e A = ( 1 2 0 3 ) .

{\displaystyle f(A)=e^{At}\qquad \mathrm {where} \qquad A={\begin{pmatrix}1&2\\0&3\end{pmatrix}}.} The characteristic polynomial is p ( x ) = ( x − 1)( x − 3) = x 2 − 4 x + 3 , and the eigenvalues are λ = 1, 3 . Let r ( x ) = c 0 + c 1 x . Evaluating f ( λ ) = r ( λ ) at the eigenvalues, one obtains two linear equations, e t = c 0 + c 1 and e 3 t = c 0 + 3 c 1 .

Solving the equations yields c 0 = (3 e t − e 3 t )/2 and c 1 = ( e 3 t − e t )/2 . Thus, it follows that e A t = c 0 I 2 + c 1 A = ( c 0 + c 1 2 c 1 0 c 0 + 3 c 1 ) = ( e t e 3 t − − e t 0 e 3 t ) .

{\displaystyle e^{At}=c_{0}I_{2}+c_{1}A={\begin{pmatrix}c_{0}+c_{1}&2c_{1}\\0&c_{0}+3c_{1}\end{pmatrix}}={\begin{pmatrix}e^{t}&e^{3t}-e^{t}\\0&e^{3t}\end{pmatrix}}.} If, instead, the function were f ( A ) = sin At , then the coefficients would have been c 0 = (3 sin t − sin 3 t )/2 and c 1 = (sin 3 t − sin t )/2 ; hence sin ⁡ ⁡ ( A t ) = c 0 I 2 + c 1 A = ( sin ⁡ ⁡ t sin ⁡ ⁡ 3 t − − sin ⁡ ⁡ t 0 sin ⁡ ⁡ 3 t ) .

{\displaystyle \sin(At)=c_{0}I_{2}+c_{1}A={\begin{pmatrix}\sin t&\sin 3t-\sin t\\0&\sin 3t\end{pmatrix}}.} As a further example, when considering f ( A ) = e A t w h e r e A = ( 0 1 − − 1 0 ) , {\displaystyle f(A)=e^{At}\qquad \mathrm {where} \qquad A={\begin{pmatrix}0&1\\-1&0\end{pmatrix}},} then the characteristic polynomial is p ( x ) = x 2 + 1 , and the eigenvalues are λ = ± i .

As before, evaluating the function at the eigenvalues gives us the linear equations e it = c 0 + i c 1 and e − it = c 0 − ic 1 ; the solution of which gives, c 0 = ( e it + e − it )/2 = cos t and c 1 = ( e it − e − it )/2 i = sin t . Thus, for this case, e A t = ( cos ⁡ ⁡ t ) I 2 + ( sin ⁡ ⁡ t ) A = ( cos ⁡ ⁡ t sin ⁡ ⁡ t − − sin ⁡ ⁡ t cos ⁡ ⁡ t ) , {\displaystyle e^{At}=(\cos t)I_{2}+(\sin t)A={\begin{pmatrix}\cos t&\sin t\\-\sin t&\cos t\end{pmatrix}},} which is a rotation matrix .

Standard examples of such usage is the exponential map from the Lie algebra of a matrix Lie group into the group. It is given by a matrix exponential , exp : g → → G ; t X ↦ ↦ e t X = ∑ ∑ n = 0 ∞ ∞ t n X n n !

= I + t X + t 2 X 2 2 + ⋯ ⋯ , t ∈ ∈ R , X ∈ ∈ g .

{\displaystyle \exp :{\mathfrak {g}}\rightarrow G;\qquad tX\mapsto e^{tX}=\sum _{n=0}^{\infty }{\frac {t^{n}X^{n}}{n!}}=I+tX+{\frac {t^{2}X^{2}}{2}}+\cdots ,t\in \mathbb {R} ,X\in {\mathfrak {g}}.} Such expressions have long been known for SU(2) , e i ( θ θ / 2 ) ( n ^ ^ ⋅ ⋅ σ σ ) = I 2 cos ⁡ ⁡ θ θ 2 + i ( n ^ ^ ⋅ ⋅ σ σ ) sin ⁡ ⁡ θ θ 2 , {\displaystyle e^{i(\theta /2)({\hat {\mathbf {n} }}\cdot \sigma )}=I_{2}\cos {\frac {\theta }{2}}+i({\hat {\mathbf {n} }}\cdot \sigma )\sin {\frac {\theta }{2}},} where the σ are the Pauli matrices and for SO(3) , e i θ θ ( n ^ ^ ⋅ ⋅ J ) = I 3 + i ( n ^ ^ ⋅ ⋅ J ) sin ⁡ ⁡ θ θ + ( n ^ ^ ⋅ ⋅ J ) 2 ( cos ⁡ ⁡ θ θ − − 1 ) , {\displaystyle e^{i\theta ({\hat {\mathbf {n} }}\cdot \mathbf {J} )}=I_{3}+i({\hat {\mathbf {n} }}\cdot \mathbf {J} )\sin \theta +({\hat {\mathbf {n} }}\cdot \mathbf {J} )^{2}(\cos \theta -1),} which is Rodrigues' rotation formula . For the notation, see 3D rotation group#A note on Lie algebras .

More recently, expressions have appeared for other groups, like the Lorentz group SO(3, 1) , [ 10 ] O(4, 2) [ 11 ] and SU(2, 2) , [ 12 ] as well as GL( n , R ) .

[ 13 ] The group O(4, 2) is the conformal group of spacetime , SU(2, 2) its simply connected cover (to be precise, the simply connected cover of the connected component SO + (4, 2) of O(4, 2) ). The expressions obtained apply to the standard representation of these groups. They require knowledge of (some of) the eigenvalues of the matrix to exponentiate. For SU(2) (and hence for SO(3) ), closed expressions have been obtained for all irreducible representations, i.e. of any spin.

[ 14 ] Ferdinand Georg Frobenius (1849–1917), German mathematician. His main interests were elliptic functions , differential equations , and later group theory .

In 1878 he gave the first full proof of the Cayley–Hamilton theorem.

[ 9 ] Algebraic number theory [ edit ] The Cayley–Hamilton theorem is an effective tool for computing the minimal polynomial of algebraic integers . For example, given a finite extension Q [ α α 1 , … … , α α k ] {\displaystyle \mathbb {Q} [\alpha _{1},\ldots ,\alpha _{k}]} of Q {\displaystyle \mathbb {Q} } and an algebraic integer α α ∈ ∈ Q [ α α 1 , … … , α α k ] {\displaystyle \alpha \in \mathbb {Q} [\alpha _{1},\ldots ,\alpha _{k}]} which is a non-zero linear combination of the α α 1 n 1 ⋯ ⋯ α α k n k {\displaystyle \alpha _{1}^{n_{1}}\cdots \alpha _{k}^{n_{k}}} we can compute the minimal polynomial of α α {\displaystyle \alpha } by finding a matrix representing the Q {\displaystyle \mathbb {Q} } - linear transformation ⋅ ⋅ α α : Q [ α α 1 , … … , α α k ] → → Q [ α α 1 , … … , α α k ] {\displaystyle \cdot \alpha :\mathbb {Q} [\alpha _{1},\ldots ,\alpha _{k}]\to \mathbb {Q} [\alpha _{1},\ldots ,\alpha _{k}]} If we call this transformation matrix A {\displaystyle A} , then we can find the minimal polynomial by applying the Cayley–Hamilton theorem to A {\displaystyle A} .

[ 15 ] Proofs [ edit ] The Cayley–Hamilton theorem is an immediate consequence of the existence of the Jordan normal form for matrices over algebraically closed fields , see Jordan normal form § Cayley–Hamilton theorem . In this section, direct proofs are presented.

As the examples above show, obtaining the statement of the Cayley–Hamilton theorem for an n × × n {\displaystyle n\times n} matrix A = ( a i j ) i , j = 1 n {\displaystyle A=\left(a_{ij}\right)_{i,j=1}^{n}} requires two steps: first the coefficients c i of the characteristic polynomial are determined by development as a polynomial in t of the determinant p ( t ) = det ( t I n − − A ) = | t − − a 1 , 1 − − a 1 , 2 ⋯ ⋯ − − a 1 , n − − a 2 , 1 t − − a 2 , 2 ⋯ ⋯ − − a 2 , n ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ − − a n , 1 − − a n , 2 ⋯ ⋯ t − − a n , n | = t n + c n − − 1 t n − − 1 + ⋯ ⋯ + c 1 t + c 0 , {\displaystyle {\begin{aligned}p(t)&=\det(tI_{n}-A)={\begin{vmatrix}t-a_{1,1}&-a_{1,2}&\cdots &-a_{1,n}\\-a_{2,1}&t-a_{2,2}&\cdots &-a_{2,n}\\\vdots &\vdots &\ddots &\vdots \\-a_{n,1}&-a_{n,2}&\cdots &t-a_{n,n}\end{vmatrix}}\\[5pt]&=t^{n}+c_{n-1}t^{n-1}+\cdots +c_{1}t+c_{0},\end{aligned}}} and then these coefficients are used in a linear combination of powers of A that is equated to the n × × n {\displaystyle n\times n} zero matrix: A n + c n − − 1 A n − − 1 + ⋯ ⋯ + c 1 A + c 0 I n = ( 0 ⋯ ⋯ 0 ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 ⋯ ⋯ 0 ) .

{\displaystyle A^{n}+c_{n-1}A^{n-1}+\cdots +c_{1}A+c_{0}I_{n}={\begin{pmatrix}0&\cdots &0\\\vdots &\ddots &\vdots \\0&\cdots &0\end{pmatrix}}.} The left-hand side can be worked out to an n × × n {\displaystyle n\times n} matrix whose entries are (enormous) polynomial expressions in the set of entries a i , j of A , so the Cayley–Hamilton theorem states that each of these n 2 expressions equals 0 . For any fixed value of n , these identities can be obtained by tedious but straightforward algebraic manipulations. None of these computations, however, can show why the Cayley–Hamilton theorem should be valid for matrices of all possible sizes n , so a uniform proof for all n is needed.

Preliminaries [ edit ] If a vector v of size n is an eigenvector of A with eigenvalue λ , in other words if A ⋅ v = λv , then p ( A ) ⋅ ⋅ v = A n ⋅ ⋅ v + c n − − 1 A n − − 1 ⋅ ⋅ v + ⋯ ⋯ + c 1 A ⋅ ⋅ v + c 0 I n ⋅ ⋅ v = λ λ n v + c n − − 1 λ λ n − − 1 v + ⋯ ⋯ + c 1 λ λ v + c 0 v = p ( λ λ ) v , {\displaystyle {\begin{aligned}p(A)\cdot v&=A^{n}\cdot v+c_{n-1}A^{n-1}\cdot v+\cdots +c_{1}A\cdot v+c_{0}I_{n}\cdot v\\[6pt]&=\lambda ^{n}v+c_{n-1}\lambda ^{n-1}v+\cdots +c_{1}\lambda v+c_{0}v=p(\lambda )v,\end{aligned}}} which is the zero vector since p ( λ ) = 0 (the eigenvalues of A are precisely the roots of p ( t ) ). This holds for all possible eigenvalues λ , so the two matrices equated by the theorem certainly give the same (null) result when applied to any eigenvector. Now if A admits a basis of eigenvectors, in other words if A is diagonalizable , then the Cayley–Hamilton theorem must hold for A , since two matrices that give the same values when applied to each element of a basis must be equal.

A = X D X − − 1 , D = diag ⁡ ⁡ ( λ λ i ) , i = 1 , 2 , .

.

.

, n {\displaystyle A=XDX^{-1},\quad D=\operatorname {diag} (\lambda _{i}),\quad i=1,2,...,n} p A ( λ λ ) = | λ λ I − − A | = ∏ ∏ i = 1 n ( λ λ − − λ λ i ) ≡ ≡ ∑ ∑ k = 0 n c k λ λ k {\displaystyle p_{A}(\lambda )=|\lambda I-A|=\prod _{i=1}^{n}(\lambda -\lambda _{i})\equiv \sum _{k=0}^{n}c_{k}\lambda ^{k}} p A ( A ) = ∑ ∑ c k A k = X p A ( D ) X − − 1 = X C X − − 1 {\displaystyle p_{A}(A)=\sum c_{k}A^{k}=Xp_{A}(D)X^{-1}=XCX^{-1}} C i i = ∑ ∑ k = 0 n c k λ λ i k = ∏ ∏ j = 1 n ( λ λ i − − λ λ j ) = 0 , C i , j ≠ ≠ i = 0 {\displaystyle C_{ii}=\sum _{k=0}^{n}c_{k}\lambda _{i}^{k}=\prod _{j=1}^{n}(\lambda _{i}-\lambda _{j})=0,\qquad C_{i,j\neq i}=0} ∴ ∴ p A ( A ) = X C X − − 1 = O .

{\displaystyle \therefore p_{A}(A)=XCX^{-1}=O.} Consider now the function e : : M n → → M n {\displaystyle e\colon M_{n}\to M_{n}} which maps n × × n {\displaystyle n\times n} matrices to n × × n {\displaystyle n\times n} matrices given by the formula e ( A ) = p A ( A ) {\displaystyle e(A)=p_{A}(A)} , i.e. which takes a matrix A {\displaystyle A} and plugs it into its own characteristic polynomial. Not all matrices are diagonalizable, but for matrices with complex coefficients many of them are: the set D {\displaystyle D} of diagonalizable complex square matrices of a given size is dense in the set of all such square matrices [ 16 ] (for a matrix to be diagonalizable it suffices for instance that its characteristic polynomial not have any multiple roots ). Now viewed as a function e : : C n 2 → → C n 2 {\displaystyle e\colon \mathbb {C} ^{n^{2}}\to \mathbb {C} ^{n^{2}}} (since matrices have n 2 {\displaystyle n^{2}} entries) we see that this function is continuous . This is true because the entries of the image of a matrix are given by polynomials in the entries of the matrix. Since e ( D ) = { ( 0 ⋯ ⋯ 0 ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 ⋯ ⋯ 0 ) } {\displaystyle e(D)=\left\{{\begin{pmatrix}0&\cdots &0\\\vdots &\ddots &\vdots \\0&\cdots &0\end{pmatrix}}\right\}} and since the set D {\displaystyle D} is dense, by continuity this function must map the entire set of n × × n {\displaystyle n\times n} matrices to the zero matrix. Therefore, the Cayley–Hamilton theorem is true for complex numbers, and must therefore also hold for Q {\displaystyle \mathbb {Q} } - or R {\displaystyle \mathbb {R} } -valued matrices.

While this provides a valid proof, the argument is not very satisfactory, since the identities represented by the theorem do not in any way depend on the nature of the matrix (diagonalizable or not), nor on the kind of entries allowed (for matrices with real entries the diagonalizable ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any commutative ring .

There is a great variety of such proofs of the Cayley–Hamilton theorem, of which several will be given here. They vary in the amount of abstract algebraic notions required to understand the proof. The simplest proofs use just those notions needed to formulate the theorem (matrices, polynomials with numeric entries, determinants), but involve technical computations that render somewhat mysterious the fact that they lead precisely to the correct conclusion. It is possible to avoid such details, but at the price of involving more subtle algebraic notions: polynomials with coefficients in a non-commutative ring, or matrices with unusual kinds of entries.

Adjugate matrices [ edit ] All proofs below use the notion of the adjugate matrix adj( M ) of an n × × n {\displaystyle n\times n} matrix M , the transpose of its cofactor matrix . This is a matrix whose coefficients are given by polynomial expressions in the coefficients of M (in fact, by certain ( n − − 1 ) × × ( n − − 1 ) {\displaystyle (n-1)\times (n-1)} determinants), in such a way that the following fundamental relations hold, adj ⁡ ⁡ ( M ) ⋅ ⋅ M = det ( M ) I n = M ⋅ ⋅ adj ⁡ ⁡ ( M ) .

{\displaystyle \operatorname {adj} (M)\cdot M=\det(M)I_{n}=M\cdot \operatorname {adj} (M)~.} These relations are a direct consequence of the basic properties of determinants: evaluation of the ( i , j ) entry of the matrix product on the left gives the expansion by column j of the determinant of the matrix obtained from M by replacing column i by a copy of column j , which is det( M ) if i = j and zero otherwise; the matrix product on the right is similar, but for expansions by rows.

Being a consequence of just algebraic expression manipulation, these relations are valid for matrices with entries in any commutative ring (commutativity must be assumed for determinants to be defined in the first place). This is important to note here, because these relations will be applied below for matrices with non-numeric entries such as polynomials.

A direct algebraic proof [ edit ] This proof uses just the kind of objects needed to formulate the Cayley–Hamilton theorem: matrices with polynomials as entries. The matrix t I n − − A {\displaystyle tI_{n}-A} whose determinant is the characteristic polynomial of A is such a matrix, and since polynomials form a commutative ring, it has an adjugate B = adj ⁡ ⁡ ( t I n − − A ) .

{\displaystyle B=\operatorname {adj} (tI_{n}-A).} Then, according to the right-hand fundamental relation of the adjugate, one has ( t I n − − A ) B = det ( t I n − − A ) I n = p ( t ) I n .

{\displaystyle (tI_{n}-A)B=\det(tI_{n}-A)I_{n}=p(t)I_{n}.} Since B is also a matrix with polynomials in t as entries, one can, for each i , collect the coefficients of t i {\displaystyle t^{i}} in each entry to form a matrix B i of numbers, such that one has B = ∑ ∑ i = 0 n − − 1 t i B i .

{\displaystyle B=\sum _{i=0}^{n-1}t^{i}B_{i}.} (The way the entries of B are defined makes clear that no powers higher than t n −1 occur). While this looks like a polynomial with matrices as coefficients, we shall not consider such a notion; it is just a way to write a matrix with polynomial entries as a linear combination of n constant matrices, and the coefficient t i {\displaystyle t^{i}} has been written to the left of the matrix to stress this point of view.

Now, one can expand the matrix product in our equation: p ( t ) I n = ( t I n − − A ) B = ( t I n − − A ) ∑ ∑ i = 0 n − − 1 t i B i = ∑ ∑ i = 0 n − − 1 t I n ⋅ ⋅ t i B i − − ∑ ∑ i = 0 n − − 1 A ⋅ ⋅ t i B i = ∑ ∑ i = 0 n − − 1 t i + 1 B i − − ∑ ∑ i = 0 n − − 1 t i A B i = t n B n − − 1 + ∑ ∑ i = 1 n − − 1 t i ( B i − − 1 − − A B i ) − − A B 0 .

{\displaystyle {\begin{aligned}p(t)I_{n}&=(tI_{n}-A)B\\&=(tI_{n}-A)\sum _{i=0}^{n-1}t^{i}B_{i}\\&=\sum _{i=0}^{n-1}tI_{n}\cdot t^{i}B_{i}-\sum _{i=0}^{n-1}A\cdot t^{i}B_{i}\\&=\sum _{i=0}^{n-1}t^{i+1}B_{i}-\sum _{i=0}^{n-1}t^{i}AB_{i}\\&=t^{n}B_{n-1}+\sum _{i=1}^{n-1}t^{i}(B_{i-1}-AB_{i})-AB_{0}.\end{aligned}}} Writing p ( t ) I n = t n I n + t n − − 1 c n − − 1 I n + ⋯ ⋯ + t c 1 I n + c 0 I n , {\displaystyle p(t)I_{n}=t^{n}I_{n}+t^{n-1}c_{n-1}I_{n}+\cdots +tc_{1}I_{n}+c_{0}I_{n},} one obtains an equality of two matrices with polynomial entries, written as linear combinations of constant matrices with powers of t as coefficients.

Such an equality can hold only if in any matrix position the entry that is multiplied by a given power t i {\displaystyle t^{i}} is the same on both sides; it follows that the constant matrices with coefficient t i {\displaystyle t^{i}} in both expressions must be equal. Writing these equations then for i from n down to 0, one finds B n − − 1 = I n , B i − − 1 − − A B i = c i I n for 1 ≤ ≤ i ≤ ≤ n − − 1 , − − A B 0 = c 0 I n .

{\displaystyle B_{n-1}=I_{n},\qquad B_{i-1}-AB_{i}=c_{i}I_{n}\quad {\text{for }}1\leq i\leq n-1,\qquad -AB_{0}=c_{0}I_{n}.} Finally, multiply the equation of the coefficients of t i {\displaystyle t^{i}} from the left by A i {\displaystyle A^{i}} , and sum up: A n B n − − 1 + ∑ ∑ i = 1 n − − 1 ( A i B i − − 1 − − A i + 1 B i ) − − A B 0 = A n + c n − − 1 A n − − 1 + ⋯ ⋯ + c 1 A + c 0 I n .

{\displaystyle A^{n}B_{n-1}+\sum \limits _{i=1}^{n-1}\left(A^{i}B_{i-1}-A^{i+1}B_{i}\right)-AB_{0}=A^{n}+c_{n-1}A^{n-1}+\cdots +c_{1}A+c_{0}I_{n}.} The left-hand sides form a telescoping sum and cancel completely; the right-hand sides add up to p ( A ) {\displaystyle p(A)} : 0 = p ( A ) .

{\displaystyle 0=p(A).} This completes the proof.

A proof using polynomials with matrix coefficients [ edit ] This proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting.

Notably, while arithmetic of polynomials over a commutative ring models the arithmetic of polynomial functions , this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in t with matrix coefficients, the variable t must not be thought of as an "unknown", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set t to a specific value.

( f + g ) ( x ) = ∑ ∑ i ( f i + g i ) x i = ∑ ∑ i f i x i + ∑ ∑ i g i x i = f ( x ) + g ( x ) .

{\displaystyle (f+g)(x)=\sum _{i}\left(f_{i}+g_{i}\right)x^{i}=\sum _{i}{f_{i}x^{i}}+\sum _{i}{g_{i}x^{i}}=f(x)+g(x).} Let M ( n , R ) {\displaystyle M(n,R)} be the ring of n × n matrices with entries in some ring R (such as the real or complex numbers) that has A as an element. Matrices with as coefficients polynomials in t , such as t I n − − A {\displaystyle tI_{n}-A} or its adjugate B in the first proof, are elements of M ( n , R [ t ] ) {\displaystyle M(n,R[t])} .

By collecting like powers of t , such matrices can be written as "polynomials" in t with constant matrices as coefficients; write M ( n , R ) [ t ] {\displaystyle M(n,R)[t]} for the set of such polynomials. Since this set is in bijection with M ( n , R [ t ] ) {\displaystyle M(n,R[t])} , one defines arithmetic operations on it correspondingly, in particular multiplication is given by ( ∑ ∑ i M i t i ) ( ∑ ∑ j N j t j ) = ∑ ∑ i , j ( M i N j ) t i + j , {\displaystyle \left(\sum _{i}M_{i}t^{i}\right)\!\!\left(\sum _{j}N_{j}t^{j}\right)=\sum _{i,j}(M_{i}N_{j})t^{i+j},} respecting the order of the coefficient matrices from the two operands; obviously this gives a non-commutative multiplication.

Thus, the identity ( t I n − − A ) B = p ( t ) I n .

{\displaystyle (tI_{n}-A)B=p(t)I_{n}.} from the first proof can be viewed as one involving a multiplication of elements in M ( n , R ) [ t ] {\displaystyle M(n,R)[t]} .

At this point, it is tempting to simply set t equal to the matrix A , which makes the first factor on the left equal to the zero matrix, and the right hand side equal to p ( A ) ; however, this is not an allowed operation when coefficients do not commute. It is possible to define a "right-evaluation map" ev A : M [ t ] → M , which replaces each t i by the matrix power A i of A , where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient. But this map is not a ring homomorphism : the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product M t i N t j = ( M ⋅ ⋅ N ) t i + j {\displaystyle Mt^{i}Nt^{j}=(M\cdot N)t^{i+j}} is defined assuming that t commutes with N , but this may fail if t is replaced by the matrix A .

One can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix A is in the center of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting t with coefficients is now justified after evaluation).

Now, A is not always in the center of M , but we may replace M with a smaller ring provided it contains all the coefficients of the polynomials in question: I n {\displaystyle I_{n}} , A , and the coefficients B i {\displaystyle B_{i}} of the polynomial B . The obvious choice for such a subring is the centralizer Z of A , the subring of all matrices that commute with A ; by definition A is in the center of Z .

This centralizer obviously contains I n {\displaystyle I_{n}} , and A , but one has to show that it contains the matrices B i {\displaystyle B_{i}} . To do this, one combines the two fundamental relations for adjugates, writing out the adjugate B as a polynomial: ( ∑ ∑ i = 0 m B i t i ) ( t I n − − A ) = ( t I n − − A ) ∑ ∑ i = 0 m B i t i ∑ ∑ i = 0 m B i t i + 1 − − ∑ ∑ i = 0 m B i A t i = ∑ ∑ i = 0 m B i t i + 1 − − ∑ ∑ i = 0 m A B i t i ∑ ∑ i = 0 m B i A t i = ∑ ∑ i = 0 m A B i t i .

{\displaystyle {\begin{aligned}\left(\sum _{i=0}^{m}B_{i}t^{i}\right)\!(tI_{n}-A)&=(tI_{n}-A)\sum _{i=0}^{m}B_{i}t^{i}\\\sum _{i=0}^{m}B_{i}t^{i+1}-\sum _{i=0}^{m}B_{i}At^{i}&=\sum _{i=0}^{m}B_{i}t^{i+1}-\sum _{i=0}^{m}AB_{i}t^{i}\\\sum _{i=0}^{m}B_{i}At^{i}&=\sum _{i=0}^{m}AB_{i}t^{i}.\end{aligned}}} Equating the coefficients shows that for each i , we have AB i = B i A as desired. Having found the proper setting in which ev A is indeed a homomorphism of rings, one can complete the proof as suggested above: ev A ⁡ ⁡ ( p ( t ) I n ) = ev A ⁡ ⁡ ( ( t I n − − A ) B ) p ( A ) = ev A ⁡ ⁡ ( t I n − − A ) ⋅ ⋅ ev A ⁡ ⁡ ( B ) p ( A ) = ( A I n − − A ) ⋅ ⋅ ev A ⁡ ⁡ ( B ) = O ⋅ ⋅ ev A ⁡ ⁡ ( B ) = O .

{\displaystyle {\begin{aligned}\operatorname {ev} _{A}\left(p(t)I_{n}\right)&=\operatorname {ev} _{A}((tI_{n}-A)B)\\[5pt]p(A)&=\operatorname {ev} _{A}(tI_{n}-A)\cdot \operatorname {ev} _{A}(B)\\[5pt]p(A)&=(AI_{n}-A)\cdot \operatorname {ev} _{A}(B)=O\cdot \operatorname {ev} _{A}(B)=O.\end{aligned}}} This completes the proof.

A synthesis of the first two proofs [ edit ] In the first proof, one was able to determine the coefficients B i of B based on the right-hand fundamental relation for the adjugate only. In fact the first n equations derived can be interpreted as determining the quotient B of the Euclidean division of the polynomial p ( t ) I n on the left by the monic polynomial I n t − A , while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial P is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes P to be a factor (here that is to the left).

To see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write P Q + r = P Q ′ + r ′ {\displaystyle PQ+r=PQ'+r'} as P ( Q − − Q ′ ) = r ′ − − r {\displaystyle P(Q-Q')=r'-r} and observe that since P is monic, P ( Q − Q ′) cannot have a degree less than that of P , unless Q = Q ′ .

But the dividend p ( t ) I n and divisor I n t − A used here both lie in the subring ( R [ A ])[ t ] , where R [ A ] is the subring of the matrix ring M ( n , R ) generated by A : the R -linear span of all powers of A . Therefore, the Euclidean division can in fact be performed within that commutative polynomial ring, and of course it then gives the same quotient B and remainder 0 as in the larger ring; in particular this shows that B in fact lies in ( R [ A ])[ t ] .

But, in this commutative setting, it is valid to set t to A in the equation p ( t ) I n = ( t I n − − A ) B ; {\displaystyle p(t)I_{n}=(tI_{n}-A)B;} in other words, to apply the evaluation map ev A : ( R [ A ] ) [ t ] → → R [ A ] {\displaystyle \operatorname {ev} _{A}:(R[A])[t]\to R[A]} which is a ring homomorphism, giving p ( A ) = 0 ⋅ ⋅ ev A ⁡ ⁡ ( B ) = 0 {\displaystyle p(A)=0\cdot \operatorname {ev} _{A}(B)=0} just like in the second proof, as desired.

In addition to proving the theorem, the above argument tells us that the coefficients B i of B are polynomials in A , while from the second proof we only knew that they lie in the centralizer Z of A ; in general Z is a larger subring than R [ A ] , and not necessarily commutative. In particular the constant term B 0 = adj(− A ) lies in R [ A ] . Since A is an arbitrary square matrix, this proves that adj( A ) can always be expressed as a polynomial in A (with coefficients that depend on A ) .

In fact, the equations found in the first proof allow successively expressing B n − − 1 , … … , B 1 , B 0 {\displaystyle B_{n-1},\ldots ,B_{1},B_{0}} as polynomials in A , which leads to the identity adj ⁡ ⁡ ( − − A ) = ∑ ∑ i = 1 n c i A i − − 1 , {\displaystyle \operatorname {adj} (-A)=\sum _{i=1}^{n}c_{i}A^{i-1},} valid for all n × n matrices, where p ( t ) = t n + c n − − 1 t n − − 1 + ⋯ ⋯ + c 1 t + c 0 {\displaystyle p(t)=t^{n}+c_{n-1}t^{n-1}+\cdots +c_{1}t+c_{0}} is the characteristic polynomial of A .

Note that this identity also implies the statement of the Cayley–Hamilton theorem: one may move adj(− A ) to the right hand side, multiply the resulting equation (on the left or on the right) by A , and use the fact that − − A ⋅ ⋅ adj ⁡ ⁡ ( − − A ) = adj ⁡ ⁡ ( − − A ) ⋅ ⋅ ( − − A ) = det ( − − A ) I n = c 0 I n .

{\displaystyle -A\cdot \operatorname {adj} (-A)=\operatorname {adj} (-A)\cdot (-A)=\det(-A)I_{n}=c_{0}I_{n}.} See also: Faddeev–LeVerrier algorithm A proof using matrices of endomorphisms [ edit ] As was mentioned above, the matrix p ( A ) in statement of the theorem is obtained by first evaluating the determinant and then substituting the matrix A for t ; doing that substitution into the matrix t I n − − A {\displaystyle tI_{n}-A} before evaluating the determinant is not meaningful. Nevertheless, it is possible to give an interpretation where p ( A ) is obtained directly as the value of a certain determinant, but this requires a more complicated setting, one of matrices over a ring in which one can interpret both the entries A i , j {\displaystyle A_{i,j}} of A , and all of A itself. One could take for this the ring M ( n , R ) of n × n matrices over R , where the entry A i , j {\displaystyle A_{i,j}} is realised as A i , j I n {\displaystyle A_{i,j}I_{n}} , and A as itself. But considering matrices with matrices as entries might cause confusion with block matrices , which is not intended, as that gives the wrong notion of determinant (recall that the determinant of a matrix is defined as a sum of products of its entries, and in the case of a block matrix this is generally not the same as the corresponding sum of products of its blocks!). It is clearer to distinguish A from the endomorphism φ of an n - dimensional vector space V (or free R -module if R is not a field) defined by it in a basis e 1 , … … , e n {\displaystyle e_{1},\ldots ,e_{n}} , and to take matrices over the ring End( V ) of all such endomorphisms. Then φ ∈ End( V ) is a possible matrix entry, while A designates the element of M ( n , End( V )) whose i , j entry is endomorphism of scalar multiplication by A i , j {\displaystyle A_{i,j}} ; similarly I n {\displaystyle I_{n}} will be interpreted as element of M ( n , End( V )) . However, since End( V ) is not a commutative ring, no determinant is defined on M ( n , End( V )) ; this can only be done for matrices over a commutative subring of End( V ) . Now the entries of the matrix φ φ I n − − A {\displaystyle \varphi I_{n}-A} all lie in the subring R [ φ ] generated by the identity and φ , which is commutative. Then a determinant map M ( n , R [ φ ]) → R [ φ ] is defined, and det ( φ φ I n − − A ) {\displaystyle \det(\varphi I_{n}-A)} evaluates to the value p ( φ ) of the characteristic polynomial of A at φ (this holds independently of the relation between A and φ ); the Cayley–Hamilton theorem states that p ( φ ) is the null endomorphism.

In this form, the following proof can be obtained from that of Atiyah & MacDonald (1969 , Prop. 2.4) (which in fact is the more general statement related to the Nakayama lemma ; one takes for the ideal in that proposition the whole ring R ). The fact that A is the matrix of φ in the basis e 1 , ..., e n means that φ φ ( e i ) = ∑ ∑ j = 1 n A j , i e j for i = 1 , … … , n .

{\displaystyle \varphi (e_{i})=\sum _{j=1}^{n}A_{j,i}e_{j}\quad {\text{for }}i=1,\ldots ,n.} One can interpret these as n components of one equation in V n , whose members can be written using the matrix-vector product M ( n , End( V )) × V n → V n that is defined as usual, but with individual entries ψ ∈ End( V ) and v in V being "multiplied" by forming ψ ψ ( v ) {\displaystyle \psi (v)} ; this gives: φ φ I n ⋅ ⋅ E = A tr ⋅ ⋅ E , {\displaystyle \varphi I_{n}\cdot E=A^{\operatorname {tr} }\cdot E,} where E ∈ ∈ V n {\displaystyle E\in V^{n}} is the element whose component i is e i (in other words it is the basis e 1 , ..., e n of V written as a column of vectors). Writing this equation as ( φ φ I n − − A tr ) ⋅ ⋅ E = 0 ∈ ∈ V n {\displaystyle (\varphi I_{n}-A^{\operatorname {tr} })\cdot E=0\in V^{n}} one recognizes the transpose of the matrix φ φ I n − − A {\displaystyle \varphi I_{n}-A} considered above, and its determinant (as element of M ( n , R [ φ ])) is also p ( φ ). To derive from this equation that p ( φ ) = 0 ∈ End( V ) , one left-multiplies by the adjugate matrix of φ φ I n − − A tr {\displaystyle \varphi I_{n}-A^{\operatorname {tr} }} , which is defined in the matrix ring M ( n , R [ φ ]) , giving 0 = adj ⁡ ⁡ ( φ φ I n − − A tr ) ⋅ ⋅ ( ( φ φ I n − − A tr ) ⋅ ⋅ E ) = ( adj ⁡ ⁡ ( φ φ I n − − A tr ) ⋅ ⋅ ( φ φ I n − − A tr ) ) ⋅ ⋅ E = ( det ( φ φ I n − − A tr ) I n ) ⋅ ⋅ E = ( p ( φ φ ) I n ) ⋅ ⋅ E ; {\displaystyle {\begin{aligned}0&=\operatorname {adj} (\varphi I_{n}-A^{\operatorname {tr} })\cdot \left((\varphi I_{n}-A^{\operatorname {tr} })\cdot E\right)\\[1ex]&=\left(\operatorname {adj} (\varphi I_{n}-A^{\operatorname {tr} })\cdot (\varphi I_{n}-A^{\operatorname {tr} })\right)\cdot E\\[1ex]&=\left(\det(\varphi I_{n}-A^{\operatorname {tr} })I_{n}\right)\cdot E\\[1ex]&=(p(\varphi )I_{n})\cdot E;\end{aligned}}} the associativity of matrix-matrix and matrix-vector multiplication used in the first step is a purely formal property of those operations, independent of the nature of the entries. Now component i of this equation says that p ( φ )( e i ) = 0 ∈ V ; thus p ( φ ) vanishes on all e i , and since these elements generate V it follows that p ( φ ) = 0 ∈ End( V ) , completing the proof.

One additional fact that follows from this proof is that the matrix A whose characteristic polynomial is taken need not be identical to the value φ substituted into that polynomial; it suffices that φ be an endomorphism of V satisfying the initial equations φ φ ( e i ) = ∑ ∑ j A j , i e j {\displaystyle \varphi (e_{i})=\sum _{j}A_{j,i}e_{j}} for some sequence of elements e 1 , ..., e n that generate V (which space might have smaller dimension than n , or in case the ring R is not a field it might not be a free module at all).

A bogus "proof": p ( A ) = det( AI n − A ) = det( A − A ) = 0 [ edit ] One persistent elementary but incorrect argument [ 17 ] for the theorem is to "simply" take the definition p ( λ λ ) = det ( λ λ I n − − A ) {\displaystyle p(\lambda )=\det(\lambda I_{n}-A)} and substitute A for λ , obtaining p ( A ) = det ( A I n − − A ) = det ( A − − A ) = det ( 0 ) = 0.

{\displaystyle p(A)=\det(AI_{n}-A)=\det(A-A)=\det(\mathbf {0} )=0.} There are many ways to see why this argument is wrong. First, in the Cayley–Hamilton theorem, p ( A ) is an n × n matrix . However, the right hand side of the above equation is the value of a determinant, which is a scalar . So they cannot be equated unless n = 1 (i.e.

A is just a scalar). Second, in the expression det ( λ λ I n − − A ) {\displaystyle \det(\lambda I_{n}-A)} , the variable λ actually occurs at the diagonal entries of the matrix λ λ I n − − A {\displaystyle \lambda I_{n}-A} . To illustrate, consider the characteristic polynomial in the previous example again: det ( λ λ − − 1 − − 2 − − 3 λ λ − − 4 ) .

{\displaystyle \det \!{\begin{pmatrix}\lambda -1&-2\\-3&\lambda -4\end{pmatrix}}.} If one substitutes the entire matrix A for λ in those positions, one obtains det ( ( 1 2 3 4 ) − − 1 − − 2 − − 3 ( 1 2 3 4 ) − − 4 ) , {\displaystyle \det \!{\begin{pmatrix}{\begin{pmatrix}1&2\\3&4\end{pmatrix}}-1&-2\\-3&{\begin{pmatrix}1&2\\3&4\end{pmatrix}}-4\end{pmatrix}},} in which the "matrix" expression is simply not a valid one. Note, however, that if scalar multiples of identity matrices
instead of scalars are subtracted in the above, i.e. if the substitution is performed as det ( ( 1 2 3 4 ) − − I 2 − − 2 I 2 − − 3 I 2 ( 1 2 3 4 ) − − 4 I 2 ) , {\displaystyle \det \!{\begin{pmatrix}{\begin{pmatrix}1&2\\3&4\end{pmatrix}}-I_{2}&-2I_{2}\\-3I_{2}&{\begin{pmatrix}1&2\\3&4\end{pmatrix}}-4I_{2}\end{pmatrix}},} then the determinant is indeed zero, but the expanded matrix in question does not evaluate to A I n − − A {\displaystyle AI_{n}-A} ; nor can its determinant (a scalar) be compared to p ( A ) (a matrix). So the argument that p ( A ) = det ( A I n − − A ) = 0 {\displaystyle p(A)=\det(AI_{n}-A)=0} still does not apply.

Actually, if such an argument holds, it should also hold when other multilinear forms instead of determinant is used. For instance, if we consider the permanent function and define q ( λ λ ) = perm ⁡ ⁡ ( λ λ I n − − A ) {\displaystyle q(\lambda )=\operatorname {perm} (\lambda I_{n}-A)} , then by the same argument, we should be able to "prove" that q ( A ) = 0 . But this statement is demonstrably wrong: in the 2-dimensional case, for instance, the permanent of a matrix is given by perm ( a b c d ) = a d + b c .

{\displaystyle \operatorname {perm} \!{\begin{pmatrix}a&b\\c&d\end{pmatrix}}=ad+bc.} So, for the matrix A in the previous example, q ( λ λ ) = perm ⁡ ⁡ ( λ λ I 2 − − A ) = perm ( λ λ − − 1 − − 2 − − 3 λ λ − − 4 ) = ( λ λ − − 1 ) ( λ λ − − 4 ) + ( − − 2 ) ( − − 3 ) = λ λ 2 − − 5 λ λ + 10.

{\displaystyle {\begin{aligned}q(\lambda )&=\operatorname {perm} (\lambda I_{2}-A)=\operatorname {perm} \!{\begin{pmatrix}\lambda -1&-2\\-3&\lambda -4\end{pmatrix}}\\[6pt]&=(\lambda -1)(\lambda -4)+(-2)(-3)=\lambda ^{2}-5\lambda +10.\end{aligned}}} Yet one can verify that q ( A ) = A 2 − − 5 A + 10 I 2 = 12 I 2 ≠ ≠ 0.

{\displaystyle q(A)=A^{2}-5A+10I_{2}=12I_{2}\neq 0.} One of the proofs for Cayley–Hamilton theorem above bears some similarity to the argument that p ( A ) = det ( A I n − − A ) = 0 {\displaystyle p(A)=\det(AI_{n}-A)=0} . By introducing a matrix with non-numeric coefficients, one can actually let A live inside a matrix entry, but then A I n {\displaystyle AI_{n}} is not equal to A , and the conclusion is reached differently.

Proofs using methods of abstract algebra [ edit ] Basic properties of Hasse–Schmidt derivations on the exterior algebra A = ⋀ ⋀ M {\textstyle A=\bigwedge M} of some B - module M (supposed to be free and of finite rank) have been used by Gatto & Salehyan (2016 , §4) to prove the Cayley–Hamilton theorem. See also Gatto & Scherbak (2015) .

A combinatorial proof [ edit ] A proof based on developing the Leibniz formula for the characteristic polynomial was given by Straubing [ 18 ] and a generalization was given using trace monoid theory of Foata and Cartier.

Abstraction and generalizations [ edit ] The above proofs show that the Cayley–Hamilton theorem holds for matrices with entries in any commutative ring R , and that p ( φ ) = 0 will hold whenever φ is an endomorphism of an R -module generated by elements e 1 ,..., e n that satisfies φ φ ( e j ) = ∑ ∑ a i j e i , j = 1 , … … , n .

{\displaystyle \varphi (e_{j})=\sum a_{ij}e_{i},\qquad j=1,\ldots ,n.} This more general version of the theorem is the source of the celebrated Nakayama lemma in commutative algebra and algebraic geometry .

The Cayley-Hamilton theorem also holds for matrices over the quaternions , a noncommutative ring .

[ 19 ] [ nb 3 ] See also [ edit ] Companion matrix Remarks [ edit ] ^ See Sect. 2 of Krivoruchenko (2016) . An explicit expression for the coefficients c i is provided by Kondratyuk & Krivoruchenko (1992) : c i = ∑ ∑ k 1 , k 2 , … … , k n ∏ ∏ l = 1 n ( − − 1 ) k l + 1 l k l k l !

tr ⁡ ⁡ ( A l ) k l , {\displaystyle c_{i}=\sum _{k_{1},k_{2},\ldots ,k_{n}}\prod _{l=1}^{n}{\frac {(-1)^{k_{l}+1}}{l^{k_{l}}k_{l}!}}\operatorname {tr} (A^{l})^{k_{l}},} where the sum is taken over the sets of all integer partitions k l ≥ 0 satisfying the equation ∑ ∑ l = 1 n l k l = n − − i .

{\displaystyle \sum _{l=1}^{n}lk_{l}=n-i.} ^ See, e.g., p. 54 of Brown 1994 , which solves Jacobi's formula , ∂ ∂ p ( λ λ ) ∂ ∂ λ λ = p ( λ λ ) ∑ ∑ m = 0 ∞ ∞ λ λ − − ( m + 1 ) tr ⁡ ⁡ A m = p ( λ λ ) tr ⁡ ⁡ I λ λ I − − A ≡ ≡ tr ⁡ ⁡ B , {\displaystyle {\frac {\partial p(\lambda )}{\partial \lambda }}=p(\lambda )\sum _{m=0}^{\infty }\lambda ^{-(m+1)}\operatorname {tr} A^{m}=p(\lambda )~\operatorname {tr} {\frac {I}{\lambda I-A}}\equiv \operatorname {tr} B~,} where B is the adjugate matrix of the next section.

There also exists an equivalent, related recursive algorithm introduced by Urbain Le Verrier and Dmitry Konstantinovich Faddeev —the Faddeev–LeVerrier algorithm , which reads M 0 ≡ ≡ O c n = 1 ( k = 0 ) M k ≡ ≡ A M k − − 1 − − 1 k − − 1 ( tr ⁡ ⁡ ( A M k − − 1 ) ) I c n − − k = − − 1 k tr ⁡ ⁡ ( A M k ) k = 1 , … … , n .

{\displaystyle {\begin{aligned}M_{0}&\equiv O&c_{n}&=1\qquad &(k=0)\\[5pt]M_{k}&\equiv AM_{k-1}-{\frac {1}{k-1}}(\operatorname {tr} (AM_{k-1}))I\qquad \qquad &c_{n-k}&=-{\frac {1}{k}}\operatorname {tr} (AM_{k})\qquad &k=1,\ldots ,n~.\end{aligned}}} (see, e.g., Gantmacher 1960 , p. 88.) Observe A −1 = − M n / c 0 as the recursion terminates.
See the algebraic proof in the following section, which relies on the modes of the adjugate, B k ≡ M n − k .
Specifically, ( λ λ I − − A ) B = I p ( λ λ ) {\displaystyle (\lambda I-A)B=Ip(\lambda )} and the above derivative of p when one traces it yields λ λ p ′ − − n p = tr ⁡ ⁡ ( A B ) , {\displaystyle \lambda p'-np=\operatorname {tr} (AB)~,} ( Hou 1998 ), and the above recursions, in turn.

^ Due to the non-commutative nature of the multiplication operation for quaternions and related constructions, care needs to be taken with definitions, most notably in this context, for the determinant. The theorem holds as well for the slightly less well-behaved split-quaternions , see Alagös, Oral & Yüce (2012) . The rings of quaternions and split-quaternions can both be represented by certain 2 × 2 complex matrices. (When restricted to unit norm, these are the groups SU(2) and SU(1,1) respectively.) Therefore it is not surprising that the theorem holds.

There is no such matrix representation for the octonions , since the multiplication operation is not associative in this case. However, a modified Cayley–Hamilton theorem still holds for the octonions, see Tian (2000) .

Notes [ edit ] ^ a b Crilly 1998 ^ a b Hamilton 1864a ^ a b Hamilton 1864b ^ a b Hamilton 1862 ^ Atiyah & MacDonald 1969 ^ Hamilton 1853 , p. 562 ^ Cayley 1858 , pp. 17–37 ^ Cayley 1889 , pp. 475–496 ^ a b Frobenius 1878 ^ Zeni & Rodrigues 1992 ^ Barut, Zeni & Laufer 1994a ^ Barut, Zeni & Laufer 1994b ^ Laufer 1997 ^ Curtright, Fairlie & Zachos 2014 ^ Stein, William.

Algebraic Number Theory, a Computational Approach (PDF) . p. 29.

^ Bhatia 1997 , p. 7 ^ Garrett 2007 , p. 381 ^ Straubing, Howard (1983-01-01).

"A combinatorial proof of the Cayley–Hamilton theorem" .

Discrete Mathematics .

43 (2): 273– 279.

doi : 10.1016/0012-365X(83)90164-4 .

ISSN 0012-365X .

^ Zhang 1997 References [ edit ] Alagös, Y.; Oral, K.; Yüce, S. (2012).

"Split Quaternion Matrices" .

Miskolc Mathematical Notes .

13 (2): 223– 232.

doi : 10.18514/MMN.2012.364 .

ISSN 1787-2405 (open access) Atiyah, M. F.

; MacDonald, I. G.

(1969), Introduction to Commutative Algebra , Westview Press, ISBN 978-0-201-40751-8 Barut, A. O.

; Zeni, J. R.; Laufer, A. (1994a). "The exponential map for the conformal group O(2,4)".

J. Phys. A: Math. Gen .

27 (15): 5239– 5250.

arXiv : hep-th/9408105 .

Bibcode : 1994JPhA...27.5239B .

doi : 10.1088/0305-4470/27/15/022 .

Barut, A. O.

; Zeni, J. R.; Laufer, A. (1994b). "The exponential map for the unitary group SU(2,2)".

J. Phys. A: Math. Gen .

27 (20): 6799– 6806.

arXiv : hep-th/9408145 .

Bibcode : 1994JPhA...27.6799B .

doi : 10.1088/0305-4470/27/20/017 .

S2CID 16495633 .

Bhatia, R. (1997).

Matrix Analysis . Graduate texts in mathematics. Vol. 169. Springer.

ISBN 978-0387948461 .

Brown, Lowell S. (1994).

Quantum Field Theory .

Cambridge University Press .

ISBN 978-0-521-46946-3 .

Cayley, A.

(1858). "A Memoir on the Theory of Matrices".

Philos. Trans .

148 .

Cayley, A. (1889).

The Collected Mathematical Papers of Arthur Cayley . (Classic Reprint). Vol. 2. Forgotten books.

ASIN B008HUED9O .

Crilly, T. (1998). "The young Arthur Cayley".

Notes Rec. R. Soc. Lond .

52 (2): 267– 282.

doi : 10.1098/rsnr.1998.0050 .

S2CID 146669911 .

Curtright, T L ; Fairlie, D B ; Zachos, C K (2014). "A compact formula for rotations as spin matrix polynomials".

SIGMA .

10 (2014): 084.

arXiv : 1402.3541 .

Bibcode : 2014SIGMA..10..084C .

doi : 10.3842/SIGMA.2014.084 .

S2CID 18776942 .

Frobenius, G.

(1878). "Ueber lineare Substutionen und bilineare Formen".

J. Reine Angew. Math .

1878 (84): 1– 63.

doi : 10.1515/crll.1878.84.1 (inactive 1 July 2025).

{{ cite journal }} :  CS1 maint: DOI inactive as of July 2025 ( link ) Gantmacher, F.R. (1960).

The Theory of Matrices . NY: Chelsea Publishing.

ISBN 978-0-8218-1376-8 .

{{ cite book }} : ISBN / Date incompatibility ( help ) Gatto, Letterio; Salehyan, Parham (2016), Hasse–Schmidt derivations on Grassmann algebras , Springer, doi : 10.1007/978-3-319-31842-4 , ISBN 978-3-319-31842-4 , MR 3524604 Gatto, Letterio; Scherbak, Inna (2015), Remarks on the Cayley-Hamilton Theorem , arXiv : 1510.03022 Garrett, Paul B. (2007).

Abstract Algebra . NY: Chapman and Hall/CRC.

ISBN 978-1584886891 .

Hamilton, W. R.

(1853).

Lectures on Quaternions . Dublin.

{{ cite book }} :  CS1 maint: location missing publisher ( link ) Hamilton, W. R. (1864a). "On a New and General Method of Inverting a Linear and Quaternion Function of a Quaternion".

Proceedings of the Royal Irish Academy .

viii : 182– 183.

(communicated on June 9, 1862) Hamilton, W. R. (1864b). "On the Existence of a Symbolic and Biquadratic Equation, which is satisfied by the Symbol of Linear Operation in Quaternions".

Proceedings of the Royal Irish Academy .

viii : 190– 101.

(communicated on June 23, 1862) Hou, S. H. (1998). "Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm".

SIAM Review .

40 (3): 706– 709.

Bibcode : 1998SIAMR..40..706H .

doi : 10.1137/S003614459732076X .

"Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm" Hamilton, W. R. (1862).

"On the Existence of a Symbolic and Biquadratic Equation which is satisfied by the Symbol of Linear or Distributive Operation on a Quaternion" .

The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science . series iv .

24 : 127– 128.

ISSN 1478-6435 . Retrieved 2015-02-14 .

Householder, Alston S.

(2006).

The Theory of Matrices in Numerical Analysis . Dover Books on Mathematics.

ISBN 978-0486449722 .

Krivoruchenko, M. I. (2016). "Trace Identities for Skew-Symmetric Matrices".

arXiv : 1605.00447 [ math-ph ].

Kondratyuk, L. A.; Krivoruchenko, M. I. (1992). "Superconducting quark matter in SU(2) color group".

Zeitschrift für Physik A .

344 (1): 99– 115.

Bibcode : 1992ZPhyA.344...99K .

doi : 10.1007/BF01291027 .

S2CID 120467300 .

Laufer, A. (1997). "The exponential map of GL(N)".

J. Phys. A: Math. Gen .

30 (15): 5455– 5470.

arXiv : hep-th/9604049 .

Bibcode : 1997JPhA...30.5455L .

doi : 10.1088/0305-4470/30/15/029 .

S2CID 10699434 .

Tian, Y. (2000). "Matrix representations of octonions and their application".

Advances in Applied Clifford Algebras .

10 (1): 61– 90.

arXiv : math/0003166 .

Bibcode : 2000math......3166T .

CiteSeerX 10.1.1.237.2217 .

doi : 10.1007/BF03042010 .

ISSN 0188-7009 .

S2CID 14465054 .

Zeni, J. R.; Rodrigues, W.A. (1992). "A thoughtful study of Lorentz transformations by Clifford algebras".

Int. J. Mod. Phys. A .

7 (8): 1793 pp.

Bibcode : 1992IJMPA...7.1793Z .

doi : 10.1142/S0217751X92000776 .

Zhang, F. (1997).

"Quaternions and matrices of quaternions" .

Linear Algebra and Its Applications .

251 : 21– 57.

doi : 10.1016/0024-3795(95)00543-9 .

ISSN 0024-3795 (open archive).

External links [ edit ] "Cayley–Hamilton theorem" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] A proof from PlanetMath.

The Cayley–Hamilton theorem at MathPages NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐xwdw8
Cached time: 20250812002930
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 1.030 seconds
Real time usage: 1.215 seconds
Preprocessor visited node count: 16132/1000000
Revision size: 66182/2097152 bytes
Post‐expand include size: 120116/2097152 bytes
Template argument size: 23147/2097152 bytes
Highest expansion depth: 10/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 89723/5000000 bytes
Lua time usage: 0.457/10.000 seconds
Lua memory usage: 7349586/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  814.599      1 -total
 34.86%  283.982    264 Template:Math
 16.97%  138.261      2 Template:Reflist
 13.62%  110.981      8 Template:Cite_book
 13.47%  109.730      1 Template:Short_description
 11.99%   97.674     17 Template:Cite_journal
  9.66%   78.722      2 Template:Pagetype
  9.01%   73.390    269 Template:Main_other
  5.81%   47.291      7 Template:Harvtxt
  2.48%   20.162      3 Template:Citation Saved in parser cache with key enwiki:pcache:173547:|#|:idhash:canonical and timestamp 20250812002930 and revision id 1304129966. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Cayley–Hamilton_theorem&oldid=1304129966 " Categories : Theorems in linear algebra Matrix theory William Rowan Hamilton Hidden categories: Articles with short description Short description is different from Wikidata CS1 maint: DOI inactive as of July 2025 CS1 errors: ISBN date CS1 maint: location missing publisher Articles containing proofs Pages that use a deprecated format of the math tags This page was last edited on 4 August 2025, at 04:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Cayley–Hamilton theorem 28 languages Add topic

