Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Occurrence 3 Probability density function Toggle Probability density function subsection 3.1 Spectral density 4 Use in Bayesian statistics Toggle Use in Bayesian statistics subsection 4.1 Choice of parameters 5 Properties Toggle Properties subsection 5.1 Log-expectation 5.2 Log-variance 5.3 Entropy 5.4 Cross-entropy 5.5 KL-divergence 5.6 Characteristic function 6 Theorem Toggle Theorem subsection 6.1 Corollary 1 6.2 Corollary 2 7 Estimator of the multivariate normal distribution 8 Bartlett decomposition 9 Marginal distribution of matrix elements 10 The range of the shape parameter 11 Relationships to other distributions 12 See also 13 References 14 External links Toggle the table of contents Wishart distribution 12 languages Català Deutsch فارسی Français Italiano עברית Nederlands 日本語 Slovenčina Sunda Svenska 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Generalization of gamma distribution to multiple dimensions Wishart Notation X ~ W p ( V , n ) Parameters n degrees of freedom ( real ) V > 0 scale matrix ( p × p pos. def ) Support X ( p × p ) positive definite matrix PDF f X ( X ) = | X | ( n − − p − − 1 ) / 2 e − − tr ⁡ ⁡ ( V − − 1 X ) / 2 2 ( n p ) / 2 | V | n / 2 Γ Γ p ( n 2 ) {\displaystyle f_{\mathbf {X} }(\mathbf {X} )={\frac {|\mathbf {X} |^{(n-p-1)/2}e^{-\operatorname {tr} (\mathbf {V} ^{-1}\mathbf {X} )/2}}{2^{(np)/2}|{\mathbf {V} }|^{n/2}\Gamma _{p}({\frac {n}{2}})}}} Γ p is the multivariate gamma function tr is the trace function Mean E ⁡ ⁡ [ X ] = n V {\displaystyle \operatorname {E} [\mathbf {\mathbf {X} } ]=n{\mathbf {V} }} Mode ( n − p − 1) V for n ≥ p + 1 Variance Var ⁡ ⁡ ( X i j ) = n ( v i j 2 + v i i v j j ) {\displaystyle \operatorname {Var} (\mathbf {X} _{ij})=n\left(v_{ij}^{2}+v_{ii}v_{jj}\right)} Entropy see below CF Θ Θ ↦ ↦ | I − − 2 i Θ Θ V | − − n 2 {\displaystyle \Theta \mapsto \left|{\mathbf {I} }-2i\,{\mathbf {\Theta } }{\mathbf {V} }\right|^{-{\frac {n}{2}}}} In statistics , the Wishart distribution is a generalization of the gamma distribution to multiple dimensions. It is named in honor of John Wishart , who first formulated the distribution in 1928.

[ 1 ] Other names include Wishart ensemble (in random matrix theory , probability distributions over matrices are usually called "ensembles"), or Wishart–Laguerre ensemble (since its eigenvalue distribution involve Laguerre polynomials ), or LOE, LUE, LSE (in analogy with GOE, GUE, GSE ).

[ 2 ] It is a family of probability distributions defined over symmetric, positive-definite random matrices (i.e.

matrix -valued random variables ). These distributions are of great importance in the estimation of covariance matrices in multivariate statistics . In Bayesian statistics , the Wishart distribution is the conjugate prior of the inverse covariance-matrix of a multivariate-normal random vector .

[ 3 ] Definition [ edit ] Suppose G is a p × n matrix, each column of which is independently drawn from a p -variate normal distribution with zero mean: G = ( g 1 , … … , g n ) ∼ ∼ N p ( 0 , V ) .

{\displaystyle G=(g_{1},\dots ,g_{n})\sim {\mathcal {N}}_{p}(0,V).} It means : g i = ( g i , 1 , … … , g i , p ) T ∼ ∼ i i d N p ( 0 , V ) ∀ ∀ i ∈ ∈ { 1 , … … , n } {\displaystyle g_{i}=(g_{i,1},\dots ,g_{i,p})^{T}\ {\overset {iid}{\sim }}\ {\mathcal {N}}_{p}(0,V)\ \forall i\in \{1,\dots ,n\}} Then the Wishart distribution is the probability distribution of the p × p random matrix [ 4 ] S = G G T = ∑ ∑ i = 1 n g i g i T {\displaystyle S=GG^{T}=\sum _{i=1}^{n}g_{i}g_{i}^{T}} known as the scatter matrix . One indicates that S has that probability distribution by writing S ∼ ∼ W p ( V , n ) .

{\displaystyle S\sim W_{p}(V,n).} The positive integer n is the number of degrees of freedom .  Sometimes this is written W ( V , p , n ) . For n ≥ p the matrix S is invertible with probability 1 if V is invertible.

If p = V = 1 then this distribution is a chi-squared distribution with n degrees of freedom.

Occurrence [ edit ] The Wishart distribution arises as the distribution of the sample covariance matrix for a sample from a multivariate normal distribution .  It occurs frequently in likelihood-ratio tests in multivariate statistical analysis.  It also arises in the spectral theory of random matrices [ citation needed ] and in multidimensional Bayesian analysis.

[ 5 ] It is also encountered in wireless communications, while analyzing the performance of Rayleigh fading MIMO wireless channels .

[ 6 ] Probability density function [ edit ] Spectral density of Wishart-Laguerre ensemble with dimensions (8, 15). A reconstruction of Figure 1 of [ 7 ] .

The Wishart distribution can be characterized by its probability density function as follows: Let X be a p × p symmetric matrix of random variables that is positive semi-definite . Let V be a (fixed) symmetric positive definite matrix of size p × p .

Then, if n ≥ p , X has a Wishart distribution with n degrees of freedom if it has the probability density function f X ( X ) = 1 2 n p / 2 | V | n / 2 Γ Γ p ( n 2 ) | X | ( n − − p − − 1 ) / 2 e − − 1 2 tr ⁡ ⁡ ( V − − 1 X ) {\displaystyle f_{\mathbf {X} }(\mathbf {X} )={\frac {1}{2^{np/2}\left|{\mathbf {V} }\right|^{n/2}\Gamma _{p}\left({\frac {n}{2}}\right)}}{\left|\mathbf {X} \right|}^{(n-p-1)/2}e^{-{\frac {1}{2}}\operatorname {tr} ({\mathbf {V} }^{-1}\mathbf {X} )}} where | X | {\displaystyle \left|{\mathbf {X} }\right|} is the determinant of X {\displaystyle \mathbf {X} } and Γ p is the multivariate gamma function defined as Γ Γ p ( n 2 ) = π π p ( p − − 1 ) / 4 ∏ ∏ j = 1 p Γ Γ ( n 2 − − j − − 1 2 ) .

{\displaystyle \Gamma _{p}\left({\frac {n}{2}}\right)=\pi ^{p(p-1)/4}\prod _{j=1}^{p}\Gamma \left({\frac {n}{2}}-{\frac {j-1}{2}}\right).} The density above is not the joint density of all the p 2 {\displaystyle p^{2}} elements of the random matrix X (such p 2 {\displaystyle p^{2}} -dimensional density does not exist because of the symmetry constrains X i j = X j i {\displaystyle X_{ij}=X_{ji}} ), it is rather the joint density of p ( p + 1 ) / 2 {\displaystyle p(p+1)/2} elements X i j {\displaystyle X_{ij}} for i ≤ ≤ j {\displaystyle i\leq j} (, [ 1 ] page 38). Also, the density formula above applies only to positive definite matrices x ; {\displaystyle \mathbf {x} ;} for other matrices the density is equal to zero.

Spectral density [ edit ] The joint-eigenvalue density for the eigenvalues λ λ 1 , … … , λ λ p ≥ ≥ 0 {\displaystyle \lambda _{1},\dots ,\lambda _{p}\geq 0} of a random matrix X ∼ ∼ W p ( I , n ) {\displaystyle \mathbf {X} \sim W_{p}(\mathbf {I} ,n)} is, [ 8 ] [ 9 ] c n , p e − − 1 2 ∑ ∑ i λ λ i ∏ ∏ λ λ i ( n − − p − − 1 ) / 2 ∏ ∏ i < j | λ λ i − − λ λ j | {\displaystyle c_{n,p}e^{-{\frac {1}{2}}\sum _{i}\lambda _{i}}\prod \lambda _{i}^{(n-p-1)/2}\prod _{i<j}|\lambda _{i}-\lambda _{j}|} where c n , p {\displaystyle c_{n,p}} is a constant. The spectral density can be marginalized to yield the density of a single eigenvalue, by evaluating a Selberg integral .

In fact the above definition can be extended to any real n > p − 1 .  If n ≤ p − 1 , then the Wishart no longer has a density—instead it represents a singular distribution that takes values in a lower-dimension subspace of the space of p × p matrices.

[ 10 ] Use in Bayesian statistics [ edit ] In Bayesian statistics , in the context of the multivariate normal distribution , the Wishart distribution is the conjugate prior to the precision matrix Ω = Σ −1 , where Σ is the covariance matrix.

[ 11 ] : 135 [ 12 ] Choice of parameters [ edit ] The least informative, proper Wishart prior is obtained by setting n = p .

[ citation needed ] A common choice for V leverages the fact that the mean of X ~ W p ( V , n ) is n V . Then V is chosen so that n V equals an initial guess for X . For instance, when estimating a precision matrix Σ −1 ~ W p ( V , n ) a reasonable choice for V would be n −1 Σ 0 −1 , where Σ 0 is some prior estimate for the covariance matrix Σ .

Properties [ edit ] Log-expectation [ edit ] The following formula plays a role in variational Bayes derivations for Bayes networks involving the Wishart distribution. From equation (2.63), [ 13 ] E ⁡ ⁡ [ ln ⁡ ⁡ | X | ] = ψ ψ p ( n 2 ) + p ln ⁡ ⁡ ( 2 ) + ln ⁡ ⁡ | V | {\displaystyle \operatorname {E} [\,\ln \left|\mathbf {X} \right|\,]=\psi _{p}\left({\frac {n}{2}}\right)+p\,\ln(2)+\ln |\mathbf {V} |} where ψ ψ p {\displaystyle \psi _{p}} is the multivariate digamma function (the derivative of the log of the multivariate gamma function ).

Log-variance [ edit ] The following variance computation could be of help in Bayesian statistics: Var ⁡ ⁡ [ ln ⁡ ⁡ | X | ] = ∑ ∑ i = 1 p ψ ψ 1 ( n + 1 − − i 2 ) {\displaystyle \operatorname {Var} \left[\,\ln \left|\mathbf {X} \right|\,\right]=\sum _{i=1}^{p}\psi _{1}\left({\frac {n+1-i}{2}}\right)} where ψ ψ 1 {\displaystyle \psi _{1}} is the trigamma function. This comes up when computing the Fisher information of the Wishart random variable.

Entropy [ edit ] The information entropy of the distribution has the following formula: [ 11 ] : 693 H ⁡ ⁡ [ X ] = − − ln ⁡ ⁡ ( B ( V , n ) ) − − n − − p − − 1 2 E ⁡ ⁡ [ ln ⁡ ⁡ | X | ] + n p 2 {\displaystyle \operatorname {H} \left[\,\mathbf {X} \,\right]=-\ln \left(B(\mathbf {V} ,n)\right)-{\frac {n-p-1}{2}}\operatorname {E} \left[\,\ln \left|\mathbf {X} \right|\,\right]+{\frac {np}{2}}} where B ( V , n ) is the normalizing constant of the distribution: B ( V , n ) = 1 | V | n / 2 2 n p / 2 Γ Γ p ( n 2 ) .

{\displaystyle B(\mathbf {V} ,n)={\frac {1}{\left|\mathbf {V} \right|^{n/2}2^{np/2}\Gamma _{p}\left({\frac {n}{2}}\right)}}.} This can be expanded as follows: H ⁡ ⁡ [ X ] = n 2 ln ⁡ ⁡ | V | + n p 2 ln ⁡ ⁡ 2 + ln ⁡ ⁡ Γ Γ p ( n 2 ) − − n − − p − − 1 2 E ⁡ ⁡ [ ln ⁡ ⁡ | X | ] + n p 2 = n 2 ln ⁡ ⁡ | V | + n p 2 ln ⁡ ⁡ 2 + ln ⁡ ⁡ Γ Γ p ( n 2 ) − − n − − p − − 1 2 ( ψ ψ p ( n 2 ) + p ln ⁡ ⁡ 2 + ln ⁡ ⁡ | V | ) + n p 2 = n 2 ln ⁡ ⁡ | V | + n p 2 ln ⁡ ⁡ 2 + ln ⁡ ⁡ Γ Γ p ( n 2 ) − − n − − p − − 1 2 ψ ψ p ( n 2 ) − − n − − p − − 1 2 ( p ln ⁡ ⁡ 2 + ln ⁡ ⁡ | V | ) + n p 2 = p + 1 2 ln ⁡ ⁡ | V | + 1 2 p ( p + 1 ) ln ⁡ ⁡ 2 + ln ⁡ ⁡ Γ Γ p ( n 2 ) − − n − − p − − 1 2 ψ ψ p ( n 2 ) + n p 2 {\displaystyle {\begin{aligned}\operatorname {H} \left[\,\mathbf {X} \,\right]&={\frac {n}{2}}\ln \left|\mathbf {V} \right|+{\frac {np}{2}}\ln 2+\ln \Gamma _{p}\left({\frac {n}{2}}\right)-{\frac {n-p-1}{2}}\operatorname {E} \left[\,\ln \left|\mathbf {X} \right|\,\right]+{\frac {np}{2}}\\[8pt]&={\frac {n}{2}}\ln \left|\mathbf {V} \right|+{\frac {np}{2}}\ln 2+\ln \Gamma _{p}\left({\frac {n}{2}}\right)-{\frac {n-p-1}{2}}\left(\psi _{p}\left({\frac {n}{2}}\right)+p\ln 2+\ln \left|\mathbf {V} \right|\right)+{\frac {np}{2}}\\[8pt]&={\frac {n}{2}}\ln \left|\mathbf {V} \right|+{\frac {np}{2}}\ln 2+\ln \Gamma _{p}\left({\frac {n}{2}}\right)-{\frac {n-p-1}{2}}\psi _{p}\left({\frac {n}{2}}\right)-{\frac {n-p-1}{2}}\left(p\ln 2+\ln \left|\mathbf {V} \right|\right)+{\frac {np}{2}}\\[8pt]&={\frac {p+1}{2}}\ln \left|\mathbf {V} \right|+{\frac {1}{2}}p(p+1)\ln 2+\ln \Gamma _{p}\left({\frac {n}{2}}\right)-{\frac {n-p-1}{2}}\psi _{p}\left({\frac {n}{2}}\right)+{\frac {np}{2}}\end{aligned}}} Cross-entropy [ edit ] The cross-entropy of two Wishart distributions p 0 {\displaystyle p_{0}} with parameters n 0 , V 0 {\displaystyle n_{0},V_{0}} and p 1 {\displaystyle p_{1}} with parameters n 1 , V 1 {\displaystyle n_{1},V_{1}} is H ( p 0 , p 1 ) = E p 0 ⁡ ⁡ [ − − log ⁡ ⁡ p 1 ] = E p 0 ⁡ ⁡ [ − − log ⁡ ⁡ | X | ( n 1 − − p 1 − − 1 ) / 2 e − − tr ⁡ ⁡ ( V 1 − − 1 X ) / 2 2 n 1 p 1 / 2 | V 1 | n 1 / 2 Γ Γ p 1 ( n 1 2 ) ] = n 1 p 1 2 log ⁡ ⁡ 2 + n 1 2 log ⁡ ⁡ | V 1 | + log ⁡ ⁡ Γ Γ p 1 ( n 1 2 ) − − n 1 − − p 1 − − 1 2 E p 0 ⁡ ⁡ [ log ⁡ ⁡ | X | ] + 1 2 E p 0 ⁡ ⁡ [ tr ⁡ ⁡ ( V 1 − − 1 X ) ] = n 1 p 1 2 log ⁡ ⁡ 2 + n 1 2 log ⁡ ⁡ | V 1 | + log ⁡ ⁡ Γ Γ p 1 ( n 1 2 ) − − n 1 − − p 1 − − 1 2 ( ψ ψ p 0 ( n 0 2 ) + p 0 log ⁡ ⁡ 2 + log ⁡ ⁡ | V 0 | ) + 1 2 tr ⁡ ⁡ ( V 1 − − 1 n 0 V 0 ) = − − n 1 2 log ⁡ ⁡ | V 1 − − 1 V 0 | + p 1 + 1 2 log ⁡ ⁡ | V 0 | + n 0 2 tr ⁡ ⁡ ( V 1 − − 1 V 0 ) + log ⁡ ⁡ Γ Γ p 1 ( n 1 2 ) − − n 1 − − p 1 − − 1 2 ψ ψ p 0 ( n 0 2 ) + n 1 ( p 1 − − p 0 ) + p 0 ( p 1 + 1 ) 2 log ⁡ ⁡ 2 {\displaystyle {\begin{aligned}H(p_{0},p_{1})&=\operatorname {E} _{p_{0}}[\,-\log p_{1}\,]\\[8pt]&=\operatorname {E} _{p_{0}}\left[\,-\log {\frac {\left|\mathbf {X} \right|^{(n_{1}-p_{1}-1)/2}e^{-\operatorname {tr} (\mathbf {V} _{1}^{-1}\mathbf {X} )/2}}{2^{n_{1}p_{1}/2}\left|\mathbf {V} _{1}\right|^{n_{1}/2}\Gamma _{p_{1}}\left({\tfrac {n_{1}}{2}}\right)}}\right]\\[8pt]&={\tfrac {n_{1}p_{1}}{2}}\log 2+{\tfrac {n_{1}}{2}}\log \left|\mathbf {V} _{1}\right|+\log \Gamma _{p_{1}}({\tfrac {n_{1}}{2}})-{\tfrac {n_{1}-p_{1}-1}{2}}\operatorname {E} _{p_{0}}\left[\,\log \left|\mathbf {X} \right|\,\right]+{\tfrac {1}{2}}\operatorname {E} _{p_{0}}\left[\,\operatorname {tr} \left(\,\mathbf {V} _{1}^{-1}\mathbf {X} \,\right)\,\right]\\[8pt]&={\tfrac {n_{1}p_{1}}{2}}\log 2+{\tfrac {n_{1}}{2}}\log \left|\mathbf {V} _{1}\right|+\log \Gamma _{p_{1}}({\tfrac {n_{1}}{2}})-{\tfrac {n_{1}-p_{1}-1}{2}}\left(\psi _{p_{0}}({\tfrac {n_{0}}{2}})+p_{0}\log 2+\log \left|\mathbf {V} _{0}\right|\right)+{\tfrac {1}{2}}\operatorname {tr} \left(\,\mathbf {V} _{1}^{-1}n_{0}\mathbf {V} _{0}\,\right)\\[8pt]&=-{\tfrac {n_{1}}{2}}\log \left|\,\mathbf {V} _{1}^{-1}\mathbf {V} _{0}\,\right|+{\tfrac {p_{1}+1}{2}}\log \left|\mathbf {V} _{0}\right|+{\tfrac {n_{0}}{2}}\operatorname {tr} \left(\,\mathbf {V} _{1}^{-1}\mathbf {V} _{0}\right)+\log \Gamma _{p_{1}}\left({\tfrac {n_{1}}{2}}\right)-{\tfrac {n_{1}-p_{1}-1}{2}}\psi _{p_{0}}({\tfrac {n_{0}}{2}})+{\tfrac {n_{1}(p_{1}-p_{0})+p_{0}(p_{1}+1)}{2}}\log 2\end{aligned}}} Note that when p 0 = p 1 {\displaystyle p_{0}=p_{1}} and n 0 = n 1 {\displaystyle n_{0}=n_{1}} we recover the entropy.

KL-divergence [ edit ] The Kullback–Leibler divergence of p 1 {\displaystyle p_{1}} from p 0 {\displaystyle p_{0}} is D K L ( p 0 ‖ ‖ p 1 ) = H ( p 0 , p 1 ) − − H ( p 0 ) = − − n 1 2 log ⁡ ⁡ | V 1 − − 1 V 0 | + n 0 2 ( tr ⁡ ⁡ ( V 1 − − 1 V 0 ) − − p ) + log ⁡ ⁡ Γ Γ p ( n 1 2 ) Γ Γ p ( n 0 2 ) + n 0 − − n 1 2 ψ ψ p ( n 0 2 ) {\displaystyle {\begin{aligned}D_{KL}(p_{0}\|p_{1})&=H(p_{0},p_{1})-H(p_{0})\\[6pt]&=-{\frac {n_{1}}{2}}\log |\mathbf {V} _{1}^{-1}\mathbf {V} _{0}|+{\frac {n_{0}}{2}}(\operatorname {tr} (\mathbf {V} _{1}^{-1}\mathbf {V} _{0})-p)+\log {\frac {\Gamma _{p}\left({\frac {n_{1}}{2}}\right)}{\Gamma _{p}\left({\frac {n_{0}}{2}}\right)}}+{\tfrac {n_{0}-n_{1}}{2}}\psi _{p}\left({\frac {n_{0}}{2}}\right)\end{aligned}}} Characteristic function [ edit ] The characteristic function of the Wishart distribution is Θ Θ ↦ ↦ E ⁡ ⁡ [ exp ⁡ ⁡ ( i tr ⁡ ⁡ ( X Θ Θ ) ) ] = | 1 − − 2 i Θ Θ V | − − n / 2 {\displaystyle \Theta \mapsto \operatorname {E} \left[\,\exp \left(\,i\operatorname {tr} \left(\,\mathbf {X} {\mathbf {\Theta } }\,\right)\,\right)\,\right]=\left|\,1-2i\,{\mathbf {\Theta } }\,{\mathbf {V} }\,\right|^{-n/2}} where E[⋅] denotes expectation. (Here Θ is any matrix with the same dimensions as V , 1 indicates the identity matrix, and i is a square root of −1 ).

[ 9 ] Properly interpreting this formula requires a little care, because noninteger complex powers are multivalued ; when n is noninteger, the correct branch must be determined via analytic continuation .

[ 14 ] Theorem [ edit ] If a p × p random matrix X has a Wishart distribution with m degrees of freedom and variance matrix V — write X ∼ ∼ W p ( V , m ) {\displaystyle \mathbf {X} \sim {\mathcal {W}}_{p}({\mathbf {V} },m)} — and C is a q × p matrix of rank q , then [ 15 ] C X C T ∼ ∼ W q ( C V C T , m ) .

{\displaystyle \mathbf {C} \mathbf {X} {\mathbf {C} }^{T}\sim {\mathcal {W}}_{q}\left({\mathbf {C} }{\mathbf {V} }{\mathbf {C} }^{T},m\right).} Corollary 1 [ edit ] If z is a nonzero p × 1 constant vector, then: [ 15 ] σ σ z − − 2 z T X z ∼ ∼ χ χ m 2 .

{\displaystyle \sigma _{z}^{-2}\,{\mathbf {z} }^{T}\mathbf {X} {\mathbf {z} }\sim \chi _{m}^{2}.} In this case, χ χ m 2 {\displaystyle \chi _{m}^{2}} is the chi-squared distribution and σ σ z 2 = z T V z {\displaystyle \sigma _{z}^{2}={\mathbf {z} }^{T}{\mathbf {V} }{\mathbf {z} }} (note that σ σ z 2 {\displaystyle \sigma _{z}^{2}} is a constant; it is positive because V is positive definite).

Corollary 2 [ edit ] Consider the case where z T = (0, ..., 0, 1, 0, ..., 0) (that is, the j -th element is one and all others zero). Then corollary 1 above shows that σ σ j j − − 1 w j j ∼ ∼ χ χ m 2 {\displaystyle \sigma _{jj}^{-1}\,w_{jj}\sim \chi _{m}^{2}} gives the marginal distribution of each of the elements on the matrix's diagonal.

George Seber points out that the Wishart distribution is not called the “multivariate chi-squared distribution” because the marginal distribution of the off-diagonal elements is not chi-squared. Seber prefers to reserve the term multivariate for the case when all univariate marginals belong to the same family.

[ 16 ] Estimator of the multivariate normal distribution [ edit ] The Wishart distribution is the sampling distribution of the maximum-likelihood estimator (MLE) of the covariance matrix of a multivariate normal distribution .

[ 17 ] A derivation of the MLE uses the spectral theorem .

Bartlett decomposition [ edit ] The Bartlett decomposition of a matrix X from a p -variate Wishart distribution with scale matrix V and n degrees of freedom is the factorization: X = L A A T L T , {\displaystyle \mathbf {X} ={\textbf {L}}{\textbf {A}}{\textbf {A}}^{T}{\textbf {L}}^{T},} where L is the Cholesky factor of V , and: A = ( c 1 0 0 ⋯ ⋯ 0 n 21 c 2 0 ⋯ ⋯ 0 n 31 n 32 c 3 ⋯ ⋯ 0 ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ n p 1 n p 2 n p 3 ⋯ ⋯ c p ) {\displaystyle \mathbf {A} ={\begin{pmatrix}c_{1}&0&0&\cdots &0\\n_{21}&c_{2}&0&\cdots &0\\n_{31}&n_{32}&c_{3}&\cdots &0\\\vdots &\vdots &\vdots &\ddots &\vdots \\n_{p1}&n_{p2}&n_{p3}&\cdots &c_{p}\end{pmatrix}}} where c i 2 ∼ ∼ χ χ n − − i + 1 2 {\displaystyle c_{i}^{2}\sim \chi _{n-i+1}^{2}} and n ij ~ N (0, 1) independently.

[ 18 ] This provides a useful method for obtaining random samples from a Wishart distribution.

[ 19 ] Marginal distribution of matrix elements [ edit ] Let V be a 2 × 2 variance matrix characterized by correlation coefficient −1 < ρ < 1 and L its lower Cholesky factor: V = ( σ σ 1 2 ρ ρ σ σ 1 σ σ 2 ρ ρ σ σ 1 σ σ 2 σ σ 2 2 ) , L = ( σ σ 1 0 ρ ρ σ σ 2 1 − − ρ ρ 2 σ σ 2 ) {\displaystyle \mathbf {V} ={\begin{pmatrix}\sigma _{1}^{2}&\rho \sigma _{1}\sigma _{2}\\\rho \sigma _{1}\sigma _{2}&\sigma _{2}^{2}\end{pmatrix}},\qquad \mathbf {L} ={\begin{pmatrix}\sigma _{1}&0\\\rho \sigma _{2}&{\sqrt {1-\rho ^{2}}}\sigma _{2}\end{pmatrix}}} Multiplying through the Bartlett decomposition above, we find that a random sample from the 2 × 2 Wishart distribution is X = ( σ σ 1 2 c 1 2 σ σ 1 σ σ 2 ( ρ ρ c 1 2 + 1 − − ρ ρ 2 c 1 n 21 ) σ σ 1 σ σ 2 ( ρ ρ c 1 2 + 1 − − ρ ρ 2 c 1 n 21 ) σ σ 2 2 ( ( 1 − − ρ ρ 2 ) c 2 2 + ( 1 − − ρ ρ 2 n 21 + ρ ρ c 1 ) 2 ) ) {\displaystyle \mathbf {X} ={\begin{pmatrix}\sigma _{1}^{2}c_{1}^{2}&\sigma _{1}\sigma _{2}\left(\rho c_{1}^{2}+{\sqrt {1-\rho ^{2}}}c_{1}n_{21}\right)\\\sigma _{1}\sigma _{2}\left(\rho c_{1}^{2}+{\sqrt {1-\rho ^{2}}}c_{1}n_{21}\right)&\sigma _{2}^{2}\left(\left(1-\rho ^{2}\right)c_{2}^{2}+\left({\sqrt {1-\rho ^{2}}}n_{21}+\rho c_{1}\right)^{2}\right)\end{pmatrix}}} The diagonal elements, most evidently in the first element, follow the χ 2 distribution with n degrees of freedom (scaled by σ 2 ) as expected.  The off-diagonal element is less familiar but can be identified as a normal variance-mean mixture where the mixing density is a χ 2 distribution.  The corresponding marginal probability density for the off-diagonal element is therefore the variance-gamma distribution f ( x 12 ) = | x 12 | n − − 1 2 Γ Γ ( n 2 ) 2 n − − 1 π π ( 1 − − ρ ρ 2 ) ( σ σ 1 σ σ 2 ) n + 1 ⋅ ⋅ K n − − 1 2 ( | x 12 | σ σ 1 σ σ 2 ( 1 − − ρ ρ 2 ) ) exp ⁡ ⁡ ( ρ ρ x 12 σ σ 1 σ σ 2 ( 1 − − ρ ρ 2 ) ) {\displaystyle f(x_{12})={\frac {\left|x_{12}\right|^{\frac {n-1}{2}}}{\Gamma \left({\frac {n}{2}}\right){\sqrt {2^{n-1}\pi \left(1-\rho ^{2}\right)\left(\sigma _{1}\sigma _{2}\right)^{n+1}}}}}\cdot K_{\frac {n-1}{2}}\left({\frac {\left|x_{12}\right|}{\sigma _{1}\sigma _{2}\left(1-\rho ^{2}\right)}}\right)\exp {\left({\frac {\rho x_{12}}{\sigma _{1}\sigma _{2}(1-\rho ^{2})}}\right)}} where K ν ( z ) is the modified Bessel function of the second kind .

[ 20 ] Similar results may be found for higher dimensions. In general, if X {\displaystyle X} follows a Wishart distribution with parameters, Σ Σ , n {\displaystyle \Sigma ,n} , then for i ≠ ≠ j {\displaystyle i\neq j} , the off-diagonal elements X i j ∼ ∼ VG ( n , Σ Σ i j , ( Σ Σ i i Σ Σ j j − − Σ Σ i j 2 ) 1 / 2 , 0 ) {\displaystyle X_{ij}\sim {\text{VG}}(n,\Sigma _{ij},(\Sigma _{ii}\Sigma _{jj}-\Sigma _{ij}^{2})^{1/2},0)} .

[ 21 ] It is also possible to write down the moment-generating function even in the noncentral case (essentially the n th power of Craig (1936) [ 22 ] equation 10) although the probability density becomes an infinite sum of Bessel functions.

The range of the shape parameter [ edit ] It can be shown [ 23 ] that the Wishart distribution can be defined if and only if the shape parameter n belongs to the set Λ Λ p := { 0 , … … , p − − 1 } ∪ ∪ ( p − − 1 , ∞ ∞ ) .

{\displaystyle \Lambda _{p}:=\{0,\ldots ,p-1\}\cup \left(p-1,\infty \right).} This set is named after Simon Gindikin , who introduced it [ 24 ] in the 1970s in the context of gamma distributions on homogeneous cones. However, for the new parameters in the discrete spectrum of the Gindikin ensemble, namely, Λ Λ p ∗ ∗ := { 0 , … … , p − − 1 } , {\displaystyle \Lambda _{p}^{*}:=\{0,\ldots ,p-1\},} the corresponding Wishart distribution has no Lebesgue density.

Relationships to other distributions [ edit ] The Wishart distribution is related to the inverse-Wishart distribution , denoted by W p − − 1 {\displaystyle W_{p}^{-1}} , as follows: If X ~ W p ( V , n ) and if we do the change of variables C = X −1 , then C ∼ ∼ W p − − 1 ( V − − 1 , n ) {\displaystyle \mathbf {C} \sim W_{p}^{-1}(\mathbf {V} ^{-1},n)} . This relationship may be derived by noting that the absolute value of the Jacobian determinant of this change of variables is | C | p +1 , see for example equation (15.15) in.

[ 25 ] In Bayesian statistics , the Wishart distribution is a conjugate prior for the precision parameter of the multivariate normal distribution , when the mean parameter is known.

[ 11 ] A generalization is the multivariate gamma distribution .

A different type of generalization is the normal-Wishart distribution , essentially the product of a multivariate normal distribution with a Wishart distribution.

See also [ edit ] Chi-squared distribution Complex Wishart distribution F-distribution Gamma distribution Hotelling's T-squared distribution Inverse-Wishart distribution Multivariate gamma distribution Student's t-distribution Wilks' lambda distribution References [ edit ] ^ a b Wishart, J.

(1928). "The generalised product moment distribution in samples from a normal multivariate population".

Biometrika .

20A ( 1– 2): 32– 52.

doi : 10.1093/biomet/20A.1-2.32 .

JFM 54.0565.02 .

JSTOR 2331939 .

^ Livan, Giacomo; Novaes, Marcel; Vivo, Pierpaolo (2018), "Classical Ensembles: Wishart-Laguerre" , Introduction to Random Matrices: Theory and Practice , SpringerBriefs in Mathematical Physics, Cham: Springer International Publishing, pp.

89– 95, doi : 10.1007/978-3-319-70885-0_13 , ISBN 978-3-319-70885-0 ^ Koop, Gary; Korobilis, Dimitris (2010).

"Bayesian Multivariate Time Series Methods for Empirical Macroeconomics" .

Foundations and Trends in Econometrics .

3 (4): 267– 358.

doi : 10.1561/0800000013 .

^ Gupta, A. K.; Nagar, D. K. (2000).

Matrix Variate Distributions . Chapman & Hall /CRC.

ISBN 1584880465 .

^ Gelman, Andrew (2003).

Bayesian Data Analysis (2nd ed.). Boca Raton, Fla.: Chapman & Hall. p. 582.

ISBN 158488388X . Retrieved 3 June 2015 .

^ Zanella, A.; Chiani, M.; Win, M.Z. (April 2009).

"On the marginal distribution of the eigenvalues of wishart matrices" (PDF) .

IEEE Transactions on Communications .

57 (4): 1050– 1060.

doi : 10.1109/TCOMM.2009.04.070143 .

hdl : 1721.1/66900 .

S2CID 12437386 .

^ Livan, Giacomo; Vivo, Pierpaolo (2011). "Moments of Wishart-Laguerre and Jacobi ensembles of random matrices: application to the quantum transport problem in chaotic cavities".

Acta Physica Polonica B .

42 (5): 1081.

arXiv : 1103.2638 .

doi : 10.5506/APhysPolB.42.1081 .

ISSN 0587-4254 .

S2CID 119599157 .

^ Muirhead, Robb J. (2005).

Aspects of Multivariate Statistical Theory (2nd ed.). Wiley Interscience.

ISBN 0471769851 .

^ a b Anderson, T. W.

(2003).

An Introduction to Multivariate Statistical Analysis (3rd ed.). Hoboken, N. J.: Wiley Interscience . p. 259.

ISBN 0-471-36091-0 .

^ Uhlig, H. (1994).

"On Singular Wishart and Singular Multivariate Beta Distributions" .

The Annals of Statistics .

22 : 395– 405.

doi : 10.1214/aos/1176325375 .

^ a b c Bishop, C. M.

(2006).

Pattern Recognition and Machine Learning . Springer.

^ Hoff, Peter D. (2009).

A First Course in Bayesian Statistical Methods . New York: Springer. pp.

109– 111.

ISBN 978-0-387-92299-7 .

^ Nguyen, Duy.

"AN IN DEPTH INTRODUCTION TO VARIATIONAL BAYES NOTE" .

SSRN 4541076 . Retrieved 15 August 2023 .

^ Mayerhofer, Eberhard (2019-01-27). "Reforming the Wishart characteristic function".

arXiv : 1901.09347 [ math.PR ].

^ a b Rao, C. R.

(1965).

Linear Statistical Inference and its Applications . Wiley. p. 535.

^ Seber, George A. F.

(2004).

Multivariate Observations .

Wiley .

ISBN 978-0471691211 .

^ Chatfield, C.; Collins, A. J. (1980).

Introduction to Multivariate Analysis . London: Chapman and Hall. pp.

103–108 .

ISBN 0-412-16030-7 .

^ Anderson, T. W.

(2003).

An Introduction to Multivariate Statistical Analysis (3rd ed.). Hoboken, N. J.: Wiley Interscience . p. 257.

ISBN 0-471-36091-0 .

^ Smith, W. B.; Hocking, R. R. (1972). "Algorithm AS 53: Wishart Variate Generator".

Journal of the Royal Statistical Society, Series C .

21 (3): 341– 345.

JSTOR 2346290 .

^ Pearson, Karl ; Jeffery, G. B.

; Elderton, Ethel M.

(December 1929). "On the Distribution of the First Product Moment-Coefficient, in Samples Drawn from an Indefinitely Large Normal Population".

Biometrika .

21 (1/4). Biometrika Trust: 164– 201.

doi : 10.2307/2332556 .

JSTOR 2332556 .

^ Fischer, Adrian; Gaunt, Robert E.; Andrey, Sarantsev (2023). "The Variance-Gamma Distribution: A Review".

arXiv : 2303.05615 [ math.ST ].

^ Craig, Cecil C. (1936).

"On the Frequency Function of xy" .

Ann. Math. Statist .

7 : 1– 15.

doi : 10.1214/aoms/1177732541 .

^ Peddada, Shyamal Das; Richards, Donald St. P.

(1991).

"Proof of a Conjecture of M. L. Eaton on the Characteristic Function of the Wishart Distribution" .

Annals of Probability .

19 (2): 868– 874.

doi : 10.1214/aop/1176990455 .

^ Gindikin, S.G.

(1975). "Invariant generalized functions in homogeneous domains".

Funct. Anal. Appl.

9 (1): 50– 52.

doi : 10.1007/BF01078179 .

S2CID 123288172 .

^ Dwyer, Paul S. (1967). "Some Applications of Matrix Derivatives in Multivariate Analysis".

J. Amer. Statist. Assoc.

62 (318): 607– 625.

doi : 10.1080/01621459.1967.10482934 .

JSTOR 2283988 .

External links [ edit ] A C++ library for random matrix generator v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Wishart_distribution&oldid=1299041139 " Categories : Continuous distributions Multivariate continuous distributions Covariance and correlation Random matrices Conjugate prior distributions Exponential family distributions Hidden categories: Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from October 2010 Articles with unsourced statements from June 2014 This page was last edited on 6 July 2025, at 06:33 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Wishart distribution 12 languages Add topic

