Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Examples Toggle Examples subsection 1.1 Coins 1.2 Dice 1.3 Cards 2 Criticism Toggle Criticism subsection 2.1 Application to continuous variables 2.2 History 2.3 See also 2.4 References Toggle the table of contents Principle of indifference 8 languages Deutsch Español Euskara فارسی 한국어 עברית 日本語 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In probability theory, a rule for assigning epistemic probabilities This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( April 2010 ) ( Learn how and when to remove this message ) Part of a series on Bayesian statistics Posterior = Likelihood × Prior ÷ Evidence Background Bayesian inference Bayesian probability Bayes' theorem Bernstein–von Mises theorem Coherence Cox's theorem Cromwell's rule Likelihood principle Principle of indifference Principle of maximum entropy Model building Conjugate prior Linear regression Empirical Bayes Hierarchical model Posterior approximation Markov chain Monte Carlo Laplace's approximation Integrated nested Laplace approximations Variational inference Approximate Bayesian computation Estimators Bayesian estimator Credible interval Maximum a posteriori estimation Evidence approximation Evidence lower bound Nested sampling Model evaluation Bayes factor ( Schwarz criterion ) Model averaging Posterior predictive Mathematics portal v t e The principle of indifference (also called principle of insufficient reason ) is a rule for assigning epistemic probabilities . The principle of indifference states that in the absence of any relevant evidence, agents should distribute their credence (or "degrees of belief") equally among all the possible outcomes under consideration.

[ 1 ] It can be viewed as
an application of the principle of parsimony and as a special case of the principle of maximum entropy .
In Bayesian probability , this is the simplest non-informative prior .

Examples [ edit ] The textbook examples for the application of the principle of indifference are coins , dice , and cards .

Coins [ edit ] A symmetric coin has two sides, arbitrarily labeled heads (many coins have the head of a person portrayed on one side) and tails . Assuming that the coin must land on one side or the other, the outcomes of a coin toss are mutually exclusive, exhaustive, and interchangeable. According to the principle of indifference, we assign each of the possible outcomes a probability of 1/2.

It is implicit in this analysis that the forces acting on the coin are not known with any precision. If the momentum imparted to the coin as it is launched were known with sufficient accuracy, the flight of the coin could be predicted according to the laws of mechanics. Thus the uncertainty in the outcome of a coin toss is derived (for the most part) from the uncertainty with respect to initial conditions. This point is discussed at greater length in the article on coin flipping .

Dice [ edit ] A symmetric die has n faces, arbitrarily labeled from 1 to n . An ordinary cubical die has n = 6 faces, although a symmetric die with different numbers of faces can be constructed; see Dice . We assume that the die will land with one face or another upward, and there are no other possible outcomes. Applying the principle of indifference, we assign each of the possible outcomes a probability of 1/ n . As with coins, it is assumed that the initial conditions of throwing the dice are not known with enough precision to predict the outcome according to the laws of mechanics. Dice are typically thrown so as to bounce on a table or other surface(s). This interaction makes prediction of the outcome much more difficult.

The assumption of symmetry is crucial here. Suppose that we are asked to bet for or against the outcome "6". We might reason that there are two relevant outcomes here "6" or "not 6", and that these are mutually exclusive and exhaustive. A common fallacy is assigning the probability 1/2 to each of the two outcomes, when "not 6" is five times more likely than "6." Cards [ edit ] A standard deck contains 52 cards, each given a unique label in an arbitrary fashion, i.e. arbitrarily ordered. We draw a card from the deck; applying the principle of indifference, we assign each of the possible outcomes a probability of 1/52.

Criticism [ edit ] The deck-of-cards example, more than the others, shows the difficulty of actually applying the principle of indifference in real situations. What we really mean by the phrase "arbitrarily ordered" is simply that we don't have any information that would lead us to favor a particular card. In actual practice, this is rarely the case: a new deck of cards is certainly not in arbitrary order, and neither is a deck immediately after a hand of cards. In practice, we therefore shuffle the cards; this does not destroy the information we have, but instead (hopefully) renders our information practically unusable, although it is still usable in principle. In fact, some expert blackjack players can track aces through the deck; for them, the condition for applying the principle of indifference is not satisfied.

In a macroscopic system, at least, it must be assumed that the physical laws that govern the system are not known well enough to predict the outcome. As observed some centuries ago by John Arbuthnot (in the preface of Of the Laws of Chance , 1692), It is impossible for a Die, with such determin'd force and direction, not to fall on such determin'd side, only I don't know the force and direction which makes it fall on such determin'd side, and therefore I call it Chance, which is nothing but the want of art....

Under the principle of causal determinism , given enough time and resources, there is no fundamental reason to suppose that suitably precise measurements could not be made, which would enable the prediction of the outcome of coins, dice, and cards with high accuracy: Persi Diaconis 's work with coin-flipping machines is a practical example of this.

[ 2 ] Application to continuous variables [ edit ] Applying the principle of indifference incorrectly can easily lead to nonsensical results, especially in the case of multivariate, continuous variables. A typical case of misuse is the following example: Suppose there is a cube hidden in a box. A label on the box says the cube has a side length between 3 and 5 cm.

We don't know the actual side length, but we might assume that all values are equally likely and simply pick the mid-value of 4 cm.

The information on the label allows us to calculate that the surface area of the cube is between 54 and 150 cm 2 . We don't know the actual surface area, but we might assume that all values are equally likely and simply pick the mid-value of 102 cm 2 .

The information on the label allows us to calculate that the volume of the cube is between 27 and 125 cm 3 . We don't know the actual volume, but we might assume that all values are equally likely and simply pick the mid-value of 76 cm 3 .

However, we have now reached the impossible conclusion that the cube has a side length of 4 cm, a surface area of 102 cm 2 , and a volume of 76 cm 3 !

In this example, mutually contradictory estimates of the length, surface area, and volume of the cube arise because we have assumed three mutually contradictory distributions for these parameters: a uniform distribution for any one of the variables implies a non-uniform distribution for the other two. In general, the principle of indifference does not indicate which variable (e.g. in this case, length, surface area, or volume) is to have a uniform epistemic probability distribution .

Another classic example of this kind of misuse is the Bertrand paradox .

Edwin T. Jaynes introduced the principle of transformation groups , which can yield an epistemic probability distribution for this problem. This generalises the principle of indifference, by saying that one is indifferent between equivalent problems rather than indifferent between propositions. This still reduces to the ordinary principle of indifference when one considers a permutation of the labels as generating equivalent problems (i.e. using the permutation transformation group). To apply this to the above box example, we have three random variables related by geometric equations. If we have no reason to favour one trio of values over another, then our prior probabilities must be related by the rule for changing variables in continuous distributions. Let L be the length, and V be the volume. Then we must have f L ( L ) = | ∂ ∂ V ∂ ∂ L | f V ( V ) = 3 L 2 f V ( L 3 ) {\displaystyle f_{L}(L)=\left|{\partial V \over \partial L}\right|f_{V}(V)=3L^{2}f_{V}(L^{3})} , where f L , f V {\displaystyle f_{L},\,f_{V}} are the probability density functions (pdf) of the stated variables. This equation has a general solution: f ( L ) = K L {\displaystyle f(L)={K \over L}} , where K is a normalization constant, determined by the range of L , in this case equal to: K − − 1 = ∫ ∫ 3 5 d L L = log ⁡ ⁡ ( 5 3 ) {\displaystyle K^{-1}=\int _{3}^{5}{dL \over L}=\log \left({5 \over 3}\right)} To put this "to the test", we ask for the probability that the length is less than 4. This has probability of: P r ( L < 4 ) = ∫ ∫ 3 4 d L L log ⁡ ⁡ ( 5 3 ) = log ⁡ ⁡ ( 4 3 ) log ⁡ ⁡ ( 5 3 ) ≈ ≈ 0.56 {\displaystyle Pr(L<4)=\int _{3}^{4}{dL \over L\log({5 \over 3})}={\log({4 \over 3}) \over \log({5 \over 3})}\approx 0.56} .

For the volume, this should be equal to the probability that the volume is less than 4 3 = 64. The pdf of the volume is f ( V 1 3 ) 1 3 V − − 2 3 = 1 3 V log ⁡ ⁡ ( 5 3 ) {\displaystyle f(V^{1 \over 3}){1 \over 3}V^{-{2 \over 3}}={1 \over 3V\log({5 \over 3})}} .

And then probability of volume less than 64 is P r ( V < 64 ) = ∫ ∫ 27 64 d V 3 V log ⁡ ⁡ ( 5 3 ) = log ⁡ ⁡ ( 64 27 ) 3 log ⁡ ⁡ ( 5 3 ) = 3 log ⁡ ⁡ ( 4 3 ) 3 log ⁡ ⁡ ( 5 3 ) = log ⁡ ⁡ ( 4 3 ) log ⁡ ⁡ ( 5 3 ) ≈ ≈ 0.56 {\displaystyle Pr(V<64)=\int _{27}^{64}{dV \over 3V\log({5 \over 3})}={\log({64 \over 27}) \over 3\log({5 \over 3})}={3\log({4 \over 3}) \over 3\log({5 \over 3})}={\log({4 \over 3}) \over \log({5 \over 3})}\approx 0.56} .

Thus we have achieved invariance with respect to volume and length. One can also show the same invariance with respect to surface area being less than 6(4 2 ) = 96. However, note that this probability assignment is not necessarily a "correct" one. For the exact distribution of lengths, volume, or surface area will depend on how the "experiment" is conducted.

The fundamental hypothesis of statistical physics , that any two microstates of a system with the same total energy are equally probable at equilibrium , is in a sense an example of the principle of indifference. However, when the microstates are described by continuous variables (such as positions and momenta), an additional physical basis is needed in order to explain under which parameterization the probability density will be uniform.

Liouville's theorem justifies the use of canonically conjugate variables , such as positions and their conjugate momenta.

The wine/water paradox shows a dilemma with linked variables, and which one to choose.

History [ edit ] This principle stems from Epicurus ' principle of "multiple explanations" (pleonachos tropos), [ 3 ] according to which "if more than one theory is consistent with the data, keep them all”. The epicurean Lucretius developed this point with an analogy of the multiple causes of death of a corpse.

[ 4 ] The original writers on probability, primarily Jacob Bernoulli and Pierre Simon Laplace , considered the principle of indifference to be intuitively obvious and did not even bother to give it a name. Laplace wrote: The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.

These earlier writers, Laplace in particular, naively generalized the principle of indifference to the case of continuous parameters, giving the so-called "uniform prior probability distribution", a function that is constant over all real numbers. He used this function to express a complete lack of knowledge as to the value of a parameter. According to Stigler (page 135), Laplace's assumption of uniform prior probabilities was not a meta-physical assumption.

[ 5 ] It was an implicit assumption made for the ease of analysis.

The principle of insufficient reason was its first name, given to it by Johannes von Kries , [ 6 ] possibly as a play on Leibniz 's principle of sufficient reason . These later writers ( George Boole , John Venn , and others) objected to the use of the uniform prior for two reasons. The first reason is that the constant function is not normalizable, and thus is not a proper probability distribution. The second reason is its inapplicability to continuous variables, as described above.

The "principle of insufficient reason" was renamed the "principle of indifference" by John Maynard Keynes ( 1921 ), [ 7 ] who was careful to note that it applies only when there is no knowledge indicating unequal probabilities.

Attempts to put the notion on firmer philosophical ground have generally begun with the concept of equipossibility and progressed from it to equiprobability .

The principle of indifference can be given a deeper logical justification by noting that equivalent states of knowledge should be assigned equivalent epistemic probabilities. This argument was propounded by Edwin Thompson Jaynes : it leads to two generalizations, namely the principle of transformation groups as in the Jeffreys prior , and the principle of maximum entropy .

[ 8 ] More generally, one speaks of uninformative priors .

See also [ edit ] Bayesian epistemology Clinical equipoise Rule of succession : a formula for estimating underlying probabilities when there are few observations, or for events that have not been observed to occur at all in (finite) sample data References [ edit ] ^ Eva, Benjamin (30 April 2019).

"Principles of Indifference" .

philsci-archive.pitt.edu (Preprint) . Retrieved 30 September 2019 .

^ Diaconis, Persi; Keller, Joseph B. (1989). "Fair Dice".

The American Mathematical Monthly .

96 (4): 337– 339.

doi : 10.2307/2324089 .

JSTOR 2324089 .

(Discussion of dice that are fair "by symmetry" and "by continuity".) ^ Verde, Francesco (2020-07-06).

"Epicurean Meteorology, Lucretius, and the Aetna" .

Lucretius Poet and Philosopher . De Gruyter. pp.

83– 102.

doi : 10.1515/9783110673487-006 .

ISBN 978-3-11-067348-7 .

S2CID 243676846 .

^ Rathmanner, Samuel; Hutter, Marcus (2011-06-03).

"A Philosophical Treatise of Universal Induction" .

Entropy .

13 (6): 1076– 1136.

arXiv : 1105.5721 .

Bibcode : 2011Entrp..13.1076R .

doi : 10.3390/e13061076 .

ISSN 1099-4300 .

^ Stigler, Stephen M. (1986).

The History of Statistics: The Measurement of Uncertainty Before 1900 . Cambridge, Mass: Belknap Press of Harvard University Press.

ISBN 0-674-40340-1 .

^ Howson, Colin; Urbach, Peter (1989). "Subjective Probability".

Scientific Reasoning : The Bayesian Approach . La Salle: Open Court. pp.

39– 76.

ISBN 0-8126-9084-2 .

^ Keynes, John Maynard (1921).

"Chapter IV. The Principle of Indifference" .

A Treatise on Probability . Vol. 4. Macmillan and Co. pp.

41– 64.

ISBN 9780404145637 .

{{ cite book }} : ISBN / Date incompatibility ( help ) ^ Jaynes, Edwin Thompson (2003). "Ignorance Priors and Transformation Groups".

Probability Theory: The Logic of Science .

Cambridge University Press . pp.

327– 347.

ISBN 0-521-59271-2 .

v t e Decision theory Core concepts Ambiguity aversion Bounded rationality Choice architecture Expected utility Expected value Hyperbolic discounting Leximin Loss aversion Multi-attribute utility Path dependence Principle of indifference Prospect theory Rational choice theory Risk aversion Risk-seeking Satisficing Strategic dominance Subjective expected utility Sure-thing Utility theorem Decision models Anscombe-Aumann framework Causal decision Decision field theory Emotional choice Evidential decision Fuzzy-trace theory Intertemporal choice Naturalistic decision Normative model Quantum cognition Recognition-primed decision Rubicon model Savage's subjective expected utility model Decision analysis tools Analytic hierarchy process Analytic network process Cost–benefit analysis Cost-effectiveness analysis Cost–utility analysis Decision conferencing Decision curve analysis Decision rule Decision support system Decision table Decision tree Decision matrix Decisional balance sheet Gittins index Influence diagram Minimax MCDA Scoring rule Value of information perfect sample uncertainty Paradoxes and biases Allais paradox Certainty effect Cognitive bias Decoy effect Disposition effect Ellsberg paradox Endowment effect Framing effect Heuristics Newcomb's paradox Pseudocertainty effect Reference dependence Regret St. Petersburg paradox Status quo bias Sunk cost Uncertainty and risk Deep uncertainty Exploration–exploitation Info-gap Pignistic probability Robust decision-making Related fields Behavioral economics Game theory Operations research Social choice theory Utility theory Key people David Blackwell Bruno de Finetti Morris H. DeGroot Peter C. Fishburn Gerd Gigerenzer Itzhak Gilboa Daniel Kahneman R. Duncan Luce Oskar Morgenstern Howard Raiffa Leonard J. Savage David Schmeidler Herbert Simon Amos Tversky John von Neumann Category Retrieved from " https://en.wikipedia.org/w/index.php?title=Principle_of_indifference&oldid=1298139567 " Categories : Probability theory Statistical principles Decision theory Hidden categories: CS1 errors: ISBN date Articles with short description Short description matches Wikidata Articles lacking in-text citations from April 2010 All articles lacking in-text citations This page was last edited on 30 June 2025, at 18:40 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Principle of indifference 8 languages Add topic

