Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Stating the theorem Toggle Stating the theorem subsection 1.1 Linear transformations 1.2 Matrices 2 Proofs Toggle Proofs subsection 2.1 First proof 2.2 Second proof 3 A third fundamental subspace 4 Reformulations and generalizations 5 Citations 6 References 7 External links Toggle the table of contents Rank–nullity theorem 16 languages Čeština Deutsch Español Français 한국어 Bahasa Indonesia Italiano עברית Magyar 日本語 Polski Português Svenska தமிழ் Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia "Rank theorem" redirects here. For the rank theorem of multivariable calculus, see constant rank theorem .

In linear algebra, relation between 3 dimensions Rank–nullity theorem The rank–nullity theorem is a theorem in linear algebra , which asserts: the number of columns of a matrix M is the sum of the rank of M and the nullity of M ; and the dimension of the domain of a linear transformation f is the sum of the rank of f (the dimension of the image of f ) and the nullity of f (the dimension of the kernel of f ).

[ 1 ] [ 2 ] [ 3 ] [ 4 ] It follows that for linear transformations of vector spaces of equal finite dimension, either injectivity or surjectivity implies bijectivity .

Stating the theorem [ edit ] Linear transformations [ edit ] Let T : V → → W {\displaystyle T:V\to W} be a linear transformation between two vector spaces where T {\displaystyle T} 's domain V {\displaystyle V} is finite dimensional. Then rank ⁡ ⁡ ( T ) + nullity ⁡ ⁡ ( T ) = dim ⁡ ⁡ V , {\displaystyle \operatorname {rank} (T)~+~\operatorname {nullity} (T)~=~\dim V,} where rank ⁡ ⁡ ( T ) {\textstyle \operatorname {rank} (T)} is the rank of T {\displaystyle T} (the dimension of its image ) and nullity ⁡ ⁡ ( T ) {\displaystyle \operatorname {nullity} (T)} is the nullity of T {\displaystyle T} (the dimension of its kernel ).  In other words, dim ⁡ ⁡ ( Im ⁡ ⁡ T ) + dim ⁡ ⁡ ( Ker ⁡ ⁡ T ) = dim ⁡ ⁡ ( Domain ⁡ ⁡ ( T ) ) .

{\displaystyle \dim(\operatorname {Im} T)+\dim(\operatorname {Ker} T)=\dim(\operatorname {Domain} (T)).} This theorem can be refined via the splitting lemma to be a statement about an isomorphism of spaces, not just dimensions. Explicitly, since T {\displaystyle T} induces an isomorphism from V / Ker ⁡ ⁡ ( T ) {\displaystyle V/\operatorname {Ker} (T)} to Im ⁡ ⁡ ( T ) , {\displaystyle \operatorname {Im} (T),} the existence of a basis for V {\displaystyle V} that extends any given basis of Ker ⁡ ⁡ ( T ) {\displaystyle \operatorname {Ker} (T)} implies, via the splitting lemma, that Im ⁡ ⁡ ( T ) ⊕ ⊕ Ker ⁡ ⁡ ( T ) ≅ ≅ V .

{\displaystyle \operatorname {Im} (T)\oplus \operatorname {Ker} (T)\cong V.} Taking dimensions, the rank–nullity theorem follows.

Matrices [ edit ] Linear maps can be represented with matrices . More precisely, an m × × n {\displaystyle m\times n} matrix M represents a linear map f : F n → → F m , {\displaystyle f:F^{n}\to F^{m},} where F {\displaystyle F} is the underlying field .

[ 5 ] So, the dimension of the domain of f {\displaystyle f} is n , the number of columns of M , and the rank–nullity theorem for an m × × n {\displaystyle m\times n} matrix M is rank ⁡ ⁡ ( M ) + nullity ⁡ ⁡ ( M ) = n .

{\displaystyle \operatorname {rank} (M)+\operatorname {nullity} (M)=n.} Proofs [ edit ] Here we provide two proofs. The first [ 2 ] operates in the general case, using linear maps. The second proof [ 6 ] looks at the homogeneous system A x = 0 , {\displaystyle \mathbf {Ax} =\mathbf {0} ,} where A {\displaystyle \mathbf {A} } is a m × × n {\displaystyle m\times n} with rank r , {\displaystyle r,} and shows explicitly that there exists a set of n − − r {\displaystyle n-r} linearly independent solutions that span the null space of A {\displaystyle \mathbf {A} } .

While the theorem requires that the domain of the linear map be finite-dimensional, there is no such assumption on the codomain . This means that there are linear maps not given by matrices for which the theorem applies. Despite this, the first proof is not actually more general than the second: since the image of the linear map is finite-dimensional, we can represent the map from its domain to its image by a matrix, prove the theorem for that matrix, then compose with the inclusion of the image into the full codomain.

First proof [ edit ] Let V , W {\displaystyle V,W} be vector spaces over some field F , {\displaystyle F,} and T {\displaystyle T} defined as in the statement of the theorem with dim ⁡ ⁡ V = n {\displaystyle \dim V=n} .

As Ker ⁡ ⁡ T ⊂ ⊂ V {\displaystyle \operatorname {Ker} T\subset V} is a subspace , there exists a basis for it. Suppose dim ⁡ ⁡ Ker ⁡ ⁡ T = k {\displaystyle \dim \operatorname {Ker} T=k} and let K := { v 1 , … … , v k } ⊂ ⊂ Ker ⁡ ⁡ ( T ) {\displaystyle {\mathcal {K}}:=\{v_{1},\ldots ,v_{k}\}\subset \operatorname {Ker} (T)} be such a basis.

We may now, by the Steinitz exchange lemma , extend K {\displaystyle {\mathcal {K}}} with n − − k {\displaystyle n-k} linearly independent vectors w 1 , … … , w n − − k {\displaystyle w_{1},\ldots ,w_{n-k}} to form a full basis of V {\displaystyle V} .

Let S := { w 1 , … … , w n − − k } ⊂ ⊂ V ∖ ∖ Ker ⁡ ⁡ ( T ) {\displaystyle {\mathcal {S}}:=\{w_{1},\ldots ,w_{n-k}\}\subset V\setminus \operatorname {Ker} (T)} such that B := K ∪ ∪ S = { v 1 , … … , v k , w 1 , … … , w n − − k } ⊂ ⊂ V {\displaystyle {\mathcal {B}}:={\mathcal {K}}\cup {\mathcal {S}}=\{v_{1},\ldots ,v_{k},w_{1},\ldots ,w_{n-k}\}\subset V} is a basis for V {\displaystyle V} .
From this, we know that Im ⁡ ⁡ T = Span ⁡ ⁡ T ( B ) = Span ⁡ ⁡ { T ( v 1 ) , … … , T ( v k ) , T ( w 1 ) , … … , T ( w n − − k ) } {\displaystyle \operatorname {Im} T=\operatorname {Span} T({\mathcal {B}})=\operatorname {Span} \{T(v_{1}),\ldots ,T(v_{k}),T(w_{1}),\ldots ,T(w_{n-k})\}} = Span ⁡ ⁡ { T ( w 1 ) , … … , T ( w n − − k ) } = Span ⁡ ⁡ T ( S ) .

{\displaystyle =\operatorname {Span} \{T(w_{1}),\ldots ,T(w_{n-k})\}=\operatorname {Span} T({\mathcal {S}}).} We now claim that T ( S ) {\displaystyle T({\mathcal {S}})} is a basis for Im ⁡ ⁡ T {\displaystyle \operatorname {Im} T} .
The above equality already states that T ( S ) {\displaystyle T({\mathcal {S}})} is a generating set for Im ⁡ ⁡ T {\displaystyle \operatorname {Im} T} ; it remains to be shown that it is also linearly independent to conclude that it is a basis.

Suppose T ( S ) {\displaystyle T({\mathcal {S}})} is not linearly independent, and let ∑ ∑ j = 1 n − − k α α j T ( w j ) = 0 W {\displaystyle \sum _{j=1}^{n-k}\alpha _{j}T(w_{j})=0_{W}} for some α α j ∈ ∈ F {\displaystyle \alpha _{j}\in F} .

Thus, owing to the linearity of T {\displaystyle T} , it follows that T ( ∑ ∑ j = 1 n − − k α α j w j ) = 0 W ⟹ ⟹ ( ∑ ∑ j = 1 n − − k α α j w j ) ∈ ∈ Ker ⁡ ⁡ T = Span ⁡ ⁡ K ⊂ ⊂ V .

{\displaystyle T\left(\sum _{j=1}^{n-k}\alpha _{j}w_{j}\right)=0_{W}\implies \left(\sum _{j=1}^{n-k}\alpha _{j}w_{j}\right)\in \operatorname {Ker} T=\operatorname {Span} {\mathcal {K}}\subset V.} This is a contradiction to B {\displaystyle {\mathcal {B}}} being a basis, unless all α α j {\displaystyle \alpha _{j}} are equal to zero. This shows that T ( S ) {\displaystyle T({\mathcal {S}})} is linearly independent, and more specifically that it is a basis for Im ⁡ ⁡ T {\displaystyle \operatorname {Im} T} .

To summarize, we have K {\displaystyle {\mathcal {K}}} , a basis for Ker ⁡ ⁡ T {\displaystyle \operatorname {Ker} T} , and T ( S ) {\displaystyle T({\mathcal {S}})} , a basis for Im ⁡ ⁡ T {\displaystyle \operatorname {Im} T} .

Finally we may state that Rank ⁡ ⁡ ( T ) + Nullity ⁡ ⁡ ( T ) = dim ⁡ ⁡ Im ⁡ ⁡ T + dim ⁡ ⁡ Ker ⁡ ⁡ T {\displaystyle \operatorname {Rank} (T)+\operatorname {Nullity} (T)=\dim \operatorname {Im} T+\dim \operatorname {Ker} T} = | T ( S ) | + | K | = ( n − − k ) + k = n = dim ⁡ ⁡ V .

{\displaystyle =|T({\mathcal {S}})|+|{\mathcal {K}}|=(n-k)+k=n=\dim V.} This concludes our proof.

Second proof [ edit ] Let A {\displaystyle \mathbf {A} } be an m × × n {\displaystyle m\times n} matrix with r {\displaystyle r} linearly independent columns (i.e.

Rank ⁡ ⁡ ( A ) = r {\displaystyle \operatorname {Rank} (\mathbf {A} )=r} ). We will show that: There exists a set of n − − r {\displaystyle n-r} linearly independent solutions to the homogeneous system A x = 0 {\displaystyle \mathbf {Ax} =\mathbf {0} } .

That every other solution is a linear combination of these n − − r {\displaystyle n-r} solutions.

To do this, we will produce an n × × ( n − − r ) {\displaystyle n\times (n-r)} matrix X {\displaystyle \mathbf {X} } whose columns form a basis of the null space of A {\displaystyle \mathbf {A} } .

Without loss of generality , assume that the first r {\displaystyle r} columns of A {\displaystyle \mathbf {A} } are linearly independent. So, we can write A = ( A 1 A 2 ) , {\displaystyle \mathbf {A} ={\begin{pmatrix}\mathbf {A} _{1}&\mathbf {A} _{2}\end{pmatrix}},} where A 1 {\displaystyle \mathbf {A} _{1}} is an m × × r {\displaystyle m\times r} matrix with r {\displaystyle r} linearly independent column vectors, and A 2 {\displaystyle \mathbf {A} _{2}} is an m × × ( n − − r ) {\displaystyle m\times (n-r)} matrix such that each of its n − − r {\displaystyle n-r} columns is linear combinations of the columns of A 1 {\displaystyle \mathbf {A} _{1}} .

This means that A 2 = A 1 B {\displaystyle \mathbf {A} _{2}=\mathbf {A} _{1}\mathbf {B} } for some r × × ( n − − r ) {\displaystyle r\times (n-r)} matrix B {\displaystyle \mathbf {B} } (see rank factorization ) and, hence, A = ( A 1 A 1 B ) .

{\displaystyle \mathbf {A} ={\begin{pmatrix}\mathbf {A} _{1}&\mathbf {A} _{1}\mathbf {B} \end{pmatrix}}.} Let X = ( − − B I n − − r ) , {\displaystyle \mathbf {X} ={\begin{pmatrix}-\mathbf {B} \\\mathbf {I} _{n-r}\end{pmatrix}},} where I n − − r {\displaystyle \mathbf {I} _{n-r}} is the ( n − − r ) × × ( n − − r ) {\displaystyle (n-r)\times (n-r)} identity matrix . So, X {\displaystyle \mathbf {X} } is an n × × ( n − − r ) {\displaystyle n\times (n-r)} matrix such that A X = ( A 1 A 1 B ) ( − − B I n − − r ) = − − A 1 B + A 1 B = 0 m × × ( n − − r ) .

{\displaystyle \mathbf {A} \mathbf {X} ={\begin{pmatrix}\mathbf {A} _{1}&\mathbf {A} _{1}\mathbf {B} \end{pmatrix}}{\begin{pmatrix}-\mathbf {B} \\\mathbf {I} _{n-r}\end{pmatrix}}=-\mathbf {A} _{1}\mathbf {B} +\mathbf {A} _{1}\mathbf {B} =\mathbf {0} _{m\times (n-r)}.} Therefore, each of the n − − r {\displaystyle n-r} columns of X {\displaystyle \mathbf {X} } are particular solutions of A x = 0 F m {\displaystyle \mathbf {Ax} ={0}_{{F}^{m}}} .

Furthermore, the n − − r {\displaystyle n-r} columns of X {\displaystyle \mathbf {X} } are linearly independent because X u = 0 F n {\displaystyle \mathbf {Xu} =\mathbf {0} _{{F}^{n}}} will imply u = 0 F n − − r {\displaystyle \mathbf {u} =\mathbf {0} _{{F}^{n-r}}} for u ∈ ∈ F n − − r {\displaystyle \mathbf {u} \in {F}^{n-r}} : X u = 0 F n ⟹ ⟹ ( − − B I n − − r ) u = 0 F n ⟹ ⟹ ( − − B u u ) = ( 0 F r 0 F n − − r ) ⟹ ⟹ u = 0 F n − − r .

{\displaystyle \mathbf {X} \mathbf {u} =\mathbf {0} _{{F}^{n}}\implies {\begin{pmatrix}-\mathbf {B} \\\mathbf {I} _{n-r}\end{pmatrix}}\mathbf {u} =\mathbf {0} _{{F}^{n}}\implies {\begin{pmatrix}-\mathbf {B} \mathbf {u} \\\mathbf {u} \end{pmatrix}}={\begin{pmatrix}\mathbf {0} _{{F}^{r}}\\\mathbf {0} _{{F}^{n-r}}\end{pmatrix}}\implies \mathbf {u} =\mathbf {0} _{{F}^{n-r}}.} Therefore, the column vectors of X {\displaystyle \mathbf {X} } constitute a set of n − − r {\displaystyle n-r} linearly independent solutions for A x = 0 F m {\displaystyle \mathbf {Ax} =\mathbf {0} _{\mathbb {F} ^{m}}} .

We next prove that any solution of A x = 0 F m {\displaystyle \mathbf {Ax} =\mathbf {0} _{{F}^{m}}} must be a linear combination of the columns of X {\displaystyle \mathbf {X} } .

For this, let u = ( u 1 u 2 ) ∈ ∈ F n {\displaystyle \mathbf {u} ={\begin{pmatrix}\mathbf {u} _{1}\\\mathbf {u} _{2}\end{pmatrix}}\in {F}^{n}} be any vector such that A u = 0 F m {\displaystyle \mathbf {Au} =\mathbf {0} _{{F}^{m}}} . Since the columns of A 1 {\displaystyle \mathbf {A} _{1}} are linearly independent, A 1 x = 0 F m {\displaystyle \mathbf {A} _{1}\mathbf {x} =\mathbf {0} _{{F}^{m}}} implies x = 0 F r {\displaystyle \mathbf {x} =\mathbf {0} _{{F}^{r}}} .

Therefore, A u = 0 F m ⟹ ⟹ ( A 1 A 1 B ) ( u 1 u 2 ) = A 1 u 1 + A 1 B u 2 = A 1 ( u 1 + B u 2 ) = 0 F m ⟹ ⟹ u 1 + B u 2 = 0 F r ⟹ ⟹ u 1 = − − B u 2 {\displaystyle {\begin{array}{rcl}\mathbf {A} \mathbf {u} &=&\mathbf {0} _{{F}^{m}}\\\implies {\begin{pmatrix}\mathbf {A} _{1}&\mathbf {A} _{1}\mathbf {B} \end{pmatrix}}{\begin{pmatrix}\mathbf {u} _{1}\\\mathbf {u} _{2}\end{pmatrix}}&=&\mathbf {A} _{1}\mathbf {u} _{1}+\mathbf {A} _{1}\mathbf {B} \mathbf {u} _{2}&=&\mathbf {A} _{1}(\mathbf {u} _{1}+\mathbf {B} \mathbf {u} _{2})&=&\mathbf {0} _{\mathbb {F} ^{m}}\\\implies \mathbf {u} _{1}+\mathbf {B} \mathbf {u} _{2}&=&\mathbf {0} _{{F}^{r}}\\\implies \mathbf {u} _{1}&=&-\mathbf {B} \mathbf {u} _{2}\end{array}}} ⟹ ⟹ u = ( u 1 u 2 ) = ( − − B I n − − r ) u 2 = X u 2 .

{\displaystyle \implies \mathbf {u} ={\begin{pmatrix}\mathbf {u} _{1}\\\mathbf {u} _{2}\end{pmatrix}}={\begin{pmatrix}-\mathbf {B} \\\mathbf {I} _{n-r}\end{pmatrix}}\mathbf {u} _{2}=\mathbf {X} \mathbf {u} _{2}.} This proves that any vector u {\displaystyle \mathbf {u} } that is a solution of A x = 0 {\displaystyle \mathbf {Ax} =\mathbf {0} } must be a linear combination of the n − − r {\displaystyle n-r} special solutions given by the columns of X {\displaystyle \mathbf {X} } . And we have already seen that the columns of X {\displaystyle \mathbf {X} } are linearly independent. Hence, the columns of X {\displaystyle \mathbf {X} } constitute a basis for the null space of A {\displaystyle \mathbf {A} } . Therefore, the nullity of A {\displaystyle \mathbf {A} } is n − − r {\displaystyle n-r} . Since r {\displaystyle r} equals rank of A {\displaystyle \mathbf {A} } , it follows that Rank ⁡ ⁡ ( A ) + Nullity ⁡ ⁡ ( A ) = n {\displaystyle \operatorname {Rank} (\mathbf {A} )+\operatorname {Nullity} (\mathbf {A} )=n} . This concludes our proof.

A third fundamental subspace [ edit ] When T : V → → W {\displaystyle T:V\to W} is a linear transformation between two finite-dimensional subspaces, with n = dim ⁡ ⁡ ( V ) {\displaystyle n=\dim(V)} and m = dim ⁡ ⁡ ( W ) {\displaystyle m=\dim(W)} (so can be represented by an m × × n {\displaystyle m\times n} matrix M {\displaystyle M} ), the rank–nullity theorem asserts that if T {\displaystyle T} has rank r {\displaystyle r} , then n − − r {\displaystyle n-r} is the dimension of the null space of M {\displaystyle M} , which represents the kernel of T {\displaystyle T} .  In some texts, a third fundamental subspace associated to T {\displaystyle T} is considered alongside its image and kernel: the cokernel of T {\displaystyle T} is the quotient space W / Im ⁡ ⁡ ( T ) {\displaystyle W/\operatorname {Im} (T)} , and its dimension is m − − r {\displaystyle m-r} .  This dimension formula (which might also be rendered dim ⁡ ⁡ Im ⁡ ⁡ ( T ) + dim ⁡ ⁡ Coker ⁡ ⁡ ( T ) = dim ⁡ ⁡ ( W ) {\displaystyle \dim \operatorname {Im} (T)+\dim \operatorname {Coker} (T)=\dim(W)} ) together with the rank–nullity theorem is sometimes called the fundamental theorem of linear algebra .

[ 7 ] [ 8 ] Reformulations and generalizations [ edit ] This theorem is a statement of the first isomorphism theorem of algebra for the case of vector spaces; it generalizes to the splitting lemma .

In more modern language, the theorem can also be phrased as saying that each short exact sequence of vector spaces splits. Explicitly, given that 0 → → U → → V → → T R → → 0 {\displaystyle 0\rightarrow U\rightarrow V\mathbin {\overset {T}{\rightarrow }} R\rightarrow 0} is a short exact sequence of vector spaces, then U ⊕ ⊕ R ≅ ≅ V {\displaystyle U\oplus R\cong V} , hence dim ⁡ ⁡ ( U ) + dim ⁡ ⁡ ( R ) = dim ⁡ ⁡ ( V ) .

{\displaystyle \dim(U)+\dim(R)=\dim(V).} Here R {\displaystyle R} plays the role of Im ⁡ ⁡ T {\displaystyle \operatorname {Im} T} and U {\displaystyle U} is Ker ⁡ ⁡ T {\displaystyle \operatorname {Ker} T} , i.e.

0 → → ker ⁡ ⁡ T ↪ ↪ V → → T im ⁡ ⁡ T → → 0 {\displaystyle 0\rightarrow \ker T\mathbin {\hookrightarrow } V\mathbin {\overset {T}{\rightarrow }} \operatorname {im} T\rightarrow 0} In the finite-dimensional case, this formulation is susceptible to a generalization: if 0 → → V 1 → → V 2 → → ⋯ ⋯ V r → → 0 {\displaystyle 0\rightarrow V_{1}\rightarrow V_{2}\rightarrow \cdots V_{r}\rightarrow 0} is an exact sequence of finite-dimensional vector spaces, then [ 9 ] ∑ ∑ i = 1 r ( − − 1 ) i dim ⁡ ⁡ ( V i ) = 0.

{\displaystyle \sum _{i=1}^{r}(-1)^{i}\dim(V_{i})=0.} The rank–nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the index of a linear map. The index of a linear map T ∈ ∈ Hom ⁡ ⁡ ( V , W ) {\displaystyle T\in \operatorname {Hom} (V,W)} , where V {\displaystyle V} and W {\displaystyle W} are finite-dimensional, is defined by index ⁡ ⁡ T = dim ⁡ ⁡ Ker ⁡ ⁡ ( T ) − − dim ⁡ ⁡ Coker ⁡ ⁡ T .

{\displaystyle \operatorname {index} T=\dim \operatorname {Ker} (T)-\dim \operatorname {Coker} T.} Intuitively, dim ⁡ ⁡ Ker ⁡ ⁡ T {\displaystyle \dim \operatorname {Ker} T} is the number of independent solutions v {\displaystyle v} of the equation T v = 0 {\displaystyle Tv=0} , and dim ⁡ ⁡ Coker ⁡ ⁡ T {\displaystyle \dim \operatorname {Coker} T} is the number of independent restrictions that have to be put on w {\displaystyle w} to make T v = w {\displaystyle Tv=w} solvable. The rank–nullity theorem for finite-dimensional vector spaces is equivalent to the statement index ⁡ ⁡ T = dim ⁡ ⁡ V − − dim ⁡ ⁡ W .

{\displaystyle \operatorname {index} T=\dim V-\dim W.} We see that we can easily read off the index of the linear map T {\displaystyle T} from the involved spaces, without any need to analyze T {\displaystyle T} in detail. This effect also occurs in a much deeper result: the Atiyah–Singer index theorem states that the index of certain differential operators can be read off the geometry of the involved spaces.

Citations [ edit ] ^ Axler (2015) p. 63, §3.22 ^ a b Friedberg, Insel & Spence (2014) p. 70, §2.1, Theorem 2.3 ^ Katznelson & Katznelson (2008) p. 52, §2.5.1 ^ Valenza (1993) p. 71, §4.3 ^ Friedberg, Insel & Spence (2014) pp. 103-104, §2.4, Theorem 2.20 ^ Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics , Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388 ^ * Strang, Gilbert .

Linear Algebra and Its Applications . 3rd ed. Orlando: Saunders, 1988.

^ Strang, Gilbert (1993), "The fundamental theorem of linear algebra" (PDF) , American Mathematical Monthly , 100 (9): 848– 855, CiteSeerX 10.1.1.384.2309 , doi : 10.2307/2324660 , JSTOR 2324660 ^ Zaman, Ragib.

"Dimensions of vector spaces in an exact sequence" .

Mathematics Stack Exchange . Retrieved 27 October 2015 .

References [ edit ] Axler, Sheldon (2015).

Linear Algebra Done Right .

Undergraduate Texts in Mathematics (3rd ed.).

Springer .

ISBN 978-3-319-11079-0 .

Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics , Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388 Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (2014).

Linear Algebra (4th ed.).

Pearson Education .

ISBN 978-0130084514 .

Meyer, Carl D. (2000), Matrix Analysis and Applied Linear Algebra , SIAM , ISBN 978-0-89871-454-8 .

Katznelson, Yitzhak ; Katznelson, Yonatan R. (2008).

A (Terse) Introduction to Linear Algebra .

American Mathematical Society .

ISBN 978-0-8218-4419-9 .

Valenza, Robert J. (1993) [1951].

Linear Algebra: An Introduction to Abstract Mathematics .

Undergraduate Texts in Mathematics (3rd ed.).

Springer .

ISBN 3-540-94099-5 .

External links [ edit ] Gilbert Strang , MIT Linear Algebra Lecture on the Four Fundamental Subspaces , from MIT OpenCourseWare Retrieved from " https://en.wikipedia.org/w/index.php?title=Rank–nullity_theorem&oldid=1303848986 " Categories : Theorems in linear algebra Isomorphism theorems Hidden categories: Articles with short description Short description is different from Wikidata Articles containing proofs This page was last edited on 2 August 2025, at 11:48 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Rank–nullity theorem 16 languages Add topic

