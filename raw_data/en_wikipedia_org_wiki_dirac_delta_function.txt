Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Motivation and overview 2 History 3 Definitions Toggle Definitions subsection 3.1 As a measure 3.2 As a distribution 3.3 Generalizations 4 Properties Toggle Properties subsection 4.1 Scaling and symmetry 4.2 Algebraic properties 4.3 Translation 4.4 Composition with a function 4.5 Indefinite integral 4.6 Properties in n dimensions 5 Derivatives Toggle Derivatives subsection 5.1 Higher dimensions 6 Representations Toggle Representations subsection 6.1 Approximations to the identity 6.2 Probabilistic considerations 6.3 Semigroups 6.3.1 The heat kernel 6.3.2 The Poisson kernel 6.4 Oscillatory integrals 6.5 Plane wave decomposition 6.6 Fourier transform 6.6.1 Fourier kernels 6.7 Hilbert space theory 6.7.1 Sobolev spaces 6.7.2 Spaces of holomorphic functions 6.7.3 Resolutions of the identity 6.8 Infinitesimal delta functions 7 Dirac comb 8 Sokhotski–Plemelj theorem 9 Relationship to the Kronecker delta 10 Applications Toggle Applications subsection 10.1 Probability theory 10.2 Quantum mechanics 10.3 Structural mechanics 11 See also 12 Notes 13 References 14 External links Toggle the table of contents Dirac delta function 45 languages العربية বাংলা Беларуская Български Català Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto فارسی Français 한국어 हिन्दी Bahasa Indonesia Íslenska Italiano עברית ქართული Latviešu Magyar Nederlands 日本語 Norsk bokmål Oʻzbekcha / ўзбекча Polski Português Română Русский Shqip සිංහල Simple English Slovenščina Српски / srpski Suomi Svenska Татарча / tatarça ไทย Türkçe Українська Tiếng Việt 吴语 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Generalized function whose value is zero everywhere except at zero "Delta function" redirects here. For other uses, see Delta function (disambiguation) .

Schematic representation of the Dirac delta function by a line surmounted by an arrow. The height of the arrow is usually meant to specify the value of any multiplicative constant, which will give the area under the function. The other convention is to write the area next to the arrowhead.

The Dirac delta as the limit as a → → 0 {\displaystyle a\to 0} (in the sense of distributions ) of the sequence of zero-centered normal distributions δ δ a ( x ) = 1 | a | π π e − − ( x / a ) 2 {\displaystyle \delta _{a}(x)={\frac {1}{\left|a\right|{\sqrt {\pi }}}}e^{-(x/a)^{2}}} Differential equations Scope Fields Natural sciences Engineering Astronomy Physics Chemistry Biology Geology Applied mathematics Continuum mechanics Chaos theory Dynamical systems Social sciences Economics Population dynamics List of named differential equations Classification Types Ordinary Partial Differential-algebraic Integro-differential Fractional Linear Non-linear By variable type Dependent and independent variables Autonomous Coupled / Decoupled Exact Homogeneous / Nonhomogeneous Features Order Operator Notation Relation to processes Difference (discrete analogue) Stochastic Stochastic partial Delay Solution Existence and uniqueness Picard–Lindelöf theorem Peano existence theorem Carathéodory's existence theorem Cauchy–Kowalevski theorem General topics Initial conditions Boundary values Dirichlet Neumann Robin Cauchy problem Wronskian Phase portrait Lyapunov / Asymptotic / Exponential stability Rate of convergence Series / Integral solutions Numerical integration Dirac delta function Solution methods Inspection Method of characteristics Euler Exponential response formula Finite difference ( Crank–Nicolson ) Finite element Infinite element Finite volume Galerkin Petrov–Galerkin Green's function Integrating factor Integral transforms Perturbation theory Runge–Kutta Separation of variables Undetermined coefficients Variation of parameters People List Isaac Newton Gottfried Leibniz Jacob Bernoulli Leonhard Euler Joseph-Louis Lagrange Józef Maria Hoene-Wroński Joseph Fourier Augustin-Louis Cauchy George Green Carl David Tolmé Runge Martin Kutta Rudolf Lipschitz Ernst Lindelöf Émile Picard Phyllis Nicolson John Crank v t e In mathematical analysis , the Dirac delta function (or δ distribution ), also known as the unit impulse , [ 1 ] is a generalized function on the real numbers , whose value is zero everywhere except at zero, and whose integral over the entire real line is equal to one.

[ 2 ] [ 3 ] [ 4 ] Thus it can be represented heuristically as δ δ ( x ) = { 0 , x ≠ ≠ 0 ∞ ∞ , x = 0 {\displaystyle \delta (x)={\begin{cases}0,&x\neq 0\\{\infty },&x=0\end{cases}}} such that ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( x ) d x = 1.

{\displaystyle \int _{-\infty }^{\infty }\delta (x)dx=1.} Since there is no function having this property, modelling the delta "function" rigorously involves the use of limits or, as is common in mathematics, measure theory and the theory of distributions .

The delta function was introduced by physicist Paul Dirac , and has since been applied routinely in physics and engineering to model point masses and instantaneous impulses.  It is called the delta function because it is a continuous analogue of the Kronecker delta function, which is usually defined on a discrete domain and takes values 0 and 1. The mathematical rigor of the delta function was disputed until Laurent Schwartz developed the theory of distributions, where it is defined as a linear form acting on functions.

Motivation and overview [ edit ] The graph of the Dirac delta is usually thought of as following the whole x -axis and the positive y -axis.

[ 5 ] The Dirac delta is used to model a tall narrow spike function (an impulse ), and other similar abstractions such as a point charge or point mass .

[ 6 ] [ 7 ] For example, to calculate the dynamics of a billiard ball being struck, one can approximate the force of the impact by a Dirac delta. In doing so, one can simplify the equations and calculate the motion of the ball by only considering the total impulse of the collision, without a detailed model of all of the elastic energy transfer at subatomic levels (for instance).

To be specific, suppose that a billiard ball is at rest.  At time t = 0 {\displaystyle t=0} it is struck by another ball, imparting it with a momentum P , with units kg⋅m⋅s −1 .  The exchange of momentum is not actually instantaneous, being mediated by elastic processes at the molecular and subatomic level, but for practical purposes it is convenient to consider that energy transfer as effectively instantaneous.  The force therefore is P δ ( t ) ; the units of δ ( t ) are s −1 .

To model this situation more rigorously, suppose that the force instead is uniformly distributed over a small time interval Δ Δ t = [ 0 , T ] {\displaystyle \Delta t=[0,T]} .

That is, F Δ Δ t ( t ) = { P / Δ Δ t 0 < t ≤ ≤ T , 0 otherwise .

{\displaystyle F_{\Delta t}(t)={\begin{cases}P/\Delta t&0<t\leq T,\\0&{\text{otherwise}}.\end{cases}}} Then the momentum at any time t is found by integration: p ( t ) = ∫ ∫ 0 t F Δ Δ t ( τ τ ) d τ τ = { P t ≥ ≥ T P t / Δ Δ t 0 ≤ ≤ t ≤ ≤ T 0 otherwise.

{\displaystyle p(t)=\int _{0}^{t}F_{\Delta t}(\tau )\,d\tau ={\begin{cases}P&t\geq T\\P\,t/\Delta t&0\leq t\leq T\\0&{\text{otherwise.}}\end{cases}}} Now, the model situation of an instantaneous transfer of momentum requires taking the limit as Δ t → 0 , giving a result everywhere except at 0 : p ( t ) = { P t > 0 0 t < 0.

{\displaystyle p(t)={\begin{cases}P&t>0\\0&t<0.\end{cases}}} Here the functions F Δ Δ t {\displaystyle F_{\Delta t}} are thought of as useful approximations to the idea of instantaneous transfer of momentum.

The delta function allows us to construct an idealized limit of these approximations. Unfortunately, the actual limit of the functions (in the sense of pointwise convergence ) lim Δ Δ t → → 0 + F Δ Δ t {\textstyle \lim _{\Delta t\to 0^{+}}F_{\Delta t}} is zero everywhere but a single point, where it is infinite. To make proper sense of the Dirac delta, we should instead insist that the property ∫ ∫ − − ∞ ∞ ∞ ∞ F Δ Δ t ( t ) d t = P , {\displaystyle \int _{-\infty }^{\infty }F_{\Delta t}(t)\,dt=P,} which holds for all Δ Δ t > 0 {\displaystyle \Delta t>0} , should continue to hold in the limit. So, in the equation F ( t ) = P δ δ ( t ) = lim Δ Δ t → → 0 F Δ Δ t ( t ) {\textstyle F(t)=P\,\delta (t)=\lim _{\Delta t\to 0}F_{\Delta t}(t)} , it is understood that the limit is always taken outside the integral .

In applied mathematics, as we have done here, the delta function is often manipulated as a kind of limit (a weak limit ) of a sequence of functions, each member of which has a tall spike at the origin: for example, a sequence of Gaussian distributions centered at the origin with variance tending to zero. (However, even in some applications, highly oscillatory functions are used as approximations to the delta function, see below .) The Dirac delta is not truly a function, at least not a usual one with domain and range in real numbers .

[ 4 ] For example, the objects f ( x ) = δ ( x ) and g ( x ) = 0 are equal everywhere except at x = 0 yet have integrals that are different.  According to Lebesgue integration theory , if f and g are functions such that f = g almost everywhere , then f is integrable if and only if g is integrable and the integrals of f and g are identical.

[ 8 ] A rigorous approach to regarding the Dirac delta function as a mathematical object in its own right uses measure theory or the theory of distributions .

[ 9 ] History [ edit ] In physics, the Dirac delta function was popularized by Paul Dirac in this book The Principles of Quantum Mechanics published in 1930.

[ 3 ] However, Oliver Heaviside , 35 years before Dirac, described an impulsive function called the Heaviside step function for purposes and with properties analogous to Dirac's work. Even earlier several mathematicians and physicists used limits of sharply peaked functions in derivations.

[ 10 ] An infinitesimal formula for an infinitely tall, unit impulse delta function (infinitesimal version of Cauchy distribution ) explicitly appears in an 1827 text of Augustin-Louis Cauchy .

[ 11 ] Siméon Denis Poisson considered the issue in connection with the study of wave propagation as did Gustav Kirchhoff somewhat later. Kirchhoff and Hermann von Helmholtz also introduced the unit impulse as a limit of Gaussians , which also corresponded to Lord Kelvin 's notion of a point heat source.

[ 12 ] The Dirac delta function as such was introduced by Paul Dirac in his 1927 paper The Physical Interpretation of the Quantum Dynamics.

[ 13 ] He called it the "delta function" since he used it as a continuum analogue of the discrete Kronecker delta .

[ 3 ] Mathematicians refer to the same concept as a distribution rather than a function.

[ 14 ] : 33 Joseph Fourier presented what is now called the Fourier integral theorem in his treatise Théorie analytique de la chaleur in the form: [ 15 ] f ( x ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ d α α f ( α α ) ∫ ∫ − − ∞ ∞ ∞ ∞ d p cos ⁡ ⁡ ( p x − − p α α ) , {\displaystyle f(x)={\frac {1}{2\pi }}\int _{-\infty }^{\infty }\ \ d\alpha \,f(\alpha )\ \int _{-\infty }^{\infty }dp\ \cos(px-p\alpha )\ ,} which is tantamount to the introduction of the δ -function in the form: [ 16 ] δ δ ( x − − α α ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ d p cos ⁡ ⁡ ( p x − − p α α ) .

{\displaystyle \delta (x-\alpha )={\frac {1}{2\pi }}\int _{-\infty }^{\infty }dp\ \cos(px-p\alpha )\ .} Later, Augustin Cauchy expressed the theorem using exponentials: [ 17 ] [ 18 ] f ( x ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ e i p x ( ∫ ∫ − − ∞ ∞ ∞ ∞ e − − i p α α f ( α α ) d α α ) d p .

{\displaystyle f(x)={\frac {1}{2\pi }}\int _{-\infty }^{\infty }\ e^{ipx}\left(\int _{-\infty }^{\infty }e^{-ip\alpha }f(\alpha )\,d\alpha \right)\,dp.} Cauchy pointed out that in some circumstances the order of integration is significant in this result (contrast Fubini's theorem ).

[ 19 ] [ 20 ] As justified using the theory of distributions , the Cauchy equation can be rearranged to resemble Fourier's original formulation and expose the δ -function as f ( x ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ e i p x ( ∫ ∫ − − ∞ ∞ ∞ ∞ e − − i p α α f ( α α ) d α α ) d p = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ ( ∫ ∫ − − ∞ ∞ ∞ ∞ e i p x e − − i p α α d p ) f ( α α ) d α α = ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( x − − α α ) f ( α α ) d α α , {\displaystyle {\begin{aligned}f(x)&={\frac {1}{2\pi }}\int _{-\infty }^{\infty }e^{ipx}\left(\int _{-\infty }^{\infty }e^{-ip\alpha }f(\alpha )\,d\alpha \right)\,dp\\[4pt]&={\frac {1}{2\pi }}\int _{-\infty }^{\infty }\left(\int _{-\infty }^{\infty }e^{ipx}e^{-ip\alpha }\,dp\right)f(\alpha )\,d\alpha =\int _{-\infty }^{\infty }\delta (x-\alpha )f(\alpha )\,d\alpha ,\end{aligned}}} where the δ -function is expressed as δ δ ( x − − α α ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ e i p ( x − − α α ) d p .

{\displaystyle \delta (x-\alpha )={\frac {1}{2\pi }}\int _{-\infty }^{\infty }e^{ip(x-\alpha )}\,dp\ .} A rigorous interpretation of the exponential form and the various limitations upon the function f necessary for its application extended over several centuries. The problems with a classical interpretation are explained as follows: [ 21 ] The greatest drawback of the classical Fourier transformation is a rather narrow class of functions (originals) for which it can be effectively computed. Namely, it is necessary that these functions decrease sufficiently rapidly to zero (in the neighborhood of infinity) to ensure the existence of the Fourier integral. For example, the Fourier transform of such simple functions as polynomials does not exist in the classical sense. The extension of the classical Fourier transformation to distributions considerably enlarged the class of functions that could be transformed and this removed many obstacles.

Further developments included generalization of the Fourier integral, "beginning with Plancherel's pathbreaking L 2 -theory (1910), continuing with Wiener's and Bochner's works (around 1930) and culminating with the amalgamation into L. Schwartz's theory of distributions (1945) ...", [ 22 ] and leading to the formal development of the Dirac delta function.

Definitions [ edit ] The Dirac delta function δ δ ( x ) {\displaystyle \delta (x)} can be loosely thought of as a function on the real line which is zero everywhere except at the origin, where it is infinite, δ δ ( x ) ≃ ≃ { + ∞ ∞ , x = 0 0 , x ≠ ≠ 0 {\displaystyle \delta (x)\simeq {\begin{cases}+\infty ,&x=0\\0,&x\neq 0\end{cases}}} and which is also constrained to satisfy the identity [ 23 ] ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( x ) d x = 1.

{\displaystyle \int _{-\infty }^{\infty }\delta (x)\,dx=1.} This is merely a heuristic characterization. The Dirac delta is not a function in the traditional sense as no extended real number valued function defined on the real numbers has these properties.

[ 24 ] As a measure [ edit ] One way to rigorously capture the notion of the Dirac delta function is to define a measure , called Dirac measure , which accepts a subset A of the real line R as an argument, and returns δ ( A ) = 1 if 0 ∈ A , and δ ( A ) = 0 otherwise.

[ 25 ] If the delta function is conceptualized as modeling an idealized point mass at 0, then δ ( A ) represents the mass contained in the set A .  One may then define the integral against δ as the integral of a function against this mass distribution.  Formally, the Lebesgue integral provides the necessary analytic device. The Lebesgue integral with respect to the measure δ satisfies ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) δ δ ( d x ) = f ( 0 ) {\displaystyle \int _{-\infty }^{\infty }f(x)\,\delta (dx)=f(0)} for all continuous compactly supported functions f . The measure δ is not absolutely continuous with respect to the Lebesgue measure —in fact, it is a singular measure . Consequently, the delta measure has no Radon–Nikodym derivative (with respect to Lebesgue measure)—no true function for which the property ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) δ δ ( x ) d x = f ( 0 ) {\displaystyle \int _{-\infty }^{\infty }f(x)\,\delta (x)\,dx=f(0)} holds.

[ 26 ] As a result, the latter notation is a convenient abuse of notation , and not a standard ( Riemann or Lebesgue ) integral.

[ 27 ] As a probability measure on R , the delta measure is characterized by its cumulative distribution function , which is the unit step function .

[ 28 ] H ( x ) = { 1 if x ≥ ≥ 0 0 if x < 0.

{\displaystyle H(x)={\begin{cases}1&{\text{if }}x\geq 0\\0&{\text{if }}x<0.\end{cases}}} This means that H ( x ) is the integral of the cumulative indicator function 1 (−∞, x ] with respect to the measure δ ; to wit, H ( x ) = ∫ ∫ R 1 ( − − ∞ ∞ , x ] ( t ) δ δ ( d t ) = δ δ ( ( − − ∞ ∞ , x ] ) , {\displaystyle H(x)=\int _{\mathbf {R} }\mathbf {1} _{(-\infty ,x]}(t)\,\delta (dt)=\delta \!\left((-\infty ,x]\right),} the latter being the measure of this interval. Thus in particular the integration of the delta function against a continuous function can be properly understood as a Riemann–Stieltjes integral : [ 29 ] ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) δ δ ( d x ) = ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) d H ( x ) .

{\displaystyle \int _{-\infty }^{\infty }f(x)\,\delta (dx)=\int _{-\infty }^{\infty }f(x)\,dH(x).} All higher moments of δ are zero.  In particular, characteristic function and moment generating function are both equal to one.

[ 30 ] As a distribution [ edit ] In the theory of distributions , a generalized function is considered not a function in itself but only through how it affects other functions when "integrated" against them.

[ 31 ] In keeping with this philosophy, to define the delta function properly, it is enough to say what the "integral" of the delta function is against a sufficiently "good" test function φ .

[ 4 ] If the delta function is already understood as a measure, then the Lebesgue integral of a test function against that measure supplies the necessary integral.

[ 32 ] A typical space of test functions consists of all smooth functions on R with compact support that have as many derivatives as required.  As a distribution, the Dirac delta is a linear functional on the space of test functions and is defined by [ 33 ] δ δ [ φ φ ] = φ φ ( 0 ) {\displaystyle \delta [\varphi ]=\varphi (0)} 1 for every test function φ .

For δ to be properly a distribution, it must be continuous in a suitable topology on the space of test functions.  In general, for a linear functional S on the space of test functions to define a distribution, it is necessary and sufficient that, for every positive integer N there is an integer M N and a constant C N such that for every test function φ , one has the inequality [ 34 ] | S [ φ φ ] | ≤ ≤ C N ∑ ∑ k = 0 M N sup x ∈ ∈ [ − − N , N ] | φ φ ( k ) ( x ) | {\displaystyle \left|S[\varphi ]\right|\leq C_{N}\sum _{k=0}^{M_{N}}\sup _{x\in [-N,N]}\left|\varphi ^{(k)}(x)\right|} where sup represents the supremum . With the δ distribution, one has such an inequality (with C N = 1) with M N = 0 for all N .  Thus δ is a distribution of order zero. It is, furthermore, a distribution with compact support (the support being {0} ).

The delta distribution can also be defined in several equivalent ways.  For instance, it is the distributional derivative of the Heaviside step function . This means that for every test function φ , one has δ δ [ φ φ ] = − − ∫ ∫ − − ∞ ∞ ∞ ∞ φ φ ′ ( x ) H ( x ) d x .

{\displaystyle \delta [\varphi ]=-\int _{-\infty }^{\infty }\varphi '(x)\,H(x)\,dx.} Intuitively, if integration by parts were permitted, then the latter integral should simplify to ∫ ∫ − − ∞ ∞ ∞ ∞ φ φ ( x ) H ′ ( x ) d x = ∫ ∫ − − ∞ ∞ ∞ ∞ φ φ ( x ) δ δ ( x ) d x , {\displaystyle \int _{-\infty }^{\infty }\varphi (x)\,H'(x)\,dx=\int _{-\infty }^{\infty }\varphi (x)\,\delta (x)\,dx,} and indeed, a form of integration by parts is permitted for the Stieltjes integral, and in that case, one does have − − ∫ ∫ − − ∞ ∞ ∞ ∞ φ φ ′ ( x ) H ( x ) d x = ∫ ∫ − − ∞ ∞ ∞ ∞ φ φ ( x ) d H ( x ) .

{\displaystyle -\int _{-\infty }^{\infty }\varphi '(x)\,H(x)\,dx=\int _{-\infty }^{\infty }\varphi (x)\,dH(x).} In the context of measure theory, the Dirac measure gives rise to distribution by integration.  Conversely, equation ( 1 ) defines a Daniell integral on the space of all compactly supported continuous functions φ which, by the Riesz representation theorem , can be represented as the Lebesgue integral of φ with respect to some Radon measure .

[ 35 ] } Generally, when the term Dirac delta function is used, it is in the sense of distributions rather than measures, the Dirac measure being among several terms for the corresponding notion in measure theory.  Some sources may also use the term Dirac delta distribution .

Generalizations [ edit ] The delta function can be defined in n -dimensional Euclidean space R n as the measure such that ∫ ∫ R n f ( x ) δ δ ( d x ) = f ( 0 ) {\displaystyle \int _{\mathbf {R} ^{n}}f(\mathbf {x} )\,\delta (d\mathbf {x} )=f(\mathbf {0} )} for every compactly supported continuous function f .  As a measure, the n -dimensional delta function is the product measure of the 1-dimensional delta functions in each variable separately. Thus, formally, with x = ( x 1 , x 2 , ..., x n ) , one has [ 36 ] δ δ ( x ) = δ δ ( x 1 ) δ δ ( x 2 ) ⋯ ⋯ δ δ ( x n ) .

{\displaystyle \delta (\mathbf {x} )=\delta (x_{1})\,\delta (x_{2})\cdots \delta (x_{n}).} 2 The delta function can also be defined in the sense of distributions exactly as above in the one-dimensional case.

[ 37 ] However, despite widespread use in engineering contexts, ( 2 ) should be manipulated with care, since the product of distributions can only be defined under quite narrow circumstances.

[ 38 ] [ 39 ] The notion of a Dirac measure makes sense on any set.

[ 40 ] Thus if X is a set, x 0 ∈ X is a marked point, and Σ is any sigma algebra of subsets of X , then the measure defined on sets A ∈ Σ by δ δ x 0 ( A ) = { 1 if x 0 ∈ ∈ A 0 if x 0 ∉ ∉ A {\displaystyle \delta _{x_{0}}(A)={\begin{cases}1&{\text{if }}x_{0}\in A\\0&{\text{if }}x_{0}\notin A\end{cases}}} is the delta measure or unit mass concentrated at x 0 .

Another common generalization of the delta function is to a differentiable manifold where most of its properties as a distribution can also be exploited because of the differentiable structure .  The delta function on a manifold M centered at the point x 0 ∈ M is defined as the following distribution: δ δ x 0 [ φ φ ] = φ φ ( x 0 ) {\displaystyle \delta _{x_{0}}[\varphi ]=\varphi (x_{0})} 3 for all compactly supported smooth real-valued functions φ on M .

[ 41 ] A common special case of this construction is a case in which M is an open set in the Euclidean space R n .

On a locally compact Hausdorff space X , the Dirac delta measure concentrated at a point x is the Radon measure associated with the Daniell integral ( 3 ) on compactly supported continuous functions φ .

[ 42 ] At this level of generality, calculus as such is no longer possible, however a variety of techniques from abstract analysis are available. For instance, the mapping x 0 ↦ ↦ δ δ x 0 {\displaystyle x_{0}\mapsto \delta _{x_{0}}} is a continuous embedding of X into the space of finite Radon measures on X , equipped with its vague topology . Moreover, the convex hull of the image of X under this embedding is dense in the space of probability measures on X .

[ 43 ] Properties [ edit ] Scaling and symmetry [ edit ] The delta function satisfies the following scaling property for a non-zero scalar α : [ 3 ] [ 44 ] ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( α α x ) d x = ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( u ) d u | α α | = 1 | α α | {\displaystyle \int _{-\infty }^{\infty }\delta (\alpha x)\,dx=\int _{-\infty }^{\infty }\delta (u)\,{\frac {du}{|\alpha |}}={\frac {1}{|\alpha |}}} and so δ δ ( α α x ) = δ δ ( x ) | α α | .

{\displaystyle \delta (\alpha x)={\frac {\delta (x)}{|\alpha |}}.} 4 Scaling property proof: ∫ ∫ − − ∞ ∞ ∞ ∞ d x g ( x ) δ δ ( a x ) = 1 a ∫ ∫ − − ∞ ∞ ∞ ∞ d x ′ g ( x ′ a ) δ δ ( x ′ ) = 1 a g ( 0 ) .

{\displaystyle \int \limits _{-\infty }^{\infty }dx\ g(x)\delta (ax)={\frac {1}{a}}\int \limits _{-\infty }^{\infty }dx'\ g\left({\frac {x'}{a}}\right)\delta (x')={\frac {1}{a}}g(0).} where a change of variable x′ = ax is used. If a is negative, i.e., a = −| a | , then ∫ ∫ − − ∞ ∞ ∞ ∞ d x g ( x ) δ δ ( a x ) = 1 − − | a | ∫ ∫ ∞ ∞ − − ∞ ∞ d x ′ g ( x ′ a ) δ δ ( x ′ ) = 1 | a | ∫ ∫ − − ∞ ∞ ∞ ∞ d x ′ g ( x ′ a ) δ δ ( x ′ ) = 1 | a | g ( 0 ) .

{\displaystyle \int \limits _{-\infty }^{\infty }dx\ g(x)\delta (ax)={\frac {1}{-\left\vert a\right\vert }}\int \limits _{\infty }^{-\infty }dx'\ g\left({\frac {x'}{a}}\right)\delta (x')={\frac {1}{\left\vert a\right\vert }}\int \limits _{-\infty }^{\infty }dx'\ g\left({\frac {x'}{a}}\right)\delta (x')={\frac {1}{\left\vert a\right\vert }}g(0).} Thus, δ δ ( a x ) = 1 | a | δ δ ( x ) {\displaystyle \delta (ax)={\frac {1}{\left\vert a\right\vert }}\delta (x)} .

In particular, the delta function is an even distribution (symmetry), in the sense that δ δ ( − − x ) = δ δ ( x ) {\displaystyle \delta (-x)=\delta (x)} which is homogeneous of degree −1 .

Algebraic properties [ edit ] The distributional product of δ with x is equal to zero: x δ δ ( x ) = 0.

{\displaystyle x\,\delta (x)=0.} More generally, ( x − − a ) n δ δ ( x − − a ) = 0 {\displaystyle (x-a)^{n}\delta (x-a)=0} for all positive integers n {\displaystyle n} .

Conversely, if xf ( x ) = xg ( x ) , where f and g are distributions, then f ( x ) = g ( x ) + c δ δ ( x ) {\displaystyle f(x)=g(x)+c\delta (x)} for some constant c .

[ 45 ] Translation [ edit ] The integral of any function multiplied by the time-delayed Dirac delta δ δ T ( t ) = δ δ ( t − − T ) {\displaystyle \delta _{T}(t){=}\delta (t{-}T)} is ∫ ∫ − − ∞ ∞ ∞ ∞ f ( t ) δ δ ( t − − T ) d t = f ( T ) .

{\displaystyle \int _{-\infty }^{\infty }f(t)\,\delta (t-T)\,dt=f(T).} This is sometimes referred to as the sifting property [ 46 ] or the sampling property .

[ 47 ] The delta function is said to "sift out" the value of f(t) at t = T .

[ 48 ] It follows that the effect of convolving a function f ( t ) with the time-delayed Dirac delta  is to time-delay f ( t ) by the same amount: [ 49 ] ( f ∗ ∗ δ δ T ) ( t ) = d e f ∫ ∫ − − ∞ ∞ ∞ ∞ f ( τ τ ) δ δ ( t − − T − − τ τ ) d τ τ = ∫ ∫ − − ∞ ∞ ∞ ∞ f ( τ τ ) δ δ ( τ τ − − ( t − − T ) ) d τ τ since δ δ ( − − x ) = δ δ ( x ) by (4) = f ( t − − T ) .

{\displaystyle {\begin{aligned}(f*\delta _{T})(t)\ &{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )\,\delta (t-T-\tau )\,d\tau \\&=\int _{-\infty }^{\infty }f(\tau )\,\delta (\tau -(t-T))\,d\tau \qquad {\text{since}}~\delta (-x)=\delta (x)~~{\text{by (4)}}\\&=f(t-T).\end{aligned}}} The sifting property holds under the precise condition that f be a tempered distribution (see the discussion of the Fourier transform below ). As a special case, for instance, we have the identity (understood in the distribution sense) ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( ξ ξ − − x ) δ δ ( x − − η η ) d x = δ δ ( η η − − ξ ξ ) .

{\displaystyle \int _{-\infty }^{\infty }\delta (\xi -x)\delta (x-\eta )\,dx=\delta (\eta -\xi ).} Composition with a function [ edit ] More generally, the delta distribution may be composed with a smooth function g ( x ) in such a way that the familiar change of variables formula holds (where u = g ( x ) {\displaystyle u=g(x)} ), that ∫ ∫ R δ δ ( g ( x ) ) f ( g ( x ) ) | g ′ ( x ) | d x = ∫ ∫ g ( R ) δ δ ( u ) f ( u ) d u {\displaystyle \int _{\mathbb {R} }\delta {\bigl (}g(x){\bigr )}f{\bigl (}g(x){\bigr )}\left|g'(x)\right|dx=\int _{g(\mathbb {R} )}\delta (u)\,f(u)\,du} provided that g is a continuously differentiable function with g′ nowhere zero.

[ 50 ] That is, there is a unique way to assign meaning to the distribution δ δ ∘ ∘ g {\displaystyle \delta \circ g} so that this identity holds for all compactly supported test functions f .  Therefore, the domain must be broken up to exclude the g′ = 0 point.  This distribution satisfies δ ( g ( x )) = 0 if g is nowhere zero, and otherwise if g has a real root at x 0 , then δ δ ( g ( x ) ) = δ δ ( x − − x 0 ) | g ′ ( x 0 ) | .

{\displaystyle \delta (g(x))={\frac {\delta (x-x_{0})}{|g'(x_{0})|}}.} It is natural therefore to define the composition δ ( g ( x )) for continuously differentiable functions g by δ δ ( g ( x ) ) = ∑ ∑ i δ δ ( x − − x i ) | g ′ ( x i ) | {\displaystyle \delta (g(x))=\sum _{i}{\frac {\delta (x-x_{i})}{|g'(x_{i})|}}} where the sum extends over all roots of g ( x ) , which are assumed to be simple .  Thus, for example δ δ ( x 2 − − α α 2 ) = 1 2 | α α | [ δ δ ( x + α α ) + δ δ ( x − − α α ) ] .

{\displaystyle \delta \left(x^{2}-\alpha ^{2}\right)={\frac {1}{2|\alpha |}}{\Big [}\delta \left(x+\alpha \right)+\delta \left(x-\alpha \right){\Big ]}.} In the integral form, the generalized scaling property may be written as ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) δ δ ( g ( x ) ) d x = ∑ ∑ i f ( x i ) | g ′ ( x i ) | .

{\displaystyle \int _{-\infty }^{\infty }f(x)\,\delta (g(x))\,dx=\sum _{i}{\frac {f(x_{i})}{|g'(x_{i})|}}.} Indefinite integral [ edit ] For a constant a ∈ ∈ R {\displaystyle a\in \mathbb {R} } and a "well-behaved" arbitrary real-valued function y ( x ) , ∫ ∫ y ( x ) δ δ ( x − − a ) d x = y ( a ) H ( x − − a ) + c , {\displaystyle \displaystyle {\int }y(x)\delta (x-a)dx=y(a)H(x-a)+c,} where H ( x ) is the Heaviside step function and c is an integration constant.

Properties in n dimensions [ edit ] The delta distribution in an n -dimensional space satisfies the following scaling property instead, δ δ ( α α x ) = | α α | − − n δ δ ( x ) , {\displaystyle \delta (\alpha {\boldsymbol {x}})=|\alpha |^{-n}\delta ({\boldsymbol {x}})~,} so that δ is a homogeneous distribution of degree − n .

Under any reflection or rotation ρ , the delta function is invariant, δ δ ( ρ ρ x ) = δ δ ( x ) .

{\displaystyle \delta (\rho {\boldsymbol {x}})=\delta ({\boldsymbol {x}})~.} As in the one-variable case, it is possible to define the composition of δ with a bi-Lipschitz function [ 51 ] g : R n → R n uniquely so that the following holds ∫ ∫ R n δ δ ( g ( x ) ) f ( g ( x ) ) | det g ′ ( x ) | d x = ∫ ∫ g ( R n ) δ δ ( u ) f ( u ) d u {\displaystyle \int _{\mathbb {R} ^{n}}\delta (g({\boldsymbol {x}}))\,f(g({\boldsymbol {x}}))\left|\det g'({\boldsymbol {x}})\right|d{\boldsymbol {x}}=\int _{g(\mathbb {R} ^{n})}\delta ({\boldsymbol {u}})f({\boldsymbol {u}})\,d{\boldsymbol {u}}} for all compactly supported functions f .

Using the coarea formula from geometric measure theory , one can also define the composition of the delta function with a submersion from one Euclidean space to another one of different dimension; the result is a type of current . In the special case of a continuously differentiable function g : R n → R such that the gradient of g is nowhere zero, the following identity holds [ 52 ] ∫ ∫ R n f ( x ) δ δ ( g ( x ) ) d x = ∫ ∫ g − − 1 ( 0 ) f ( x ) | ∇ ∇ g | d σ σ ( x ) {\displaystyle \int _{\mathbb {R} ^{n}}f({\boldsymbol {x}})\,\delta (g({\boldsymbol {x}}))\,d{\boldsymbol {x}}=\int _{g^{-1}(0)}{\frac {f({\boldsymbol {x}})}{|{\boldsymbol {\nabla }}g|}}\,d\sigma ({\boldsymbol {x}})} where the integral on the right is over g −1 (0) , the ( n − 1) -dimensional surface defined by g ( x ) = 0 with respect to the Minkowski content measure.  This is known as a simple layer integral.

More generally, if S is a smooth hypersurface of R n , then we can associate to S the distribution that integrates any compactly supported smooth function g over S : δ δ S [ g ] = ∫ ∫ S g ( s ) d σ σ ( s ) {\displaystyle \delta _{S}[g]=\int _{S}g({\boldsymbol {s}})\,d\sigma ({\boldsymbol {s}})} where σ is the hypersurface measure associated to S .  This generalization is associated with the potential theory of simple layer potentials on S .  If D is a domain in R n with smooth boundary S , then δ S is equal to the normal derivative of the indicator function of D in the distribution sense, − − ∫ ∫ R n g ( x ) ∂ ∂ 1 D ( x ) ∂ ∂ n d x = ∫ ∫ S g ( s ) d σ σ ( s ) , {\displaystyle -\int _{\mathbb {R} ^{n}}g({\boldsymbol {x}})\,{\frac {\partial 1_{D}({\boldsymbol {x}})}{\partial n}}\,d{\boldsymbol {x}}=\int _{S}\,g({\boldsymbol {s}})\,d\sigma ({\boldsymbol {s}}),} where n is the outward normal.

[ 53 ] [ 54 ] In three dimensions, the delta function is represented in spherical coordinates by: δ δ ( r − − r 0 ) = { 1 r 2 sin ⁡ ⁡ θ θ δ δ ( r − − r 0 ) δ δ ( θ θ − − θ θ 0 ) δ δ ( ϕ ϕ − − ϕ ϕ 0 ) x 0 , y 0 , z 0 ≠ ≠ 0 1 2 π π r 2 sin ⁡ ⁡ θ θ δ δ ( r − − r 0 ) δ δ ( θ θ − − θ θ 0 ) x 0 = y 0 = 0 , z 0 ≠ ≠ 0 1 4 π π r 2 δ δ ( r − − r 0 ) x 0 = y 0 = z 0 = 0 {\displaystyle \delta ({\boldsymbol {r}}-{\boldsymbol {r}}_{0})={\begin{cases}\displaystyle {\frac {1}{r^{2}\sin \theta }}\delta (r-r_{0})\delta (\theta -\theta _{0})\delta (\phi -\phi _{0})&x_{0},y_{0},z_{0}\neq 0\\\displaystyle {\frac {1}{2\pi r^{2}\sin \theta }}\delta (r-r_{0})\delta (\theta -\theta _{0})&x_{0}=y_{0}=0,\ z_{0}\neq 0\\\displaystyle {\frac {1}{4\pi r^{2}}}\delta (r-r_{0})&x_{0}=y_{0}=z_{0}=0\end{cases}}} Derivatives [ edit ] The derivative of the Dirac delta distribution, denoted δ′ and also called the Dirac delta prime or Dirac delta derivative , is defined on compactly supported smooth test functions φ by [ 55 ] δ δ ′ [ φ φ ] = − − δ δ [ φ φ ′ ] = − − φ φ ′ ( 0 ) .

{\displaystyle \delta '[\varphi ]=-\delta [\varphi ']=-\varphi '(0).} The first equality here is a kind of integration by parts , for if δ were a true function then ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ′ ( x ) φ φ ( x ) d x = δ δ ( x ) φ φ ( x ) | − − ∞ ∞ ∞ ∞ − − ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( x ) φ φ ′ ( x ) d x = − − ∫ ∫ − − ∞ ∞ ∞ ∞ δ δ ( x ) φ φ ′ ( x ) d x = − − φ φ ′ ( 0 ) .

{\displaystyle \int _{-\infty }^{\infty }\delta '(x)\varphi (x)\,dx=\delta (x)\varphi (x)|_{-\infty }^{\infty }-\int _{-\infty }^{\infty }\delta (x)\varphi '(x)\,dx=-\int _{-\infty }^{\infty }\delta (x)\varphi '(x)\,dx=-\varphi '(0).} By mathematical induction , the k -th derivative of δ is defined similarly as the distribution given on test functions by δ δ ( k ) [ φ φ ] = ( − − 1 ) k φ φ ( k ) ( 0 ) .

{\displaystyle \delta ^{(k)}[\varphi ]=(-1)^{k}\varphi ^{(k)}(0).} In particular, δ is an infinitely differentiable distribution.

The first derivative of the delta function is the distributional limit of the difference quotients: [ 56 ] δ δ ′ ( x ) = lim h → → 0 δ δ ( x + h ) − − δ δ ( x ) h .

{\displaystyle \delta '(x)=\lim _{h\to 0}{\frac {\delta (x+h)-\delta (x)}{h}}.} More properly, one has δ δ ′ = lim h → → 0 1 h ( τ τ h δ δ − − δ δ ) {\displaystyle \delta '=\lim _{h\to 0}{\frac {1}{h}}(\tau _{h}\delta -\delta )} where τ h is the translation operator, defined on functions by τ h φ ( x ) = φ ( x + h ) , and on a distribution S by ( τ τ h S ) [ φ φ ] = S [ τ τ − − h φ φ ] .

{\displaystyle (\tau _{h}S)[\varphi ]=S[\tau _{-h}\varphi ].} In the theory of electromagnetism , the first derivative of the delta function represents a point magnetic dipole situated at the origin.  Accordingly, it is referred to as a dipole or the doublet function .

[ 57 ] The derivative of the delta function satisfies a number of basic properties, including: [ 58 ] δ δ ′ ( − − x ) = − − δ δ ′ ( x ) x δ δ ′ ( x ) = − − δ δ ( x ) {\displaystyle {\begin{aligned}\delta '(-x)&=-\delta '(x)\\x\delta '(x)&=-\delta (x)\end{aligned}}} which can be shown by applying a test function and integrating by parts.

The latter of these properties can also be demonstrated by applying distributional derivative definition, Leibniz 's theorem and linearity of inner product: [ 59 ] [ better source needed ] ⟨ ⟨ x δ δ ′ , φ φ ⟩ ⟩ = ⟨ ⟨ δ δ ′ , x φ φ ⟩ ⟩ = − − ⟨ ⟨ δ δ , ( x φ φ ) ′ ⟩ ⟩ = − − ⟨ ⟨ δ δ , x ′ φ φ + x φ φ ′ ⟩ ⟩ = − − ⟨ ⟨ δ δ , x ′ φ φ ⟩ ⟩ − − ⟨ ⟨ δ δ , x φ φ ′ ⟩ ⟩ = − − x ′ ( 0 ) φ φ ( 0 ) − − x ( 0 ) φ φ ′ ( 0 ) = − − x ′ ( 0 ) ⟨ ⟨ δ δ , φ φ ⟩ ⟩ − − x ( 0 ) ⟨ ⟨ δ δ , φ φ ′ ⟩ ⟩ = − − x ′ ( 0 ) ⟨ ⟨ δ δ , φ φ ⟩ ⟩ + x ( 0 ) ⟨ ⟨ δ δ ′ , φ φ ⟩ ⟩ = ⟨ ⟨ x ( 0 ) δ δ ′ − − x ′ ( 0 ) δ δ , φ φ ⟩ ⟩ ⟹ ⟹ x ( t ) δ δ ′ ( t ) = x ( 0 ) δ δ ′ ( t ) − − x ′ ( 0 ) δ δ ( t ) = − − x ′ ( 0 ) δ δ ( t ) = − − δ δ ( t ) {\displaystyle {\begin{aligned}\langle x\delta ',\varphi \rangle \,&=\,\langle \delta ',x\varphi \rangle \,=\,-\langle \delta ,(x\varphi )'\rangle \,=\,-\langle \delta ,x'\varphi +x\varphi '\rangle \,=\,-\langle \delta ,x'\varphi \rangle -\langle \delta ,x\varphi '\rangle \,=\,-x'(0)\varphi (0)-x(0)\varphi '(0)\\&=\,-x'(0)\langle \delta ,\varphi \rangle -x(0)\langle \delta ,\varphi '\rangle \,=\,-x'(0)\langle \delta ,\varphi \rangle +x(0)\langle \delta ',\varphi \rangle \,=\,\langle x(0)\delta '-x'(0)\delta ,\varphi \rangle \\\Longrightarrow x(t)\delta '(t)&=x(0)\delta '(t)-x'(0)\delta (t)=-x'(0)\delta (t)=-\delta (t)\end{aligned}}} Furthermore, the convolution of δ′ with a compactly-supported, smooth function f is δ δ ′ ∗ ∗ f = δ δ ∗ ∗ f ′ = f ′ , {\displaystyle \delta '*f=\delta *f'=f',} which follows from the properties of the distributional derivative of a convolution.

Higher dimensions [ edit ] More generally, on an open set U in the n -dimensional Euclidean space R n {\displaystyle \mathbb {R} ^{n}} , the Dirac delta distribution centered at a point a ∈ U is defined by [ 60 ] δ δ a [ φ φ ] = φ φ ( a ) {\displaystyle \delta _{a}[\varphi ]=\varphi (a)} for all φ φ ∈ ∈ C c ∞ ∞ ( U ) {\displaystyle \varphi \in C_{c}^{\infty }(U)} , the space of all smooth functions with compact support on U .  If α α = ( α α 1 , … … , α α n ) {\displaystyle \alpha =(\alpha _{1},\ldots ,\alpha _{n})} is any multi-index with | α α | = α α 1 + ⋯ ⋯ + α α n {\displaystyle |\alpha |=\alpha _{1}+\cdots +\alpha _{n}} and ∂ ∂ α α {\displaystyle \partial ^{\alpha }} denotes the associated mixed partial derivative operator, then the α -th derivative ∂ α δ a of δ a is given by [ 60 ] ⟨ ∂ ∂ α α δ δ a , φ φ ⟩ = ( − − 1 ) | α α | ⟨ δ δ a , ∂ ∂ α α φ φ ⟩ = ( − − 1 ) | α α | ∂ ∂ α α φ φ ( x ) | x = a for all φ φ ∈ ∈ C c ∞ ∞ ( U ) .

{\displaystyle \left\langle \partial ^{\alpha }\delta _{a},\,\varphi \right\rangle =(-1)^{|\alpha |}\left\langle \delta _{a},\partial ^{\alpha }\varphi \right\rangle =(-1)^{|\alpha |}\partial ^{\alpha }\varphi (x){\Big |}_{x=a}\quad {\text{ for all }}\varphi \in C_{c}^{\infty }(U).} That is, the α -th derivative of δ a is the distribution whose value on any test function φ is the α -th derivative of φ at a (with the appropriate positive or negative sign).

The first partial derivatives of the delta function are thought of as double layers along the coordinate planes.  More generally, the normal derivative of a simple layer supported on a surface is a double layer supported on that surface and represents a laminar magnetic monopole.  Higher derivatives of the delta function are known in physics as multipoles .

[ 61 ] Higher derivatives enter into mathematics naturally as the building blocks for the complete structure of distributions with point support.  If S is any distribution on U supported on the set { a } consisting of a single point, then there is an integer m and coefficients c α such that [ 60 ] [ 62 ] S = ∑ ∑ | α α | ≤ ≤ m c α α ∂ ∂ α α δ δ a .

{\displaystyle S=\sum _{|\alpha |\leq m}c_{\alpha }\partial ^{\alpha }\delta _{a}.} Representations [ edit ] The delta function can be viewed as the limit of a sequence of functions δ δ ( x ) = lim ε ε → → 0 + η η ε ε ( x ) .

{\displaystyle \delta (x)=\lim _{\varepsilon \to 0^{+}}\eta _{\varepsilon }(x).} This limit is meant in a weak sense: either that lim ε ε → → 0 + ∫ ∫ − − ∞ ∞ ∞ ∞ η η ε ε ( x ) f ( x ) d x = f ( 0 ) {\displaystyle \lim _{\varepsilon \to 0^{+}}\int _{-\infty }^{\infty }\eta _{\varepsilon }(x)f(x)\,dx=f(0)} 5 for all continuous functions f having compact support , or that this limit holds for all smooth functions f with compact support. The former is convergence in the vague topology of measures, and the latter is convergence in the sense of distributions .

Approximations to the identity [ edit ] An approximate delta function η ε can be constructed in the following manner.  Let η be an absolutely integrable function on R of total integral 1 , and define η η ε ε ( x ) = ε ε − − 1 η η ( x ε ε ) .

{\displaystyle \eta _{\varepsilon }(x)=\varepsilon ^{-1}\eta \left({\frac {x}{\varepsilon }}\right).} In n dimensions, one uses instead the scaling η η ε ε ( x ) = ε ε − − n η η ( x ε ε ) .

{\displaystyle \eta _{\varepsilon }(x)=\varepsilon ^{-n}\eta \left({\frac {x}{\varepsilon }}\right).} Then a simple change of variables shows that η ε also has integral 1 .  One may show that ( 5 ) holds for all continuous compactly supported functions f , [ 63 ] and so η ε converges weakly to δ in the sense of measures.

The η ε constructed in this way are known as an approximation to the identity .

[ 64 ] This terminology is because the space L 1 ( R ) of absolutely integrable functions is closed under the operation of convolution of functions: f ∗ g ∈ L 1 ( R ) whenever f and g are in L 1 ( R ) .  However, there is no identity in L 1 ( R ) for the convolution product: no element h such that f ∗ h = f for all f .  Nevertheless, the sequence η ε does approximate such an identity in the sense that f ∗ ∗ η η ε ε → → f as ε ε → → 0.

{\displaystyle f*\eta _{\varepsilon }\to f\quad {\text{as }}\varepsilon \to 0.} This limit holds in the sense of mean convergence (convergence in L 1 ).  Further conditions on the η ε , for instance that it be a mollifier associated to a compactly supported function, [ 65 ] are needed to ensure pointwise convergence almost everywhere .

If the initial η = η 1 is itself smooth and compactly supported then the sequence is called a mollifier .  The standard mollifier is obtained by choosing η to be a suitably normalized bump function , for instance η η ( x ) = { 1 I n exp ⁡ ⁡ ( − − 1 1 − − | x | 2 ) if | x | < 1 0 if | x | ≥ ≥ 1.

{\displaystyle \eta (x)={\begin{cases}{\frac {1}{I_{n}}}\exp {\Big (}-{\frac {1}{1-|x|^{2}}}{\Big )}&{\text{if }}|x|<1\\0&{\text{if }}|x|\geq 1.\end{cases}}} ( I n {\displaystyle I_{n}} ensuring that the total integral is 1).

In some situations such as numerical analysis , a piecewise linear approximation to the identity is desirable.  This can be obtained by taking η 1 to be a hat function .  With this choice of η 1 , one has η η ε ε ( x ) = ε ε − − 1 max ( 1 − − | x ε ε | , 0 ) {\displaystyle \eta _{\varepsilon }(x)=\varepsilon ^{-1}\max \left(1-\left|{\frac {x}{\varepsilon }}\right|,0\right)} which are all continuous and compactly supported, although not smooth and so not a mollifier.

Probabilistic considerations [ edit ] In the context of probability theory , it is natural to impose the additional condition that the initial η 1 in an approximation to the identity should be positive, as such a function then represents a probability distribution .  Convolution with a probability distribution is sometimes favorable because it does not result in overshoot or undershoot, as the output is a convex combination of the input values, and thus falls between the maximum and minimum of the input function.  Taking η 1 to be any probability distribution at all, and letting η ε ( x ) = η 1 ( x / ε )/ ε as above will give rise to an approximation to the identity.  In general this converges more rapidly to a delta function if, in addition, η has mean 0 and has small higher moments. For instance, if η 1 is the uniform distribution on [ − − 1 2 , 1 2 ] {\textstyle \left[-{\frac {1}{2}},{\frac {1}{2}}\right]} , also known as the rectangular function , then: [ 66 ] η η ε ε ( x ) = 1 ε ε rect ⁡ ⁡ ( x ε ε ) = { 1 ε ε , − − ε ε 2 < x < ε ε 2 , 0 , otherwise .

{\displaystyle \eta _{\varepsilon }(x)={\frac {1}{\varepsilon }}\operatorname {rect} \left({\frac {x}{\varepsilon }}\right)={\begin{cases}{\frac {1}{\varepsilon }},&-{\frac {\varepsilon }{2}}<x<{\frac {\varepsilon }{2}},\\0,&{\text{otherwise}}.\end{cases}}} Another example is with the Wigner semicircle distribution η η ε ε ( x ) = { 2 π π ε ε 2 ε ε 2 − − x 2 , − − ε ε < x < ε ε , 0 , otherwise .

{\displaystyle \eta _{\varepsilon }(x)={\begin{cases}{\frac {2}{\pi \varepsilon ^{2}}}{\sqrt {\varepsilon ^{2}-x^{2}}},&-\varepsilon <x<\varepsilon ,\\0,&{\text{otherwise}}.\end{cases}}} This is continuous and compactly supported, but not a mollifier because it is not smooth.

Semigroups [ edit ] Approximations to the delta functions often arise as convolution semigroups .

[ 67 ] This amounts to the further constraint that the convolution of η ε with η δ must satisfy η η ε ε ∗ ∗ η η δ δ = η η ε ε + δ δ {\displaystyle \eta _{\varepsilon }*\eta _{\delta }=\eta _{\varepsilon +\delta }} for all ε , δ > 0 . Convolution semigroups in L 1 that approximate the delta function are always an approximation to the identity in the above sense, however the semigroup condition is quite a strong restriction.

In practice, semigroups approximating the delta function arise as fundamental solutions or Green's functions to physically motivated elliptic or parabolic partial differential equations .  In the context of applied mathematics , semigroups arise as the output of a linear time-invariant system .  Abstractly, if A is a linear operator acting on functions of x , then a convolution semigroup arises by solving the initial value problem { ∂ ∂ ∂ ∂ t η η ( t , x ) = A η η ( t , x ) , t > 0 lim t → → 0 + η η ( t , x ) = δ δ ( x ) {\displaystyle {\begin{cases}{\dfrac {\partial }{\partial t}}\eta (t,x)=A\eta (t,x),\quad t>0\\[5pt]\displaystyle \lim _{t\to 0^{+}}\eta (t,x)=\delta (x)\end{cases}}} in which the limit is as usual understood in the weak sense.  Setting η ε ( x ) = η ( ε , x ) gives the associated approximate delta function.

Some examples of physically important convolution semigroups arising from such a fundamental solution include the following.

The heat kernel [ edit ] The heat kernel , defined by [ 68 ] η η ε ε ( x ) = 1 2 π π ε ε e − − x 2 2 ε ε {\displaystyle \eta _{\varepsilon }(x)={\frac {1}{\sqrt {2\pi \varepsilon }}}\mathrm {e} ^{-{\frac {x^{2}}{2\varepsilon }}}} represents the temperature in an infinite wire at time t > 0 , if a unit of heat energy is stored at the origin of the wire at time t = 0 . This semigroup evolves according to the one-dimensional heat equation : ∂ ∂ u ∂ ∂ t = 1 2 ∂ ∂ 2 u ∂ ∂ x 2 .

{\displaystyle {\frac {\partial u}{\partial t}}={\frac {1}{2}}{\frac {\partial ^{2}u}{\partial x^{2}}}.} In probability theory , η ε ( x ) is a normal distribution of variance ε and mean 0 . It represents the probability density at time t = ε of the position of a particle starting at the origin following a standard Brownian motion . In this context, the semigroup condition is then an expression of the Markov property of Brownian motion.

In higher-dimensional Euclidean space R n , the heat kernel is η η ε ε = 1 ( 2 π π ε ε ) n / 2 e − − x ⋅ ⋅ x 2 ε ε , {\displaystyle \eta _{\varepsilon }={\frac {1}{(2\pi \varepsilon )^{n/2}}}\mathrm {e} ^{-{\frac {x\cdot x}{2\varepsilon }}},} and has the same physical interpretation, mutatis mutandis . It also represents an approximation to the delta function in the sense that η ε → δ in the distribution sense as ε → 0 .

The Poisson kernel [ edit ] The Poisson kernel η η ε ε ( x ) = 1 π π I m { 1 x − − i ε ε } = 1 π π ε ε ε ε 2 + x 2 = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ e i ξ ξ x − − | ε ε ξ ξ | d ξ ξ {\displaystyle \eta _{\varepsilon }(x)={\frac {1}{\pi }}\mathrm {Im} \left\{{\frac {1}{x-\mathrm {i} \varepsilon }}\right\}={\frac {1}{\pi }}{\frac {\varepsilon }{\varepsilon ^{2}+x^{2}}}={\frac {1}{2\pi }}\int _{-\infty }^{\infty }\mathrm {e} ^{\mathrm {i} \xi x-|\varepsilon \xi |}\,d\xi } is the fundamental solution of the Laplace equation in the upper half-plane.

[ 69 ] It represents the electrostatic potential in a semi-infinite plate whose potential along the edge is held at fixed at the delta function. The Poisson kernel is also closely related to the Cauchy distribution and Epanechnikov and Gaussian kernel functions.

[ 70 ] This semigroup evolves according to the equation ∂ ∂ u ∂ ∂ t = − − ( − − ∂ ∂ 2 ∂ ∂ x 2 ) 1 2 u ( t , x ) {\displaystyle {\frac {\partial u}{\partial t}}=-\left(-{\frac {\partial ^{2}}{\partial x^{2}}}\right)^{\frac {1}{2}}u(t,x)} where the operator is rigorously defined as the Fourier multiplier F [ ( − − ∂ ∂ 2 ∂ ∂ x 2 ) 1 2 f ] ( ξ ξ ) = | 2 π π ξ ξ | F f ( ξ ξ ) .

{\displaystyle {\mathcal {F}}\left[\left(-{\frac {\partial ^{2}}{\partial x^{2}}}\right)^{\frac {1}{2}}f\right](\xi )=|2\pi \xi |{\mathcal {F}}f(\xi ).} Oscillatory integrals [ edit ] In areas of physics such as wave propagation and wave mechanics , the equations involved are hyperbolic and so may have more singular solutions.  As a result, the approximate delta functions that arise as fundamental solutions of the associated Cauchy problems are generally oscillatory integrals .  An example, which comes from a solution of the Euler–Tricomi equation of transonic gas dynamics , [ 71 ] is the rescaled Airy function ε ε − − 1 / 3 Ai ⁡ ⁡ ( x ε ε − − 1 / 3 ) .

{\displaystyle \varepsilon ^{-1/3}\operatorname {Ai} \left(x\varepsilon ^{-1/3}\right).} Although using the Fourier transform, it is easy to see that this generates a semigroup in some sense—it is not absolutely integrable and so cannot define a semigroup in the above strong sense.  Many approximate delta functions constructed as oscillatory integrals only converge in the sense of distributions (an example is the Dirichlet kernel below), rather than in the sense of measures.

Another example is the Cauchy problem for the wave equation in R 1+1 : [ 72 ] c − − 2 ∂ ∂ 2 u ∂ ∂ t 2 − − Δ Δ u = 0 u = 0 , ∂ ∂ u ∂ ∂ t = δ δ for t = 0.

{\displaystyle {\begin{aligned}c^{-2}{\frac {\partial ^{2}u}{\partial t^{2}}}-\Delta u&=0\\u=0,\quad {\frac {\partial u}{\partial t}}=\delta &\qquad {\text{for }}t=0.\end{aligned}}} The solution u represents the displacement from equilibrium of an infinite elastic string, with an initial disturbance at the origin.

Other approximations to the identity of this kind include the sinc function (used widely in electronics and telecommunications) η η ε ε ( x ) = 1 π π x sin ⁡ ⁡ ( x ε ε ) = 1 2 π π ∫ ∫ − − 1 ε ε 1 ε ε cos ⁡ ⁡ ( k x ) d k {\displaystyle \eta _{\varepsilon }(x)={\frac {1}{\pi x}}\sin \left({\frac {x}{\varepsilon }}\right)={\frac {1}{2\pi }}\int _{-{\frac {1}{\varepsilon }}}^{\frac {1}{\varepsilon }}\cos(kx)\,dk} and the Bessel function η η ε ε ( x ) = 1 ε ε J 1 ε ε ( x + 1 ε ε ) .

{\displaystyle \eta _{\varepsilon }(x)={\frac {1}{\varepsilon }}J_{\frac {1}{\varepsilon }}\left({\frac {x+1}{\varepsilon }}\right).} Plane wave decomposition [ edit ] One approach to the study of a linear partial differential equation L [ u ] = f , {\displaystyle L[u]=f,} where L is a differential operator on R n , is to seek first a fundamental solution, which is a solution of the equation L [ u ] = δ δ .

{\displaystyle L[u]=\delta .} When L is particularly simple, this problem can often be resolved using the Fourier transform directly (as in the case of the Poisson kernel and heat kernel already mentioned).  For more complicated operators, it is sometimes easier first to consider an equation of the form L [ u ] = h {\displaystyle L[u]=h} where h is a plane wave function, meaning that it has the form h = h ( x ⋅ ⋅ ξ ξ ) {\displaystyle h=h(x\cdot \xi )} for some vector ξ .  Such an equation can be resolved (if the coefficients of L are analytic functions ) by the Cauchy–Kovalevskaya theorem or (if the coefficients of L are constant) by quadrature.  So, if the delta function can be decomposed into plane waves, then one can in principle solve linear partial differential equations.

Such a decomposition of the delta function into plane waves was part of a general technique first introduced essentially by Johann Radon , and then developed in this form by Fritz John ( 1955 ).

[ 73 ] Choose k so that n + k is an even integer, and for a real number s , put g ( s ) = Re ⁡ ⁡ [ − − s k log ⁡ ⁡ ( − − i s ) k !

( 2 π π i ) n ] = { | s | k 4 k !

( 2 π π i ) n − − 1 n odd − − | s | k log ⁡ ⁡ | s | k !

( 2 π π i ) n n even.

{\displaystyle g(s)=\operatorname {Re} \left[{\frac {-s^{k}\log(-is)}{k!(2\pi i)^{n}}}\right]={\begin{cases}{\frac {|s|^{k}}{4k!(2\pi i)^{n-1}}}&n{\text{ odd}}\\[5pt]-{\frac {|s|^{k}\log |s|}{k!(2\pi i)^{n}}}&n{\text{ even.}}\end{cases}}} Then δ is obtained by applying a power of the Laplacian to the integral with respect to the unit sphere measure dω of g ( x · ξ ) for ξ in the unit sphere S n −1 : δ δ ( x ) = Δ Δ x ( n + k ) / 2 ∫ ∫ S n − − 1 g ( x ⋅ ⋅ ξ ξ ) d ω ω ξ ξ .

{\displaystyle \delta (x)=\Delta _{x}^{(n+k)/2}\int _{S^{n-1}}g(x\cdot \xi )\,d\omega _{\xi }.} The Laplacian here is interpreted as a weak derivative, so that this equation is taken to mean that, for any test function φ , φ φ ( x ) = ∫ ∫ R n φ φ ( y ) d y Δ Δ x n + k 2 ∫ ∫ S n − − 1 g ( ( x − − y ) ⋅ ⋅ ξ ξ ) d ω ω ξ ξ .

{\displaystyle \varphi (x)=\int _{\mathbf {R} ^{n}}\varphi (y)\,dy\,\Delta _{x}^{\frac {n+k}{2}}\int _{S^{n-1}}g((x-y)\cdot \xi )\,d\omega _{\xi }.} The result follows from the formula for the Newtonian potential (the fundamental solution of Poisson's equation). This is essentially a form of the inversion formula for the Radon transform because it recovers the value of φ ( x ) from its integrals over hyperplanes.

[ 74 ] For instance, if n is odd and k = 1 , then the integral on the right hand side is c n Δ Δ x n + 1 2 ∬ ∬ S n − − 1 φ φ ( y ) | ( y − − x ) ⋅ ⋅ ξ ξ | d ω ω ξ ξ d y = c n Δ Δ x ( n + 1 ) / 2 ∫ ∫ S n − − 1 d ω ω ξ ξ ∫ ∫ − − ∞ ∞ ∞ ∞ | p | R φ φ ( ξ ξ , p + x ⋅ ⋅ ξ ξ ) d p {\displaystyle {\begin{aligned}&c_{n}\Delta _{x}^{\frac {n+1}{2}}\iint _{S^{n-1}}\varphi (y)|(y-x)\cdot \xi |\,d\omega _{\xi }\,dy\\[5pt]&\qquad =c_{n}\Delta _{x}^{(n+1)/2}\int _{S^{n-1}}\,d\omega _{\xi }\int _{-\infty }^{\infty }|p|R\varphi (\xi ,p+x\cdot \xi )\,dp\end{aligned}}} where Rφ ( ξ , p ) is the Radon transform of φ : R φ φ ( ξ ξ , p ) = ∫ ∫ x ⋅ ⋅ ξ ξ = p f ( x ) d n − − 1 x .

{\displaystyle R\varphi (\xi ,p)=\int _{x\cdot \xi =p}f(x)\,d^{n-1}x.} An alternative equivalent expression of the plane wave decomposition is: [ 75 ] δ δ ( x ) = { ( n − − 1 ) !

( 2 π π i ) n ∫ ∫ S n − − 1 ( x ⋅ ⋅ ξ ξ ) − − n d ω ω ξ ξ n even 1 2 ( 2 π π i ) n − − 1 ∫ ∫ S n − − 1 δ δ ( n − − 1 ) ( x ⋅ ⋅ ξ ξ ) d ω ω ξ ξ n odd .

{\displaystyle \delta (x)={\begin{cases}{\frac {(n-1)!}{(2\pi i)^{n}}}\displaystyle \int _{S^{n-1}}(x\cdot \xi )^{-n}\,d\omega _{\xi }&n{\text{ even}}\\{\frac {1}{2(2\pi i)^{n-1}}}\displaystyle \int _{S^{n-1}}\delta ^{(n-1)}(x\cdot \xi )\,d\omega _{\xi }&n{\text{ odd}}.\end{cases}}} Fourier transform [ edit ] The delta function is a tempered distribution , and therefore it has a well-defined Fourier transform .  Formally, one finds [ 76 ] δ δ ^ ^ ( ξ ξ ) = ∫ ∫ − − ∞ ∞ ∞ ∞ e − − 2 π π i x ξ ξ δ δ ( x ) d x = 1.

{\displaystyle {\widehat {\delta }}(\xi )=\int _{-\infty }^{\infty }e^{-2\pi ix\xi }\,\delta (x)dx=1.} Properly speaking, the Fourier transform of a distribution is defined by imposing self-adjointness of the Fourier transform under the duality pairing ⟨ ⟨ ⋅ ⋅ , ⋅ ⋅ ⟩ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } of tempered distributions with Schwartz functions .  Thus δ δ ^ ^ {\displaystyle {\widehat {\delta }}} is defined as the unique tempered distribution satisfying ⟨ ⟨ δ δ ^ ^ , φ φ ⟩ ⟩ = ⟨ ⟨ δ δ , φ φ ^ ^ ⟩ ⟩ {\displaystyle \langle {\widehat {\delta }},\varphi \rangle =\langle \delta ,{\widehat {\varphi }}\rangle } for all Schwartz functions φ . And indeed it follows from this that δ δ ^ ^ = 1.

{\displaystyle {\widehat {\delta }}=1.} As a result of this identity, the convolution of the delta function with any other tempered distribution S is simply S : S ∗ ∗ δ δ = S .

{\displaystyle S*\delta =S.} That is to say that δ is an identity element for the convolution on tempered distributions, and in fact, the space of compactly supported distributions under convolution is an associative algebra with identity the delta function.  This property is fundamental in signal processing , as convolution with a tempered distribution is a linear time-invariant system , and applying the linear time-invariant system measures its impulse response .  The impulse response can be computed to any desired degree of accuracy by choosing a suitable approximation for δ , and once it is known, it characterizes the system completely. See LTI system theory § Impulse response and convolution .

The inverse Fourier transform of the tempered distribution f ( ξ ) = 1 is the delta function. Formally, this is expressed as ∫ ∫ − − ∞ ∞ ∞ ∞ 1 ⋅ ⋅ e 2 π π i x ξ ξ d ξ ξ = δ δ ( x ) {\displaystyle \int _{-\infty }^{\infty }1\cdot e^{2\pi ix\xi }\,d\xi =\delta (x)} and more rigorously, it follows since ⟨ ⟨ 1 , f ^ ^ ⟩ ⟩ = f ( 0 ) = ⟨ ⟨ δ δ , f ⟩ ⟩ {\displaystyle \langle 1,{\widehat {f}}\rangle =f(0)=\langle \delta ,f\rangle } for all Schwartz functions f .

In these terms, the delta function provides a suggestive statement of the orthogonality property of the Fourier kernel on R .  Formally, one has ∫ ∫ − − ∞ ∞ ∞ ∞ e i 2 π π ξ ξ 1 t [ e i 2 π π ξ ξ 2 t ] ∗ ∗ d t = ∫ ∫ − − ∞ ∞ ∞ ∞ e − − i 2 π π ( ξ ξ 2 − − ξ ξ 1 ) t d t = δ δ ( ξ ξ 2 − − ξ ξ 1 ) .

{\displaystyle \int _{-\infty }^{\infty }e^{i2\pi \xi _{1}t}\left[e^{i2\pi \xi _{2}t}\right]^{*}\,dt=\int _{-\infty }^{\infty }e^{-i2\pi (\xi _{2}-\xi _{1})t}\,dt=\delta (\xi _{2}-\xi _{1}).} This is, of course, shorthand for the assertion that the Fourier transform of the tempered distribution f ( t ) = e i 2 π π ξ ξ 1 t {\displaystyle f(t)=e^{i2\pi \xi _{1}t}} is f ^ ^ ( ξ ξ 2 ) = δ δ ( ξ ξ 1 − − ξ ξ 2 ) {\displaystyle {\widehat {f}}(\xi _{2})=\delta (\xi _{1}-\xi _{2})} which again follows by imposing self-adjointness of the Fourier transform.

By analytic continuation of the Fourier transform, the Laplace transform of the delta function is found to be [ 77 ] ∫ ∫ 0 ∞ ∞ δ δ ( t − − a ) e − − s t d t = e − − s a .

{\displaystyle \int _{0}^{\infty }\delta (t-a)\,e^{-st}\,dt=e^{-sa}.} Fourier kernels [ edit ] See also: Convergence of Fourier series In the study of Fourier series , a major question consists of determining whether and in what sense the Fourier series associated with a periodic function converges to the function.  The n -th partial sum of the Fourier series of a function f of period 2π is defined by convolution (on the interval [−π,π] ) with the Dirichlet kernel : D N ( x ) = ∑ ∑ n = − − N N e i n x = sin ⁡ ⁡ ( ( N + 1 2 ) x ) sin ⁡ ⁡ ( x / 2 ) .

{\displaystyle D_{N}(x)=\sum _{n=-N}^{N}e^{inx}={\frac {\sin \left(\left(N+{\frac {1}{2}}\right)x\right)}{\sin(x/2)}}.} Thus, s N ( f ) ( x ) = D N ∗ ∗ f ( x ) = ∑ ∑ n = − − N N a n e i n x {\displaystyle s_{N}(f)(x)=D_{N}*f(x)=\sum _{n=-N}^{N}a_{n}e^{inx}} where a n = 1 2 π π ∫ ∫ − − π π π π f ( y ) e − − i n y d y .

{\displaystyle a_{n}={\frac {1}{2\pi }}\int _{-\pi }^{\pi }f(y)e^{-iny}\,dy.} A fundamental result of elementary Fourier series states that the Dirichlet kernel restricted to the interval [−π,π] tends to a multiple of the delta function as N → ∞ .  This is interpreted in the distribution sense, that s N ( f ) ( 0 ) = ∫ ∫ − − π π π π D N ( x ) f ( x ) d x → → 2 π π f ( 0 ) {\displaystyle s_{N}(f)(0)=\int _{-\pi }^{\pi }D_{N}(x)f(x)\,dx\to 2\pi f(0)} for every compactly supported smooth function f .  Thus, formally one has δ δ ( x ) = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ e i n x {\displaystyle \delta (x)={\frac {1}{2\pi }}\sum _{n=-\infty }^{\infty }e^{inx}} on the interval [−π,π] .

Despite this, the result does not hold for all compactly supported continuous functions: that is D N does not converge weakly in the sense of measures. The lack of convergence of the Fourier series has led to the introduction of a variety of summability methods to produce convergence.  The method of Cesàro summation leads to the Fejér kernel [ 78 ] F N ( x ) = 1 N ∑ ∑ n = 0 N − − 1 D n ( x ) = 1 N ( sin ⁡ ⁡ N x 2 sin ⁡ ⁡ x 2 ) 2 .

{\displaystyle F_{N}(x)={\frac {1}{N}}\sum _{n=0}^{N-1}D_{n}(x)={\frac {1}{N}}\left({\frac {\sin {\frac {Nx}{2}}}{\sin {\frac {x}{2}}}}\right)^{2}.} The Fejér kernels tend to the delta function in a stronger sense that [ 79 ] ∫ ∫ − − π π π π F N ( x ) f ( x ) d x → → 2 π π f ( 0 ) {\displaystyle \int _{-\pi }^{\pi }F_{N}(x)f(x)\,dx\to 2\pi f(0)} for every compactly supported continuous function f .  The implication is that the Fourier series of any continuous function is Cesàro summable to the value of the function at every point.

Hilbert space theory [ edit ] The Dirac delta distribution is a densely defined unbounded linear functional on the Hilbert space L 2 of square-integrable functions .

[ 80 ] Indeed, smooth compactly supported functions are dense in L 2 , and the action of the delta distribution on such functions is well-defined.  In many applications, it is possible to identify subspaces of L 2 and to give a stronger topology on which the delta function defines a bounded linear functional .

Sobolev spaces [ edit ] The Sobolev embedding theorem for Sobolev spaces on the real line R implies that any square-integrable function f such that ‖ ‖ f ‖ ‖ H 1 2 = ∫ ∫ − − ∞ ∞ ∞ ∞ | f ^ ^ ( ξ ξ ) | 2 ( 1 + | ξ ξ | 2 ) d ξ ξ < ∞ ∞ {\displaystyle \|f\|_{H^{1}}^{2}=\int _{-\infty }^{\infty }|{\widehat {f}}(\xi )|^{2}(1+|\xi |^{2})\,d\xi <\infty } is automatically continuous, and satisfies in particular δ δ [ f ] = | f ( 0 ) | < C ‖ ‖ f ‖ ‖ H 1 .

{\displaystyle \delta [f]=|f(0)|<C\|f\|_{H^{1}}.} Thus δ is a bounded linear functional on the Sobolev space H 1 .

[ 81 ] Equivalently δ is an element of the continuous dual space H −1 of H 1 . More generally, in n dimensions, one has δ ∈ H − s ( R n ) provided s > ⁠ n / 2 ⁠ .

Spaces of holomorphic functions [ edit ] In complex analysis , the delta function enters via Cauchy's integral formula , which asserts that if D is a domain in the complex plane with smooth boundary, then f ( z ) = 1 2 π π i ∮ ∮ ∂ ∂ D f ( ζ ζ ) d ζ ζ ζ ζ − − z , z ∈ ∈ D {\displaystyle f(z)={\frac {1}{2\pi i}}\oint _{\partial D}{\frac {f(\zeta )\,d\zeta }{\zeta -z}},\quad z\in D} for all holomorphic functions f in D that are continuous on the closure of D .  As a result, the delta function δ z is represented in this class of holomorphic functions by the Cauchy integral: δ δ z [ f ] = f ( z ) = 1 2 π π i ∮ ∮ ∂ ∂ D f ( ζ ζ ) d ζ ζ ζ ζ − − z .

{\displaystyle \delta _{z}[f]=f(z)={\frac {1}{2\pi i}}\oint _{\partial D}{\frac {f(\zeta )\,d\zeta }{\zeta -z}}.} Moreover, let H 2 (∂ D ) be the Hardy space consisting of the closure in L 2 (∂ D ) of all holomorphic functions in D continuous up to the boundary of D .  Then functions in H 2 (∂ D ) uniquely extend to holomorphic functions in D , and the Cauchy integral formula continues to hold.  In particular for z ∈ D , the delta function δ z is a continuous linear functional on H 2 (∂ D ) .  This is a special case of the situation in several complex variables in which, for smooth domains D , the Szegő kernel plays the role of the Cauchy integral.

[ 82 ] Another representation of the delta function in a space of holomorphic functions is on the space H ( D ) ∩ ∩ L 2 ( D ) {\displaystyle H(D)\cap L^{2}(D)} of square-integrable holomorphic functions in an open set D ⊂ ⊂ C n {\displaystyle D\subset \mathbb {C} ^{n}} .  This is a closed subspace of L 2 ( D ) {\displaystyle L^{2}(D)} , and therefore is a Hilbert space.  On the other hand, the functional that evaluates a holomorphic function in H ( D ) ∩ ∩ L 2 ( D ) {\displaystyle H(D)\cap L^{2}(D)} at a point z {\displaystyle z} of D {\displaystyle D} is a continuous functional, and so by the Riesz representation theorem, is represented by integration against a kernel K z ( ζ ζ ) {\displaystyle K_{z}(\zeta )} , the Bergman kernel .

[ 83 ] This kernel is the analog of the delta function in this Hilbert space.  A Hilbert space having such a kernel is called a reproducing kernel Hilbert space .  In the special case of the unit disc, one has δ δ w [ f ] = f ( w ) = 1 π π ∬ ∬ | z | < 1 f ( z ) d x d y ( 1 − − z ¯ ¯ w ) 2 .

{\displaystyle \delta _{w}[f]=f(w)={\frac {1}{\pi }}\iint _{|z|<1}{\frac {f(z)\,dx\,dy}{(1-{\bar {z}}w)^{2}}}.} Resolutions of the identity [ edit ] Given a complete orthonormal basis set of functions { φ n } in a separable Hilbert space, for example, the normalized eigenvectors of a compact self-adjoint operator , any vector f can be expressed as f = ∑ ∑ n = 1 ∞ ∞ α α n φ φ n .

{\displaystyle f=\sum _{n=1}^{\infty }\alpha _{n}\varphi _{n}.} The coefficients {α n } are found as α α n = ⟨ ⟨ φ φ n , f ⟩ ⟩ , {\displaystyle \alpha _{n}=\langle \varphi _{n},f\rangle ,} which may be represented by the notation: α α n = φ φ n † † f , {\displaystyle \alpha _{n}=\varphi _{n}^{\dagger }f,} a form of the bra–ket notation of Dirac.

[ 84 ] Adopting this notation, the expansion of f takes the dyadic form: [ 85 ] f = ∑ ∑ n = 1 ∞ ∞ φ φ n ( φ φ n † † f ) .

{\displaystyle f=\sum _{n=1}^{\infty }\varphi _{n}\left(\varphi _{n}^{\dagger }f\right).} Letting I denote the identity operator on the Hilbert space, the expression I = ∑ ∑ n = 1 ∞ ∞ φ φ n φ φ n † † , {\displaystyle I=\sum _{n=1}^{\infty }\varphi _{n}\varphi _{n}^{\dagger },} is called a resolution of the identity . When the Hilbert space is the space L 2 ( D ) of square-integrable functions on a domain D , the quantity: φ φ n φ φ n † † , {\displaystyle \varphi _{n}\varphi _{n}^{\dagger },} is an integral operator, and the expression for f can be rewritten f ( x ) = ∑ ∑ n = 1 ∞ ∞ ∫ ∫ D ( φ φ n ( x ) φ φ n ∗ ∗ ( ξ ξ ) ) f ( ξ ξ ) d ξ ξ .

{\displaystyle f(x)=\sum _{n=1}^{\infty }\int _{D}\,\left(\varphi _{n}(x)\varphi _{n}^{*}(\xi )\right)f(\xi )\,d\xi .} The right-hand side converges to f in the L 2 sense.  It need not hold in a pointwise sense, even when f is a continuous function.  Nevertheless, it is common to abuse notation and write f ( x ) = ∫ ∫ δ δ ( x − − ξ ξ ) f ( ξ ξ ) d ξ ξ , {\displaystyle f(x)=\int \,\delta (x-\xi )f(\xi )\,d\xi ,} resulting in the representation of the delta function: [ 86 ] δ δ ( x − − ξ ξ ) = ∑ ∑ n = 1 ∞ ∞ φ φ n ( x ) φ φ n ∗ ∗ ( ξ ξ ) .

{\displaystyle \delta (x-\xi )=\sum _{n=1}^{\infty }\varphi _{n}(x)\varphi _{n}^{*}(\xi ).} With a suitable rigged Hilbert space (Φ, L 2 ( D ), Φ*) where Φ ⊂ L 2 ( D ) contains all compactly supported smooth functions, this summation may converge in Φ* , depending on the properties of the basis φ n .  In most cases of practical interest, the orthonormal basis comes from an integral or differential operator (e.g. the heat kernel ), in which case the series converges in the distribution sense.

[ 87 ] Infinitesimal delta functions [ edit ] Cauchy used an infinitesimal α to write down a unit impulse, infinitely tall and narrow Dirac-type delta function δ α satisfying ∫ ∫ F ( x ) δ δ α α ( x ) d x = F ( 0 ) {\textstyle \int F(x)\delta _{\alpha }(x)\,dx=F(0)} in a number of articles in 1827.

[ 88 ] Cauchy defined an infinitesimal in Cours d'Analyse (1827) in terms of a sequence tending to zero.  Namely, such a null sequence becomes an infinitesimal in Cauchy's and Lazare Carnot 's terminology.

Non-standard analysis allows one to rigorously treat infinitesimals. The article by Yamashita (2007) contains a bibliography on modern Dirac delta functions in the context of an infinitesimal-enriched continuum provided by the hyperreals .  Here the Dirac delta can be given by an actual function, having the property that for every real function F one has ∫ ∫ F ( x ) δ δ α α ( x ) d x = F ( 0 ) {\textstyle \int F(x)\delta _{\alpha }(x)\,dx=F(0)} as anticipated by Fourier and Cauchy.

Dirac comb [ edit ] Main article: Dirac comb A Dirac comb is an infinite series of Dirac delta functions spaced at intervals of T A so-called uniform "pulse train" of Dirac delta measures, which is known as a Dirac comb , or as the Sha distribution, creates a sampling function, often used in digital signal processing (DSP) and discrete time signal analysis.  The Dirac comb is given as the infinite sum , whose limit is understood in the distribution sense, Ш ⁡ ⁡ ( x ) = ∑ ∑ n = − − ∞ ∞ ∞ ∞ δ δ ( x − − n ) , {\displaystyle \operatorname {\text{Ш}} (x)=\sum _{n=-\infty }^{\infty }\delta (x-n),} which is a sequence of point masses at each of the integers.

Up to an overall normalizing constant, the Dirac comb is equal to its own Fourier transform.  This is significant because if f is any Schwartz function , then the periodization of f is given by the convolution ( f ∗ ∗ Ш ) ( x ) = ∑ ∑ n = − − ∞ ∞ ∞ ∞ f ( x − − n ) .

{\displaystyle (f*\operatorname {\text{Ш}} )(x)=\sum _{n=-\infty }^{\infty }f(x-n).} In particular, ( f ∗ ∗ Ш ) ∧ ∧ = f ^ ^ Ш ^ ^ = f ^ ^ Ш {\displaystyle (f*\operatorname {\text{Ш}} )^{\wedge }={\widehat {f}}{\widehat {\operatorname {\text{Ш}} }}={\widehat {f}}\operatorname {\text{Ш}} } is precisely the Poisson summation formula .

[ 89 ] [ 90 ] More generally, this formula remains to be true if f is a tempered distribution of rapid descent or, equivalently, if f ^ ^ {\displaystyle {\widehat {f}}} is a slowly growing, ordinary function within the space of tempered distributions.

Sokhotski–Plemelj theorem [ edit ] The Sokhotski–Plemelj theorem , important in quantum mechanics, relates the delta function to the distribution p.v.

⁠ 1 / x ⁠ , the Cauchy principal value of the function ⁠ 1 / x ⁠ , defined by ⟨ p .

v .

⁡ ⁡ 1 x , φ φ ⟩ = lim ε ε → → 0 + ∫ ∫ | x | > ε ε φ φ ( x ) x d x .

{\displaystyle \left\langle \operatorname {p.v.} {\frac {1}{x}},\varphi \right\rangle =\lim _{\varepsilon \to 0^{+}}\int _{|x|>\varepsilon }{\frac {\varphi (x)}{x}}\,dx.} Sokhotsky's formula states that [ 91 ] lim ε ε → → 0 + 1 x ± ± i ε ε = p .

v .

⁡ ⁡ 1 x ∓ ∓ i π π δ δ ( x ) , {\displaystyle \lim _{\varepsilon \to 0^{+}}{\frac {1}{x\pm i\varepsilon }}=\operatorname {p.v.} {\frac {1}{x}}\mp i\pi \delta (x),} Here the limit is understood in the distribution sense, that for all compactly supported smooth functions f , ∫ ∫ − − ∞ ∞ ∞ ∞ lim ε ε → → 0 + f ( x ) x ± ± i ε ε d x = ∓ ∓ i π π f ( 0 ) + lim ε ε → → 0 + ∫ ∫ | x | > ε ε f ( x ) x d x .

{\displaystyle \int _{-\infty }^{\infty }\lim _{\varepsilon \to 0^{+}}{\frac {f(x)}{x\pm i\varepsilon }}\,dx=\mp i\pi f(0)+\lim _{\varepsilon \to 0^{+}}\int _{|x|>\varepsilon }{\frac {f(x)}{x}}\,dx.} Relationship to the Kronecker delta [ edit ] The Kronecker delta δ ij is the quantity defined by δ δ i j = { 1 i = j 0 i ≠ j {\displaystyle \delta _{ij}={\begin{cases}1&i=j\\0&i\not =j\end{cases}}} for all integers i , j .  This function then satisfies the following analog of the sifting property: if a i (for i in the set of all integers) is any doubly infinite sequence , then ∑ ∑ i = − − ∞ ∞ ∞ ∞ a i δ δ i k = a k .

{\displaystyle \sum _{i=-\infty }^{\infty }a_{i}\delta _{ik}=a_{k}.} Similarly, for any real or complex valued continuous function f on R , the Dirac delta satisfies the sifting property ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x ) δ δ ( x − − x 0 ) d x = f ( x 0 ) .

{\displaystyle \int _{-\infty }^{\infty }f(x)\delta (x-x_{0})\,dx=f(x_{0}).} This exhibits the Kronecker delta function as a discrete analog of the Dirac delta function.

[ 92 ] Applications [ edit ] Probability theory [ edit ] See also: Probability distribution § Dirac delta representation In probability theory and statistics , the Dirac delta function is often used to represent a discrete distribution , or a partially discrete, partially continuous distribution , using a probability density function (which is normally used to represent absolutely continuous distributions).  For example, the probability density function f ( x ) of a discrete distribution consisting of points x = { x 1 , ..., x n } , with corresponding probabilities p 1 , ..., p n , can be written as [ 93 ] f ( x ) = ∑ ∑ i = 1 n p i δ δ ( x − − x i ) .

{\displaystyle f(x)=\sum _{i=1}^{n}p_{i}\delta (x-x_{i}).} As another example, consider a distribution in which 6/10 of the time returns a standard normal distribution , and 4/10 of the time returns exactly the value 3.5 (i.e. a partly continuous, partly discrete mixture distribution ).  The density function of this distribution can be written as f ( x ) = 0.6 1 2 π π e − − x 2 2 + 0.4 δ δ ( x − − 3.5 ) .

{\displaystyle f(x)=0.6\,{\frac {1}{\sqrt {2\pi }}}e^{-{\frac {x^{2}}{2}}}+0.4\,\delta (x-3.5).} The delta function is also used to represent the resulting probability density function of a random variable that is transformed by continuously differentiable function. If Y = g( X ) is a continuous differentiable function, then the density of Y can be written as f Y ( y ) = ∫ ∫ − − ∞ ∞ + ∞ ∞ f X ( x ) δ δ ( y − − g ( x ) ) d x .

{\displaystyle f_{Y}(y)=\int _{-\infty }^{+\infty }f_{X}(x)\delta (y-g(x))\,dx.} The delta function is also used in a completely different way to represent the local time of a diffusion process (like Brownian motion ).

[ 94 ] The local time of a stochastic process B ( t ) is given by ℓ ℓ ( x , t ) = ∫ ∫ 0 t δ δ ( x − − B ( s ) ) d s {\displaystyle \ell (x,t)=\int _{0}^{t}\delta (x-B(s))\,ds} and represents the amount of time that the process spends at the point x in the range of the process.  More precisely, in one dimension this integral can be written ℓ ℓ ( x , t ) = lim ε ε → → 0 + 1 2 ε ε ∫ ∫ 0 t 1 [ x − − ε ε , x + ε ε ] ( B ( s ) ) d s {\displaystyle \ell (x,t)=\lim _{\varepsilon \to 0^{+}}{\frac {1}{2\varepsilon }}\int _{0}^{t}\mathbf {1} _{[x-\varepsilon ,x+\varepsilon ]}(B(s))\,ds} where 1 [ x − − ε ε , x + ε ε ] {\displaystyle \mathbf {1} _{[x-\varepsilon ,x+\varepsilon ]}} is the indicator function of the interval [ x − − ε ε , x + ε ε ] .

{\displaystyle [x-\varepsilon ,x+\varepsilon ].} Quantum mechanics [ edit ] The delta function is expedient in quantum mechanics . The wave function of a particle gives the probability amplitude of finding a particle within a given region of space.  Wave functions are assumed to be elements of the Hilbert space L 2 of square-integrable functions , and the total probability of finding a particle within a given interval is the integral of the magnitude of the wave function squared over the interval. A set { | φ n ⟩ } of wave functions is orthonormal if ⟨ ⟨ φ φ n ∣ ∣ φ φ m ⟩ ⟩ = δ δ n m , {\displaystyle \langle \varphi _{n}\mid \varphi _{m}\rangle =\delta _{nm},} where δ nm is the Kronecker delta.  A set of orthonormal wave functions is complete in the space of square-integrable functions if any wave function |ψ⟩ can be expressed as a linear combination of the { | φ n ⟩ } with complex coefficients: ψ ψ = ∑ ∑ c n φ φ n , {\displaystyle \psi =\sum c_{n}\varphi _{n},} where c n = ⟨ φ n | ψ ⟩ .  Complete orthonormal systems of wave functions appear naturally as the eigenfunctions of the Hamiltonian (of a bound system ) in quantum mechanics that measures the energy levels, which are called the eigenvalues.  The set of eigenvalues, in this case, is known as the spectrum of the Hamiltonian.  In bra–ket notation this equality implies the resolution of the identity : I = ∑ ∑ | φ φ n ⟩ ⟩ ⟨ ⟨ φ φ n | .

{\displaystyle I=\sum |\varphi _{n}\rangle \langle \varphi _{n}|.} Here the eigenvalues are assumed to be discrete, but the set of eigenvalues of an observable can also be continuous.  An example is the position operator , Qψ ( x ) = x ψ( x ) .  The spectrum of the position (in one dimension) is the entire real line and is called a continuous spectrum .  However, unlike the Hamiltonian, the position operator lacks proper eigenfunctions.  The conventional way to overcome this shortcoming is to widen the class of available functions by allowing distributions as well, i.e., to replace the Hilbert space with a rigged Hilbert space .

[ 95 ] In this context, the position operator has a complete set of generalized eigenfunctions , [ 96 ] labeled by the points y of the real line, given by φ φ y ( x ) = δ δ ( x − − y ) .

{\displaystyle \varphi _{y}(x)=\delta (x-y).} The generalized eigenfunctions of the position operator are called the eigenkets and are denoted by φ y = | y ⟩ .

[ 97 ] Similar considerations apply to any other (unbounded) self-adjoint operator with continuous spectrum and no degenerate eigenvalues, such as the momentum operator P . In that case, there is a set Ω of real numbers (the spectrum) and a collection of distributions φ y with y ∈ Ω such that P φ φ y = y φ φ y .

{\displaystyle P\varphi _{y}=y\varphi _{y}.} That is, φ y are the generalized eigenvectors of P . If they form an "orthonormal basis" in the distribution sense, that is: ⟨ ⟨ φ φ y , φ φ y ′ ⟩ ⟩ = δ δ ( y − − y ′ ) , {\displaystyle \langle \varphi _{y},\varphi _{y'}\rangle =\delta (y-y'),} then for any test function ψ , ψ ψ ( x ) = ∫ ∫ Ω Ω c ( y ) φ φ y ( x ) d y {\displaystyle \psi (x)=\int _{\Omega }c(y)\varphi _{y}(x)\,dy} where c ( y ) = ⟨ ψ , φ y ⟩ . That is, there is a resolution of the identity I = ∫ ∫ Ω Ω | φ φ y ⟩ ⟩ ⟨ ⟨ φ φ y | d y {\displaystyle I=\int _{\Omega }|\varphi _{y}\rangle \,\langle \varphi _{y}|\,dy} where the operator-valued integral is again understood in the weak sense.  If the spectrum of P has both continuous and discrete parts, then the resolution of the identity involves a summation over the discrete spectrum and an integral over the continuous spectrum.

The delta function also has many more specialized applications in quantum mechanics, such as the delta potential models for a single and double potential well.

Structural mechanics [ edit ] The delta function can be used in structural mechanics to describe transient loads or point loads acting on structures. The governing equation of a simple mass–spring system excited by a sudden force impulse I at time t = 0 can be written [ 98 ] [ 99 ] m d 2 ξ ξ d t 2 + k ξ ξ = I δ δ ( t ) , {\displaystyle m{\frac {d^{2}\xi }{dt^{2}}}+k\xi =I\delta (t),} where m is the mass, ξ is the deflection, and k is the spring constant .

As another example, the equation governing the static deflection of a slender beam is, according to Euler–Bernoulli theory , E I d 4 w d x 4 = q ( x ) , {\displaystyle EI{\frac {d^{4}w}{dx^{4}}}=q(x),} where EI is the bending stiffness of the beam, w is the deflection , x is the spatial coordinate, and q ( x ) is the load distribution. If a beam is loaded by a point force F at x = x 0 , the load distribution is written q ( x ) = F δ δ ( x − − x 0 ) .

{\displaystyle q(x)=F\delta (x-x_{0}).} As the integration of the delta function results in the Heaviside step function , it follows that the static deflection of a slender beam subject to multiple point loads is described by a set of piecewise polynomials .

Also, a point moment acting on a beam can be described by delta functions. Consider two opposing point forces F at a distance d apart. They then produce a moment M = Fd acting on the beam. Now, let the distance d approach the limit zero, while M is kept constant. The load distribution, assuming a clockwise moment acting at x = 0 , is written q ( x ) = lim d → → 0 ( F δ δ ( x ) − − F δ δ ( x − − d ) ) = lim d → → 0 ( M d δ δ ( x ) − − M d δ δ ( x − − d ) ) = M lim d → → 0 δ δ ( x ) − − δ δ ( x − − d ) d = M δ δ ′ ( x ) .

{\displaystyle {\begin{aligned}q(x)&=\lim _{d\to 0}{\Big (}F\delta (x)-F\delta (x-d){\Big )}\\[4pt]&=\lim _{d\to 0}\left({\frac {M}{d}}\delta (x)-{\frac {M}{d}}\delta (x-d)\right)\\[4pt]&=M\lim _{d\to 0}{\frac {\delta (x)-\delta (x-d)}{d}}\\[4pt]&=M\delta '(x).\end{aligned}}} Point moments can thus be represented by the derivative of the delta function. Integration of the beam equation again results in piecewise polynomial deflection.

See also [ edit ] Atom (measure theory) Degenerate distribution Laplacian of the indicator Uncertainty principle Notes [ edit ] ^ Jeffrey 1993 , p. 639.

^ Arfken & Weber 2000 , p. 84.

^ a b c d Dirac 1930 , §22 The δ function.

^ a b c Gelfand & Shilov 1966–1968 , Volume I, §1.1.

^ Zhao 2011 , p.

174 .

^ Bracewell 2000 , p. 74.

^ Snieder 2004 , p.

212 .

^ Schwartz 1950 , p. 19.

^ Schwartz 1950 , p. 5.

^ Jackson, J. D. (2008-08-01).

"Examples of the zeroth theorem of the history of science" .

American Journal of Physics .

76 (8): 704– 719.

arXiv : 0708.4249 .

Bibcode : 2008AmJPh..76..704J .

doi : 10.1119/1.2904468 .

ISSN 0002-9505 .

^ Laugwitz 1989 , p. 230.

^ A more complete historical account can be found in van der Pol & Bremmer 1987 , §V.4.

^ Dirac, P. A. M. (January 1927).

"The physical interpretation of the quantum dynamics" .

Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character .

113 (765): 621– 641.

Bibcode : 1927RSPSA.113..621D .

doi : 10.1098/rspa.1927.0012 .

ISSN 0950-1207 .

S2CID 122855515 .

^ Zee, Anthony (2013).

Einstein Gravity in a Nutshell . In a Nutshell Series (1st ed.). Princeton: Princeton University Press.

ISBN 978-0-691-14558-7 .

^ Fourier, JB (1822).

The Analytical Theory of Heat (English translation by Alexander Freeman, 1878 ed.). The University Press. p.

[1] .

, cf.

https://books.google.com/books?id=-N8EAAAAYAAJ&pg=PA449 and pp. 546–551.

Original French text .

^ Komatsu, Hikosaburo (2002).

"Fourier's hyperfunctions and Heaviside's pseudodifferential operators" . In Takahiro Kawai ; Keiko Fujita (eds.).

Microlocal Analysis and Complex Fourier Analysis . World Scientific. p.

[2] .

ISBN 978-981-238-161-3 .

^ Myint-U., Tyn; Debnath, Lokenath (2007).

Linear Partial Differential Equations for Scientists And Engineers (4th ed.). Springer. p.

[3] .

ISBN 978-0-8176-4393-5 .

^ Debnath, Lokenath; Bhatta, Dambaru (2007).

Integral Transforms And Their Applications (2nd ed.).

CRC Press . p.

[4] .

ISBN 978-1-58488-575-7 .

^ Grattan-Guinness, Ivor (2009).

Convolutions in French Mathematics, 1800–1840: From the Calculus and Mechanics to Mathematical Analysis and Mathematical Physics, Volume 2 . Birkhäuser. p.

653 .

ISBN 978-3-7643-2238-0 .

^ See, for example, Cauchy, Augustin-Louis (1789-1857) Auteur du texte (1882–1974).

"Des intégrales doubles qui se présentent sous une forme indéterminèe" .

Oeuvres complètes d'Augustin Cauchy. Série 1, tome 1 / publiées sous la direction scientifique de l'Académie des sciences et sous les auspices de M. le ministre de l'Instruction publique...

{{ cite book }} :  CS1 maint: numeric names: authors list ( link ) ^ Mitrović, Dragiša; Žubrinić, Darko (1998).

Fundamentals of Applied Functional Analysis: Distributions, Sobolev Spaces . CRC Press. p.

62 .

ISBN 978-0-582-24694-2 .

^ Kracht, Manfred; Kreyszig, Erwin (1989).

"On singular integral operators and generalizations" . In Themistocles M. Rassias (ed.).

Topics in Mathematical Analysis: A Volume Dedicated to the Memory of A.L. Cauchy . World Scientific. p.

https://books.google.com/books?id=xIsPrSiDlZIC&pg=PA553 553].

ISBN 978-9971-5-0666-7 .

^ Gelfand & Shilov 1966–1968 , Volume I, §1.1, p. 1.

^ Dirac 1930 , p. 63.

^ Rudin 1966 , §1.20 ^ Hewitt & Stromberg 1963 , §19.61.

^ Gelfand & Shilov , p. 1.

sfn error: no target: CITEREFGelfandShilov ( help ) ^ Driggers 2003 , p. 2321  See also Bracewell 1986 , Chapter 5 for a different interpretation.  Other conventions for the assigning the value of the Heaviside function at zero exist, and some of these are not consistent with what follows.

^ Hewitt & Stromberg 1963 , §9.19.

^ Billingsley 1986 , p. 356.

^ Hazewinkel 2011 , p.

41 .

^ Stein & Shararchi 2007 , p. 285.

sfn error: no target: CITEREFSteinShararchi2007 ( help ) ^ Strichartz 1994 , §2.2.

^ Hörmander 1983 , Theorem 2.1.5.

^ Schwartz 1950 .

^ Bracewell 1986 , Chapter 5.

^ Hörmander 1983 , §3.1.

^ Strichartz 1994 , §2.3.

^ Hörmander 1983 , §8.2.

^ Rudin 1966 , §1.20.

^ Dieudonné 1972 , §17.3.3.

^ Krantz, Steven G.; Parks, Harold R. (2008-12-15).

Geometric Integration Theory . Springer Science & Business Media.

ISBN 978-0-8176-4679-0 .

^ Federer 1969 , §2.5.19.

^ Strichartz 1994 , Problem 2.6.2.

^ Vladimirov 1971 , Chapter 2, Example 3(d).

^ Weisstein, Eric W.

"Sifting Property" .

MathWorld .

^ Karris, Steven T. (2003).

Signals and Systems with MATLAB Applications . Orchard Publications. p.

15 .

ISBN 978-0-9709511-6-8 .

^ Roden, Martin S. (2014-05-17).

Introduction to Communication Theory . Elsevier. p.

[5] .

ISBN 978-1-4831-4556-3 .

^ Rottwitt, Karsten; Tidemand-Lichtenberg, Peter (2014-12-11).

Nonlinear Optics: Principles and Applications . CRC Press. p.

[6] 276.

ISBN 978-1-4665-6583-8 .

^ Gelfand & Shilov 1966–1968 , Vol. 1, §II.2.5.

^ Further refinement is possible, namely to submersions , although these require a more involved change of variables formula.

^ Hörmander 1983 , §6.1.

^ Lange 2012 , pp.29–30.

^ Gelfand & Shilov 1966–1968 , p. 212.

^ Gelfand & Shilov 1966–1968 , p. 26.

^ Gelfand & Shilov 1966–1968 , §2.1.

^ Weisstein, Eric W.

"Doublet Function" .

MathWorld .

^ Bracewell 2000 , p. 86.

^ "Gugo82's comment on the distributional derivative of Dirac's delta" .

matematicamente.it . 12 September 2010.

^ a b c Hörmander 1983 , p. 56.

^ Namias, Victor (July 1977). "Application of the Dirac delta function to electric charge and multipole distributions".

American Journal of Physics .

45 (7): 624– 630.

doi : 10.1119/1.10779 .

^ Rudin 1991 , Theorem 6.25.

^ Stein & Weiss 1971 , Theorem 1.18.

^ Rudin 1991 , §II.6.31.

^ More generally, one only needs η = η 1 to have an integrable radially symmetric decreasing rearrangement.

^ Saichev & Woyczyński 1997 , §1.1 The "delta function" as viewed by a physicist and an engineer, p. 3.

^ Milovanović, Gradimir V.; Rassias, Michael Th (2014-07-08).

Analytic Number Theory, Approximation Theory, and Special Functions: In Honor of Hari M. Srivastava . Springer. p.

748 .

ISBN 978-1-4939-0258-3 .

^ Stein & Shararchi 2005 , p. 111.

sfn error: no target: CITEREFSteinShararchi2005 ( help ) ^ Stein & Weiss 1971 , §I.1.

^ Mader, Heidy M. (2006).

Statistics in Volcanology . Geological Society of London. p.

81 .

ISBN 978-1-86239-208-3 .

^ Vallée & Soares 2004 , §7.2.

^ Hörmander 1983 , §7.8.

^ Courant & Hilbert 1962 , §14.

^ John 1955 .

^ Gelfand & Shilov 1966–1968 , I, §3.10.

^ The numerical factors depend on the conventions for the Fourier transform.

^ Bracewell 1986 .

^ Lang 1997 , p. 312.

^ In the terminology of Lang (1997) , the Fejér kernel is a Dirac sequence, whereas the Dirichlet kernel is not.

^ Reed & Simon 1980 , Ch. II–III, VIII.

^ Adams & Fournier 2003 , p. 71.

^ Hazewinkel 1995 , p.

357 .

^ Zhu 2007 , Ch. 4.

^ Levin 2002 , p. 109.

^ Davis & Thomson 2000 , p. 343.

^ Davis & Thomson 2000 , p. 344.

^ de la Madrid, Bohm & Gadella 2002 .

^ Laugwitz 1989 .

^ Córdoba 1988 .

^ Hörmander 1983 , §7.2 .

^ Vladimirov 1971 , §5.7.

^ Hartmann 1997 , pp. 154–155.

^ Kanwal, Ram P. (1997). "15.1. Applications to Probability and Random Processes".

Generalized Functions Theory and Technique . Boston, MA: Birkhäuser Boston.

doi : 10.1007/978-1-4684-0035-9 .

ISBN 978-1-4684-0037-3 .

^ Karatzas & Shreve 1998 , p. 204.

^ Isham 1995 , §6.2.

^ Gelfand & Shilov 1966–1968 , Vol. 4, §I.4.1.

^ de la Madrid Modino 2001 , pp. 96, 106.

^ Arfken & Weber 2005 , pp. 975–976.

^ Boyce, DiPrima & Meade 2017 , pp. 270–273.

References [ edit ] Adams, Robert A.; Fournier, John J. F. (2003).

Sobolev Spaces (2nd ed.). Academic Press.

ISBN 978-0-12-044143-3 .

Arfken, George Brown; Weber, Hans-Jurgen (2005).

Mathematical Methods for Physicists . Boston: Academic Press.

ISBN 0-12-088584-0 .

Aratyn, Henrik; Rasinariu, Constantin (2006), A short course in mathematical methods with Maple , World Scientific, ISBN 978-981-256-461-0 .

Arfken, G. B.

; Weber, H. J. (2000), Mathematical Methods for Physicists (5th ed.), Boston, Massachusetts: Academic Press , ISBN 978-0-12-059825-0 .

atis (2013), ATIS Telecom Glossary , archived from the original on 2013-03-13 Billingsley (1986), Probability and measure (2nd ed.) Boyce, William E.; DiPrima, Richard C.; Meade, Douglas B. (2017).

Elementary differential equations and boundary value problems . Hoboken, NJ: Wiley.

ISBN 978-1-119-37792-4 .

Bracewell, R. N.

(1986), The Fourier Transform and Its Applications (2nd ed.), McGraw-Hill, Bibcode : 1986ftia.book.....B .

Bracewell, R. N.

(2000), The Fourier Transform and Its Applications (3rd ed.), McGraw-Hill .

Córdoba, A. (1988), "La formule sommatoire de Poisson", Comptes Rendus de l'Académie des Sciences, Série I , 306 : 373– 376 .

Courant, Richard ; Hilbert, David (1962), Methods of Mathematical Physics, Volume II , Wiley-Interscience .

Davis, Howard Ted; Thomson, Kendall T (2000), Linear algebra and linear operators in engineering with applications in Mathematica , Academic Press, ISBN 978-0-12-206349-7 Dieudonné, Jean (1976), Treatise on analysis. Vol. II , New York: Academic Press [Harcourt Brace Jovanovich Publishers], ISBN 978-0-12-215502-4 , MR 0530406 .

Dieudonné, Jean (1972), Treatise on analysis. Vol. III , Boston, Massachusetts: Academic Press, MR 0350769 Dirac, Paul (1930), The Principles of Quantum Mechanics (1st ed.), Oxford University Press .

Driggers, Ronald G. (2003), Encyclopedia of Optical Engineering , CRC Press, Bibcode : 2003eoe..book.....D , ISBN 978-0-8247-0940-2 .

Duistermaat, Hans ; Kolk (2010), Distributions: Theory and applications , Springer .

Federer, Herbert (1969), Geometric measure theory , Die Grundlehren der mathematischen Wissenschaften, vol. 153, New York: Springer-Verlag, pp. xiv+676, ISBN 978-3-540-60656-7 , MR 0257325 .

Gannon, Terry (2008), "Vertex operator algebras" , Princeton Companion to Mathematics , Princeton University Press, ISBN 978-1400830398 .

Gelfand, I. M.

; Shilov, G. E. (1966–1968), Generalized functions , vol.

1– 5, Academic Press, ISBN 9781483262246 .

Hartmann, William M. (1997), Signals, sound, and sensation , Springer, ISBN 978-1-56396-283-7 .

Hazewinkel, Michiel (1995).

Encyclopaedia of Mathematics (set) . Springer Science & Business Media.

ISBN 978-1-55608-010-4 .

Hazewinkel, Michiel (2011).

Encyclopaedia of mathematics . Vol. 10. Springer.

ISBN 978-90-481-4896-7 .

OCLC 751862625 .

Hewitt, E ; Stromberg, K (1963), Real and abstract analysis , Springer-Verlag .

Hörmander, L.

(1983), The analysis of linear partial differential operators I , Grundl. Math. Wissenschaft., vol. 256, Springer, doi : 10.1007/978-3-642-96750-4 , ISBN 978-3-540-12104-6 , MR 0717035 .

Isham, C. J. (1995), Lectures on quantum theory: mathematical and structural foundations , Imperial College Press, Bibcode : 1995lqtm.book.....I , ISBN 978-81-7764-190-5 .

Jeffrey, Alan (1993), Linear Algebra and Ordinary Differential Equations , CRC Press, p. 639 John, Fritz (1955), Plane waves and spherical means applied to partial differential equations , Interscience Publishers, New York-London, MR 0075429 .

Reprinted , Dover Publications, 2004, ISBN 9780486438047 .

Karatzas, Ioannis; Shreve, Steven E. (1998), Brownian Motion and Stochastic Calculus , vol. 113, New York, NY: Springer New York, doi : 10.1007/978-1-4612-0949-2 , ISBN 978-0-387-97655-6 Lang, Serge (1997), Undergraduate analysis , Undergraduate Texts in Mathematics (2nd ed.), Berlin, New York: Springer-Verlag, doi : 10.1007/978-1-4757-2698-5 , ISBN 978-0-387-94841-6 , MR 1476913 .

Lange, Rutger-Jan (2012), "Potential theory, path integrals and the Laplacian of the indicator", Journal of High Energy Physics , 2012 (11) 32: 29– 30, arXiv : 1302.0864 , Bibcode : 2012JHEP...11..032L , doi : 10.1007/JHEP11(2012)032 , S2CID 56188533 .

Laugwitz, D.

(1989), "Definite values of infinite sums: aspects of the foundations of infinitesimal analysis around 1820", Arch. Hist. Exact Sci.

, 39 (3): 195– 245, doi : 10.1007/BF00329867 , S2CID 120890300 .

Levin, Frank S. (2002), "Coordinate-space wave functions and completeness" , An introduction to quantum theory , Cambridge University Press, pp. 109 ff , ISBN 978-0-521-59841-5 Li, Y. T.; Wong, R. (2008), "Integral and series representations of the Dirac delta function", Commun. Pure Appl. Anal.

, 7 (2): 229– 247, arXiv : 1303.1943 , doi : 10.3934/cpaa.2008.7.229 , MR 2373214 , S2CID 119319140 .

de la Madrid Modino, R. (2001).

Quantum mechanics in rigged Hilbert space language (PhD thesis). Universidad de Valladolid.

de la Madrid, R.; Bohm, A.; Gadella, M. (2002), "Rigged Hilbert Space Treatment of Continuous Spectrum", Fortschr. Phys.

, 50 (2): 185– 216, arXiv : quant-ph/0109154 , Bibcode : 2002ForPh..50..185D , doi : 10.1002/1521-3978(200203)50:2<185::AID-PROP185>3.0.CO;2-S , S2CID 9407651 .

McMahon, D. (2005-11-22), "An Introduction to State Space" (PDF) , Quantum Mechanics Demystified, A Self-Teaching Guide , Demystified Series, New York: McGraw-Hill, p. 108, ISBN 978-0-07-145546-6 , retrieved 2008-03-17 .

van der Pol, Balth.; Bremmer, H. (1987), Operational calculus (3rd ed.), New York: Chelsea Publishing Co., ISBN 978-0-8284-0327-6 , MR 0904873 .

Reed, Michael; Simon, Barry (1980).

Methods of Modern Mathematical Physics, Volume I: Functional Analysis . Academic Press.

ISBN 9780125850506 .

Rudin, Walter (1966). Devine, Peter R. (ed.).

Real and complex analysis (3rd ed.). New York: McGraw-Hill (published 1987).

ISBN 0-07-100276-6 .

Rudin, Walter (1991), Functional Analysis (2nd ed.), McGraw-Hill, ISBN 978-0-07-054236-5 .

Vallée, Olivier; Soares, Manuel (2004), Airy functions and applications to physics , London: Imperial College Press, ISBN 9781911299486 .

Saichev, A I; Woyczyński, Wojbor Andrzej (1997), "Chapter1: Basic definitions and operations" , Distributions in the Physical and Engineering Sciences: Distributional and fractal calculus, integral transforms, and wavelets , Birkhäuser, ISBN 978-0-8176-3924-2 Schwartz, L.

(1950), Théorie des distributions , vol. 1, Hermann .

Schwartz, L.

(1951), Théorie des distributions , vol. 2, Hermann .

Snieder, Roel (2004), A Guided Tour of Mathematical Methods: For the Physical Sciences , Cambridge University Press Stein, Elias ; Weiss, Guido (1971), Introduction to Fourier Analysis on Euclidean Spaces , Princeton University Press, ISBN 978-0-691-08078-9 .

Strichartz, R.

(1994), A Guide to Distribution Theory and Fourier Transforms , CRC Press, ISBN 978-0-8493-8273-4 .

Vladimirov, V. S. (1971), Equations of mathematical physics , Marcel Dekker, ISBN 978-0-8247-1713-1 .

Weisstein, Eric W.

"Delta Function" .

MathWorld .

Yamashita, H. (2006), "Pointwise analysis of scalar fields: A nonstandard approach", Journal of Mathematical Physics , 47 (9): 092301, Bibcode : 2006JMP....47i2301Y , doi : 10.1063/1.2339017 Yamashita, H. (2007), "Comment on "Pointwise analysis of scalar fields: A nonstandard approach" [J. Math. Phys. 47, 092301 (2006)]", Journal of Mathematical Physics , 48 (8): 084101, Bibcode : 2007JMP....48h4101Y , doi : 10.1063/1.2771422 Zhao, Ji-Cheng (2011), Methods for Phase Diagram Determination , Elsevier, ISBN 978-0-08-054996-5 Zhu, Kehe (2007).

Operator Theory in Function Spaces . Mathematical Surveys and Monographs. Vol. 138. American Mathematical Society.

External links [ edit ] Media related to Dirac distribution at Wikimedia Commons "Delta-function" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] KhanAcademy.org video lesson The Dirac Delta function , a tutorial on the Dirac delta function.

Video Lectures – Lecture 23 , a lecture by Arthur Mattuck .

The Dirac delta measure is a hyperfunction We show the existence of a unique solution and analyze a finite element approximation when the source term is a Dirac delta measure Non-Lebesgue measures on R. Lebesgue-Stieltjes measure, Dirac delta measure.

Archived 2008-03-07 at the Wayback Machine v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons v t e Differential equations Classification Operations Differential operator Notation for differentiation Ordinary Partial Differential-algebraic Integro-differential Fractional Linear Non-linear Holonomic Attributes of variables Dependent and independent variables Homogeneous Nonhomogeneous Coupled Decoupled Order Degree Autonomous Exact differential equation On jet bundles Relation to processes Difference (discrete analogue) Stochastic Stochastic partial Delay Solutions Existence/uniqueness Picard–Lindelöf theorem Peano existence theorem Carathéodory's existence theorem Cauchy–Kowalevski theorem Solution topics Wronskian Phase portrait Phase space Lyapunov stability Asymptotic stability Exponential stability Rate of convergence Series solutions Integral solutions Numerical integration Dirac delta function Solution methods Inspection Substitution Separation of variables Method of undetermined coefficients Variation of parameters Integrating factor Integral transforms Euler method Finite difference method Crank–Nicolson method Runge–Kutta methods Finite element method Finite volume method Galerkin method Perturbation theory Examples List of named differential equations List of linear ordinary differential equations List of nonlinear ordinary differential equations List of nonlinear partial differential equations Mathematicians Isaac Newton Gottfried Wilhelm Leibniz Leonhard Euler Jacob Bernoulli Émile Picard Józef Maria Hoene-Wroński Ernst Lindelöf Rudolf Lipschitz Joseph-Louis Lagrange Augustin-Louis Cauchy John Crank Phyllis Nicolson Carl David Tolmé Runge Martin Kutta Sofya Kovalevskaya Retrieved from " https://en.wikipedia.org/w/index.php?title=Dirac_delta_function&oldid=1306069258 " Categories : Fourier analysis Generalized functions Measure theory Digital signal processing Paul Dirac Schwartz distributions Hidden categories: CS1 maint: numeric names: authors list Harv and Sfn no-target errors Articles with short description Short description is different from Wikidata Use American English from January 2019 All Wikipedia articles written in American English All articles lacking reliable references Articles lacking reliable references from July 2025 Articles containing Latin-language text Commons category link from Wikidata Webarchive template wayback links Good articles This page was last edited on 15 August 2025, at 19:20 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Dirac delta function 45 languages Add topic

