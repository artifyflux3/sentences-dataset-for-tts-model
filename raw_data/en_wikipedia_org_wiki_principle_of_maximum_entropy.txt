Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Overview 3 Testable information 4 Applications Toggle Applications subsection 4.1 Prior probabilities 4.2 Posterior probabilities 4.3 Maximum entropy models 4.4 Probability density estimation 5 General solution for the maximum entropy distribution with linear constraints Toggle General solution for the maximum entropy distribution with linear constraints subsection 5.1 Discrete case 5.2 Continuous case 5.3 Examples 6 Justifications for the principle of maximum entropy Toggle Justifications for the principle of maximum entropy subsection 6.1 Information entropy as a measure of 'uninformativeness' 6.2 The Wallis derivation 6.3 Compatibility with Bayes' theorem 7 Relevance to physics 8 See also 9 Notes 10 References 11 Further reading Toggle the table of contents Principle of maximum entropy 9 languages Català Deutsch Español فارسی Français 한국어 日本語 Português Русский Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Principle in Bayesian statistics For other uses of "Maximum entropy", see Maximum entropy (disambiguation) .

This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( September 2008 ) ( Learn how and when to remove this message ) Part of a series on Bayesian statistics Posterior = Likelihood × Prior ÷ Evidence Background Bayesian inference Bayesian probability Bayes' theorem Bernstein–von Mises theorem Coherence Cox's theorem Cromwell's rule Likelihood principle Principle of indifference Principle of maximum entropy Model building Conjugate prior Linear regression Empirical Bayes Hierarchical model Posterior approximation Markov chain Monte Carlo Laplace's approximation Integrated nested Laplace approximations Variational inference Approximate Bayesian computation Estimators Bayesian estimator Credible interval Maximum a posteriori estimation Evidence approximation Evidence lower bound Nested sampling Model evaluation Bayes factor ( Schwarz criterion ) Model averaging Posterior predictive Mathematics portal v t e The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge about a system is the one with largest entropy , in the context of precisely stated prior data (such as a proposition that expresses testable information ).

Another way of stating this:  Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the best choice.

History [ edit ] The principle was first expounded by E. T. Jaynes in two papers in 1957, [ 1 ] [ 2 ] where he emphasized a natural correspondence between statistical mechanics and information theory . In particular, Jaynes argued that the Gibbsian method of statistical mechanics is sound by also arguing that the entropy of statistical mechanics and the information entropy of information theory are the same concept. Consequently, statistical mechanics should be considered a particular application of a general tool of logical inference and information theory.

Overview [ edit ] In most practical cases, the stated prior data or testable information is given by a set of conserved quantities (average values of some moment functions), associated with the probability distribution in question. This is the way the maximum entropy principle is most often used in statistical thermodynamics . Another possibility is to prescribe some symmetries of the probability distribution. The equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method.

The maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, statistical mechanics and logical inference in particular.

The maximum entropy principle makes explicit our freedom in using different forms of prior data . As a special case, a uniform prior probability density (Laplace's principle of indifference , sometimes called the principle of insufficient reason), may be adopted. Thus, the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics, but represents a significant conceptual generalization of those methods.

However these statements do not imply that thermodynamical systems need not be shown to be ergodic to justify treatment as a statistical ensemble .

In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.

Testable information [ edit ] The principle of maximum entropy is useful explicitly only when applied to testable information . Testable information is a statement about a probability distribution whose truth or falsity is well-defined. For example, the statements the expectation of the variable x {\displaystyle x} is 2.87 and p 2 + p 3 > 0.6 {\displaystyle p_{2}+p_{3}>0.6} (where p 2 {\displaystyle p_{2}} and p 3 {\displaystyle p_{3}} are probabilities of events) are statements of testable information.

Given testable information, the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy , subject to the constraints of the information. This constrained optimization problem is typically solved using the method of Lagrange multipliers .

[ 3 ] Entropy maximization with no testable information respects the universal "constraint" that the sum of the probabilities is one. Under this constraint, the maximum entropy discrete probability distribution is the uniform distribution , p i = 1 n f o r a l l i ∈ ∈ { 1 , … … , n } .

{\displaystyle p_{i}={\frac {1}{n}}\ {\rm {for\ all}}\ i\in \{\,1,\dots ,n\,\}.} Applications [ edit ] The principle of maximum entropy is commonly applied in two ways to inferential problems: Prior probabilities [ edit ] The principle of maximum entropy is often used to obtain prior probability distributions for Bayesian inference . Jaynes was a strong advocate of this approach, claiming the maximum entropy distribution represented the least informative distribution.

[ 4 ] A large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding .

[ 5 ] [ 6 ] [ 7 ] [ 8 ] Posterior probabilities [ edit ] Maximum entropy is a sufficient updating rule for radical probabilism .

Richard Jeffrey 's probability kinematics is a special case of maximum entropy inference . However, maximum entropy is not a generalisation of all such sufficient updating rules.

[ 9 ] Maximum entropy models [ edit ] Alternatively, the principle is often invoked for model specification: in this case the observed data itself is assumed to be the testable information. Such models are widely used in natural language processing . An example of such a model is logistic regression , which corresponds to the maximum entropy classifier for independent observations.

The maximum entropy principle has also been applied in economics and resource allocation. For example, the Boltzmann fair division model uses the maximum entropy (Boltzmann) distribution to allocate resources or income among individuals, providing a probabilistic approach to distributive justice.

[ 10 ] Probability density estimation [ edit ] One of the main applications of the maximum entropy principle is in discrete and continuous density estimation .

[ 11 ] [ 12 ] Similar to support vector machine estimators, 
the maximum entropy principle may require the solution to a quadratic programming problem, and thus provide 
a sparse mixture model as the optimal density estimator. One important advantage of the method is its ability to incorporate prior information in the density estimation.

[ 13 ] General solution for the maximum entropy distribution with linear constraints [ edit ] Main article: Maximum entropy probability distribution Discrete case [ edit ] We have some testable information I about a quantity x taking values in { x 1 , x 2 ,..., x n }. We assume this information has the form of m constraints on the expectations of the functions f k ; that is, we require our probability distribution to satisfy the moment inequality/equality constraints: ∑ ∑ i = 1 n Pr ( x i ) f k ( x i ) ≥ ≥ F k k = 1 , … … , m .

{\displaystyle \sum _{i=1}^{n}\Pr(x_{i})f_{k}(x_{i})\geq F_{k}\qquad k=1,\ldots ,m.} where the F k {\displaystyle F_{k}} are observables.  We also require the probability density to sum to one, which may be viewed as a primitive constraint on the identity function and an observable equal to 1 giving the constraint ∑ ∑ i = 1 n Pr ( x i ) = 1.

{\displaystyle \sum _{i=1}^{n}\Pr(x_{i})=1.} The probability distribution with maximum information entropy subject to these inequality/equality constraints is of the form: [ 11 ] Pr ( x i ) = 1 Z ( λ λ 1 , … … , λ λ m ) exp ⁡ ⁡ [ λ λ 1 f 1 ( x i ) + ⋯ ⋯ + λ λ m f m ( x i ) ] , {\displaystyle \Pr(x_{i})={\frac {1}{Z(\lambda _{1},\ldots ,\lambda _{m})}}\exp \left[\lambda _{1}f_{1}(x_{i})+\cdots +\lambda _{m}f_{m}(x_{i})\right],} for some λ λ 1 , … … , λ λ m {\displaystyle \lambda _{1},\ldots ,\lambda _{m}} . It is sometimes called the Gibbs distribution . The normalization constant is determined by: Z ( λ λ 1 , … … , λ λ m ) = ∑ ∑ i = 1 n exp ⁡ ⁡ [ λ λ 1 f 1 ( x i ) + ⋯ ⋯ + λ λ m f m ( x i ) ] , {\displaystyle Z(\lambda _{1},\ldots ,\lambda _{m})=\sum _{i=1}^{n}\exp \left[\lambda _{1}f_{1}(x_{i})+\cdots +\lambda _{m}f_{m}(x_{i})\right],} and is conventionally called the partition function .  (The Pitman–Koopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution.) The λ k parameters are Lagrange multipliers. In the case of equality constraints their values are determined from the solution of the nonlinear equations F k = ∂ ∂ ∂ ∂ λ λ k log ⁡ ⁡ Z ( λ λ 1 , … … , λ λ m ) .

{\displaystyle F_{k}={\frac {\partial }{\partial \lambda _{k}}}\log Z(\lambda _{1},\ldots ,\lambda _{m}).} In the case of inequality constraints, the Lagrange multipliers are determined from the solution of a convex optimization program with linear constraints.

[ 11 ] In both cases, there is no closed form solution , and the computation of the Lagrange multipliers  usually requires numerical methods .

Continuous case [ edit ] For continuous distributions , the Shannon entropy cannot be used, as it is only defined for discrete probability spaces.  Instead Edwin Jaynes (1963, 1968, 2003) gave the following formula, which is closely related to the relative entropy (see also differential entropy ).

H c = − − ∫ ∫ p ( x ) log ⁡ ⁡ p ( x ) q ( x ) d x {\displaystyle H_{c}=-\int p(x)\log {\frac {p(x)}{q(x)}}\,dx} where q ( x ), which Jaynes called the " invariant measure ", is proportional to the limiting density of discrete points . For now, we shall assume that q is known; we will discuss it further after the solution equations are given.

A closely related quantity, the relative entropy, is usually defined as the Kullback–Leibler divergence of p from q (although it is sometimes, confusingly, defined as the negative of this).  The inference principle of minimizing this, due to Kullback, is known as the Principle of Minimum Discrimination Information .

We have some testable information I about a quantity x which takes values in some interval of the real numbers (all integrals below are over this interval). We assume this information has the form of m constraints on the expectations of the functions f k , i.e. we require our probability density function to satisfy the inequality (or purely equality) moment constraints: ∫ ∫ p ( x ) f k ( x ) d x ≥ ≥ F k k = 1 , … … , m .

{\displaystyle \int p(x)f_{k}(x)\,dx\geq F_{k}\qquad k=1,\dotsc ,m.} where the F k {\displaystyle F_{k}} are observables.  We also require the probability density to integrate to one, which may be viewed as a primitive constraint on the identity function and an observable equal to 1 giving the constraint ∫ ∫ p ( x ) d x = 1.

{\displaystyle \int p(x)\,dx=1.} The probability density function with maximum H c subject to these constraints is: [ 12 ] p ( x ) = 1 Z ( λ λ 1 , … … , λ λ m ) q ( x ) exp ⁡ ⁡ [ λ λ 1 f 1 ( x ) + ⋯ ⋯ + λ λ m f m ( x ) ] {\displaystyle p(x)={\frac {1}{Z(\lambda _{1},\dotsc ,\lambda _{m})}}q(x)\exp \left[\lambda _{1}f_{1}(x)+\dotsb +\lambda _{m}f_{m}(x)\right]} with the partition function determined by Z ( λ λ 1 , … … , λ λ m ) = ∫ ∫ q ( x ) exp ⁡ ⁡ [ λ λ 1 f 1 ( x ) + ⋯ ⋯ + λ λ m f m ( x ) ] d x .

{\displaystyle Z(\lambda _{1},\dotsc ,\lambda _{m})=\int q(x)\exp \left[\lambda _{1}f_{1}(x)+\dotsb +\lambda _{m}f_{m}(x)\right]\,dx.} As in the discrete case, in the case where all moment constraints are equalities, the values of the λ λ k {\displaystyle \lambda _{k}} parameters are determined by the system of nonlinear equations: F k = ∂ ∂ ∂ ∂ λ λ k log ⁡ ⁡ Z ( λ λ 1 , … … , λ λ m ) .

{\displaystyle F_{k}={\frac {\partial }{\partial \lambda _{k}}}\log Z(\lambda _{1},\dotsc ,\lambda _{m}).} In the case with  inequality moment  constraints the Lagrange multipliers are determined from the solution of a convex optimization program.

[ 12 ] The invariant measure function q ( x ) can be best understood by supposing that x is known to take values only in the bounded interval ( a , b ), and that no other information is given. Then the maximum entropy probability density function is p ( x ) = A ⋅ ⋅ q ( x ) , a < x < b {\displaystyle p(x)=A\cdot q(x),\qquad a<x<b} where A is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'.  It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the principle of transformation groups or marginalization theory .

Examples [ edit ] For several examples of maximum entropy distributions, see the article on maximum entropy probability distributions .

Justifications for the principle of maximum entropy [ edit ] Proponents of the principle of maximum entropy justify its use in assigning probabilities in several ways, including the following two arguments. These arguments take the use of Bayesian probability as given, and are thus subject to the same postulates.

Information entropy as a measure of 'uninformativeness' [ edit ] Consider a discrete probability distribution among m {\displaystyle m} mutually exclusive propositions . The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, log ⁡ ⁡ m {\displaystyle \log m} . The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is, ranging from zero (completely informative) to log ⁡ ⁡ m {\displaystyle \log m} (completely uninformative).

By choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess. Thus the maximum entropy distribution is the only reasonable distribution. The dependence of the solution on the dominating measure represented by m ( x ) {\displaystyle m(x)} is however a source of criticisms of the approach since this dominating measure is in fact arbitrary.

[ 14 ] The Wallis derivation [ edit ] The following argument is the result of a suggestion made by Graham Wallis to E. T. Jaynes in 1962.

[ 15 ] It is essentially the same mathematical argument used for the Maxwell–Boltzmann statistics in statistical mechanics , although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed a priori , but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.

Suppose an individual wishes to make a probability assignment among m {\displaystyle m} mutually exclusive propositions. They have some testable information, but are not sure how to go about including this information in their probability assessment. They therefore conceive of the following random experiment. They will distribute N {\displaystyle N} quanta of probability (each worth 1 / N {\displaystyle 1/N} ) at random among the m {\displaystyle m} possibilities. (One might imagine that they will throw N {\displaystyle N} balls into m {\displaystyle m} buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, they will check if the probability assignment thus obtained is consistent with their information. (For this step to be successful, the information must be a constraint given by an open set in the space of probability measures). If it is inconsistent, they will reject it and try again. If it is consistent, their assessment will be p i = n i N {\displaystyle p_{i}={\frac {n_{i}}{N}}} where p i {\displaystyle p_{i}} is the probability of the i {\displaystyle i} th proposition, while n i is the number of quanta that were assigned to the i {\displaystyle i} th proposition (i.e. the number of balls that ended up in bucket i {\displaystyle i} ).

Now, in order to reduce the 'graininess' of the probability assignment, it will be necessary to use quite a large number of quanta of probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, the protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the multinomial distribution , P r ( p ) = W ⋅ ⋅ m − − N {\displaystyle Pr(\mathbf {p} )=W\cdot m^{-N}} where W = N !

n 1 !

n 2 !

⋯ ⋯ n m !

{\displaystyle W={\frac {N!}{n_{1}!\,n_{2}!\,\dotsb \,n_{m}!}}} is sometimes known as the multiplicity of the outcome.

The most probable result is the one which maximizes the multiplicity W {\displaystyle W} . Rather than maximizing W {\displaystyle W} directly, the protagonist could equivalently maximize any monotonic increasing function of W {\displaystyle W} . They decide to maximize 1 N log ⁡ ⁡ W = 1 N log ⁡ ⁡ N !

n 1 !

n 2 !

⋯ ⋯ n m !

= 1 N log ⁡ ⁡ N !

( N p 1 ) !

( N p 2 ) !

⋯ ⋯ ( N p m ) !

= 1 N ( log ⁡ ⁡ N !

− − ∑ ∑ i = 1 m log ⁡ ⁡ ( ( N p i ) !

) ) .

{\displaystyle {\begin{aligned}{\frac {1}{N}}\log W&={\frac {1}{N}}\log {\frac {N!}{n_{1}!\,n_{2}!\,\dotsb \,n_{m}!}}\\[6pt]&={\frac {1}{N}}\log {\frac {N!}{(Np_{1})!\,(Np_{2})!\,\dotsb \,(Np_{m})!}}\\[6pt]&={\frac {1}{N}}\left(\log N!-\sum _{i=1}^{m}\log((Np_{i})!)\right).\end{aligned}}} At this point, in order to simplify the expression, the protagonist takes the limit as N → → ∞ ∞ {\displaystyle N\to \infty } , i.e. as the probability levels go from grainy  discrete values to smooth continuous values. Using Stirling's approximation , they find lim N → → ∞ ∞ ( 1 N log ⁡ ⁡ W ) = 1 N ( N log ⁡ ⁡ N − − ∑ ∑ i = 1 m N p i log ⁡ ⁡ ( N p i ) ) = log ⁡ ⁡ N − − ∑ ∑ i = 1 m p i log ⁡ ⁡ ( N p i ) = log ⁡ ⁡ N − − log ⁡ ⁡ N ∑ ∑ i = 1 m p i − − ∑ ∑ i = 1 m p i log ⁡ ⁡ p i = ( 1 − − ∑ ∑ i = 1 m p i ) log ⁡ ⁡ N − − ∑ ∑ i = 1 m p i log ⁡ ⁡ p i = − − ∑ ∑ i = 1 m p i log ⁡ ⁡ p i = H ( p ) .

{\displaystyle {\begin{aligned}\lim _{N\to \infty }\left({\frac {1}{N}}\log W\right)&={\frac {1}{N}}\left(N\log N-\sum _{i=1}^{m}Np_{i}\log(Np_{i})\right)\\[6pt]&=\log N-\sum _{i=1}^{m}p_{i}\log(Np_{i})\\[6pt]&=\log N-\log N\sum _{i=1}^{m}p_{i}-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=\left(1-\sum _{i=1}^{m}p_{i}\right)\log N-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=H(\mathbf {p} ).\end{aligned}}} All that remains for the protagonist to do is to maximize entropy under the constraints of their testable information. They have found that the maximum entropy distribution is the most probable of all "fair" random distributions, in the limit as the probability levels go from discrete to continuous.

Compatibility with Bayes' theorem [ edit ] Giffin and Caticha (2007) state that Bayes' theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the "method of maximum relative entropy". They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as empirical likelihood and exponentially tilted empirical likelihood – see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.

Jaynes stated Bayes' theorem was a way to calculate a probability, while maximum entropy was a way to assign a prior probability distribution.

[ 16 ] It is however, possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross-entropy (or the Principle of Maximum Entropy being a special case of using a uniform distribution as the given prior), independently of any Bayesian considerations by treating the problem formally as a constrained optimisation problem, the Entropy functional being the objective function.  For the case of given average values as testable information (averaged over the sought after probability distribution), the sought after distribution is formally the Gibbs (or Boltzmann) distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information.

Relevance to physics [ edit ] The principle of maximum entropy bears a relation to a key assumption of kinetic theory of gases known as molecular chaos or Stosszahlansatz . This asserts that the distribution function characterizing particles entering a collision can be factorized. Though this statement can be understood as a strictly physical hypothesis, it can also be interpreted as a heuristic hypothesis regarding the most probable configuration of particles before colliding.

[ 17 ] See also [ edit ] Akaike information criterion Dissipation Info-metrics Maximum entropy classifier Maximum entropy probability distribution Maximum entropy spectral estimation Maximum entropy thermodynamics Principle of maximum caliber Thermodynamic equilibrium Molecular chaos Boltzmann fair division Notes [ edit ] ^ Jaynes, E. T.

(1957).

"Information Theory and Statistical Mechanics" (PDF) .

Physical Review . Series II.

106 (4): 620– 630.

Bibcode : 1957PhRv..106..620J .

doi : 10.1103/PhysRev.106.620 .

MR 0087305 .

^ Jaynes, E. T.

(1957).

"Information Theory and Statistical Mechanics II" (PDF) .

Physical Review . Series II.

108 (2): 171– 190.

Bibcode : 1957PhRv..108..171J .

doi : 10.1103/PhysRev.108.171 .

MR 0096414 .

^ Sivia, Devinderjit; Skilling, John (2006-06-02).

Data Analysis: A Bayesian Tutorial . OUP Oxford.

ISBN 978-0-19-154670-9 .

^ Jaynes, E. T.

(1968).

"Prior Probabilities" (PDF) .

IEEE Transactions on Systems Science and Cybernetics .

4 (3): 227– 241.

doi : 10.1109/TSSC.1968.300117 .

^ Clarke, B. (2006). "Information optimality and Bayesian modelling".

Journal of Econometrics .

138 (2): 405– 429.

doi : 10.1016/j.jeconom.2006.05.003 .

^ Soofi, E.S. (2000). "Principal Information Theoretic Approaches".

Journal of the American Statistical Association .

95 (452): 1349– 1353.

doi : 10.2307/2669786 .

JSTOR 2669786 .

MR 1825292 .

^ Bousquet, N. (2008). "Eliciting vague but proper maximal entropy priors in Bayesian experiments".

Statistical Papers .

51 (3): 613– 628.

doi : 10.1007/s00362-008-0149-9 .

S2CID 119657859 .

^ Palmieri, Francesco A. N.; Ciuonzo, Domenico (2013-04-01). "Objective priors from maximum entropy in data classification".

Information Fusion .

14 (2): 186– 198.

CiteSeerX 10.1.1.387.4515 .

doi : 10.1016/j.inffus.2012.01.012 .

^ Skyrms, B (1987). "Updating, supposing and MAXENT".

Theory and Decision .

22 (3): 225– 46.

doi : 10.1007/BF00134086 .

S2CID 121847242 .

^ Park, J.-W., Kim, J. U., Ghim, C.-M., & Kim, C. U. (2022). The Boltzmann Fair Division for Distributive Justice.

Scientific Reports , 12(1), 16179.

https://doi.org/10.1038/s41598-022-19792-3 Park, J.-W., & Kim, C. U. (2021). Getting to a Feasible Income Equality.

PLOS ONE , 16(3), e0249204.

https://doi.org/10.1371/journal.pone.0249204 Park, J.-W., Kim, C. U., & Isard, W. (2012). Permit Allocation in Emissions Trading Using the Boltzmann Distribution.

Physica A , 391, 4883–4890.

https://doi.org/10.1016/j.physa.2012.05.006 ^ a b c Botev, Z. I.; Kroese, D. P. (2008). "Non-asymptotic Bandwidth Selection for Density Estimation of Discrete Data".

Methodology and Computing in Applied Probability .

10 (3): 435.

doi : 10.1007/s11009-007-9057-z .

S2CID 122047337 .

^ a b c Botev, Z. I.; Kroese, D. P. (2011).

"The Generalized Cross Entropy Method, with Applications to Probability Density Estimation" (PDF) .

Methodology and Computing in Applied Probability .

13 (1): 1– 27.

doi : 10.1007/s11009-009-9133-7 .

S2CID 18155189 .

^ Kesavan, H. K.; Kapur, J. N. (1990). "Maximum Entropy and Minimum Cross-Entropy Principles". In Fougère, P. F. (ed.).

Maximum Entropy and Bayesian Methods . pp.

419 –432.

doi : 10.1007/978-94-009-0683-9_29 .

ISBN 978-94-010-6792-8 .

^ Druilhet, Pierre; Marin, Jean-Michel (2007).

"Invariant {HPD} credible sets and {MAP} estimators" .

Bayesian Anal .

2 : 681– 691.

doi : 10.1214/07-BA227 .

^ Jaynes, E. T. (2003) Probability Theory: The Logic of Science , Cambridge University Press, p. 351-355.

ISBN 978-0521592710 ^ Jaynes, E. T. (1988) "The Relation of Bayesian and Maximum Entropy Methods" , in Maximum-Entropy and Bayesian Methods in Science and Engineering (Vol. 1) , Kluwer Academic Publishers, p. 25-29.

^ Chliamovitch, G.; Malaspinas, O.; Chopard, B. (2017).

"Kinetic theory beyond the Stosszahlansatz" .

Entropy .

19 (8): 381.

Bibcode : 2017Entrp..19..381C .

doi : 10.3390/e19080381 .

References [ edit ] Bajkova, A. T. (1992). "The generalization of maximum entropy method for reconstruction of complex functions".

Astronomical and Astrophysical Transactions .

1 (4): 313– 320.

Bibcode : 1992A&AT....1..313B .

doi : 10.1080/10556799208230532 .

Fornalski, K.W.; Parzych, G.; Pylak, M.; Satuła, D.; Dobrzyński, L. (2010).

"Application of Bayesian reasoning and the Maximum Entropy Method to some reconstruction problems" (PDF) .

Acta Physica Polonica A .

117 (6): 892– 899.

Bibcode : 2010AcPPA.117..892F .

doi : 10.12693/APhysPolA.117.892 .

Giffin, A. and Caticha, A., 2007, Updating Probabilities with Data and Moments Guiasu, S.; Shenitzer, A. (1985). "The principle of maximum entropy".

The Mathematical Intelligencer .

7 (1): 42– 48.

doi : 10.1007/bf03023004 .

S2CID 53059968 .

Harremoës, P.; Topsøe (2001).

"Maximum entropy fundamentals" .

Entropy .

3 (3): 191– 226.

Bibcode : 2001Entrp...3..191H .

doi : 10.3390/e3030191 .

Jaynes, E. T.

(1963).

"Information Theory and Statistical Mechanics" . In Ford, K. (ed.).

Statistical Physics . New York: Benjamin. p. 181.

Jaynes, E. T., 1986 (new version online 1996), " Monkeys, kangaroos and N ", in Maximum-Entropy and Bayesian Methods in Applied Statistics , J. H. Justice (ed.), Cambridge University Press, Cambridge, p. 26.

Kapur, J. N.; and Kesavan, H. K.

, 1992, Entropy Optimization Principles with Applications , Boston: Academic Press.

ISBN 0-12-397670-7 Kitamura, Y., 2006, Empirical Likelihood Methods in Econometrics: Theory and Practice , Cowles Foundation Discussion Papers 1569, Cowles Foundation, Yale University.

Lazar, N (2003). "Bayesian empirical likelihood".

Biometrika .

90 (2): 319– 326.

doi : 10.1093/biomet/90.2.319 .

Owen, A. B., 2001, Empirical Likelihood , Chapman and Hall/CRC.

ISBN 1-58-488071-6 .

Schennach, S. M. (2005). "Bayesian exponentially tilted empirical likelihood".

Biometrika .

92 (1): 31– 46.

doi : 10.1093/biomet/92.1.31 .

Uffink, Jos (1995).

"Can the Maximum Entropy Principle be explained as a consistency requirement?" (PDF) .

Studies in History and Philosophy of Modern Physics .

26B (3): 223– 261.

Bibcode : 1995SHPMP..26..223U .

CiteSeerX 10.1.1.27.6392 .

doi : 10.1016/1355-2198(95)00015-1 .

hdl : 1874/2649 . Archived from the original (PDF) on 2006-06-03.

Further reading [ edit ] Boyd, Stephen; Lieven Vandenberghe (2004).

Convex Optimization (PDF) .

Cambridge University Press . p. 362.

ISBN 0-521-83378-7 . Retrieved 2008-08-24 .

Ratnaparkhi A. (1997) "A simple introduction to maximum entropy models for natural language processing" Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania. An easy-to-read introduction to maximum entropy methods in the context of natural language processing.

Tang, A.; Jackson, D.; Hobbs, J.; Chen, W.; Smith, J. L.; Patel, H.; Prieto, A.; Petrusca, D.; Grivich, M. I.; Sher, A.; Hottowy, P.; Dabrowski, W.; Litke, A. M.; Beggs, J. M. (2008).

"A Maximum Entropy Model Applied to Spatial and Temporal Correlations from Cortical Networks in Vitro" .

Journal of Neuroscience .

28 (2): 505– 518.

doi : 10.1523/JNEUROSCI.3359-07.2008 .

PMC 6670549 .

PMID 18184793 .

Open access article containing pointers to various papers and software implementations of Maximum Entropy Model on the net.

v t e Statistical mechanics Theory Principle of maximum entropy ergodic theory Statistical thermodynamics Ensembles partition functions equations of state thermodynamic potential : U H F G Maxwell relations Models Ferromagnetism models Ising Potts Heisenberg percolation Particles with force field depletion force Lennard-Jones potential Mathematical approaches Boltzmann equation H-theorem Vlasov equation BBGKY hierarchy stochastic process mean-field theory and conformal field theory Critical phenomena Phase transition Critical exponents correlation length size scaling Entropy Boltzmann Shannon Tsallis Rényi von Neumann Applications Statistical field theory elementary particle superfluidity Condensed matter physics Complex system chaos information theory Boltzmann machine Authority control databases National United States Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐9k626
Cached time: 20250812021350
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.617 seconds
Real time usage: 0.965 seconds
Preprocessor visited node count: 2356/1000000
Revision size: 32192/2097152 bytes
Post‐expand include size: 83800/2097152 bytes
Template argument size: 889/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 112252/5000000 bytes
Lua time usage: 0.399/10.000 seconds
Lua memory usage: 7283807/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  608.339      1 -total
 35.49%  215.893     20 Template:Cite_journal
 33.39%  203.109      1 Template:Reflist
 15.01%   91.282      1 Template:Bayesian_statistics
 12.44%   75.685      1 Template:Short_description
  8.53%   51.878      1 Template:More_footnotes
  7.73%   47.017      2 Template:Pagetype
  7.43%   45.203      1 Template:Ambox
  6.00%   36.526      1 Template:Statistical_mechanics_topics
  5.55%   33.745      1 Template:Navbox Saved in parser cache with key enwiki:pcache:201718:|#|:idhash:canonical and timestamp 20250812021350 and revision id 1298206034. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Principle_of_maximum_entropy&oldid=1298206034 " Categories : Entropy and information Bayesian statistics Statistical principles Probability assessment Mathematical principles Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from September 2008 All articles lacking in-text citations This page was last edited on 1 July 2025, at 03:15 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Principle of maximum entropy 9 languages Add topic

