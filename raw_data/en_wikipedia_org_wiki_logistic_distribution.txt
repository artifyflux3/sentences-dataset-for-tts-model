Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Specification Toggle Specification subsection 1.1 Cumulative distribution function 1.2 Probability density function 1.3 Quantile function 1.4 Alternative parameterization 2 Applications Toggle Applications subsection 2.1 Logistic regression 2.2 Physics 2.3 Hydrology 2.4 Chess ratings 3 Related distributions 4 Derivations Toggle Derivations subsection 4.1 Higher-order moments 5 See also 6 Notes 7 References Toggle the table of contents Logistic distribution 18 languages Català Dansk Deutsch Español فارسی Français 한국어 Italiano עברית Nederlands 日本語 Polski Português Русский Slovenščina Српски / srpski Türkçe Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Continuous probability distribution Logistic distribution Probability density function Cumulative distribution function Parameters μ μ , {\displaystyle \mu ,} location ( real ) s > 0 , {\displaystyle s>0,} scale (real) Support x ∈ ∈ ( − − ∞ ∞ , ∞ ∞ ) {\displaystyle x\in (-\infty ,\infty )} PDF e − − ( x − − μ μ ) / s s ( 1 + e − − ( x − − μ μ ) / s ) 2 {\displaystyle {\frac {e^{-(x-\mu )/s}}{s\left(1+e^{-(x-\mu )/s}\right)^{2}}}} CDF 1 1 + e − − ( x − − μ μ ) / s = 1 + tanh ⁡ ⁡ x − − μ μ 2 s 2 {\displaystyle {\frac {1}{1+e^{-(x-\mu )/s}}}={\frac {1+\tanh {\frac {x-\mu }{2s}}}{2}}} Quantile μ μ + s log ⁡ ⁡ ( p 1 − − p ) {\displaystyle \mu +s\log \left({\frac {p}{1-p}}\right)} Mean μ μ {\displaystyle \mu } Median μ μ {\displaystyle \mu } Mode μ μ {\displaystyle \mu } Variance s 2 π π 2 3 {\displaystyle {\frac {s^{2}\pi ^{2}}{3}}} Skewness 0 {\displaystyle 0} Excess kurtosis 6 / 5 {\displaystyle 6/5} Entropy ln ⁡ ⁡ s + 2 {\displaystyle \ln s+2} MGF e μ μ t B ( 1 − − s t , 1 + s t ) {\displaystyle e^{\mu t}\mathrm {B} (1-st,1+st)} for t ∈ ∈ ( − − 1 / s , 1 / s ) {\displaystyle t\in (-1/s,1/s)} and B {\displaystyle \mathrm {B} } is the Beta function CF e i t μ μ π π s t sinh ⁡ ⁡ ( π π s t ) {\displaystyle e^{it\mu }{\frac {\pi st}{\sinh(\pi st)}}} Expected shortfall μ μ + s H ( p ) 1 − − p {\displaystyle \mu +{\frac {sH(p)}{1-p}}} where H ( p ) {\displaystyle H(p)} is the binary entropy function [ 1 ] H ( p ) = − − p ln ⁡ ⁡ ( p ) − − ( 1 − − p ) ln ⁡ ⁡ ( 1 − − p ) {\displaystyle H(p)=-p\ln(p)-(1-p)\ln(1-p)} In probability theory and statistics , the logistic distribution is a continuous probability distribution . Its cumulative distribution function is the logistic function , which appears in logistic regression and feedforward neural networks . It resembles the normal distribution in shape but has heavier tails (higher kurtosis ).  The logistic distribution is a special case of the Tukey lambda distribution .

Specification [ edit ] Cumulative distribution function [ edit ] The logistic distribution receives its name from its cumulative distribution function , which is an instance of the family of logistic functions. The cumulative distribution function of the logistic distribution is also a scaled version of the hyperbolic tangent .

F ( x ; μ μ , s ) = 1 1 + e − − ( x − − μ μ ) / s = 1 2 + 1 2 tanh ⁡ ⁡ ( x − − μ μ 2 s ) .

{\displaystyle F(x;\mu ,s)={\frac {1}{1+e^{-(x-\mu )/s}}}={\frac {1}{2}}+{\frac {1}{2}}\operatorname {tanh} \left({\frac {x-\mu }{2s}}\right).} In this equation μ is the mean , and s is a scale parameter proportional to the standard deviation .

Probability density function [ edit ] The probability density function is the partial derivative of the cumulative distribution function: f ( x ; μ μ , s ) = ∂ ∂ F ( x ; μ μ , s ) ∂ ∂ x = e − − ( x − − μ μ ) / s s ( 1 + e − − ( x − − μ μ ) / s ) 2 = 1 s ( e ( x − − μ μ ) / ( 2 s ) + e − − ( x − − μ μ ) / ( 2 s ) ) 2 = 1 4 s sech 2 ⁡ ⁡ ( x − − μ μ 2 s ) .

{\displaystyle {\begin{aligned}f(x;\mu ,s)&={\frac {\partial F(x;\mu ,s)}{\partial x}}={\frac {e^{-(x-\mu )/s}}{s\left(1+e^{-(x-\mu )/s}\right)^{2}}}\\[4pt]&={\frac {1}{s\left(e^{(x-\mu )/(2s)}+e^{-(x-\mu )/(2s)}\right)^{2}}}\\[4pt]&={\frac {1}{4s}}\operatorname {sech} ^{2}\left({\frac {x-\mu }{2s}}\right).\end{aligned}}} When the location parameter μ is 0 and the scale parameter s is 1, then the probability density function of the logistic distribution is given by f ( x ; 0 , 1 ) = e − − x ( 1 + e − − x ) 2 = 1 ( e x / 2 + e − − x / 2 ) 2 = 1 4 sech 2 ⁡ ⁡ ( x 2 ) .

{\displaystyle {\begin{aligned}f(x;0,1)&={\frac {e^{-x}}{(1+e^{-x})^{2}}}\\[4pt]&={\frac {1}{(e^{x/2}+e^{-x/2})^{2}}}\\[5pt]&={\frac {1}{4}}\operatorname {sech} ^{2}\left({\frac {x}{2}}\right).\end{aligned}}} Because this function can be expressed in terms of the square of the hyperbolic secant function "sech", it is sometimes referred to as the sech-square(d) distribution .

[ 2 ] (See also: hyperbolic secant distribution ).

Quantile function [ edit ] The inverse cumulative distribution function ( quantile function ) of the logistic distribution is a generalization of the logit function.  Its derivative is called the quantile density function.  They are defined as follows: Q ( p ; μ μ , s ) = μ μ + s ln ⁡ ⁡ ( p 1 − − p ) .

{\displaystyle Q(p;\mu ,s)=\mu +s\ln \left({\frac {p}{1-p}}\right).} Q ′ ( p ; s ) = s p ( 1 − − p ) .

{\displaystyle Q'(p;s)={\frac {s}{p(1-p)}}.} Alternative parameterization [ edit ] An alternative parameterization of the logistic distribution can be derived by expressing the scale parameter, s {\displaystyle s} , in terms of the standard deviation, σ σ {\displaystyle \sigma } , using the substitution s = q σ σ {\displaystyle s\,=\,q\,\sigma } , where q = 3 / π π = 0.551328895 … … {\displaystyle q\,=\,{\sqrt {3}}/{\pi }\,=\,0.551328895\ldots } .  The alternative forms of the above functions are reasonably straightforward.

Applications [ edit ] The logistic distribution—and the S-shaped pattern of its cumulative distribution function (the logistic function ) and quantile function (the logit function )—have been extensively used in many different areas.

Logistic regression [ edit ] One of the most common applications is in logistic regression , which is used for modeling categorical dependent variables (e.g., yes-no choices or a choice of 3 or 4 possibilities), much as standard linear regression is used for modeling continuous variables (e.g., income or population). Specifically, logistic regression models can be phrased as latent variable models with error variables following a logistic distribution. This phrasing is common in the theory of discrete choice models, where the logistic distribution plays the same role in logistic regression as the normal distribution does in probit regression . Indeed, the logistic and normal distributions have a quite similar shape. However, the logistic distribution has heavier tails , which often increases the robustness of analyses based on it compared with using the normal distribution.

Physics [ edit ] The PDF of this distribution has the same functional form as the derivative of the Fermi function . In the theory of electron properties in semiconductors and metals, this derivative sets the relative weight of the various electron energies in their contributions to electron transport. Those energy levels whose energies are closest to the distribution's "mean" ( Fermi level ) dominate processes such as electronic conduction, with some smearing induced by temperature.

[ 3 ] : 34 However the pertinent probability distribution in Fermi–Dirac statistics is actually a simple Bernoulli distribution , with the probability factor given by the Fermi function.

The logistic distribution arises as limit distribution of a finite-velocity damped random motion described by a telegraph process in which the random times between consecutive velocity changes have independent exponential distributions with linearly increasing parameters.

[ 4 ] Hydrology [ edit ] Fitted cumulative logistic distribution to October rainfalls using CumFreq , see also Distribution fitting In hydrology the distribution of long duration river discharge and rainfall (e.g., monthly and yearly totals, consisting of the sum of 30 respectively 360 daily values) is often thought to be almost normal according to the central limit theorem .

[ 5 ] The normal distribution , however, needs a numeric approximation. As the logistic distribution, which can be solved analytically, is similar to the normal distribution, it can be used instead. The blue picture illustrates an example of fitting the logistic distribution to ranked October rainfalls—that are almost normally distributed—and it shows the 90% confidence belt based on the binomial distribution . The rainfall data are represented by plotting positions as part of the cumulative frequency analysis .

Chess ratings [ edit ] The United States Chess Federation and FIDE have switched its formula for calculating chess ratings from the normal distribution to the logistic distribution; see the article on Elo rating system (itself based on the normal distribution).

Related distributions [ edit ] Logistic distribution mimics the sech distribution ; they are different cases of the Champernowne distribution .

If X ∼ ∼ L o g i s t i c ( μ μ , s ) {\displaystyle X\sim \mathrm {Logistic} (\mu ,s)} then k X + ℓ ℓ ∼ ∼ L o g i s t i c ( k μ μ + ℓ ℓ , | k | s ) {\displaystyle kX+\ell \sim \mathrm {Logistic} (k\mu +\ell ,|k|s)} .

If X ∼ ∼ {\displaystyle X\sim } U(0, 1) then μ μ + s ⋅ ⋅ logit ( X ) ∼ ∼ L o g i s t i c ( μ μ , s ) {\displaystyle \mu +s\cdot {\text{logit}}(X)\sim \mathrm {Logistic} (\mu ,s)} , where logit ( X ) = log ⁡ ⁡ X − − log ⁡ ⁡ ( 1 − − X ) {\displaystyle {\text{logit}}(X)=\log X-\log(1-X)} is the logit function.

If X ∼ ∼ G u m b e l ( μ μ X , β β ) {\displaystyle X\sim \mathrm {Gumbel} (\mu _{X},\beta )} and Y ∼ ∼ G u m b e l ( μ μ Y , β β ) {\displaystyle Y\sim \mathrm {Gumbel} (\mu _{Y},\beta )} independently then X − − Y ∼ ∼ L o g i s t i c ( μ μ X − − μ μ Y , β β ) {\displaystyle X-Y\sim \mathrm {Logistic} (\mu _{X}-\mu _{Y},\beta )\,} .

If X {\displaystyle X} and Y ∼ ∼ G u m b e l ( μ μ , β β ) {\displaystyle Y\sim \mathrm {Gumbel} (\mu ,\beta )} then X + Y ≁ ≁ L o g i s t i c ( 2 μ μ , β β ) {\displaystyle X+Y\nsim \mathrm {Logistic} (2\mu ,\beta )\,} (The sum is not a logistic distribution).

E ( X + Y ) = 2 μ μ + 2 β β γ γ ≠ ≠ 2 μ μ = E ( L o g i s t i c ( 2 μ μ , β β ) ) {\displaystyle E(X+Y)=2\mu +2\beta \gamma \neq 2\mu =E\left(\mathrm {Logistic} (2\mu ,\beta )\right)} .

If X ~ Logistic( μ , s ) then exp( X ) ~ LogLogistic ( α α = e μ μ , β β = 1 s ) {\displaystyle \left(\alpha =e^{\mu },\beta ={\frac {1}{s}}\right)} ,  and  exp( X ) + γ ~ shifted log-logistic ( α α = e μ μ , β β = 1 s , γ γ ) {\displaystyle \left(\alpha =e^{\mu },\beta ={\frac {1}{s}},\gamma \right)} .

If X ~ Exponential(1) then μ μ + s log ⁡ ⁡ ( e X − − 1 ) ∼ ∼ Logistic ⁡ ⁡ ( μ μ , s ) .

{\displaystyle \mu +s\log(e^{X}-1)\sim \operatorname {Logistic} (\mu ,s).} If X , Y ~ Exponential(λ) independently then μ μ + s log ⁡ ⁡ ( X Y ) ∼ ∼ Logistic ⁡ ⁡ ( μ μ , s ) .

{\displaystyle \mu +s\log \left({\frac {X}{Y}}\right)\sim \operatorname {Logistic} (\mu ,s).} The metalog distribution is generalization of the logistic distribution, in which power series expansions in terms of p {\displaystyle p} are substituted for logistic parameters μ μ {\displaystyle \mu } and σ σ {\displaystyle \sigma } . The resulting metalog quantile function is highly shape flexible, has a simple closed form, and can be fit to data with linear least squares.

Derivations [ edit ] Higher-order moments [ edit ] The n th-order central moment can be expressed in terms of the quantile function: E ⁡ ⁡ [ ( X − − μ μ ) n ] = ∫ ∫ − − ∞ ∞ ∞ ∞ ( x − − μ μ ) n d F ( x ) = ∫ ∫ 0 1 ( Q ( p ) − − μ μ ) n d p = s n ∫ ∫ 0 1 [ ln ( p 1 − − p ) ] n d p .

{\displaystyle {\begin{aligned}\operatorname {E} [(X-\mu )^{n}]&=\int _{-\infty }^{\infty }(x-\mu )^{n}\,dF(x)\\&=\int _{0}^{1}{\big (}Q(p)-\mu {\big )}^{n}\,dp=s^{n}\int _{0}^{1}\left[\ln \!\left({\frac {p}{1-p}}\right)\right]^{n}\,dp.\end{aligned}}} This integral is well-known [ 6 ] and can be expressed in terms of Bernoulli numbers : E ⁡ ⁡ [ ( X − − μ μ ) n ] = s n π π n ( 2 n − − 2 ) ⋅ ⋅ | B n | .

{\displaystyle \operatorname {E} [(X-\mu )^{n}]=s^{n}\pi ^{n}(2^{n}-2)\cdot |B_{n}|.} See also [ edit ] generalized logistic distribution Tukey lambda distribution log-logistic distribution half-logistic distribution logistic regression sigmoid function Notes [ edit ] ^ Norton, Matthew; Khokhlov, Valentyn; Uryasev, Stan (2019).

"Calculating CVaR and bPOE for common probability distributions with application to portfolio optimization and density estimation" (PDF) .

Annals of Operations Research .

299 ( 1– 2). Springer: 1281– 1315.

arXiv : 1811.11301 .

doi : 10.1007/s10479-019-03373-1 . Archived from the original (PDF) on Mar 1, 2023 . Retrieved 2023-02-27 .

^ Johnson, Kotz & Balakrishnan (1995, p.116).

^ Davies, John H. (1998).

The Physics of Low-dimensional Semiconductors: An Introduction . Cambridge University Press.

ISBN 9780521484916 .

^ A. Di Crescenzo, B. Martinucci (2010) "A damped telegraph random process with logistic stationary distribution", J. Appl. Prob.

, vol. 47, pp. 84–96.

^ Ritzema, H.P., ed. (1994).

Frequency and Regression Analysis . Chapter 6 in: Drainage Principles and Applications, Publication 16, International Institute for Land Reclamation and Improvement (ILRI), Wageningen, The Netherlands. pp.

175–224 .

ISBN 90-70754-33-9 .

^ OEIS : A001896 References [ edit ] Wikimedia Commons has media related to Logistic distribution .

John S. deCani & Robert A. Stine (1986). "A note on deriving the information matrix for a logistic distribution".

The American Statistician .

40 (3). American Statistical Association: 220– 222.

doi : 10.2307/2684541 .

JSTOR 2684541 .

N., Balakrishnan (1992).

Handbook of the Logistic Distribution . Marcel Dekker, New York.

ISBN 0-8247-8587-8 .

Johnson, N. L.; Kotz, S.; N., Balakrishnan (1995).

Continuous Univariate Distributions . Vol. 2 (2nd ed.).

ISBN 0-471-58494-0 .

Modis, Theodore (1992) Predictions: Society's Telltale Signature Reveals the Past and Forecasts the Future , Simon & Schuster, New York.

ISBN 0-671-75917-5 v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Authority control databases National United States France BnF data Israel Other Yale LUX Retrieved from " https://en.wikipedia.org/w/index.php?title=Logistic_distribution&oldid=1280996447 " Categories : Continuous distributions Location-scale family probability distributions Hidden categories: Articles with short description Short description matches Wikidata Commons category link from Wikidata This page was last edited on 17 March 2025, at 17:39 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Logistic distribution 18 languages Add topic

