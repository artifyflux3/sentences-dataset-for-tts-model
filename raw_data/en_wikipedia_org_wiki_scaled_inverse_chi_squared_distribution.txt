Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Characterization 2 Parameter estimation 3 Bayesian estimation of the variance of a normal distribution Toggle Bayesian estimation of the variance of a normal distribution subsection 3.1 Use as an informative prior 3.2 Estimation of variance when mean is unknown 4 Related distributions 5 References Toggle the table of contents Scaled inverse chi-squared distribution 1 language Català Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Scaled inverse chi-squared Probability density function Cumulative distribution function Parameters ν ν > 0 {\displaystyle \nu >0\,} τ τ 2 > 0 {\displaystyle \tau ^{2}>0\,} Support x ∈ ∈ ( 0 , ∞ ∞ ) {\displaystyle x\in (0,\infty )} PDF ( τ τ 2 ν ν / 2 ) ν ν / 2 Γ Γ ( ν ν / 2 ) exp ⁡ ⁡ [ − − ν ν τ τ 2 2 x ] x 1 + ν ν / 2 {\displaystyle {\frac {(\tau ^{2}\nu /2)^{\nu /2}}{\Gamma (\nu /2)}}~{\frac {\exp \left[{\frac {-\nu \tau ^{2}}{2x}}\right]}{x^{1+\nu /2}}}} CDF Γ Γ ( ν ν 2 , τ τ 2 ν ν 2 x ) / Γ Γ ( ν ν 2 ) {\displaystyle \Gamma \left({\frac {\nu }{2}},{\frac {\tau ^{2}\nu }{2x}}\right)\left/\Gamma \left({\frac {\nu }{2}}\right)\right.} Mean ν ν τ τ 2 ν ν − − 2 {\displaystyle {\frac {\nu \tau ^{2}}{\nu -2}}} for ν ν > 2 {\displaystyle \nu >2\,} Mode ν ν τ τ 2 ν ν + 2 {\displaystyle {\frac {\nu \tau ^{2}}{\nu +2}}} Variance 2 ν ν 2 τ τ 4 ( ν ν − − 2 ) 2 ( ν ν − − 4 ) {\displaystyle {\frac {2\nu ^{2}\tau ^{4}}{(\nu -2)^{2}(\nu -4)}}} for ν ν > 4 {\displaystyle \nu >4\,} Skewness 4 ν ν − − 6 2 ( ν ν − − 4 ) {\displaystyle {\frac {4}{\nu -6}}{\sqrt {2(\nu -4)}}} for ν ν > 6 {\displaystyle \nu >6\,} Excess kurtosis 12 ( 5 ν ν − − 22 ) ( ν ν − − 6 ) ( ν ν − − 8 ) {\displaystyle {\frac {12(5\nu -22)}{(\nu -6)(\nu -8)}}} for ν ν > 8 {\displaystyle \nu >8\,} Entropy ν ν 2 + ln ⁡ ⁡ ( τ τ 2 ν ν 2 Γ Γ ( ν ν 2 ) ) {\displaystyle {\frac {\nu }{2}}\!+\!\ln \left({\frac {\tau ^{2}\nu }{2}}\Gamma \left({\frac {\nu }{2}}\right)\right)} − − ( 1 + ν ν 2 ) ψ ψ ( ν ν 2 ) {\displaystyle \!-\!\left(1\!+\!{\frac {\nu }{2}}\right)\psi \left({\frac {\nu }{2}}\right)} MGF 2 Γ Γ ( ν ν 2 ) ( − − τ τ 2 ν ν t 2 ) ν ν 4 K ν ν 2 ( − − 2 τ τ 2 ν ν t ) {\displaystyle {\frac {2}{\Gamma ({\frac {\nu }{2}})}}\left({\frac {-\tau ^{2}\nu t}{2}}\right)^{\!\!{\frac {\nu }{4}}}\!\!K_{\frac {\nu }{2}}\left({\sqrt {-2\tau ^{2}\nu t}}\right)} CF 2 Γ Γ ( ν ν 2 ) ( − − i τ τ 2 ν ν t 2 ) ν ν 4 K ν ν 2 ( − − 2 i τ τ 2 ν ν t ) {\displaystyle {\frac {2}{\Gamma ({\frac {\nu }{2}})}}\left({\frac {-i\tau ^{2}\nu t}{2}}\right)^{\!\!{\frac {\nu }{4}}}\!\!K_{\frac {\nu }{2}}\left({\sqrt {-2i\tau ^{2}\nu t}}\right)} The scaled inverse chi-squared distribution ψ ψ inv- χ χ 2 ( ν ν ) {\displaystyle \psi \,{\mbox{inv-}}\chi ^{2}(\nu )} , where ψ ψ {\displaystyle \psi } is the scale parameter, equals the univariate inverse Wishart distribution W − − 1 ( ψ ψ , ν ν ) {\displaystyle {\mathcal {W}}^{-1}(\psi ,\nu )} with degrees of freedom ν ν {\displaystyle \nu } .

This family of scaled inverse chi-squared distributions is linked to the inverse-chi-squared distribution and to the chi-squared distribution : If X ∼ ∼ ψ ψ inv- χ χ 2 ( ν ν ) {\displaystyle X\sim \psi \,{\mbox{inv-}}\chi ^{2}(\nu )} then X / ψ ψ ∼ ∼ inv- χ χ 2 ( ν ν ) {\displaystyle X/\psi \sim {\mbox{inv-}}\chi ^{2}(\nu )} as well as ψ ψ / X ∼ ∼ χ χ 2 ( ν ν ) {\displaystyle \psi /X\sim \chi ^{2}(\nu )} and 1 / X ∼ ∼ ψ ψ − − 1 χ χ 2 ( ν ν ) {\displaystyle 1/X\sim \psi ^{-1}\chi ^{2}(\nu )} .

Instead of ψ ψ {\displaystyle \psi } , the scaled inverse chi-squared distribution is however most frequently
parametrized by the scale parameter τ τ 2 = ψ ψ / ν ν {\displaystyle \tau ^{2}=\psi /\nu } and the distribution ν ν τ τ 2 inv- χ χ 2 ( ν ν ) {\displaystyle \nu \tau ^{2}\,{\mbox{inv-}}\chi ^{2}(\nu )} is denoted by Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle {\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} .

In terms of τ τ 2 {\displaystyle \tau ^{2}} the above relations can be written as follows: If X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle X\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} then X ν ν τ τ 2 ∼ ∼ inv- χ χ 2 ( ν ν ) {\displaystyle {\frac {X}{\nu \tau ^{2}}}\sim {\mbox{inv-}}\chi ^{2}(\nu )} as well as ν ν τ τ 2 X ∼ ∼ χ χ 2 ( ν ν ) {\displaystyle {\frac {\nu \tau ^{2}}{X}}\sim \chi ^{2}(\nu )} and 1 / X ∼ ∼ 1 ν ν τ τ 2 χ χ 2 ( ν ν ) {\displaystyle 1/X\sim {\frac {1}{\nu \tau ^{2}}}\chi ^{2}(\nu )} .

This family of scaled inverse chi-squared distributions is a reparametrization of the inverse-gamma distribution .

Specifically, if X ∼ ∼ ψ ψ inv- χ χ 2 ( ν ν ) = Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle X\sim \psi \,{\mbox{inv-}}\chi ^{2}(\nu )={\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} then X ∼ ∼ Inv-Gamma ( ν ν 2 , ψ ψ 2 ) = Inv-Gamma ( ν ν 2 , ν ν τ τ 2 2 ) {\displaystyle X\sim {\textrm {Inv-Gamma}}\left({\frac {\nu }{2}},{\frac {\psi }{2}}\right)={\textrm {Inv-Gamma}}\left({\frac {\nu }{2}},{\frac {\nu \tau ^{2}}{2}}\right)} Either form may be used to represent the maximum entropy distribution for a fixed first inverse moment ( E ( 1 / X ) ) {\displaystyle (E(1/X))} and first logarithmic moment ( E ( ln ⁡ ⁡ ( X ) ) {\displaystyle (E(\ln(X))} .

The scaled inverse chi-squared distribution also has a particular use in Bayesian statistics .  Specifically, the scaled inverse chi-squared distribution can be used as a conjugate prior for the variance parameter of a normal distribution . 
The same prior in alternative parametrization is given by 
the inverse-gamma distribution .

Characterization [ edit ] The probability density function of the scaled inverse chi-squared distribution extends over the domain x > 0 {\displaystyle x>0} and  is f ( x ; ν ν , τ τ 2 ) = ( τ τ 2 ν ν / 2 ) ν ν / 2 Γ Γ ( ν ν / 2 ) exp ⁡ ⁡ [ − − ν ν τ τ 2 2 x ] x 1 + ν ν / 2 {\displaystyle f(x;\nu ,\tau ^{2})={\frac {(\tau ^{2}\nu /2)^{\nu /2}}{\Gamma (\nu /2)}}~{\frac {\exp \left[{\frac {-\nu \tau ^{2}}{2x}}\right]}{x^{1+\nu /2}}}} where ν ν {\displaystyle \nu } is the degrees of freedom parameter and τ τ 2 {\displaystyle \tau ^{2}} is the scale parameter . The cumulative distribution function is F ( x ; ν ν , τ τ 2 ) = Γ Γ ( ν ν 2 , τ τ 2 ν ν 2 x ) / Γ Γ ( ν ν 2 ) {\displaystyle F(x;\nu ,\tau ^{2})=\Gamma \left({\frac {\nu }{2}},{\frac {\tau ^{2}\nu }{2x}}\right)\left/\Gamma \left({\frac {\nu }{2}}\right)\right.} = Q ( ν ν 2 , τ τ 2 ν ν 2 x ) {\displaystyle =Q\left({\frac {\nu }{2}},{\frac {\tau ^{2}\nu }{2x}}\right)} where Γ Γ ( a , x ) {\displaystyle \Gamma (a,x)} is the incomplete gamma function , Γ Γ ( x ) {\displaystyle \Gamma (x)} is the gamma function and Q ( a , x ) {\displaystyle Q(a,x)} is a regularized gamma function . The characteristic function is φ φ ( t ; ν ν , τ τ 2 ) = {\displaystyle \varphi (t;\nu ,\tau ^{2})=} 2 Γ Γ ( ν ν 2 ) ( − − i τ τ 2 ν ν t 2 ) ν ν 4 K ν ν 2 ( − − 2 i τ τ 2 ν ν t ) , {\displaystyle {\frac {2}{\Gamma ({\frac {\nu }{2}})}}\left({\frac {-i\tau ^{2}\nu t}{2}}\right)^{\!\!{\frac {\nu }{4}}}\!\!K_{\frac {\nu }{2}}\left({\sqrt {-2i\tau ^{2}\nu t}}\right),} where K ν ν 2 ( z ) {\displaystyle K_{\frac {\nu }{2}}(z)} is the modified Modified Bessel function of the second kind .

Parameter estimation [ edit ] The maximum likelihood estimate of τ τ 2 {\displaystyle \tau ^{2}} is τ τ 2 = n / ∑ ∑ i = 1 n 1 x i .

{\displaystyle \tau ^{2}=n/\sum _{i=1}^{n}{\frac {1}{x_{i}}}.} The maximum likelihood estimate of ν ν 2 {\displaystyle {\frac {\nu }{2}}} can be found using Newton's method on: ln ⁡ ⁡ ( ν ν 2 ) − − ψ ψ ( ν ν 2 ) = 1 n ∑ ∑ i = 1 n ln ⁡ ⁡ ( x i ) − − ln ⁡ ⁡ ( τ τ 2 ) , {\displaystyle \ln \left({\frac {\nu }{2}}\right)-\psi \left({\frac {\nu }{2}}\right)={\frac {1}{n}}\sum _{i=1}^{n}\ln \left(x_{i}\right)-\ln \left(\tau ^{2}\right),} where ψ ψ ( x ) {\displaystyle \psi (x)} is the digamma function .  An initial estimate can be found by taking the formula for mean and solving it for ν ν .

{\displaystyle \nu .} Let x ¯ ¯ = 1 n ∑ ∑ i = 1 n x i {\displaystyle {\bar {x}}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}} be the sample mean.  Then an initial estimate for ν ν {\displaystyle \nu } is given by: ν ν 2 = x ¯ ¯ x ¯ ¯ − − τ τ 2 .

{\displaystyle {\frac {\nu }{2}}={\frac {\bar {x}}{{\bar {x}}-\tau ^{2}}}.} Bayesian estimation of the variance of a normal distribution [ edit ] The scaled inverse chi-squared distribution has a second important application, in the Bayesian estimation of the variance of a Normal distribution.

According to Bayes' theorem , the posterior probability distribution for quantities of interest is proportional to the product of a prior distribution for the quantities and a likelihood function : p ( σ σ 2 | D , I ) ∝ ∝ p ( σ σ 2 | I ) p ( D | σ σ 2 ) {\displaystyle p(\sigma ^{2}|D,I)\propto p(\sigma ^{2}|I)\;p(D|\sigma ^{2})} where D represents the data and I represents any initial information about σ 2 that we may already have.

The simplest scenario arises if the mean μ is already known; or, alternatively, if it is the conditional distribution of σ 2 that is sought, for a particular assumed value of μ.

Then the likelihood term L (σ 2 | D ) = p ( D |σ 2 ) has the familiar form L ( σ σ 2 | D , μ μ ) = 1 ( 2 π π σ σ ) n exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − μ μ ) 2 2 σ σ 2 ] {\displaystyle {\mathcal {L}}(\sigma ^{2}|D,\mu )={\frac {1}{\left({\sqrt {2\pi }}\sigma \right)^{n}}}\;\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-\mu )^{2}}{2\sigma ^{2}}}\right]} Combining this with the rescaling-invariant prior p(σ 2 | I ) = 1/σ 2 , which can be argued (e.g.

following Jeffreys ) to be the least informative possible prior for σ 2 in this problem, gives a combined posterior probability p ( σ σ 2 | D , I , μ μ ) ∝ ∝ 1 σ σ n + 2 exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − μ μ ) 2 2 σ σ 2 ] {\displaystyle p(\sigma ^{2}|D,I,\mu )\propto {\frac {1}{\sigma ^{n+2}}}\;\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-\mu )^{2}}{2\sigma ^{2}}}\right]} This form can be recognised as that of a scaled inverse chi-squared distribution, with parameters ν = n and τ 2 = s 2 = (1/ n ) Σ (x i -μ) 2 Gelman and co-authors remark that the re-appearance of this distribution, previously seen in a sampling context, may seem remarkable; but given the choice of prior "this result is not surprising." [ 1 ] In particular, the choice of a rescaling-invariant prior for σ 2 has the result that the probability for the ratio of σ 2 / s 2 has the same form (independent of the conditioning variable) when conditioned on s 2 as when conditioned on σ 2 : p ( σ σ 2 s 2 | s 2 ) = p ( σ σ 2 s 2 | σ σ 2 ) {\displaystyle p({\tfrac {\sigma ^{2}}{s^{2}}}|s^{2})=p({\tfrac {\sigma ^{2}}{s^{2}}}|\sigma ^{2})} In the sampling-theory case, conditioned on σ 2 , the probability distribution for (1/s 2 ) is a scaled inverse chi-squared distribution; and so the probability distribution for σ 2 conditioned on s 2 , given a scale-agnostic prior, is also a scaled inverse chi-squared distribution.

Use as an informative prior [ edit ] If more is known about the possible values of σ 2 , a distribution from the scaled inverse chi-squared family, such as Scale-inv-χ 2 ( n 0 , s 0 2 ) can be a convenient form to represent a more informative prior for σ 2 , as if from the result of n 0 previous observations (though n 0 need not necessarily be a whole number): p ( σ σ 2 | I ′ ′ , μ μ ) ∝ ∝ 1 σ σ n 0 + 2 exp ⁡ ⁡ [ − − n 0 s 0 2 2 σ σ 2 ] {\displaystyle p(\sigma ^{2}|I^{\prime },\mu )\propto {\frac {1}{\sigma ^{n_{0}+2}}}\;\exp \left[-{\frac {n_{0}s_{0}^{2}}{2\sigma ^{2}}}\right]} Such a prior would lead to the posterior distribution p ( σ σ 2 | D , I ′ ′ , μ μ ) ∝ ∝ 1 σ σ n + n 0 + 2 exp ⁡ ⁡ [ − − n s 2 + n 0 s 0 2 2 σ σ 2 ] {\displaystyle p(\sigma ^{2}|D,I^{\prime },\mu )\propto {\frac {1}{\sigma ^{n+n_{0}+2}}}\;\exp \left[-{\frac {ns^{2}+n_{0}s_{0}^{2}}{2\sigma ^{2}}}\right]} which is itself a scaled inverse chi-squared distribution.  The scaled inverse chi-squared distributions are thus a convenient conjugate prior family for σ 2 estimation.

Estimation of variance when mean is unknown [ edit ] If the mean is not known, the most uninformative prior that can be taken for it is arguably the translation-invariant prior p (μ| I ) ∝ const., which gives the following joint posterior distribution for μ and σ 2 , p ( μ μ , σ σ 2 ∣ ∣ D , I ) ∝ ∝ 1 σ σ n + 2 exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − μ μ ) 2 2 σ σ 2 ] = 1 σ σ n + 2 exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − x ¯ ¯ ) 2 2 σ σ 2 ] exp ⁡ ⁡ [ − − n ( μ μ − − x ¯ ¯ ) 2 2 σ σ 2 ] {\displaystyle {\begin{aligned}p(\mu ,\sigma ^{2}\mid D,I)&\propto {\frac {1}{\sigma ^{n+2}}}\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-\mu )^{2}}{2\sigma ^{2}}}\right]\\&={\frac {1}{\sigma ^{n+2}}}\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-{\bar {x}})^{2}}{2\sigma ^{2}}}\right]\exp \left[-{\frac {n(\mu -{\bar {x}})^{2}}{2\sigma ^{2}}}\right]\end{aligned}}} The marginal posterior distribution for σ 2 is obtained from the joint posterior distribution by integrating out over μ, p ( σ σ 2 | D , I ) ∝ ∝ 1 σ σ n + 2 exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − x ¯ ¯ ) 2 2 σ σ 2 ] ∫ ∫ − − ∞ ∞ ∞ ∞ exp ⁡ ⁡ [ − − n ( μ μ − − x ¯ ¯ ) 2 2 σ σ 2 ] d μ μ = 1 σ σ n + 2 exp ⁡ ⁡ [ − − ∑ ∑ i n ( x i − − x ¯ ¯ ) 2 2 σ σ 2 ] 2 π π σ σ 2 / n ∝ ∝ ( σ σ 2 ) − − ( n + 1 ) / 2 exp ⁡ ⁡ [ − − ( n − − 1 ) s 2 2 σ σ 2 ] {\displaystyle {\begin{aligned}p(\sigma ^{2}|D,I)\;\propto \;&{\frac {1}{\sigma ^{n+2}}}\;\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-{\bar {x}})^{2}}{2\sigma ^{2}}}\right]\;\int _{-\infty }^{\infty }\exp \left[-{\frac {n(\mu -{\bar {x}})^{2}}{2\sigma ^{2}}}\right]d\mu \\=\;&{\frac {1}{\sigma ^{n+2}}}\;\exp \left[-{\frac {\sum _{i}^{n}(x_{i}-{\bar {x}})^{2}}{2\sigma ^{2}}}\right]\;{\sqrt {2\pi \sigma ^{2}/n}}\\\propto \;&(\sigma ^{2})^{-(n+1)/2}\;\exp \left[-{\frac {(n-1)s^{2}}{2\sigma ^{2}}}\right]\end{aligned}}} This is again a scaled inverse chi-squared distribution, with parameters n − − 1 {\displaystyle \scriptstyle {n-1}\;} and s 2 = ∑ ∑ ( x i − − x ¯ ¯ ) 2 / ( n − − 1 ) {\displaystyle \scriptstyle {s^{2}=\sum (x_{i}-{\bar {x}})^{2}/(n-1)}} .

Related distributions [ edit ] If X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle X\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} then k X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , k τ τ 2 ) {\displaystyle kX\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,k\tau ^{2})\,} If X ∼ ∼ inv- χ χ 2 ( ν ν ) {\displaystyle X\sim {\mbox{inv-}}\chi ^{2}(\nu )\,} ( Inverse-chi-squared distribution ) then X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , 1 / ν ν ) {\displaystyle X\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,1/\nu )\,} If X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle X\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} then X τ τ 2 ν ν ∼ ∼ inv- χ χ 2 ( ν ν ) {\displaystyle {\frac {X}{\tau ^{2}\nu }}\sim {\mbox{inv-}}\chi ^{2}(\nu )\,} ( Inverse-chi-squared distribution ) If X ∼ ∼ Scale-inv- χ χ 2 ( ν ν , τ τ 2 ) {\displaystyle X\sim {\mbox{Scale-inv-}}\chi ^{2}(\nu ,\tau ^{2})} then X ∼ ∼ Inv-Gamma ( ν ν 2 , ν ν τ τ 2 2 ) {\displaystyle X\sim {\textrm {Inv-Gamma}}\left({\frac {\nu }{2}},{\frac {\nu \tau ^{2}}{2}}\right)} ( Inverse-gamma distribution ) Scaled inverse chi square distribution is a special case of type 5 Pearson distribution References [ edit ] Gelman, Andrew; et al. (2014).

Bayesian Data Analysis (Third ed.). Boca Raton: CRC Press. p. 583.

ISBN 978-1-4398-4095-5 .

^ Gelman, Andrew; et al. (2014).

Bayesian Data Analysis (Third ed.). Boca Raton: CRC Press. p. 65.

ISBN 978-1-4398-4095-5 .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Scaled_inverse_chi-squared_distribution&oldid=1279639940 " Categories : Continuous distributions Exponential family distributions Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 9 March 2025, at 18:40 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Scaled inverse chi-squared distribution 1 language Add topic

