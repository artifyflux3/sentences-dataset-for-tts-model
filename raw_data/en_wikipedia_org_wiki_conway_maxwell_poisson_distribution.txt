Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Background 2 Probability mass function and basic properties 3 Cumulative distribution function 4 The normalizing constant 5 Moments, cumulants and related results 6 Moments for the case of integer '"`UNIQ--postMath-0000004A-QINU`"' 7 Median, mode and mean deviation 8 Stein characterisation 9 Use as a limiting distribution 10 Related distributions 11 Parameter estimation Toggle Parameter estimation subsection 11.1 Weighted least squares 11.2 Maximum likelihood 12 Generalized linear model 13 References 14 External links Toggle the table of contents Conway–Maxwell–Poisson distribution 2 languages Català Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Conway–Maxwell–Poisson Probability mass function Cumulative distribution function Parameters λ λ > 0 , ν ν ≥ ≥ 0 {\displaystyle \lambda >0,\nu \geq 0} Support x ∈ ∈ { 0 , 1 , 2 , … … } {\displaystyle x\in \{0,1,2,\dots \}} PMF λ λ x ( x !

) ν ν 1 Z ( λ λ , ν ν ) {\displaystyle {\frac {\lambda ^{x}}{(x!)^{\nu }}}{\frac {1}{Z(\lambda ,\nu )}}} CDF ∑ ∑ i = 0 x Pr ( X = i ) {\displaystyle \sum _{i=0}^{x}\Pr(X=i)} Mean ∑ ∑ j = 0 ∞ ∞ j λ λ j ( j !

) ν ν Z ( λ λ , ν ν ) {\displaystyle \sum _{j=0}^{\infty }{\frac {j\lambda ^{j}}{(j!)^{\nu }Z(\lambda ,\nu )}}} Median No closed form Mode See text Variance ∑ ∑ j = 0 ∞ ∞ j 2 λ λ j ( j !

) ν ν Z ( λ λ , ν ν ) − − mean 2 {\displaystyle \sum _{j=0}^{\infty }{\frac {j^{2}\lambda ^{j}}{(j!)^{\nu }Z(\lambda ,\nu )}}-\operatorname {mean} ^{2}} Skewness Not listed Excess kurtosis Not listed Entropy Not listed MGF Z ( e t λ λ , ν ν ) Z ( λ λ , ν ν ) {\displaystyle {\frac {Z(e^{t}\lambda ,\nu )}{Z(\lambda ,\nu )}}} CF Z ( e i t λ λ , ν ν ) Z ( λ λ , ν ν ) {\displaystyle {\frac {Z(e^{it}\lambda ,\nu )}{Z(\lambda ,\nu )}}} PGF Z ( t λ λ , ν ν ) Z ( λ λ , ν ν ) {\displaystyle {\frac {Z(t\lambda ,\nu )}{Z(\lambda ,\nu )}}} In probability theory and statistics , the Conway–Maxwell–Poisson (CMP or COM–Poisson) distribution is a discrete probability distribution named after Richard W. Conway , William L. Maxwell , and Siméon Denis Poisson that generalizes the Poisson distribution by adding a parameter to model overdispersion and underdispersion . It is a member of the exponential family , [ 1 ] has the Poisson distribution and geometric distribution as special cases and the Bernoulli distribution as a limiting case .

[ 2 ] Background [ edit ] The CMP distribution was originally proposed by Conway and Maxwell in 1962 [ 3 ] as a solution to handling queueing systems with state-dependent service rates.  The CMP distribution was introduced into the statistics literature by Boatwright et al. 2003 [ 4 ] and Shmueli et al. (2005).

[ 2 ] The first detailed investigation into the
probabilistic and statistical properties of the distribution was published by Shmueli et al. (2005).

[ 2 ] Some theoretical probability results of COM-Poisson distribution is studied and reviewed by Li et al. (2019), [ 5 ] especially the characterizations of COM-Poisson distribution.

Probability mass function and basic properties [ edit ] The CMP distribution is defined to be the distribution with probability mass function P ( X = x ) = f ( x ; λ λ , ν ν ) = λ λ x ( x !

) ν ν 1 Z ( λ λ , ν ν ) .

{\displaystyle P(X=x)=f(x;\lambda ,\nu )={\frac {\lambda ^{x}}{(x!)^{\nu }}}{\frac {1}{Z(\lambda ,\nu )}}.} where : Z ( λ λ , ν ν ) = ∑ ∑ j = 0 ∞ ∞ λ λ j ( j !

) ν ν .

{\displaystyle Z(\lambda ,\nu )=\sum _{j=0}^{\infty }{\frac {\lambda ^{j}}{(j!)^{\nu }}}.} The function Z ( λ λ , ν ν ) {\displaystyle Z(\lambda ,\nu )} serves as a normalization constant so the probability mass function sums to one. Note that Z ( λ λ , ν ν ) {\displaystyle Z(\lambda ,\nu )} does not have a closed form.

The domain of admissible parameters is λ λ , ν ν > 0 {\displaystyle \lambda ,\nu >0} , and 0 < λ λ < 1 {\displaystyle 0<\lambda <1} , ν ν = 0 {\displaystyle \nu =0} .

The additional parameter ν ν {\displaystyle \nu } which does not appear in the Poisson distribution allows for adjustment of the rate of decay. This rate of decay is a non-linear decrease in ratios of successive probabilities, specifically P ( X = x − − 1 ) P ( X = x ) = x ν ν λ λ .

{\displaystyle {\frac {P(X=x-1)}{P(X=x)}}={\frac {x^{\nu }}{\lambda }}.} When ν ν = 1 {\displaystyle \nu =1} , the CMP distribution becomes the standard Poisson distribution and as ν ν → → ∞ ∞ {\displaystyle \nu \to \infty } , the distribution approaches a Bernoulli distribution with parameter λ λ / ( 1 + λ λ ) {\displaystyle \lambda /(1+\lambda )} . When ν ν = 0 {\displaystyle \nu =0} the CMP distribution reduces to a geometric distribution with probability of success 1 − − λ λ {\displaystyle 1-\lambda } provided λ λ < 1 {\displaystyle \lambda <1} .

[ 2 ] For the CMP distribution, moments can be found through the recursive formula [ 2 ] E ⁡ ⁡ [ X r + 1 ] = { λ λ E ⁡ ⁡ [ X + 1 ] 1 − − ν ν if r = 0 λ λ d d λ λ E ⁡ ⁡ [ X r ] + E ⁡ ⁡ [ X ] E ⁡ ⁡ [ X r ] if r > 0.

{\displaystyle \operatorname {E} [X^{r+1}]={\begin{cases}\lambda \,\operatorname {E} [X+1]^{1-\nu }&{\text{if }}r=0\\\lambda \,{\frac {d}{d\lambda }}\operatorname {E} [X^{r}]+\operatorname {E} [X]\operatorname {E} [X^{r}]&{\text{if }}r>0.\\\end{cases}}} Cumulative distribution function [ edit ] For general ν ν {\displaystyle \nu } , there does not exist a closed form formula for the cumulative distribution function of X ∼ ∼ C M P ( λ λ , ν ν ) {\displaystyle X\sim \mathrm {CMP} (\lambda ,\nu )} .  If ν ν ≥ ≥ 1 {\displaystyle \nu \geq 1} is an integer, we can, however, obtain the following formula in terms of the generalized hypergeometric function : [ 6 ] F ( n ) = P ( X ≤ ≤ n ) = 1 − − 1 F ν ν − − 1 ( ; n + 2 , … … , n + 2 ; λ λ ) { ( n + 1 ) !

} ν ν − − 1 0 F ν ν − − 1 ( ; 1 , … … , 1 ; λ λ ) .

{\displaystyle F(n)=P(X\leq n)=1-{\frac {_{1}F_{\nu -1}(;n+2,\ldots ,n+2;\lambda )}{{\{(n+1)!\}^{\nu -1}}_{0}F_{\nu -1}(;1,\ldots ,1;\lambda )}}.} The normalizing constant [ edit ] Many important summary statistics, such as moments and cumulants, of the CMP distribution can be expressed in terms of the normalizing constant Z ( λ λ , ν ν ) {\displaystyle Z(\lambda ,\nu )} .

[ 2 ] [ 7 ] Indeed, The probability generating function is E ⁡ ⁡ s X = Z ( s λ λ , ν ν ) / Z ( λ λ , ν ν ) {\displaystyle \operatorname {E} s^{X}=Z(s\lambda ,\nu )/Z(\lambda ,\nu )} , and the mean and variance are given by E ⁡ ⁡ X = λ λ d d λ λ { ln ⁡ ⁡ ( Z ( λ λ , ν ν ) ) } , {\displaystyle \operatorname {E} X=\lambda {\frac {d}{d\lambda }}{\big \{}\ln(Z(\lambda ,\nu )){\big \}},} var ⁡ ⁡ ( X ) = λ λ d d λ λ E ⁡ ⁡ X .

{\displaystyle \operatorname {var} (X)=\lambda {\frac {d}{d\lambda }}\operatorname {E} X.} The cumulant generating function is g ( t ) = ln ⁡ ⁡ ( E ⁡ ⁡ [ e t X ] ) = ln ⁡ ⁡ ( Z ( λ λ e t , ν ν ) ) − − ln ⁡ ⁡ ( Z ( λ λ , ν ν ) ) , {\displaystyle g(t)=\ln(\operatorname {E} [e^{tX}])=\ln(Z(\lambda e^{t},\nu ))-\ln(Z(\lambda ,\nu )),} and the cumulants are given by κ κ n = g ( n ) ( 0 ) = ∂ ∂ n ∂ ∂ t n ln ⁡ ⁡ ( Z ( λ λ e t , ν ν ) ) | t = 0 , n ≥ ≥ 1.

{\displaystyle \kappa _{n}=g^{(n)}(0)={\frac {\partial ^{n}}{\partial t^{n}}}\ln(Z(\lambda e^{t},\nu )){\bigg |}_{t=0},\quad n\geq 1.} Whilst the normalizing constant Z ( λ λ , ν ν ) = ∑ ∑ i = 0 ∞ ∞ λ λ i ( i !

) ν ν {\displaystyle Z(\lambda ,\nu )=\sum _{i=0}^{\infty }{\frac {\lambda ^{i}}{(i!)^{\nu }}}} does not in general have a closed form, there are some noteworthy special cases: Z ( λ λ , 1 ) = e λ λ {\displaystyle Z(\lambda ,1)=\mathrm {e} ^{\lambda }} Z ( λ λ , 0 ) = ( 1 − − λ λ ) − − 1 {\displaystyle Z(\lambda ,0)=(1-\lambda )^{-1}} lim ν ν → → ∞ ∞ Z ( λ λ , ν ν ) = 1 + λ λ {\displaystyle \lim _{\nu \rightarrow \infty }Z(\lambda ,\nu )=1+\lambda } Z ( λ λ , 2 ) = I 0 ( 2 λ λ ) {\displaystyle Z(\lambda ,2)=I_{0}(2{\sqrt {\lambda }})} , where I 0 ( x ) = ∑ ∑ k = 0 ∞ ∞ 1 ( k !

) 2 ( x 2 ) 2 k {\displaystyle I_{0}(x)=\sum _{k=0}^{\infty }{\frac {1}{(k!)^{2}}}{\big (}{\frac {x}{2}}{\big )}^{2k}} is a modified Bessel function of the first kind.

[ 7 ] For integer ν ν {\displaystyle \nu } , the normalizing constant can expressed [ 6 ] as a generalized hypergeometric function: Z ( λ λ , ν ν ) = 0 F ν ν − − 1 ( ; 1 , … … , 1 ; λ λ ) {\displaystyle Z(\lambda ,\nu )=_{0}F_{\nu -1}(;1,\ldots ,1;\lambda )} .

Because the normalizing constant does not in general have a closed form, the following asymptotic expansion is of interest.  Fix ν ν > 0 {\displaystyle \nu >0} .  Then, as λ λ → → ∞ ∞ {\displaystyle \lambda \rightarrow \infty } , [ 8 ] Z ( λ λ , ν ν ) = exp ⁡ ⁡ { ν ν λ λ 1 / ν ν } λ λ ( ν ν − − 1 ) / 2 ν ν ( 2 π π ) ( ν ν − − 1 ) / 2 ν ν ∑ ∑ k = 0 ∞ ∞ c k ( ν ν λ λ 1 / ν ν ) − − k , {\displaystyle Z(\lambda ,\nu )={\frac {\exp \left\{\nu \lambda ^{1/\nu }\right\}}{\lambda ^{(\nu -1)/2\nu }(2\pi )^{(\nu -1)/2}{\sqrt {\nu }}}}\sum _{k=0}^{\infty }c_{k}{\big (}\nu \lambda ^{1/\nu }{\big )}^{-k},} where the c j {\displaystyle c_{j}} are uniquely determined by the expansion ( Γ Γ ( t + 1 ) ) − − ν ν = ν ν ν ν ( t + 1 / 2 ) ( 2 π π ) ( ν ν − − 1 ) / 2 ∑ ∑ j = 0 ∞ ∞ c j Γ Γ ( ν ν t + ( 1 + ν ν ) / 2 + j ) .

{\displaystyle \left(\Gamma (t+1)\right)^{-\nu }={\frac {\nu ^{\nu (t+1/2)}}{\left(2\pi \right)^{(\nu -1)/2}}}\sum _{j=0}^{\infty }{\frac {c_{j}}{\Gamma (\nu t+(1+\nu )/2+j)}}.} In particular, c 0 = 1 {\displaystyle c_{0}=1} , c 1 = ν ν 2 − − 1 24 {\displaystyle c_{1}={\frac {\nu ^{2}-1}{24}}} , c 2 = ν ν 2 − − 1 1152 ( ν ν 2 + 23 ) {\displaystyle c_{2}={\frac {\nu ^{2}-1}{1152}}\left(\nu ^{2}+23\right)} .  Further coefficients are given in.

[ 8 ] Moments, cumulants and related results [ edit ] For general values of ν ν {\displaystyle \nu } , there does not exist closed form formulas for the mean, variance and moments of the CMP distribution.  We do, however, have the following neat formula.

[ 7 ] Let ( j ) r = j ( j − − 1 ) ⋯ ⋯ ( j − − r + 1 ) {\displaystyle (j)_{r}=j(j-1)\cdots (j-r+1)} denote the falling factorial .  Let X ∼ ∼ C M P ( λ λ , ν ν ) {\displaystyle X\sim \mathrm {CMP} (\lambda ,\nu )} , λ λ , ν ν > 0 {\displaystyle \lambda ,\nu >0} .  Then E ⁡ ⁡ [ ( ( X ) r ) ν ν ] = λ λ r , {\displaystyle \operatorname {E} [((X)_{r})^{\nu }]=\lambda ^{r},} for r ∈ ∈ N {\displaystyle r\in \mathbb {N} } .

Since in general closed form formulas are not available for moments and cumulants of the CMP distribution, the following asymptotic formulas are of interest.  Let X ∼ ∼ C M P ( λ λ , ν ν ) {\displaystyle X\sim \mathrm {CMP} (\lambda ,\nu )} , where ν ν > 0 {\displaystyle \nu >0} .  Denote the skewness γ γ 1 = κ κ 3 σ σ 3 {\displaystyle \gamma _{1}={\frac {\kappa _{3}}{\sigma ^{3}}}} and excess kurtosis γ γ 2 = κ κ 4 σ σ 4 {\displaystyle \gamma _{2}={\frac {\kappa _{4}}{\sigma ^{4}}}} , where σ σ 2 = V a r ( X ) {\displaystyle \sigma ^{2}=\mathrm {Var} (X)} .  Then, as λ λ → → ∞ ∞ {\displaystyle \lambda \rightarrow \infty } , [ 8 ] E ⁡ ⁡ X = λ λ 1 / ν ν ( 1 − − ν ν − − 1 2 ν ν λ λ − − 1 / ν ν − − ν ν 2 − − 1 24 ν ν 2 λ λ − − 2 / ν ν − − ν ν 2 − − 1 24 ν ν 3 λ λ − − 3 / ν ν + O ( λ λ − − 4 / ν ν ) ) , {\displaystyle \operatorname {E} X=\lambda ^{1/\nu }\left(1-{\frac {\nu -1}{2\nu }}\lambda ^{-1/\nu }-{\frac {\nu ^{2}-1}{24\nu ^{2}}}\lambda ^{-2/\nu }-{\frac {\nu ^{2}-1}{24\nu ^{3}}}\lambda ^{-3/\nu }+{\mathcal {O}}(\lambda ^{-4/\nu })\right),} V a r ( X ) = λ λ 1 / ν ν ν ν ( 1 + ν ν 2 − − 1 24 ν ν 2 λ λ − − 2 / ν ν + ν ν 2 − − 1 12 ν ν 3 λ λ − − 3 / ν ν + O ( λ λ − − 4 / ν ν ) ) , {\displaystyle \mathrm {Var} (X)={\frac {\lambda ^{1/\nu }}{\nu }}{\bigg (}1+{\frac {\nu ^{2}-1}{24\nu ^{2}}}\lambda ^{-2/\nu }+{\frac {\nu ^{2}-1}{12\nu ^{3}}}\lambda ^{-3/\nu }+{\mathcal {O}}(\lambda ^{-4/\nu }){\bigg )},} κ κ n = λ λ 1 / ν ν ν ν n − − 1 ( 1 + ( − − 1 ) n ( ν ν 2 − − 1 ) 24 ν ν 2 λ λ − − 2 / ν ν + ( − − 2 ) n ( ν ν 2 − − 1 ) 48 ν ν 3 λ λ − − 3 / ν ν + O ( λ λ − − 4 / ν ν ) ) , {\displaystyle \kappa _{n}={\frac {\lambda ^{1/\nu }}{\nu ^{n-1}}}{\bigg (}1+{\frac {(-1)^{n}(\nu ^{2}-1)}{24\nu ^{2}}}\lambda ^{-2/\nu }+{\frac {(-2)^{n}(\nu ^{2}-1)}{48\nu ^{3}}}\lambda ^{-3/\nu }+{\mathcal {O}}(\lambda ^{-4/\nu }){\bigg )},} γ γ 1 = λ λ − − 1 / 2 ν ν ν ν ( 1 − − 5 ( ν ν 2 − − 1 ) 48 ν ν 2 λ λ − − 2 / ν ν − − 7 ( ν ν 2 − − 1 ) 24 ν ν 3 λ λ − − 3 / ν ν + O ( λ λ − − 4 / ν ν ) ) , {\displaystyle \gamma _{1}={\frac {\lambda ^{-1/2\nu }}{\sqrt {\nu }}}{\bigg (}1-{\frac {5(\nu ^{2}-1)}{48\nu ^{2}}}\lambda ^{-2/\nu }-{\frac {7(\nu ^{2}-1)}{24\nu ^{3}}}\lambda ^{-3/\nu }+{\mathcal {O}}(\lambda ^{-4/\nu }){\bigg )},} γ γ 2 = λ λ − − 1 / ν ν ν ν ( 1 − − ( ν ν 2 − − 1 ) 24 ν ν 2 λ λ − − 2 / ν ν + ( ν ν 2 − − 1 ) 6 ν ν 3 λ λ − − 3 / ν ν + O ( λ λ − − 4 / ν ν ) ) , {\displaystyle \gamma _{2}={\frac {\lambda ^{-1/\nu }}{\nu }}{\bigg (}1-{\frac {(\nu ^{2}-1)}{24\nu ^{2}}}\lambda ^{-2/\nu }+{\frac {(\nu ^{2}-1)}{6\nu ^{3}}}\lambda ^{-3/\nu }+{\mathcal {O}}(\lambda ^{-4/\nu }){\bigg )},} E ⁡ ⁡ [ X n ] = λ λ n / ν ν ( 1 + n ( n − − ν ν ) 2 ν ν λ λ − − 1 / ν ν + a 2 λ λ − − 2 / ν ν + O ( λ λ − − 3 / ν ν ) ) , {\displaystyle \operatorname {E} [X^{n}]=\lambda ^{n/\nu }{\bigg (}1+{\frac {n(n-\nu )}{2\nu }}\lambda ^{-1/\nu }+a_{2}\lambda ^{-2/\nu }+{\mathcal {O}}(\lambda ^{-3/\nu }){\bigg )},} where a 2 = − − n ( ν ν − − 1 ) ( 6 n ν ν 2 − − 3 n ν ν − − 15 n + 4 ν ν + 10 ) 24 ν ν 2 + 1 ν ν 2 { ( n 3 ) + 3 ( n 4 ) } .

{\displaystyle a_{2}=-{\frac {n(\nu -1)(6n\nu ^{2}-3n\nu -15n+4\nu +10)}{24\nu ^{2}}}+{\frac {1}{\nu ^{2}}}{\bigg \{}{\binom {n}{3}}+3{\binom {n}{4}}{\bigg \}}.} The asymptotic series for κ κ n {\displaystyle \kappa _{n}} holds for all n ≥ ≥ 2 {\displaystyle n\geq 2} , and κ κ 1 = E ⁡ ⁡ X {\displaystyle \kappa _{1}=\operatorname {E} X} .

Moments for the case of integer ν ν {\displaystyle \nu } [ edit ] When ν ν {\displaystyle \nu } is an integer explicit formulas for moments can be obtained.  The case ν ν = 1 {\displaystyle \nu =1} corresponds to the Poisson distribution.  Suppose now that ν ν = 2 {\displaystyle \nu =2} .  For m ∈ ∈ N {\displaystyle m\in \mathbb {N} } , [ 7 ] E ⁡ ⁡ [ ( X ) m ] = λ λ m / 2 I m ( 2 λ λ ) I 0 ( 2 λ λ ) , {\displaystyle \operatorname {E} [(X)_{m}]={\frac {\lambda ^{m/2}I_{m}(2{\sqrt {\lambda }})}{I_{0}(2{\sqrt {\lambda }})}},} where I r ( x ) {\displaystyle I_{r}(x)} is the modified Bessel function of the first kind.

Using the connecting formula for moments and factorial moments gives E ⁡ ⁡ X m = ∑ ∑ k = 1 m { m k } λ λ k / 2 I k ( 2 λ λ ) I 0 ( 2 λ λ ) .

{\displaystyle \operatorname {E} X^{m}=\sum _{k=1}^{m}\left\{{m \atop k}\right\}{\frac {\lambda ^{k/2}I_{k}(2{\sqrt {\lambda }})}{I_{0}(2{\sqrt {\lambda }})}}.} In particular, the mean of X {\displaystyle X} is given by E ⁡ ⁡ X = λ λ I 1 ( 2 λ λ ) I 0 ( 2 λ λ ) .

{\displaystyle \operatorname {E} X={\frac {{\sqrt {\lambda }}I_{1}(2{\sqrt {\lambda }})}{I_{0}(2{\sqrt {\lambda }})}}.} Also, since E ⁡ ⁡ X 2 = λ λ {\displaystyle \operatorname {E} X^{2}=\lambda } , the variance is given by V a r ( X ) = λ λ ( 1 − − I 1 ( 2 λ λ ) 2 I 0 ( 2 λ λ ) 2 ) .

{\displaystyle \mathrm {Var} (X)=\lambda \left(1-{\frac {I_{1}(2{\sqrt {\lambda }})^{2}}{I_{0}(2{\sqrt {\lambda }})^{2}}}\right).} Suppose now that ν ν ≥ ≥ 1 {\displaystyle \nu \geq 1} is an integer.  Then [ 6 ] E ⁡ ⁡ [ ( X ) m ] = λ λ m ( m !

) ν ν − − 1 0 F ν ν − − 1 ( ; m + 1 , … … , m + 1 ; λ λ ) 0 F ν ν − − 1 ( ; 1 , … … , 1 ; λ λ ) .

{\displaystyle \operatorname {E} [(X)_{m}]={\frac {\lambda ^{m}}{(m!)^{\nu -1}}}{\frac {_{0}F_{\nu -1}(;m+1,\ldots ,m+1;\lambda )}{_{0}F_{\nu -1}(;1,\ldots ,1;\lambda )}}.} In particular, E ⁡ ⁡ [ X ] = λ λ 0 F ν ν − − 1 ( ; 2 , … … , 2 ; λ λ ) 0 F ν ν − − 1 ( ; 1 , … … , 1 ; λ λ ) , {\displaystyle \operatorname {E} [X]=\lambda {\frac {_{0}F_{\nu -1}(;2,\ldots ,2;\lambda )}{_{0}F_{\nu -1}(;1,\ldots ,1;\lambda )}},} and V a r ( X ) = λ λ 2 2 ν ν − − 1 0 F ν ν − − 1 ( ; 3 , … … , 3 ; λ λ ) 0 F ν ν − − 1 ( ; 1 , … … , 1 ; λ λ ) + E ⁡ ⁡ [ X ] − − ( E ⁡ ⁡ [ X ] ) 2 .

{\displaystyle \mathrm {Var} (X)={\frac {\lambda ^{2}}{2^{\nu -1}}}{\frac {_{0}F_{\nu -1}(;3,\ldots ,3;\lambda )}{_{0}F_{\nu -1}(;1,\ldots ,1;\lambda )}}+\operatorname {E} [X]-(\operatorname {E} [X])^{2}.} Median, mode and mean deviation [ edit ] Let X ∼ ∼ C M P ( λ λ , ν ν ) {\displaystyle X\sim \mathrm {CMP} (\lambda ,\nu )} .  Then the mode of X {\displaystyle X} is ⌊ ⌊ λ λ 1 / ν ν ⌋ ⌋ {\displaystyle \lfloor \lambda ^{1/\nu }\rfloor } if λ λ 1 / ν ν < m {\displaystyle \lambda ^{1/\nu }<m} is not an integer.  Otherwise, the modes of X {\displaystyle X} are λ λ 1 / ν ν {\displaystyle \lambda ^{1/\nu }} and λ λ 1 / ν ν − − 1 {\displaystyle \lambda ^{1/\nu }-1} .

[ 7 ] The mean deviation of X ν ν {\displaystyle X^{\nu }} about its mean λ λ {\displaystyle \lambda } is given by [ 7 ] E ⁡ ⁡ | X ν ν − − λ λ | = 2 Z ( λ λ , ν ν ) − − 1 λ λ ⌊ ⌊ λ λ 1 / ν ν ⌋ ⌋ + 1 ⌊ ⌊ λ λ 1 / ν ν ⌋ ⌋ !

.

{\displaystyle \operatorname {E} |X^{\nu }-\lambda |=2Z(\lambda ,\nu )^{-1}{\frac {\lambda ^{\lfloor \lambda ^{1/\nu }\rfloor +1}}{\lfloor \lambda ^{1/\nu }\rfloor !}}.} No explicit formula is known for the median of X {\displaystyle X} , but the following asymptotic result is available.

[ 7 ] Let m {\displaystyle m} be the median of X ∼ ∼ CMP ( λ λ , ν ν ) {\displaystyle X\sim {\mbox{CMP}}(\lambda ,\nu )} .  Then m = λ λ 1 / ν ν + O ( λ λ 1 / 2 ν ν ) , {\displaystyle m=\lambda ^{1/\nu }+{\mathcal {O}}\left(\lambda ^{1/2\nu }\right),} as λ λ → → ∞ ∞ {\displaystyle \lambda \rightarrow \infty } .

Stein characterisation [ edit ] Let X ∼ ∼ CMP ( λ λ , ν ν ) {\displaystyle X\sim {\mbox{CMP}}(\lambda ,\nu )} , and suppose that f : Z + ↦ ↦ R {\displaystyle f:\mathbb {Z} ^{+}\mapsto \mathbb {R} } is such that E ⁡ ⁡ | f ( X + 1 ) | < ∞ ∞ {\displaystyle \operatorname {E} |f(X+1)|<\infty } and E ⁡ ⁡ | X ν ν f ( X ) | < ∞ ∞ {\displaystyle \operatorname {E} |X^{\nu }f(X)|<\infty } .  Then E ⁡ ⁡ [ λ λ f ( X + 1 ) − − X ν ν f ( X ) ] = 0.

{\displaystyle \operatorname {E} [\lambda f(X+1)-X^{\nu }f(X)]=0.} Conversely, suppose now that W {\displaystyle W} is a real-valued random variable supported on Z + {\displaystyle \mathbb {Z} ^{+}} such that E ⁡ ⁡ [ λ λ f ( W + 1 ) − − W ν ν f ( W ) ] = 0 {\displaystyle \operatorname {E} [\lambda f(W+1)-W^{\nu }f(W)]=0} for all bounded f : Z + ↦ ↦ R {\displaystyle f:\mathbb {Z} ^{+}\mapsto \mathbb {R} } .  Then W ∼ ∼ CMP ( λ λ , ν ν ) {\displaystyle W\sim {\mbox{CMP}}(\lambda ,\nu )} .

[ 7 ] Use as a limiting distribution [ edit ] Let Y n {\displaystyle Y_{n}} have the Conway–Maxwell–binomial distribution with parameters n {\displaystyle n} , p = λ λ / n ν ν {\displaystyle p=\lambda /n^{\nu }} and ν ν {\displaystyle \nu } .  Fix λ λ > 0 {\displaystyle \lambda >0} and ν ν > 0 {\displaystyle \nu >0} .  Then, Y n {\displaystyle Y_{n}} converges in distribution to the C M P ( λ λ , ν ν ) {\displaystyle \mathrm {CMP} (\lambda ,\nu )} distribution as n → → ∞ ∞ {\displaystyle n\rightarrow \infty } .

[ 7 ] This result generalises the classical Poisson approximation of the binomial distribution.   More generally, the CMP distribution arises as a limiting distribution of Conway–Maxwell–Poisson binomial distribution.

[ 7 ] Apart from the fact that COM-binomial approximates to COM-Poisson, Zhang et al. (2018) [ 9 ] illustrates that COM-negative binomial distribution with probability mass function P ( X = k ) = ( Γ Γ ( r + k ) k !

Γ Γ ( r ) ) ν ν p k ( 1 − − p ) r ∑ ∑ i = 0 ∞ ∞ ( Γ Γ ( r + i ) i !

Γ Γ ( r ) ) ν ν p i ( 1 − − p ) r = ( Γ Γ ( r + k ) k !

Γ Γ ( r ) ) ν ν p k ( 1 − − p ) r 1 C ( r , ν ν , p ) , ( k = 0 , 1 , 2 , … … ) , {\displaystyle \mathrm {P} (X=k)={\frac {{{({\frac {\Gamma (r+k)}{k!\Gamma (r)}})}^{\nu }}{p^{k}}{{(1-p)}^{r}}}{\sum \limits _{i=0}^{\infty }{{({\frac {\Gamma (r+i)}{i!\Gamma (r)}})}^{\nu }}{p^{i}}{{(1-p)}^{r}}}}={{\left({\frac {\Gamma (r+k)}{k!\Gamma (r)}}\right)}^{\nu }}{{p^{k}}{{(1-p)}^{r}}}{\frac {1}{C(r,\nu ,p)}},\quad (k=0,1,2,\ldots ),} convergents to a limiting distribution which is the COM-Poisson, as r → → + ∞ ∞ {\displaystyle {r\to +\infty }} .

Related distributions [ edit ] X ∼ ∼ CMP ⁡ ⁡ ( λ λ , 1 ) {\displaystyle X\sim \operatorname {CMP} (\lambda ,1)} , then X {\displaystyle X} follows the Poisson distribution with parameter λ λ {\displaystyle \lambda } .

Suppose λ λ < 1 {\displaystyle \lambda <1} .  Then if X ∼ ∼ C M P ( λ λ , 0 ) {\displaystyle X\sim \mathrm {CMP} (\lambda ,0)} , we have that X {\displaystyle X} follows the geometric distribution with probability mass function P ( X = k ) = λ λ k ( 1 − − λ λ ) {\displaystyle P(X=k)=\lambda ^{k}(1-\lambda )} , k ≥ ≥ 0 {\displaystyle k\geq 0} .

The sequence of random variable X ν ν ∼ ∼ C M P ( λ λ , ν ν ) {\displaystyle X_{\nu }\sim \mathrm {CMP} (\lambda ,\nu )} converges in distribution as ν ν → → ∞ ∞ {\displaystyle \nu \rightarrow \infty } to the Bernoulli distribution with mean λ λ ( 1 + λ λ ) − − 1 {\displaystyle \lambda (1+\lambda )^{-1}} .

Parameter estimation [ edit ] There are a few methods of estimating the parameters of the CMP distribution from the data. Two methods will be discussed: weighted least squares and maximum likelihood. The weighted least squares approach is simple and efficient but lacks precision. Maximum likelihood, on the other hand, is precise, but is more complex and computationally intensive.

Weighted least squares [ edit ] The weighted least squares provides a simple, efficient method to derive rough estimates of the parameters of the CMP distribution and determine if the distribution would be an appropriate model. Following the use of this method, an alternative method should be employed to compute more accurate estimates of the parameters if the model is deemed appropriate.

This method uses the relationship of successive probabilities as discussed above. By taking logarithms of both sides of this equation, the following linear relationship arises log ⁡ ⁡ p x − − 1 p x = − − log ⁡ ⁡ λ λ + ν ν log ⁡ ⁡ x {\displaystyle \log {\frac {p_{x-1}}{p_{x}}}=-\log \lambda +\nu \log x} where p x {\displaystyle p_{x}} denotes Pr ( X = x ) {\displaystyle \Pr(X=x)} . When estimating the parameters, the probabilities can be replaced by the relative frequencies of x {\displaystyle x} and x − − 1 {\displaystyle x-1} . To determine if the CMP distribution is an appropriate model, these values should be plotted against log ⁡ ⁡ x {\displaystyle \log x} for all ratios without zero counts. If the data appear to be linear, then the model is likely to be a good fit.

Once the appropriateness of the model is determined, the parameters can be estimated by fitting a regression of log ⁡ ⁡ ( p ^ ^ x − − 1 / p ^ ^ x ) {\displaystyle \log({\hat {p}}_{x-1}/{\hat {p}}_{x})} on log ⁡ ⁡ x {\displaystyle \log x} . However, the basic assumption of homoscedasticity is violated, so a weighted least squares regression must be used. The inverse weight matrix will have the variances of each ratio on the diagonal with the one-step covariances on the first off-diagonal, both given below.

var ⁡ ⁡ [ log ⁡ ⁡ p ^ ^ x − − 1 p ^ ^ x ] ≈ ≈ 1 n p x + 1 n p x − − 1 {\displaystyle \operatorname {var} \left[\log {\frac {{\hat {p}}_{x-1}}{{\hat {p}}_{x}}}\right]\approx {\frac {1}{np_{x}}}+{\frac {1}{np_{x-1}}}} cov ( log ⁡ ⁡ p ^ ^ x − − 1 p ^ ^ x , log ⁡ ⁡ p ^ ^ x p ^ ^ x + 1 ) ≈ ≈ − − 1 n p x {\displaystyle {\text{cov}}\left(\log {\frac {{\hat {p}}_{x-1}}{{\hat {p}}_{x}}},\log {\frac {{\hat {p}}_{x}}{{\hat {p}}_{x+1}}}\right)\approx -{\frac {1}{np_{x}}}} Maximum likelihood [ edit ] The CMP likelihood function is L ( λ λ , ν ν ∣ ∣ x 1 , … … , x n ) = λ λ S 1 exp ⁡ ⁡ ( − − ν ν S 2 ) Z − − n ( λ λ , ν ν ) {\displaystyle {\mathcal {L}}(\lambda ,\nu \mid x_{1},\dots ,x_{n})=\lambda ^{S_{1}}\exp(-\nu S_{2})Z^{-n}(\lambda ,\nu )} where S 1 = ∑ ∑ i = 1 n x i {\displaystyle S_{1}=\sum _{i=1}^{n}x_{i}} and S 2 = ∑ ∑ i = 1 n log ⁡ ⁡ x i !

{\displaystyle S_{2}=\sum _{i=1}^{n}\log x_{i}!} . Maximizing the likelihood yields the following two equations E ⁡ ⁡ [ X ] = X ¯ ¯ {\displaystyle \operatorname {E} [X]={\bar {X}}} E ⁡ ⁡ [ log ⁡ ⁡ X !

] = log ⁡ ⁡ X !

¯ ¯ {\displaystyle \operatorname {E} [\log X!]={\overline {\log X!}}} which do not have an analytic solution.

Instead, the maximum likelihood estimates are approximated numerically by the Newton–Raphson method . In each iteration, the expectations, variances, and covariance of X {\displaystyle X} and log ⁡ ⁡ X !

{\displaystyle \log X!} are approximated by using the estimates for λ λ {\displaystyle \lambda } and ν ν {\displaystyle \nu } from the previous iteration in the expression E ⁡ ⁡ [ f ( x ) ] = ∑ ∑ j = 0 ∞ ∞ f ( j ) λ λ j ( j !

) ν ν Z ( λ λ , ν ν ) .

{\displaystyle \operatorname {E} [f(x)]=\sum _{j=0}^{\infty }f(j){\frac {\lambda ^{j}}{(j!)^{\nu }Z(\lambda ,\nu )}}.} This is continued until convergence of λ λ ^ ^ {\displaystyle {\hat {\lambda }}} and ν ν ^ ^ {\displaystyle {\hat {\nu }}} .

Generalized linear model [ edit ] The basic CMP distribution discussed above has also been used as the basis for a generalized linear model (GLM) using a Bayesian formulation. A dual-link GLM based on the CMP distribution has been developed, [ 10 ] and this model has been used to evaluate traffic accident data.

[ 11 ] [ 12 ] The CMP GLM developed by Guikema and Coffelt (2008) is based on a reformulation of the CMP distribution above, replacing λ λ {\displaystyle \lambda } with μ μ = λ λ 1 / ν ν {\displaystyle \mu =\lambda ^{1/\nu }} . The integral part of μ μ {\displaystyle \mu } is then the mode of the distribution. A full Bayesian estimation approach has been used with MCMC sampling implemented in WinBugs with non-informative priors for the regression parameters.

[ 10 ] [ 11 ] This approach is computationally expensive, but it yields the full posterior distributions for the regression parameters and allows expert knowledge to be incorporated through the use of informative priors.

A classical GLM formulation for a CMP regression has been developed which generalizes Poisson regression and logistic regression .

[ 13 ] This takes advantage of the exponential family properties of the CMP distribution to obtain elegant model estimation (via maximum likelihood ), inference, diagnostics, and interpretation. This approach requires substantially less computational time than the Bayesian approach, at the cost of not allowing expert knowledge to be incorporated into the model.

[ 13 ] In addition it yields standard errors for the regression parameters (via the Fisher Information matrix) compared to the full posterior distributions obtainable via the Bayesian formulation. It also provides a statistical test for the level of dispersion compared to a Poisson model. Code for fitting a CMP regression, testing for dispersion, and evaluating fit is available.

[ 14 ] The two GLM frameworks developed for the CMP distribution significantly extend the usefulness of this distribution for data analysis problems.

References [ edit ] ^ "Conway–Maxwell–Poisson Regression" .

SAS Support . SAS Institute, Inc . Retrieved 2 March 2015 .

^ a b c d e f Shmueli G., Minka T., Kadane J.B., Borle S., and Boatwright, P.B. "A useful distribution for fitting discrete data: revival of the Conway–Maxwell–Poisson distribution." Journal of the Royal Statistical Society : Series C (Applied Statistics) 54.1 (2005): 127–142.

[1] ^ Conway, R. W.; Maxwell, W. L. (1962), "A queuing model with state dependent service rates", Journal of Industrial Engineering , 12 : 132– 136 ^ Boatwright, P., Borle, S. and Kadane, J.B. "A model of the joint distribution of purchase quantity and timing." Journal of the American Statistical Association 98 (2003): 564–572.

^ Li B., Zhang H., Jiao H. "Some Characterizations and Properties of COM-Poisson Random Variables." Communications in Statistics - Theory and Methods, (2019).

[2] ^ a b c Nadarajah, S. "Useful moment and CDF formulations for the COM–Poisson distribution." Statistical Papers 50 (2009): 617–622.

^ a b c d e f g h i j Daly, F. and Gaunt, R.E. " The Conway–Maxwell–Poisson distribution: distributional theory and approximation." ALEA Latin American Journal of Probability and Mathematical Statistics 13 (2016): 635–658.

^ a b c Gaunt, R.E., Iyengar, S., Olde Daalhuis, A.B. and Simsek, B. "An asymptotic expansion for the normalizing constant of the Conway–Maxwell–Poisson distribution." To appear in Annals of the Institute of Statistical Mathematics (2017+) DOI 10.1007/s10463-017-0629-6 ^ Zhang H., Tan K., Li B. "COM-negative binomial distribution: modeling overdispersion and ultrahigh zero-inflated count data." Frontiers of Mathematics in China, 2018, 13(4): 967–998.

[3] ^ a b Guikema, S.D. and J.P. Coffelt (2008) "A Flexible Count Data Regression Model for Risk Analysis", Risk Analysis , 28 (1), 213–223.

doi : 10.1111/j.1539-6924.2008.01014.x ^ a b Lord, D., S.D. Guikema, and S.R. Geedipally (2008) "Application of the Conway–Maxwell–Poisson Generalized Linear Model for Analyzing Motor Vehicle Crashes," Accident Analysis & Prevention , 40 (3), 1123–1134.

doi : 10.1016/j.aap.2007.12.003 ^ Lord, D., S.R. Geedipally, and S.D. Guikema (2010) "Extension of the Application of Conway–Maxwell–Poisson Models: Analyzing Traffic Crash Data Exhibiting Under-Dispersion," Risk Analysis , 30 (8), 1268–1276.

doi : 10.1111/j.1539-6924.2010.01417.x ^ a b Sellers, K. S.

and Shmueli, G. (2010), "A Flexible Regression Model for Count Data" , Annals of Applied Statistics , 4 (2), 943–961 ^ Code for COM_Poisson modelling , Georgetown Univ.

External links [ edit ] Conway–Maxwell–Poisson distribution package for R (compoisson) by Jeffrey Dunn, part of Comprehensive R Archive Network (CRAN) Conway–Maxwell–Poisson distribution package for R (compoisson) by Tom Minka, third party package v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Conway–Maxwell–Poisson_distribution&oldid=1175105786 " Categories : Discrete distributions Poisson distribution Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 12 September 2023, at 20:50 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Conway–Maxwell–Poisson distribution 2 languages Add topic

