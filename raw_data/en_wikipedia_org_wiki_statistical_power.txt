Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Background 2 Description 3 Applications 4 Rule of thumb for t-test 5 Factors influencing power 6 Discussion 7 A priori vs.

post hoc analysis 8 Example Toggle Example subsection 8.1 Analytic solution 8.2 Simulation solution 9 Power in different disciplines 10 Extension Toggle Extension subsection 10.1 Bayesian power 10.2 Predictive probability of success 11 Software for power and sample size calculations 12 See also 13 References 14 Sources 15 External links Toggle the table of contents Power (statistics) 20 languages Català Deutsch Eesti Español فارسی Français Galego 한국어 עברית Magyar Nederlands Polski Português Русский Српски / srpski Sunda Svenska Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Statistical power ) Term in statistical hypothesis testing In frequentist statistics , power is the probability of detecting an effect (i.e. rejecting the null hypothesis) given that some prespecified effect actually exists using a given test in a given context. In typical use, it is a function of the specific test that is used (including the choice of test statistic and significance level ), the sample size (more data tends to provide more power), and the effect size (effects or correlations that are large relative to the variability of the data tend to provide more power).

More formally, in the case of a simple hypothesis test with two hypotheses, the power of the test is the probability that the test correctly rejects the null hypothesis ( H 0 {\displaystyle H_{0}} ) when the alternative hypothesis ( H 1 {\displaystyle H_{1}} ) is true. It is commonly denoted by 1 − − β β {\displaystyle 1-\beta } , where β β {\displaystyle \beta } is the probability of making a type II error (a false negative ) conditional on there being a true effect or association.

Background [ edit ] Main article: Statistical hypothesis test Statistical testing uses data from samples to assess, or make inferences about, a statistical population . For example, we may measure the yields of samples of two varieties of a crop, and use a two sample test to assess whether the mean values of this yield differs between varieties.

Under a frequentist hypothesis testing framework, this is done by calculating a test statistic (such as a t-statistic ) for the dataset, which has a known theoretical probability distribution if there is no difference (the so called null hypothesis). If the actual value calculated on the sample is sufficiently unlikely to arise under the null hypothesis, we say we identified a statistically significant effect.

The threshold for significance can be set small to ensure there is little chance of falsely detecting a non-existent effect. However, failing to identify a significant effect does not imply there was none. If we insist on being careful to avoid false positives, we may create false negatives instead. It may simply be too much to expect that we will be able to find satisfactorily strong evidence of a very subtle difference even if it exists. Statistical power is an attempt to quantify this issue.

In the case of the comparison of the two crop varieties, it enables us to answer questions like: Is there a big danger of two very different varieties producing samples that just happen to look indistinguishable by pure chance?

How much effort do we need to put into this comparison to avoid that danger?

How different do these varieties need to be before we can expect to notice a difference?

Description [ edit ] See also: Type I and type II errors Illustration of the power of a statistical test, for a two sided test, through the probability distribution of the test statistic under the null and alternative hypothesis.

α is shown as the blue area , the probability of rejection under null, while the red area shows power, 1 − β , the probability of correctly rejecting under the alternative.

Suppose we are conducting a hypothesis test. We define two hypotheses H 0 {\displaystyle H_{0}} the null hypothesis, and H 1 {\displaystyle H_{1}} the alternative hypothesis. If we design the test such that α is the significance level - being the probability of rejecting H 0 {\displaystyle H_{0}} when H 0 {\displaystyle H_{0}} is in fact true, then the power of the test is 1 - β where β is the probability of failing to reject H 0 {\displaystyle H_{0}} when the alternative H 1 {\displaystyle H_{1}} is true.

Probability to reject H 0 {\displaystyle H_{0}} Probability to not reject H 0 {\displaystyle H_{0}} If H 0 {\displaystyle H_{0}} is True α 1-α If H 1 {\displaystyle H_{1}} is True 1-β (power) β To make this more concrete, a typical statistical test would be based on a test statistic t calculated from the sampled data, which has a particular probability distribution under H 0 {\displaystyle H_{0}} . A desired significance level α would then define a corresponding "rejection region" (bounded by certain "critical values"), a set of values t is unlikely to take if H 0 {\displaystyle H_{0}} was correct. If we reject H 0 {\displaystyle H_{0}} in favor of H 1 {\displaystyle H_{1}} only when the sample t takes those values, we would be able to keep the probability of falsely rejecting H 0 {\displaystyle H_{0}} within our desired significance level. At the same time, if H 1 {\displaystyle H_{1}} defines its own probability distribution for t (the difference between the two distributions being a function of the effect size), the power of the test would be the probability, under H 1 {\displaystyle H_{1}} , that the sample t falls into our defined rejection region and causes H 0 {\displaystyle H_{0}} to be correctly rejected.

Statistical power is one minus the type II error probability and is also the sensitivity of the hypothesis testing procedure to detect a true effect. There is usually a trade-off between demanding more stringent tests (and so, smaller rejection regions) and trying to have a high probability of rejecting the null under the alternative hypothesis. Statistical power may also be extended to the case where multiple hypotheses are being tested based on an experiment or survey. It is thus also common to refer to the power of a study , evaluating a scientific project in terms of its ability to answer the research questions they are seeking to answer.

Applications [ edit ] The main application of statistical power is "power analysis", a calculation of power usually done before an experiment is conducted using data from pilot studies or a literature review. Power analyses can be used to calculate the minimum sample size required so that one can be reasonably likely to detect an effect of a given size (in other words, producing an acceptable level of power). For example: "How many times do I need to toss a coin to conclude it is rigged by a certain amount?" [ 1 ] If resources and thus sample sizes are fixed, power analyses can also be used to calculate the minimum effect size that is likely to be detected.

Funding agencies, ethics boards and research review panels frequently request that a researcher perform a power analysis. An underpowered study is likely be inconclusive, failing to allow one to choose between hypotheses at the desired significance level, while an overpowered study will spend great expense on being able to report significant effects even if they are tiny and so practically meaningless. If a large number of underpowered studies are done and statistically significant results published , published findings are more likely false positives than true results, contributing to a replication crisis . However, excessive demands for power could be connected to wasted resources and ethical problems, for example the use of a large number of animal test subjects when a smaller number would have been sufficient. It could also induce researchers trying to seek funding to overstate their expected effect sizes, or avoid looking for more subtle interaction effects that cannot be easily detected.

[ 2 ] Power analysis is primarily a frequentist statistics tool. In Bayesian statistics , hypothesis testing of the type used in classical power analysis is not done. In the Bayesian framework, one updates his or her prior beliefs using the data obtained in a given study. In principle, a study that would be deemed underpowered from the perspective of hypothesis testing could still be used in such an updating process. However, power remains a useful measure of how much a given experiment size can be expected to refine one's beliefs. A study with low power is unlikely to lead to a large change in beliefs.

In addition, the concept of power is used to make comparisons between different statistical testing procedures: for example, between a parametric test and a nonparametric test of the same hypothesis. Tests may have the same size , and hence the same false positive rates, but different ability to detect true effects. Consideration of their theoretical power proprieties is a key reason for the common use of likelihood ratio tests .

Rule of thumb for t-test [ edit ] Lehr's [ 3 ] [ 4 ] (rough) rule of thumb says that the sample size n {\displaystyle n} (for each group) for the common case of a two-sided two-sample t-test with power 80% ( β β = 0.2 {\displaystyle \beta =0.2} ) and significance level α α = 0.05 {\displaystyle \alpha =0.05} should be: n ≈ ≈ 16 s 2 d 2 , {\displaystyle n\approx 16{\frac {s^{2}}{d^{2}}},} where s 2 {\displaystyle s^{2}} is an estimate of the population variance and d = μ μ 1 − − μ μ 2 {\displaystyle d=\mu _{1}-\mu _{2}} the to-be-detected difference in the mean values of both samples. This expression can be rearranged, implying for example that 80% power is obtained when looking for a difference in means that exceeds about 4 times the group-wise standard error of the mean .

For a one sample t-test 16 is to be replaced with 8. Other values provide an appropriate approximation when the desired power or significance level are different.

[ 5 ] However, a full power analysis should always be performed to confirm and refine this estimate.

Factors influencing power [ edit ] An example of the relationship between sample size and power levels. Higher power requires larger sample sizes Statistical power may depend on a number of factors. Some factors may be particular to a specific testing situation, but in normal use, power depends on the following three aspects that can be potentially controlled by the practitioner: the test itself and the statistical significance criterion used the magnitude of the effect of interest the size and variability of the sample used to detect the effect For a given test, the significance criterion determines the desired degree of rigor, specifying how unlikely it is for the null hypothesis of no effect to be rejected if it is in fact true. The most commonly used threshold is a probability of rejection of 0.05, though smaller values like 0.01 or 0.001 are sometimes used. This threshold then implies that the observation must be at least that unlikely (perhaps by suggesting a sufficiently large estimate of difference) to be considered strong enough evidence against the null. Picking a smaller value to tighten the threshold, so as to reduce the chance of a false positive, would also reduce power (and so increase the chance of a false negative). Some statistical tests will inherently produce better power , albeit often at the cost of requiring stronger assumptions.

The magnitude of the effect of interest defines what is being looked for by the test. It can be the expected effect size if it exists, as a scientific hypothesis that the researcher has arrived at and wishes to test. Alternatively, in a more practical context it could be determined by the size the effect must be to be useful, for example that which is required to be clinically significant . An effect size can be a direct value of the quantity of interest (for example, a difference in mean of a particular size), or it can be a standardized measure that also accounts for the variability in the population (such as a difference in means expressed as a multiple of the standard deviation). If the researcher is looking for a larger effect, then it should be easier to find with a given experimental or analytic setup, and so power is higher.

The nature of the sample underlies the information being used in the test. This will usually involve the sample size, and the sample variability, if that is not implicit in the definition of the effect size. More broadly, the precision with which the data are measured can also be an important factor (such as the statistical reliability ), as well as the design of an experiment or observational study. Ultimately, these factors lead to an expected amount of sampling error . A smaller sampling error could be obtained by larger sample sizes from a less variability population, from more accurate measurements, or from more efficient experimental designs (for example, with the appropriate use of blocking ), and such smaller errors would lead to improved power, albeit usually at a cost in resources. How increased sample size translates to higher power is a measure of the efficiency of the test—for example, the sample size required for a given power.

[ 6 ] Discussion [ edit ] The statistical power of a hypothesis test has an impact on the interpretation of its results. Not finding a result with a more powerful study is stronger evidence against the effect existing than the same finding with a less powerful study. However, this is not completely conclusive. The effect may exist, but be smaller than what was looked for, meaning the study is in fact underpowered and the sample is thus unable to distinguish it from random chance.

[ 7 ] Many clinical trials , for instance, have low statistical power to detect differences in adverse effects of treatments, since such effects may only affect a few patients, even if this difference can be important .

[ 8 ] Conclusions about the probability of actual presence of an effect also should consider more things than a single test, especially as real world power is rarely close to 1.

Indeed, although there are no formal standards for power, many researchers and funding bodies assess power using 0.80 (or 80%) as a standard for adequacy. This convention implies a four-to-one trade off between β -risk and α -risk, as the probability of a type II error β is set as 1 - 0.8 = 0.2, while α, the probability of a type I error, is commonly set at 0.05. Some applications require much higher levels of power.

Medical tests may be designed to minimise the number of false negatives (type II errors) produced by loosening the threshold of significance, raising the risk of obtaining a false positive (a type I error). The rationale is that it is better to tell a healthy patient "we may have found something—let's test further," than to tell a diseased patient "all is well." [ 9 ] Power analysis focuses on the correct rejection of a null hypothesis. Alternative concerns may however motivate an experiment, and so lead to different needs for sample size. In many contexts, the issue is less about deciding between hypotheses but rather with getting an estimate of the population effect size of sufficient accuracy. For example, a careful power analysis can tell you that 55 pairs of normally distributed samples with a correlation of 0.5 will be sufficient to grant 80% power in rejecting a null that the correlation is no more than 0.2 (using a one-sided test, α = 0.05). But the typical 95% confidence interval with this sample would be around [0.27, 0.67]. An alternative, albeit related analysis would be required if we wish to be able to measure correlation to an accuracy of +/- 0.1, implying a different (in this case, larger) sample size. Alternatively, multiple under-powered studies can still be useful, if appropriately combined through a meta-analysis .

Many statistical analyses involve the estimation of several unknown quantities. In simple cases, all but one of these quantities are nuisance parameters . In this setting, the only relevant power pertains to the single quantity that will undergo formal statistical inference. In some settings, particularly if the goals are more "exploratory", there may be a number of quantities of interest in the analysis. For example, in a multiple regression analysis we may include several covariates of potential interest. In situations such as this where several hypotheses are under consideration, it is common that the powers associated with the different hypotheses differ. For instance, in multiple regression analysis, the power for detecting an effect of a given size is related to the variance of the covariate. Since different covariates will have different variances, their powers will differ as well.

Additional complications arise when we consider these multiple hypotheses together. For example, if we consider a false positive to be making an erroneous null rejection on any one of these hypotheses, our likelihood of this "family-wise error" will be inflated if appropriate measures are not taken. Such measures typically involve applying a higher threshold of stringency to reject a hypothesis (such as with the Bonferroni method ), and so would reduce power. Alternatively, there may be different notions of power connected with how the different hypotheses are considered. "Complete power" demands that all true effects are detected across all of the hypotheses, which is a much stronger requirement than the "minimal power" of being able to find at least one true effect, a type of power that might increase with an increasing number of hypotheses.

[ 10 ] A priori vs.

post hoc analysis [ edit ] Further information: Post hoc analysis Power analysis can either be done before ( a priori or prospective power analysis) or after ( post hoc or retrospective power analysis) data are collected.

A priori power analysis is conducted prior to the research study, and is typically used in estimating sufficient sample sizes to achieve adequate power.

Post-hoc analysis of "observed power" is conducted after a study has been completed, and uses the obtained sample size and effect size to determine what the power was in the study, assuming the effect size in the sample is equal to the effect size in the population. Whereas the utility of prospective power analysis in experimental design is universally accepted, post hoc power analysis is controversial. Many statisticians have argued that post-hoc power calculations are misleading and essentially meaningless.

[ 11 ] [ 12 ] Example [ edit ] The following is an example that shows how to compute power for a randomized experiment: Suppose the goal of an experiment is to study the effect of a treatment on some quantity, and so we shall compare research subjects by measuring the quantity before and after the treatment, analyzing the data using a one-sided paired t-test , with a significance level threshold of 0.05. We are interested in being able to detect a positive change of size θ θ > 0 {\displaystyle \theta >0} .

We first set up the problem according to our test. Let A i {\displaystyle A_{i}} and B i {\displaystyle B_{i}} denote the pre-treatment and post-treatment measures on subject i {\displaystyle i} , respectively. The possible effect of the treatment should be visible in the differences D i = B i − − A i , {\displaystyle D_{i}=B_{i}-A_{i},} which are assumed to be independent and identically Normal in distribution, with unknown mean value μ μ D {\displaystyle \mu _{D}} and variance σ σ D 2 {\displaystyle \sigma _{D}^{2}} .

Here, it is natural to choose our null hypothesis to be that the expected mean difference is zero, i.e.

H 0 : μ μ D = μ μ 0 = 0.

{\displaystyle H_{0}:\mu _{D}=\mu _{0}=0.} For our one-sided test, the alternative hypothesis would be that there is a positive effect, corresponding to H 1 : μ μ D = θ θ > 0.

{\displaystyle H_{1}:\mu _{D}=\theta >0.} The test statistic in this case is defined as: T n = D ¯ ¯ n − − μ μ 0 σ σ ^ ^ D / n = D ¯ ¯ n − − 0 σ σ ^ ^ D / n , {\displaystyle T_{n}={\frac {{\bar {D}}_{n}-\mu _{0}}{{\hat {\sigma }}_{D}/{\sqrt {n}}}}={\frac {{\bar {D}}_{n}-0}{{\hat {\sigma }}_{D}/{\sqrt {n}}}},} where μ μ 0 {\displaystyle \mu _{0}} is the mean under the null so we substitute in 0, n is the sample size (number of subjects), D ¯ ¯ n {\displaystyle {\bar {D}}_{n}} is the sample mean of the difference D ¯ ¯ n = 1 n ∑ ∑ i = 1 n D i , {\displaystyle {\bar {D}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}D_{i},} and σ σ ^ ^ D {\displaystyle {\hat {\sigma }}_{D}} is the sample standard deviation of the difference.

Analytic solution [ edit ] We can proceed according to our knowledge of statistical theory, though in practice for a standard case like this software will exist to compute more accurate answers.

Thanks to t-test theory, we know this test statistic under the null hypothesis follows a Student t-distribution with n − − 1 {\displaystyle n-1} degrees of freedom. If we wish to reject the null at significance level α α = 0.05 {\displaystyle \alpha =0.05\,} , we must find the critical value t α α {\displaystyle t_{\alpha }} such that the probability of T n > t α α {\displaystyle T_{n}>t_{\alpha }} under the null is equal to α α {\displaystyle \alpha } . If n is large, the t-distribution converges to the standard normal distribution (thus no longer involving n ) and so through use of the corresponding quantile function Φ Φ − − 1 {\displaystyle \Phi ^{-1}} , we obtain that the null should be rejected if T n > t α α ≈ ≈ Φ Φ − − 1 ( 0.95 ) ≈ ≈ 1.64 .

{\displaystyle T_{n}>t_{\alpha }\approx \Phi ^{-1}(0.95)\approx 1.64\,.} Now suppose that the alternative hypothesis H 1 {\displaystyle H_{1}} is true so μ μ D = θ θ {\displaystyle \mu _{D}=\theta } . Then, writing the power as a function of the effect size, B ( θ θ ) {\displaystyle B(\theta )} , we find the probability of T n {\displaystyle T_{n}} being above t α α {\displaystyle t_{\alpha }} under H 1 {\displaystyle H_{1}} .

B ( θ θ ) ≈ ≈ Pr ( T n > 1.64 | μ μ D = θ θ ) = Pr ( D ¯ ¯ n − − 0 σ σ ^ ^ D / n > 1.64 | μ μ D = θ θ ) = 1 − − Pr ( D ¯ ¯ n − − 0 σ σ ^ ^ D / n < 1.64 | μ μ D = θ θ ) = 1 − − Pr ( D ¯ ¯ n − − θ θ σ σ ^ ^ D / n < 1.64 − − θ θ σ σ ^ ^ D / n | μ μ D = θ θ ) {\displaystyle {\begin{aligned}B(\theta )&\approx \Pr \left(T_{n}>1.64~{\big |}~\mu _{D}=\theta \right)\\&=\Pr \left({\frac {{\bar {D}}_{n}-0}{{\hat {\sigma }}_{D}/{\sqrt {n}}}}>1.64~{\Big |}~\mu _{D}=\theta \right)\\&=1-\Pr \left({\frac {{\bar {D}}_{n}-0}{{\hat {\sigma }}_{D}/{\sqrt {n}}}}<1.64~{\Big |}~\mu _{D}=\theta \right)\\&=1-\Pr \left({\frac {{\bar {D}}_{n}-\theta }{{\hat {\sigma }}_{D}/{\sqrt {n}}}}<1.64-{\frac {\theta }{{\hat {\sigma }}_{D}/{\sqrt {n}}}}~{\Big |}~\mu _{D}=\theta \right)\\\end{aligned}}} D ¯ ¯ n − − θ θ σ σ ^ ^ D / n {\displaystyle {\frac {{\bar {D}}_{n}-\theta }{{\hat {\sigma }}_{D}/{\sqrt {n}}}}} again follows a student-t distribution under H 1 {\displaystyle H_{1}} , converging on to a standard normal distribution for large n . The estimated σ σ ^ ^ D {\displaystyle {\hat {\sigma }}_{D}} will also converge on to its population value σ σ D {\displaystyle \sigma _{D}} Thus power can be approximated as B ( θ θ ) ≈ ≈ 1 − − Φ Φ ( 1.64 − − θ θ σ σ D / n ) .

{\displaystyle B(\theta )\approx 1-\Phi \left(1.64-{\frac {\theta }{\sigma _{D}/{\sqrt {n}}}}\right).} According to this formula, the power increases with the values of the effect size θ θ {\displaystyle \theta } and the sample size n , and reduces with increasing variability σ σ D {\displaystyle \sigma _{D}} . In the trivial case of zero effect size, power is at a minimum ( infimum ) and equal to the significance level of the test α α , {\displaystyle \alpha \,,} in this example 0.05. For finite sample sizes and non-zero variability, it is the case here, as is typical, that power cannot be made equal to 1 except in the trivial case where α α = 1 {\displaystyle \alpha =1} so the null is always rejected.

We can invert B {\displaystyle B} to obtain required sample sizes: n > σ σ D θ θ ( 1.64 − − Φ Φ − − 1 ( 1 − − B ( θ θ ) ) ) .

{\displaystyle {\sqrt {n}}>{\frac {\sigma _{D}}{\theta }}\left(1.64-\Phi ^{-1}\left(1-B(\theta )\right)\right).} Suppose θ θ = 1 {\displaystyle \theta =1} and we believe σ σ D {\displaystyle \sigma _{D}} is around 2, say, then we require for a power of B ( θ θ ) = 0.8 {\displaystyle B(\theta )=0.8} , a sample size n > 4 ( 1.64 − − Φ Φ − − 1 ( 1 − − 0.8 ) ) 2 ≈ ≈ 4 ( 1.64 + 0.84 ) 2 ≈ ≈ 24.6.

{\displaystyle n>4\left(1.64-\Phi ^{-1}\left(1-0.8\right)\right)^{2}\approx 4\left(1.64+0.84\right)^{2}\approx 24.6.} Simulation solution [ edit ] Alternatively we can use a Monte Carlo simulation method that works more generally.

[ 13 ] Once again, we return to the assumption of the distribution of D n {\displaystyle D_{n}} and the definition of T n {\displaystyle T_{n}} . Suppose we have fixed values of the sample size, variability and effect size, and wish to compute power. We can adopt this process: 1. Generate a large number of sets of D n {\displaystyle D_{n}} according to the null hypothesis, N ( 0 , σ σ D ) {\displaystyle N(0,\sigma _{D})} 2. Compute the resulting test statistic T n {\displaystyle T_{n}} for each set.

3. Compute the ( 1 − − α α ) {\displaystyle (1-\alpha )} th quantile of the simulated T n {\displaystyle T_{n}} and use that as an estimate of t α α {\displaystyle t_{\alpha }} .

4. Now generate a large number of sets of D n {\displaystyle D_{n}} according to the alternative hypothesis, N ( θ θ , σ σ D ) {\displaystyle N(\theta ,\sigma _{D})} , and compute the corresponding test statistics again.

5. Look at the proportion of these simulated alternative T n {\displaystyle T_{n}} that are above the t α α {\displaystyle t_{\alpha }} calculated in step 3 and so are rejected. This is the power.

This can be done with a variety of software packages. Using this methodology with the values before, setting the sample size to 25 leads to an estimated power of around 0.78. The small discrepancy with the previous section is due mainly to inaccuracies with the normal approximation.

Power in different disciplines [ edit ] Several studies have attempted to estimate typical levels of statistical power across different academic fields. One common approach uses meta-analyses to assess whether individual studies have sufficient power to detect the average effect size estimated from the meta-analysis itself. This method essentially asks: how likely is each study to detect the consensus effect found in the broader literature? These assessments consistently find low levels of statistical power across many disciplines.  For example, using this method median power is 18% in economics, [ 14 ] 10% in political science, [ 15 ] 36% in psychology, [ 16 ] and 15% in ecology and evolutionary biology.

[ 17 ] Extension [ edit ] Bayesian power [ edit ] In the frequentist setting, parameters are assumed to have a specific value which is unlikely to be true. This issue can be addressed by assuming the parameter has a distribution. The resulting power is sometimes referred to as Bayesian power which is commonly used in clinical trial design.

Predictive probability of success [ edit ] Both frequentist power and Bayesian power use statistical significance as the success criterion. However, statistical significance is often not enough to define success. To address this issue, the power concept can be extended to the concept of predictive probability of success (PPOS). The success criterion for PPOS is not restricted to statistical significance and is commonly used in clinical trial designs.

Software for power and sample size calculations [ edit ] Numerous free and/or open source programs are available for performing power and sample size calculations. These include G*Power ( https://www.gpower.hhu.de/ ) WebPower Free online statistical power analysis ( https://webpower.psychstat.org ) Free and open source online calculators ( https://powerandsamplesize.com ) PowerUp! provides Excel-based functions to determine minimum detectable effect size and minimum required sample size for various experimental and quasi-experimental designs.

PowerUpR is R package version of PowerUp! and additionally includes functions to determine sample size for various multilevel randomized experiments with or without budgetary constraints.

R package pwr ( https://cran.r-project.org/web/packages/pwr/ ) R package WebPower ( https://cran.r-project.org/web/packages/WebPower/index.html ) R package Spower ( https://cran.r-project.org/web/packages/Spower/index.html ) for general-purpose power analyses using simulation experiments Python package statsmodels ( https://www.statsmodels.org/ ) See also [ edit ] Mathematics portal Positive and negative predictive values – Statistical measures of whether a finding is likely to be true Effect size – Statistical measure of the magnitude of a phenomenon Efficiency – Quality measure of a statistical method Neyman–Pearson lemma – Theorem about the power of the likelihood ratio test Sample size – Statistical considerations on how many observations to make Pages displaying short descriptions of redirect targets Uniformly most powerful test – Theoretically optimal hypothesis test References [ edit ] ^ "Statistical power and underpowered statistics — Statistics Done Wrong" .

www.statisticsdonewrong.com . Retrieved 30 September 2019 .

^ Nakagawa, Shinichi; Lagisz, Malgorzata; Yang, Yefeng; Drobniak, Szymon M. (2024).

"Finding the right power balance: Better study design and collaboration can reduce dependence on statistical power" .

PLOS Biology .

22 (1): e3002423.

doi : 10.1371/journal.pbio.3002423 .

PMC 10773938 .

PMID 38190355 .

^ Robert Lehr (1992), "SixteenS-squared overD-squared: A relation for crude sample size estimates", Statistics in Medicine (in German), vol. 11, no. 8, pp. 1099–1102, doi : 10.1002/sim.4780110811 , ISSN 0277-6715 , PMID 1496197 ^ van Belle, Gerald (2008-08-18).

Statistical Rules of Thumb, Second Edition . Wiley Series in Probability and Statistics. Hoboken, NJ, USA: John Wiley & Sons, Inc.

doi : 10.1002/9780470377963 .

ISBN 978-0-470-37796-3 .

^ Sample Size Estimation in Clinical Research From Randomized Controlled Trials to Observational Studies, 2020, doi: 10.1016/j.chest.2020.03.010, Xiaofeng Wang, PhD; and Xinge Ji, MS pdf ^ Everitt, Brian S. (2002).

The Cambridge Dictionary of Statistics . Cambridge University Press. p. 321.

ISBN 0-521-81099-X .

^ Ellis, Paul (2010).

The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results . Cambridge University Press. p. 52.

ISBN 978-0521142465 .

^ Tsang, R.; Colley, L.; Lynd, L.D. (2009). "Inadequate statistical power to detect clinically significant differences in adverse event rates in randomized controlled trials".

Journal of Clinical Epidemiology .

62 (6): 609– 616.

doi : 10.1016/j.jclinepi.2008.08.005 .

PMID 19013761 .

^ Ellis, Paul D. (2010).

The Essential Guide to Effect Sizes: An Introduction to Statistical Power, Meta-Analysis and the Interpretation of Research Results . United Kingdom: Cambridge University Press. p. 56.

^ "Estimating Statistical Power When Using Multiple Testing Procedures" .

mdrc.org . November 2017.

^ Hoenig; Heisey (2001). "The Abuse of Power".

The American Statistician .

55 (1): 19– 24.

doi : 10.1198/000313001300339897 .

^ Thomas, L. (1997).

"Retrospective power analysis" (PDF) .

Conservation Biology .

11 (1): 276– 280.

Bibcode : 1997ConBi..11..276T .

doi : 10.1046/j.1523-1739.1997.96102.x .

hdl : 10023/679 .

^ Graebner, Robert W. (1999).

Study design with SAS: Estimating power with Monte Carlo methods (PDF) . SUGI 24.

^ Ioannidis, John P. A.; Stanley, T. D.; Doucouliagos, Hristos (1 October 2017). "The Power of Bias in Economics Research".

The Economic Journal .

127 (605): F236 – F265 .

doi : 10.1111/ecoj.12461 .

^ Arel-Bundock, Vincent; Briggs, Ryan C; Doucouliagos, Hristos; Mendoza Aviña, Marco; Stanley, Td (13 December 2024). "Quantitative Political Science Research is Greatly Underpowered".

The Journal of Politics .

doi : 10.1086/734279 .

^ Stanley, T. D.; Carter, Evan C.; Doucouliagos, Hristos (December 2018). "What meta-analyses reveal about the replicability of psychological research".

Psychological Bulletin .

144 (12): 1325– 1346.

doi : 10.1037/bul0000169 .

PMID 30321017 .

^ Yang, Yefeng; Sánchez-Tójar, Alfredo; O’Dea, Rose E.; Noble, Daniel W. A.; Koricheva, Julia; Jennions, Michael D.; Parker, Timothy H.; Lagisz, Malgorzata; Nakagawa, Shinichi (3 April 2023).

"Publication bias impacts on effect size, statistical power, and magnitude (Type M) and sign (Type S) errors in ecology and evolutionary biology" .

BMC Biology .

21 (1) 71.

doi : 10.1186/s12915-022-01485-y .

PMC 10071700 .

PMID 37013585 .

Sources [ edit ] Cohen, J.

(1988).

Statistical Power Analysis for the Behavioral Sciences (2nd ed.). Lawrence Erlbaum Associates.

ISBN 0-8058-0283-5 .

Aberson, C.L. (2010).

Applied Power Analysis for the Behavioral Science . Routledge.

ISBN 978-1-84872-835-6 .

External links [ edit ] StatQuest: P-value pitfalls and power calculations on YouTube v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐56p2m
Cached time: 20250817205427
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.729 seconds
Real time usage: 0.995 seconds
Preprocessor visited node count: 3712/1000000
Revision size: 33910/2097152 bytes
Post‐expand include size: 185916/2097152 bytes
Template argument size: 2057/2097152 bytes
Highest expansion depth: 18/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 79953/5000000 bytes
Lua time usage: 0.433/10.000 seconds
Lua memory usage: 22760762/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  659.182      1 -total
 31.08%  204.870      1 Template:Reflist
 23.65%  155.905      1 Template:Statistics
 23.25%  153.259      1 Template:Navbox_with_collapsible_groups
 22.11%  145.743      6 Template:Annotated_link
 12.25%   80.721      2 Template:Cite_web
 11.43%   75.342      1 Template:Short_description
 10.41%   68.646      8 Template:Cite_journal
  9.82%   64.736     11 Template:Navbox
  6.84%   45.112      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:238695:|#|:idhash:canonical and timestamp 20250817205427 and revision id 1306450539. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Power_(statistics)&oldid=1306450539 " Category : Statistical hypothesis testing Hidden categories: CS1 German-language sources (de) Articles with short description Short description matches Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link This page was last edited on 17 August 2025, at 20:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Power (statistics) 20 languages Add topic

