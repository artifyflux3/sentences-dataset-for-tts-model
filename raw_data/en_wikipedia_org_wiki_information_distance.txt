Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Properties Toggle Properties subsection 1.1 Universality 1.2 Metricity 1.3 Maximum overlap 1.4 Minimum overlap 2 Applications Toggle Applications subsection 2.1 Theoretical 2.2 Practical 3 References 4 Related literature Toggle the table of contents Information distance 1 language Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Information distance is the distance between two finite objects (represented as computer files) expressed as the number of bits in the shortest program which transforms one object into the other one or vice versa on a universal computer . This is an extension of Kolmogorov complexity .

[ 1 ] The Kolmogorov complexity  of a single finite object is the information in that object; the information distance between a pair of finite objects is the minimum information required to go from one object to the other or vice versa.
Information distance was first defined and investigated in [ 2 ] based on thermodynamic principles, see also.

[ 3 ] Subsequently, it achieved final form in.

[ 4 ] It is applied in the normalized compression distance and the normalized Google distance .

Properties [ edit ] Formally the information distance I D ( x , y ) {\displaystyle ID(x,y)} between x {\displaystyle x} and y {\displaystyle y} is defined by I D ( x , y ) = min { | p | : p ( x ) = y & & p ( y ) = x } , {\displaystyle ID(x,y)=\min\{|p|:p(x)=y\;\&\;p(y)=x\},} with p {\displaystyle p} a finite binary program for the fixed universal computer with as inputs finite binary strings x , y {\displaystyle x,y} . In [ 4 ] it is proven that I D ( x , y ) = E ( x , y ) + O ( log ⋅ ⋅ max { K ( x ∣ ∣ y ) , K ( y ∣ ∣ x ) } ) {\displaystyle ID(x,y)=E(x,y)+O(\log \cdot \max\{K(x\mid y),K(y\mid x)\})} with E ( x , y ) = max { K ( x ∣ ∣ y ) , K ( y ∣ ∣ x ) } , {\displaystyle E(x,y)=\max\{K(x\mid y),K(y\mid x)\},} where K ( ⋅ ⋅ ∣ ∣ ⋅ ⋅ ) {\displaystyle K(\cdot \mid \cdot )} is the Kolmogorov complexity defined by [ 1 ] of the prefix type.

[ 5 ] This E ( x , y ) {\displaystyle E(x,y)} is the important quantity.

Universality [ edit ] Let Δ Δ {\displaystyle \Delta } be the class of upper semicomputable distances D ( x , y ) {\displaystyle D(x,y)} that satisfy the density condition ∑ ∑ x : x ≠ ≠ y 2 − − D ( x , y ) ≤ ≤ 1 , ∑ ∑ y : y ≠ ≠ x 2 − − D ( x , y ) ≤ ≤ 1 , {\displaystyle \sum _{x:x\neq y}2^{-D(x,y)}\leq 1,\;\sum _{y:y\neq x}2^{-D(x,y)}\leq 1,} This excludes irrelevant distances such as D ( x , y ) = 1 2 {\displaystyle D(x,y)={\frac {1}{2}}} for x ≠ ≠ y {\displaystyle x\neq y} ;
it takes care that if the distance growth then the number of objects within that distance of a given object grows.
If D ∈ ∈ Δ Δ {\displaystyle D\in \Delta } then E ( x , y ) ≤ ≤ D ( x , y ) {\displaystyle E(x,y)\leq D(x,y)} up to a constant additive term.

[ 4 ] The probabilistic expressions of the distance is the first cohomological class in information symmetric cohomology, [ 6 ] which may be conceived as a universality property.

Metricity [ edit ] The distance E ( x , y ) {\displaystyle E(x,y)} is a metric up to an additive O ( log .

max { K ( x ∣ ∣ y ) , K ( y ∣ ∣ x ) } ) {\displaystyle O(\log .\max\{K(x\mid y),K(y\mid x)\})} term in the metric (in)equalities.

[ 4 ] The probabilistic version of the metric is indeed unique has shown by Han in 1981.

[ 7 ] Maximum overlap [ edit ] If E ( x , y ) = K ( x ∣ ∣ y ) {\displaystyle E(x,y)=K(x\mid y)} , then there is a program p {\displaystyle p} of length K ( x ∣ ∣ y ) {\displaystyle K(x\mid y)} that converts y {\displaystyle y} to x {\displaystyle x} , and a program q {\displaystyle q} of length K ( y ∣ ∣ x ) − − K ( x ∣ ∣ y ) {\displaystyle K(y\mid x)-K(x\mid y)} such that the program q p {\displaystyle qp} converts x {\displaystyle x} to y {\displaystyle y} . (The programs are of the self-delimiting format which means that one can decide where one program ends and the other begins in concatenation of the programs.) That is, the shortest programs to convert between two objects can be made maximally overlapping: For K ( x ∣ ∣ y ) ≤ ≤ K ( y ∣ ∣ x ) {\displaystyle K(x\mid y)\leq K(y\mid x)} it can be divided into a program that converts object x {\displaystyle x} to object y {\displaystyle y} , and another program which concatenated with the first converts y {\displaystyle y} to x {\displaystyle x} while the concatenation of these two programs is a shortest program to convert between these objects.

[ 4 ] Minimum overlap [ edit ] The programs to convert between objects x {\displaystyle x} and y {\displaystyle y} can also be made minimal overlapping.
There exists a program p {\displaystyle p} of length K ( x ∣ ∣ y ) {\displaystyle K(x\mid y)} up to an additive term of O ( log ⁡ ⁡ ( max { K ( x ∣ ∣ y ) , K ( y ∣ ∣ x ) } ) ) {\displaystyle O(\log(\max\{K(x\mid y),K(y\mid x)\}))} that maps y {\displaystyle y} to x {\displaystyle x} and has small complexity when x {\displaystyle x} is known ( K ( p ∣ ∣ x ) ≈ ≈ 0 {\displaystyle K(p\mid x)\approx 0} ). Interchanging the two objects we have the other program [ 8 ] Having in mind the parallelism between Shannon information theory and Kolmogorov complexity theory, one can say that this result is parallel to the Slepian-Wolf and Körner–Imre Csiszár–Marton theorems.

Applications [ edit ] Theoretical [ edit ] The result of An.A. Muchnik on minimum overlap above is an important theoretical application showing
that certain codes exist: to go to finite target object from any object there is a program which almost only
depends on the target object! This result is fairly precise and the error term cannot be significantly improved.

[ 9 ] Information distance was material in the textbook, [ 10 ] it occurs in the Encyclopedia on Distances.

[ 11 ] Practical [ edit ] To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object). The result is the normalized compression distance (NCD) between the objects. This pertains to objects given as computer files like the genome of a mouse or text of a book.  If the objects are just given by name such as `Einstein' or `table' or the name of a book or the name `mouse', compression does not make sense. We need outside information about what the name means. Using a data base (such as the internet) and a means to search the database (such as a search engine like Google) provides this information. Every search engine on a data base that provides aggregate page counts can be used in the normalized Google distance (NGD).
A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available .

[ 12 ] References [ edit ] ^ a b A.N. Kolmogorov, Three approaches to the quantitative definition of information, Problems Inform. Transmission, 1:1(1965), 1–7 ^ M. Li, P.M.B. Vitanyi, Theory of Thermodynamics of Computation, Proc. IEEE Physics of Computation Workshop, Dallas, Texas, USA, 1992, 42–46 ^ M. Li, P.M.B. Vitanyi, Reversibility and Adiabatic Computation: Trading Time and Space for Energy,  Proc. R. Soc. Lond. A 9 April 1996 vol. 452 no. 1947 769–789 ^ a b c d e C.H. Bennett, P. Gacs, M. Li, P.M.B. Vitanyi, W. Zurek, Information distance, IEEE Transactions on Information Theory, 44:4(1998), 1407–1423 ^ L.A. Levin, Laws of Information Conservation (Nongrowth) and Aspects of the Foundation of Probability Theory, Problems Inform. Transmission, 10:3(1974), 30–35 ^ P. Baudot, The Poincaré-Shannon Machine: Statistical Physics and Machine Learning Aspects of Information Cohomology , Entropy, 21:9 - 881 (2019) ^ Te Sun Han, A uniqueness of Shannon information distance and related nonnegativity problems, Journal of combinatorics. 6:4 p.320-331 (1981), 30–35 ^ Muchnik, Andrej A. (2002).

"Conditional complexity and codes" .

Theoretical Computer Science .

271 ( 1– 2): 97– 109.

doi : 10.1016/S0304-3975(01)00033-0 .

^ N.K Vereshchagin, M.V. Vyugin, Independent minimum length programs to translate between given strings, Proc. 15th Ann. Conf. Computational Complexity, 2000, 138–144 ^ M.Hutter, Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, Springer, 1998 ^ M.M. Deza, E Deza , Encyclopedia of Distances, Springer, 2009, doi : 10.1007/978-3-642-00234-2 ^ "InfoTopo: Topological Information Data Analysis. Deep statistical unsupervised and supervised learning - File Exchange - Github" .

github.com/pierrebaudot/infotopopy/ . Retrieved 26 September 2020 .

Related literature [ edit ] Arkhangel'skii, A. V.; Pontryagin, L. S. (1990), General Topology I: Basic Concepts and Constructions Dimension Theory , Encyclopaedia of Mathematical Sciences, Springer , ISBN 3-540-18178-4 Retrieved from " https://en.wikipedia.org/w/index.php?title=Information_distance&oldid=1237711601 " Category : Statistical distance This page was last edited on 31 July 2024, at 03:56 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Information distance 1 language Add topic

