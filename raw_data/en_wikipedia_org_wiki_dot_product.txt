Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Coordinate definition 1.2 Geometric definition 1.3 Scalar projection and first properties 1.4 Equivalence of the definitions 2 Properties Toggle Properties subsection 2.1 Application to the law of cosines 3 Triple product 4 Physics 5 Generalizations Toggle Generalizations subsection 5.1 Complex vectors 5.2 Inner product 5.3 Functions 5.4 Weight function 5.5 Dyadics and matrices 5.6 Tensors 6 Computation Toggle Computation subsection 6.1 Algorithms 6.2 Libraries 7 See also 8 Notes 9 References 10 External links Toggle the table of contents Dot product 70 languages አማርኛ العربية Asturianu Azərbaycanca বাংলা Башҡортса Беларуская Български Bosanski Català Чӑвашла Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Hrvatski Bahasa Indonesia Italiano עברית ქართული Қазақша Latina Latviešu Lietuvių Magyar Македонски मराठी Bahasa Melayu Nederlands 日本語 Norsk bokmål Norsk nynorsk Oʻzbekcha / ўзбекча Piemontèis Polski Português Română Русский Саха тыла Scots Shqip Simple English Slovenčina Slovenščina کوردی Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் Татарча / tatarça ไทย Türkçe Українська اردو Tiếng Việt 吴语 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Algebraic operation on coordinate vectors "Scalar product" redirects here. For the abstract scalar product, see Inner product space . For the product of a vector and a scalar, see Scalar multiplication .

In mathematics , the dot product or scalar product [ note 1 ] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors ), and returns a single number. In Euclidean geometry , the dot product of the Cartesian coordinates of two vectors is widely used. It is often called the inner product (or rarely the projection product ) of Euclidean space , even though it is not the only inner product that can be defined on Euclidean space (see Inner product space for more). It should not be confused with the cross product .

Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry , Euclidean spaces are often defined by using vector spaces . In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle between two vectors is the quotient of their dot product by the product of their lengths).

The name "dot product" is derived from the dot operator " ⋅ " that is often used to designate this operation; [ 1 ] the alternative name "scalar product" emphasizes that the result is a scalar , rather than a vector (as with the vector product in three-dimensional space).

Definition [ edit ] The dot product may be defined algebraically or geometrically. The geometric definition is based on the notions of angle and distance ( magnitude ) of vectors. The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.

In modern presentations of Euclidean geometry , the points of space are defined in terms of their Cartesian coordinates , and Euclidean space itself is commonly identified with the real coordinate space R n {\displaystyle \mathbf {R} ^{n}} . In such a presentation, the notions of length and angle are defined by means of the dot product. The length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle between two vectors of length one is defined as their dot product.  So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.

Coordinate definition [ edit ] The dot product of two vectors a = [ a 1 , a 2 , ⋯ ⋯ , a n ] {\displaystyle \mathbf {a} =[a_{1},a_{2},\cdots ,a_{n}]} and b = [ b 1 , b 2 , ⋯ ⋯ , b n ] {\displaystyle \mathbf {b} =[b_{1},b_{2},\cdots ,b_{n}]} , specified with respect to an orthonormal basis , is defined as: [ 2 ] a ⋅ ⋅ b = ∑ ∑ i = 1 n a i b i = a 1 b 1 + a 2 b 2 + ⋯ ⋯ + a n b n {\displaystyle \mathbf {a} \cdot \mathbf {b} =\sum _{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}} where Σ Σ {\displaystyle \Sigma } ( sigma ) denotes summation and n {\displaystyle n} is the dimension of the vector space . For instance, in three-dimensional space , the dot product of vectors [ 1 , 3 , − − 5 ] {\displaystyle [1,3,-5]} and [ 4 , − − 2 , − − 1 ] {\displaystyle [4,-2,-1]} is: [ 1 , 3 , − − 5 ] ⋅ ⋅ [ 4 , − − 2 , − − 1 ] = ( 1 × × 4 ) + ( 3 × × − − 2 ) + ( − − 5 × × − − 1 ) = 4 − − 6 + 5 = 3 {\displaystyle {\begin{aligned}\ [1,3,-5]\cdot [4,-2,-1]&=(1\times 4)+(3\times -2)+(-5\times -1)\\&=4-6+5\\&=3\end{aligned}}} Likewise, the dot product of the vector [ 1 , 3 , − − 5 ] {\displaystyle [1,3,-5]} with itself is: [ 1 , 3 , − − 5 ] ⋅ ⋅ [ 1 , 3 , − − 5 ] = ( 1 × × 1 ) + ( 3 × × 3 ) + ( − − 5 × × − − 5 ) = 1 + 9 + 25 = 35 {\displaystyle {\begin{aligned}\ [1,3,-5]\cdot [1,3,-5]&=(1\times 1)+(3\times 3)+(-5\times -5)\\&=1+9+25\\&=35\end{aligned}}} If vectors are identified with column vectors , the dot product can also be written as a matrix product a ⋅ ⋅ b = a T b , {\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {a} ^{\mathsf {T}}\mathbf {b} ,} where a T {\displaystyle \mathbf {a} {^{\mathsf {T}}}} denotes the transpose of a {\displaystyle \mathbf {a} } .

Expressing the above example in this way, a 1 × 3 matrix ( row vector ) is multiplied by a 3 × 1 matrix ( column vector ) to get a 1 × 1 matrix that is identified with its unique entry: [ 1 3 − − 5 ] [ 4 − − 2 − − 1 ] = 3 .

{\displaystyle {\begin{bmatrix}1&3&-5\end{bmatrix}}{\begin{bmatrix}4\\-2\\-1\end{bmatrix}}=3\,.} Geometric definition [ edit ] Illustration showing how to find the angle between vectors using the dot product Calculating bond angles of a symmetrical tetrahedral molecular geometry using a dot product In Euclidean space , a Euclidean vector is a geometric object that possesses both a magnitude and a direction.  A vector can be pictured as an arrow.  Its magnitude is its length, and its direction is the direction to which the arrow points. The magnitude of a vector a {\displaystyle \mathbf {a} } is denoted by ‖ a ‖ {\displaystyle \left\|\mathbf {a} \right\|} . The dot product of two Euclidean vectors a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } is defined by [ 3 ] [ 4 ] [ 1 ] a ⋅ ⋅ b = ‖ a ‖ ‖ b ‖ cos ⁡ ⁡ θ θ , {\displaystyle \mathbf {a} \cdot \mathbf {b} =\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\cos \theta ,} where θ θ {\displaystyle \theta } is the angle between a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } .

In particular, if the vectors a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } are orthogonal (i.e., their angle is π π 2 {\displaystyle {\frac {\pi }{2}}} or 90 ∘ ∘ {\displaystyle 90^{\circ }} ), then cos ⁡ ⁡ π π 2 = 0 {\displaystyle \cos {\frac {\pi }{2}}=0} , which implies that a ⋅ ⋅ b = 0.

{\displaystyle \mathbf {a} \cdot \mathbf {b} =0.} At the other extreme, if they are codirectional , then the angle between them is zero with cos ⁡ ⁡ 0 = 1 {\displaystyle \cos 0=1} and a ⋅ ⋅ b = ‖ a ‖ ‖ b ‖ {\displaystyle \mathbf {a} \cdot \mathbf {b} =\left\|\mathbf {a} \right\|\,\left\|\mathbf {b} \right\|} This implies that the dot product of a vector a {\displaystyle \mathbf {a} } with itself is a ⋅ ⋅ a = ‖ a ‖ 2 , {\displaystyle \mathbf {a} \cdot \mathbf {a} =\left\|\mathbf {a} \right\|^{2},} which gives ‖ a ‖ = a ⋅ ⋅ a , {\displaystyle \left\|\mathbf {a} \right\|={\sqrt {\mathbf {a} \cdot \mathbf {a} }},} the formula for the Euclidean length of the vector.

Scalar projection and first properties [ edit ] Scalar projection The scalar projection (or scalar component) of a Euclidean vector a {\displaystyle \mathbf {a} } in the direction of a Euclidean vector b {\displaystyle \mathbf {b} } is given by a b = ‖ a ‖ cos ⁡ ⁡ θ θ , {\displaystyle a_{b}=\left\|\mathbf {a} \right\|\cos \theta ,} where θ θ {\displaystyle \theta } is the angle between a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } .

In terms of the geometric definition of the dot product, this can be rewritten as a b = a ⋅ ⋅ b ^ ^ , {\displaystyle a_{b}=\mathbf {a} \cdot {\widehat {\mathbf {b} }},} where b ^ ^ = b / ‖ b ‖ {\displaystyle {\widehat {\mathbf {b} }}=\mathbf {b} /\left\|\mathbf {b} \right\|} is the unit vector in the direction of b {\displaystyle \mathbf {b} } .

Distributive law for the dot product The dot product is thus characterized geometrically by [ 5 ] a ⋅ ⋅ b = a b ‖ b ‖ = b a ‖ a ‖ .

{\displaystyle \mathbf {a} \cdot \mathbf {b} =a_{b}\left\|\mathbf {b} \right\|=b_{a}\left\|\mathbf {a} \right\|.} The dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar α α {\displaystyle \alpha } , ( α α a ) ⋅ ⋅ b = α α ( a ⋅ ⋅ b ) = a ⋅ ⋅ ( α α b ) .

{\displaystyle (\alpha \mathbf {a} )\cdot \mathbf {b} =\alpha (\mathbf {a} \cdot \mathbf {b} )=\mathbf {a} \cdot (\alpha \mathbf {b} ).} It also satisfies the distributive law , meaning that a ⋅ ⋅ ( b + c ) = a ⋅ ⋅ b + a ⋅ ⋅ c .

{\displaystyle \mathbf {a} \cdot (\mathbf {b} +\mathbf {c} )=\mathbf {a} \cdot \mathbf {b} +\mathbf {a} \cdot \mathbf {c} .} These properties may be summarized by saying that the dot product is a bilinear form . Moreover, this bilinear form is positive definite , which means that a ⋅ ⋅ a {\displaystyle \mathbf {a} \cdot \mathbf {a} } is never negative, and is zero if and only if a = 0 {\displaystyle \mathbf {a} =\mathbf {0} } , the zero vector .

Equivalence of the definitions [ edit ] If e 1 , ⋯ ⋯ , e n {\displaystyle \mathbf {e} _{1},\cdots ,\mathbf {e} _{n}} are the standard basis vectors in R n {\displaystyle \mathbf {R} ^{n}} , then we may write a = [ a 1 , … … , a n ] = ∑ ∑ i a i e i b = [ b 1 , … … , b n ] = ∑ ∑ i b i e i .

{\displaystyle {\begin{aligned}\mathbf {a} &=[a_{1},\dots ,a_{n}]=\sum _{i}a_{i}\mathbf {e} _{i}\\\mathbf {b} &=[b_{1},\dots ,b_{n}]=\sum _{i}b_{i}\mathbf {e} _{i}.\end{aligned}}} The vectors e i {\displaystyle \mathbf {e} _{i}} are an orthonormal basis , which means that they have unit length and are at right angles to each other.  Since these vectors have unit length, e i ⋅ ⋅ e i = 1 {\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{i}=1} and since they form right angles with each other, if i ≠ ≠ j {\displaystyle i\neq j} , e i ⋅ ⋅ e j = 0.

{\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{j}=0.} Thus in general, we can say that: e i ⋅ ⋅ e j = δ δ i j , {\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{j}=\delta _{ij},} where δ δ i j {\displaystyle \delta _{ij}} is the Kronecker delta .

Vector components in an orthonormal basis Also, by the geometric definition, for any vector e i {\displaystyle \mathbf {e} _{i}} and a vector a {\displaystyle \mathbf {a} } , we note that a ⋅ ⋅ e i = ‖ a ‖ ‖ e i ‖ cos ⁡ ⁡ θ θ i = ‖ a ‖ cos ⁡ ⁡ θ θ i = a i , {\displaystyle \mathbf {a} \cdot \mathbf {e} _{i}=\left\|\mathbf {a} \right\|\left\|\mathbf {e} _{i}\right\|\cos \theta _{i}=\left\|\mathbf {a} \right\|\cos \theta _{i}=a_{i},} where a i {\displaystyle a_{i}} is the component of vector a {\displaystyle \mathbf {a} } in the direction of e i {\displaystyle \mathbf {e} _{i}} . The last step in the equality can be seen from the figure.

Now applying the distributivity of the geometric version of the dot product gives a ⋅ ⋅ b = a ⋅ ⋅ ∑ ∑ i b i e i = ∑ ∑ i b i ( a ⋅ ⋅ e i ) = ∑ ∑ i b i a i = ∑ ∑ i a i b i , {\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {a} \cdot \sum _{i}b_{i}\mathbf {e} _{i}=\sum _{i}b_{i}(\mathbf {a} \cdot \mathbf {e} _{i})=\sum _{i}b_{i}a_{i}=\sum _{i}a_{i}b_{i},} which is precisely the algebraic definition of the dot product.  So the geometric dot product equals the algebraic dot product.

Properties [ edit ] The dot product fulfills the following properties if a {\displaystyle \mathbf {a} } , b {\displaystyle \mathbf {b} } , c {\displaystyle \mathbf {c} } and d {\displaystyle \mathbf {d} } are real vectors and α α {\displaystyle \alpha } , β β {\displaystyle \beta } , γ γ {\displaystyle \gamma } and δ δ {\displaystyle \delta } are scalars .

[ 2 ] [ 3 ] Commutative a ⋅ ⋅ b = b ⋅ ⋅ a , {\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {b} \cdot \mathbf {a} ,} which follows from the definition ( θ θ {\displaystyle \theta } is the angle between a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } ): [ 6 ] a ⋅ ⋅ b = ‖ a ‖ ‖ b ‖ cos ⁡ ⁡ θ θ = ‖ b ‖ ‖ a ‖ cos ⁡ ⁡ θ θ = b ⋅ ⋅ a .

{\displaystyle \mathbf {a} \cdot \mathbf {b} =\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\cos \theta =\left\|\mathbf {b} \right\|\left\|\mathbf {a} \right\|\cos \theta =\mathbf {b} \cdot \mathbf {a} .} The commutative property can also be easily proven with the algebraic definition, and in more general spaces (where the notion of angle might not be geometrically intuitive but an analogous product can be defined) the angle between two vectors can be defined as θ θ = arccos ⁡ ⁡ ( a ⋅ ⋅ b ‖ a ‖ ‖ b ‖ ) .

{\displaystyle \theta =\operatorname {arccos} \left({\frac {\mathbf {a} \cdot \mathbf {b} }{\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|}}\right).} Bilinear (additive, distributive and scalar-multiplicative in both arguments) ( α α a + β β b ) ⋅ ⋅ ( γ γ c + δ δ d ) = α α γ γ ( a ⋅ ⋅ c ) + α α δ δ ( a ⋅ ⋅ d ) + β β γ γ ( b ⋅ ⋅ c ) + β β δ δ ( b ⋅ ⋅ d ) .

{\displaystyle {\begin{aligned}(\alpha \mathbf {a} +\beta \mathbf {b} )&\cdot (\gamma \mathbf {c} +\delta \mathbf {d} )\\&=\alpha \gamma (\mathbf {a} \cdot \mathbf {c} )+\alpha \delta (\mathbf {a} \cdot \mathbf {d} )+\beta \gamma (\mathbf {b} \cdot \mathbf {c} )+\beta \delta (\mathbf {b} \cdot \mathbf {d} ).\end{aligned}}} Not associative Because the dot product is not defined between a scalar a ⋅ ⋅ b {\displaystyle \mathbf {a} \cdot \mathbf {b} } and a vector c , {\displaystyle \mathbf {c} ,} associativity is meaningless.

[ 7 ] However, bilinearity implies c ( a ⋅ ⋅ b ) = ( c a ) ⋅ ⋅ b = a ⋅ ⋅ ( c b ) .

{\displaystyle c(\mathbf {a} \cdot \mathbf {b} )=(c\mathbf {a} )\cdot \mathbf {b} =\mathbf {a} \cdot (c\mathbf {b} ).} This property is sometimes called the "associative law for scalar and dot product", [ 8 ] and one may say that "the dot product is associative with respect to scalar multiplication".

[ 9 ] Orthogonal Two non-zero vectors a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } are orthogonal if and only if a ⋅ ⋅ b = 0 {\displaystyle \mathbf {a} \cdot \mathbf {b} =0} .

No cancellation Unlike multiplication of ordinary numbers, where if a b = a c {\displaystyle ab=ac} , then b {\displaystyle b} always equals c {\displaystyle c} unless a {\displaystyle a} is zero, the dot product does not obey the cancellation law : If a ⋅ ⋅ b = a ⋅ ⋅ c {\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {a} \cdot \mathbf {c} } and a ≠ ≠ 0 {\displaystyle \mathbf {a} \neq \mathbf {0} } , then we can write: a ⋅ ⋅ ( b − − c ) = 0 {\displaystyle \mathbf {a} \cdot (\mathbf {b} -\mathbf {c} )=0} by the distributive law ; the result above says this just means that a {\displaystyle \mathbf {a} } is perpendicular to ( b − − c ) {\displaystyle (\mathbf {b} -\mathbf {c} )} , which still allows ( b − − c ) ≠ ≠ 0 {\displaystyle (\mathbf {b} -\mathbf {c} )\neq \mathbf {0} } , and therefore allows b ≠ ≠ c {\displaystyle \mathbf {b} \neq \mathbf {c} } .

Product rule If a {\displaystyle \mathbf {a} } and b {\displaystyle \mathbf {b} } are vector-valued differentiable functions , then the derivative ( denoted by a prime ′ {\displaystyle {}'} ) of a ⋅ ⋅ b {\displaystyle \mathbf {a} \cdot \mathbf {b} } is given by the rule ( a ⋅ ⋅ b ) ′ = a ′ ⋅ ⋅ b + a ⋅ ⋅ b ′ .

{\displaystyle (\mathbf {a} \cdot \mathbf {b} )'=\mathbf {a} '\cdot \mathbf {b} +\mathbf {a} \cdot \mathbf {b} '.} Application to the law of cosines [ edit ] Triangle with vector edges a and b , separated by angle θ Main article: Law of cosines Given two vectors a {\displaystyle {\color {red}\mathbf {a} }} and b {\displaystyle {\color {blue}\mathbf {b} }} separated by angle θ θ {\displaystyle \theta } (see the upper image), they form a triangle with a third side c = a − − b {\displaystyle {\color {orange}\mathbf {c} }={\color {red}\mathbf {a} }-{\color {blue}\mathbf {b} }} . Let a {\displaystyle \color {red}a} , b {\displaystyle \color {blue}b} and c {\displaystyle \color {orange}c} denote the lengths of a {\displaystyle {\color {red}\mathbf {a} }} , b {\displaystyle {\color {blue}\mathbf {b} }} , and c {\displaystyle {\color {orange}\mathbf {c} }} , respectively. The dot product of c {\displaystyle {\color {orange}\mathbf {c} }} with itself is: c ⋅ ⋅ c = ( a − − b ) ⋅ ⋅ ( a − − b ) = a ⋅ ⋅ a − − a ⋅ ⋅ b − − b ⋅ ⋅ a + b ⋅ ⋅ b = a 2 − − a ⋅ ⋅ b − − a ⋅ ⋅ b + b 2 = a 2 − − 2 a ⋅ ⋅ b + b 2 c 2 = a 2 + b 2 − − 2 a b cos ⁡ ⁡ θ θ {\displaystyle {\begin{aligned}\mathbf {\color {orange}c} \cdot \mathbf {\color {orange}c} &=(\mathbf {\color {red}a} -\mathbf {\color {blue}b} )\cdot (\mathbf {\color {red}a} -\mathbf {\color {blue}b} )\\&=\mathbf {\color {red}a} \cdot \mathbf {\color {red}a} -\mathbf {\color {red}a} \cdot \mathbf {\color {blue}b} -\mathbf {\color {blue}b} \cdot \mathbf {\color {red}a} +\mathbf {\color {blue}b} \cdot \mathbf {\color {blue}b} \\&={\color {red}a}^{2}-\mathbf {\color {red}a} \cdot \mathbf {\color {blue}b} -\mathbf {\color {red}a} \cdot \mathbf {\color {blue}b} +{\color {blue}b}^{2}\\&={\color {red}a}^{2}-2\mathbf {\color {red}a} \cdot \mathbf {\color {blue}b} +{\color {blue}b}^{2}\\{\color {orange}c}^{2}&={\color {red}a}^{2}+{\color {blue}b}^{2}-2{\color {red}a}{\color {blue}b}\cos \mathbf {\color {purple}\theta } \\\end{aligned}}} which is the law of cosines .

Triple product [ edit ] Main article: Triple product There are two ternary operations involving dot product and cross product .

The scalar triple product of three vectors is defined as a ⋅ ⋅ ( b × × c ) = b ⋅ ⋅ ( c × × a ) = c ⋅ ⋅ ( a × × b ) .

{\displaystyle \mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} )=\mathbf {b} \cdot (\mathbf {c} \times \mathbf {a} )=\mathbf {c} \cdot (\mathbf {a} \times \mathbf {b} ).} Its value is the determinant of the matrix whose columns are the Cartesian coordinates of the three vectors. It is the signed volume of the parallelepiped defined by the three vectors, and is isomorphic to the three-dimensional special case of the exterior product of three vectors.

The vector triple product is defined by [ 2 ] [ 3 ] a × × ( b × × c ) = ( a ⋅ ⋅ c ) b − − ( a ⋅ ⋅ b ) c .

{\displaystyle \mathbf {a} \times (\mathbf {b} \times \mathbf {c} )=(\mathbf {a} \cdot \mathbf {c} )\,\mathbf {b} -(\mathbf {a} \cdot \mathbf {b} )\,\mathbf {c} .} This identity, also known as Lagrange's formula , may be remembered as "ACB minus ABC", keeping in mind which vectors are dotted together. This formula has applications in simplifying vector calculations in physics .

Physics [ edit ] In physics , the dot product takes two vectors and returns a scalar quantity. It is also known as the "scalar product". The dot product of two vectors can be defined as the product of the magnitudes of the two vectors and the cosine of the angle between the two vectors. Thus, a ⋅ ⋅ b = | a | | b | cos ⁡ ⁡ θ θ {\displaystyle \mathbf {a} \cdot \mathbf {b} =|\mathbf {a} |\,|\mathbf {b} |\cos \theta } Alternatively, it is defined as the product of the projection of the first vector onto the second vector and the magnitude of the second vector.

For example: [ 10 ] [ 11 ] Mechanical work is the dot product of force and displacement vectors, Power is the dot product of force and velocity .

Generalizations [ edit ] Complex vectors [ edit ] For vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance, the dot product of a vector with itself could be zero without the vector being the zero vector (e.g. this would happen with the vector a = [ 1 i ] {\displaystyle \mathbf {a} =[1\ i]} ).

This in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the dot product, through the alternative definition [ 12 ] [ 2 ] a ⋅ ⋅ b = ∑ ∑ i a i b i ¯ ¯ , {\displaystyle \mathbf {a} \cdot \mathbf {b} =\sum _{i}{{a_{i}}\,{\overline {b_{i}}}},} where b i ¯ ¯ {\displaystyle {\overline {b_{i}}}} is the complex conjugate of b i {\displaystyle b_{i}} . When vectors are represented by column vectors , the dot product can be expressed as a matrix product involving a conjugate transpose , denoted with the superscript H: a ⋅ ⋅ b = b H a .

{\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {b} ^{\mathsf {H}}\mathbf {a} .} In the case of vectors with real components, this definition is the same as in the real case. The dot product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However, the complex dot product is sesquilinear rather than bilinear, as it is conjugate linear and not linear in a {\displaystyle \mathbf {a} } . The dot product is not symmetric, since a ⋅ ⋅ b = b ⋅ ⋅ a ¯ ¯ .

{\displaystyle \mathbf {a} \cdot \mathbf {b} ={\overline {\mathbf {b} \cdot \mathbf {a} }}.} The angle between two complex vectors is then given by cos ⁡ ⁡ θ θ = Re ⁡ ⁡ ( a ⋅ ⋅ b ) ‖ a ‖ ‖ b ‖ .

{\displaystyle \cos \theta ={\frac {\operatorname {Re} (\mathbf {a} \cdot \mathbf {b} )}{\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|}}.} The complex dot product leads to the notions of Hermitian forms and general inner product spaces , which are widely used in mathematics and physics .

The self dot product of a complex vector a ⋅ ⋅ a = a H a {\displaystyle \mathbf {a} \cdot \mathbf {a} =\mathbf {a} ^{\mathsf {H}}\mathbf {a} } , involving the conjugate transpose of a row vector, is also known as the norm squared , a ⋅ ⋅ a = ‖ ‖ a ‖ ‖ 2 {\textstyle \mathbf {a} \cdot \mathbf {a} =\|\mathbf {a} \|^{2}} , after the Euclidean norm ; it is a vector generalization of the absolute square of a complex scalar (see also: Squared Euclidean distance ).

Inner product [ edit ] Main article: Inner product space The inner product generalizes the dot product to abstract vector spaces over a field of scalars , being either the field of real numbers R {\displaystyle \mathbb {R} } or the field of complex numbers C {\displaystyle \mathbb {C} } . It is usually denoted using angular brackets by ⟨ a , b ⟩ {\displaystyle \left\langle \mathbf {a} \,,\mathbf {b} \right\rangle } .

The inner product of two vectors over the field of complex numbers is, in general, a complex number, and is sesquilinear instead of bilinear. An inner product space is a normed vector space , and the inner product of a vector with itself is real and positive-definite.

Functions [ edit ] The dot product is defined for vectors that have a finite number of entries . Thus these vectors can be regarded as discrete functions : a length- n {\displaystyle n} vector u {\displaystyle u} is, then, a function with domain { k ∈ ∈ N : 1 ≤ ≤ k ≤ ≤ n } {\displaystyle \{k\in \mathbb {N} :1\leq k\leq n\}} , and u i {\displaystyle u_{i}} is a notation for the image of i {\displaystyle i} by the function/vector u {\displaystyle u} .

This notion can be generalized to square-integrable functions :  just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some measure space ( X , A , μ μ ) {\displaystyle (X,{\mathcal {A}},\mu )} : [ 2 ] ⟨ u , v ⟩ = ∫ ∫ X u v d μ μ .

{\displaystyle \left\langle u,v\right\rangle =\int _{X}uv\,{\text{d}}\mu .} For example, if f {\displaystyle f} and g {\displaystyle g} are continuous functions over a compact subset K {\displaystyle K} of R n {\displaystyle \mathbb {R} ^{n}} with the standard Lebesgue measure , the above definition becomes: ⟨ f , g ⟩ = ∫ ∫ K f ( x ) g ( x ) d n ⁡ ⁡ x .

{\displaystyle \left\langle f,g\right\rangle =\int _{K}f(\mathbf {x} )g(\mathbf {x} )\,\operatorname {d} ^{n}\mathbf {x} .} Generalized further to complex continuous functions ψ ψ {\displaystyle \psi } and χ χ {\displaystyle \chi } , by analogy with the complex inner product above, gives: ⟨ ψ ψ , χ χ ⟩ = ∫ ∫ K ψ ψ ( z ) χ χ ( z ) ¯ ¯ d z .

{\displaystyle \left\langle \psi ,\chi \right\rangle =\int _{K}\psi (z){\overline {\chi (z)}}\,{\text{d}}z.} Weight function [ edit ] Inner products can have a weight function (i.e., a function which weights each term of the inner product with a value). Explicitly, the inner product of functions u ( x ) {\displaystyle u(x)} and v ( x ) {\displaystyle v(x)} with respect to the weight function r ( x ) > 0 {\displaystyle r(x)>0} is ⟨ u , v ⟩ r = ∫ ∫ a b r ( x ) u ( x ) v ( x ) d x .

{\displaystyle \left\langle u,v\right\rangle _{r}=\int _{a}^{b}r(x)u(x)v(x)\,dx.} Dyadics and matrices [ edit ] A double-dot product for matrices is the Frobenius inner product , which is analogous to the dot product on vectors. It is defined as the sum of the products of the corresponding components of two matrices A {\displaystyle \mathbf {A} } and B {\displaystyle \mathbf {B} } of the same size: A : B = ∑ ∑ i ∑ ∑ j A i j B i j ¯ ¯ = tr ⁡ ⁡ ( B H A ) = tr ⁡ ⁡ ( A B H ) .

{\displaystyle \mathbf {A} :\mathbf {B} =\sum _{i}\sum _{j}A_{ij}{\overline {B_{ij}}}=\operatorname {tr} (\mathbf {B} ^{\mathsf {H}}\mathbf {A} )=\operatorname {tr} (\mathbf {A} \mathbf {B} ^{\mathsf {H}}).} And for real matrices, A : B = ∑ ∑ i ∑ ∑ j A i j B i j = tr ⁡ ⁡ ( B T A ) = tr ⁡ ⁡ ( A B T ) = tr ⁡ ⁡ ( A T B ) = tr ⁡ ⁡ ( B A T ) .

{\displaystyle \mathbf {A} :\mathbf {B} =\sum _{i}\sum _{j}A_{ij}B_{ij}=\operatorname {tr} (\mathbf {B} ^{\mathsf {T}}\mathbf {A} )=\operatorname {tr} (\mathbf {A} \mathbf {B} ^{\mathsf {T}})=\operatorname {tr} (\mathbf {A} ^{\mathsf {T}}\mathbf {B} )=\operatorname {tr} (\mathbf {B} \mathbf {A} ^{\mathsf {T}}).} Writing a matrix as a dyadic , we can define a different double-dot product (see Dyadics § Product of dyadic and dyadic ) however it is not an inner product.

Tensors [ edit ] The inner product between a tensor of order n {\displaystyle n} and a tensor of order m {\displaystyle m} is a tensor of order n + m − − 2 {\displaystyle n+m-2} , see Tensor contraction for details.

Computation [ edit ] Algorithms [ edit ] The straightforward algorithm for calculating a floating-point dot product of vectors can suffer from catastrophic cancellation . To avoid this, approaches such as the Kahan summation algorithm are used.

Libraries [ edit ] A dot product function is included in: BLAS level 1 real SDOT , DDOT ; complex CDOTU , ZDOTU = X^T * Y , CDOTC , ZDOTC = X^H * Y Fortran as dot_product(A,B) or sum(conjg(A) * B) Julia as A' * B or standard library LinearAlgebra as dot(A, B) R (programming language) as sum(A * B) for vectors or, more generally for matrices, as A %*% B Matlab as A' * B or conj(transpose(A)) * B or sum(conj(A) .* B) or dot(A, B) Python (package NumPy ) as np.matmul(A, B) or np.dot(A, B) or np.inner(A, B) GNU Octave as sum(conj(X) .* Y, dim) , and similar code as Matlab Intel oneAPI Math Kernel Library real p?dot dot  = sub(x)'*sub(y) ; complex p?dotc dotc  = conjg(sub(x)')*sub(y) See also [ edit ] Cauchy–Schwarz inequality Cross product Dot product representation of a graph Euclidean norm , the square-root of the self dot product Matrix multiplication Metric tensor Multiplication of vectors Outer product Notes [ edit ] ^ The term scalar product means literally "product with a scalar as a result". It is also used for other symmetric bilinear forms , for example in a pseudo-Euclidean space . Not to be confused with scalar multiplication .

References [ edit ] ^ a b "Dot Product" .

www.mathsisfun.com . Retrieved 2020-09-06 .

^ a b c d e S. Lipschutz; M. Lipson (2009).

Linear Algebra (Schaum's Outlines) (4th ed.). McGraw Hill.

ISBN 978-0-07-154352-1 .

^ a b c M.R. Spiegel; S. Lipschutz; D. Spellman (2009).

Vector Analysis (Schaum's Outlines) (2nd ed.). McGraw Hill.

ISBN 978-0-07-161545-7 .

^ A I Borisenko; I E Taparov (1968).

Vector and tensor analysis with applications . Translated by Richard Silverman. Dover. p. 14.

^ Arfken, G. B.; Weber, H. J. (2000).

Mathematical Methods for Physicists (5th ed.). Boston, MA: Academic Press . pp.

14– 15.

ISBN 978-0-12-059825-0 .

^ Nykamp, Duane.

"The dot product" .

Math Insight . Retrieved September 6, 2020 .

^ Weisstein, Eric W. "Dot Product". From MathWorld--A Wolfram Web Resource.

http://mathworld.wolfram.com/DotProduct.html ^ T. Banchoff; J. Wermer (1983).

Linear Algebra Through Geometry . Springer Science & Business Media. p. 12.

ISBN 978-1-4684-0161-5 .

^ A. Bedford; Wallace L. Fowler (2008).

Engineering Mechanics: Statics (5th ed.). Prentice Hall. p. 60.

ISBN 978-0-13-612915-8 .

^ K.F. Riley; M.P. Hobson; S.J. Bence (2010).

Mathematical methods for physics and engineering (3rd ed.). Cambridge University Press.

ISBN 978-0-521-86153-3 .

^ M. Mansfield; C. O'Sullivan (2011).

Understanding Physics (4th ed.). John Wiley & Sons.

ISBN 978-0-47-0746370 .

^ Berberian, Sterling K. (2014) [1992].

Linear Algebra . Dover. p. 287.

ISBN 978-0-486-78055-9 .

External links [ edit ] Wikimedia Commons has media related to Scalar product .

"Inner product" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Explanation of dot product including with complex vectors "Dot Product" by Bruce Torrence, Wolfram Demonstrations Project , 2007.

v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category v t e Tensors Glossary of tensor theory Scope Mathematics Coordinate system Differential geometry Dyadic algebra Euclidean geometry Exterior calculus Multilinear algebra Tensor algebra Tensor calculus Physics Engineering Computer vision Continuum mechanics Electromagnetism General relativity Transport phenomena Notation Abstract index notation Einstein notation Index notation Multi-index notation Penrose graphical notation Ricci calculus Tetrad (index notation) Van der Waerden notation Voigt notation Tensor definitions Tensor (intrinsic definition) Tensor field Tensor density Tensors in curvilinear coordinates Mixed tensor Antisymmetric tensor Symmetric tensor Tensor operator Tensor bundle Two-point tensor Operations Covariant derivative Exterior covariant derivative Exterior derivative Exterior product Hodge star operator Lie derivative Raising and lowering indices Symmetrization Tensor contraction Tensor product Transpose (2nd-order tensors) Related abstractions Affine connection Basis Cartan formalism (physics) Connection form Covariance and contravariance of vectors Differential form Dimension Exterior form Fiber bundle Geodesic Levi-Civita connection Linear map Manifold Matrix Multivector Pseudotensor Spinor Vector Vector space Notable tensors Mathematics Kronecker delta Levi-Civita symbol Metric tensor Nonmetricity tensor Ricci curvature Riemann curvature tensor Torsion tensor Weyl tensor Physics Moment of inertia Angular momentum tensor Spin tensor Cauchy stress tensor stress–energy tensor Einstein tensor EM tensor Gluon field strength tensor Metric tensor (GR) Mathematicians Élie Cartan Augustin-Louis Cauchy Elwin Bruno Christoffel Albert Einstein Leonhard Euler Carl Friedrich Gauss Hermann Grassmann Tullio Levi-Civita Gregorio Ricci-Curbastro Bernhard Riemann Jan Arnoldus Schouten Woldemar Voigt Hermann Weyl Authority control databases : National Germany NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐fvgbc
Cached time: 20250811235238
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.548 seconds
Real time usage: 0.880 seconds
Preprocessor visited node count: 2941/1000000
Revision size: 29070/2097152 bytes
Post‐expand include size: 64200/2097152 bytes
Template argument size: 1508/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 28/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 76379/5000000 bytes
Lua time usage: 0.259/10.000 seconds
Lua memory usage: 6367879/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  444.683      1 -total
 30.79%  136.896      2 Template:Reflist
 20.37%   90.585      4 Template:Navbox
 16.95%   75.375      1 Template:Linear_algebra
 16.23%   72.178      1 Template:Short_description
 15.66%   69.653      2 Template:Cite_web
 11.29%   50.208      2 Template:Pagetype
 11.21%   49.839      9 Template:Cite_book
  7.90%   35.123      1 Template:Commons_category
  7.45%   33.116      1 Template:Sister_project Saved in parser cache with key enwiki:pcache:157093:|#|:idhash:canonical and timestamp 20250811235238 and revision id 1296787094. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Dot_product&oldid=1296787094 " Categories : Bilinear forms Operations on vectors Analytic geometry Tensors Scalars Hidden categories: Articles with short description Short description is different from Wikidata Commons category link is on Wikidata Articles containing proofs This page was last edited on 22 June 2025, at 07:56 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Dot product 70 languages Add topic

