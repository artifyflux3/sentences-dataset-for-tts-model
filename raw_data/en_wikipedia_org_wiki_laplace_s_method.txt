Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Concept 2 General theory Toggle General theory subsection 2.1 Formal statement and proof 3 Other formulations 4 Steepest descent extension 5 Further generalizations 6 Median-point approximation generalization 7 Complex integrals 8 Example: Stirling's approximation 9 See also 10 Notes 11 References Toggle the table of contents Laplace's method 11 languages Català Čeština Deutsch Español Français 한국어 Italiano 日本語 Português Русский 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Method for approximate evaluation of integrals Not to be confused with Laplace's approximation or Laplace smoothing .

This article's tone or style may not reflect the encyclopedic tone used on Wikipedia .

See Wikipedia's guide to writing better articles for suggestions.

( March 2024 ) ( Learn how and when to remove this message ) In mathematics , Laplace's method , named after Pierre-Simon Laplace , is a technique used to approximate integrals of the form ∫ ∫ a b e M f ( x ) d x , {\displaystyle \int _{a}^{b}e^{Mf(x)}\,dx,} where f {\displaystyle f} is a twice- differentiable function , M {\displaystyle M} is a large number , and the endpoints a {\displaystyle a} and b {\displaystyle b} could be infinite. This technique was originally presented in the book by Laplace (1774) .

In Bayesian statistics , Laplace's approximation can refer to either approximating the posterior normalizing constant with Laplace's method or approximating the posterior distribution with a Gaussian centered at the maximum a posteriori estimate .

[ 1 ] [ 2 ] Laplace approximations are used in the integrated nested Laplace approximations method for fast approximations of Bayesian inference .

Concept [ edit ] f ( x ) = sin ⁡ ⁡ ( x ) x {\displaystyle f(x)={\tfrac {\sin(x)}{x}}} has a global maximum at x = 0 {\displaystyle x=0} .

e M f ( x ) {\displaystyle e^{Mf(x)}} is shown on top for M = 0.5 {\displaystyle M=0.5} and at the bottom for M = 3 {\displaystyle M=3} (both in blue). As M {\displaystyle M} grows, the approximation of this function by a Gaussian function (shown in red) improves. This observation underlies Laplace's method.

Let the function f ( x ) {\displaystyle f(x)} have a unique global maximum at x 0 {\displaystyle x_{0}} .

M > 0 {\displaystyle M>0} is a constant here. The following two functions are considered: g ( x ) = M f ( x ) , h ( x ) = e M f ( x ) .

{\displaystyle {\begin{aligned}g(x)&=Mf(x),\\h(x)&=e^{Mf(x)}.\end{aligned}}} Then, x 0 {\displaystyle x_{0}} is the global maximum of g {\displaystyle g} and h {\displaystyle h} as well. Hence: g ( x 0 ) g ( x ) = M f ( x 0 ) M f ( x ) = f ( x 0 ) f ( x ) , h ( x 0 ) h ( x ) = e M f ( x 0 ) e M f ( x ) = e M ( f ( x 0 ) − − f ( x ) ) .

{\displaystyle {\begin{aligned}{\frac {g(x_{0})}{g(x)}}&={\frac {Mf(x_{0})}{Mf(x)}}={\frac {f(x_{0})}{f(x)}},\\[4pt]{\frac {h(x_{0})}{h(x)}}&={\frac {e^{Mf(x_{0})}}{e^{Mf(x)}}}=e^{M(f(x_{0})-f(x))}.\end{aligned}}} As M increases, the ratio for h {\displaystyle h} will grow exponentially, while the ratio for g {\displaystyle g} does not change. Thus, significant contributions to the integral of this function will come only from points x {\displaystyle x} in a neighborhood of x 0 {\displaystyle x_{0}} , which can then be estimated.

General theory [ edit ] To state and motivate the method, one must make several assumptions. It is assumed that x 0 {\displaystyle x_{0}} is not an endpoint of the interval of integration and that the values f ( x ) {\displaystyle f(x)} cannot be very close to f ( x 0 ) {\displaystyle f(x_{0})} unless x {\displaystyle x} is close to x 0 {\displaystyle x_{0}} .

f ( x ) {\displaystyle f(x)} can be expanded around x 0 {\displaystyle x_{0}} by Taylor's theorem , f ( x ) = f ( x 0 ) + f ′ ( x 0 ) ( x − − x 0 ) + 1 2 f ″ ( x 0 ) ( x − − x 0 ) 2 + R {\displaystyle f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+{\frac {1}{2}}f''(x_{0})(x-x_{0})^{2}+R} where R = O ( ( x − − x 0 ) 3 ) {\displaystyle R=O\left((x-x_{0})^{3}\right)} (see: big O notation ).

Since f {\displaystyle f} has a global maximum at x 0 {\displaystyle x_{0}} , and x 0 {\displaystyle x_{0}} is not an endpoint, it is a stationary point , i.e.

f ′ ( x 0 ) = 0 {\displaystyle f'(x_{0})=0} . Therefore, the second-order Taylor polynomial approximating f ( x ) {\displaystyle f(x)} is f ( x ) ≈ ≈ f ( x 0 ) + 1 2 f ″ ( x 0 ) ( x − − x 0 ) 2 .

{\displaystyle f(x)\approx f(x_{0})+{\frac {1}{2}}f''(x_{0})(x-x_{0})^{2}.} Then, just one more step is needed to get a Gaussian distribution. Since x 0 {\displaystyle x_{0}} is a global maximum of the function f {\displaystyle f} it can be stated, by definition of the second derivative , that f ″ ( x 0 ) ≤ ≤ 0 {\displaystyle f''(x_{0})\leq 0} , thus giving the relation f ( x ) ≈ ≈ f ( x 0 ) − − 1 2 | f ″ ( x 0 ) | ( x − − x 0 ) 2 {\displaystyle f(x)\approx f(x_{0})-{\frac {1}{2}}|f''(x_{0})|(x-x_{0})^{2}} for x {\displaystyle x} close to x 0 {\displaystyle x_{0}} .  The integral can then be approximated with: ∫ ∫ a b e M f ( x ) d x ≈ ≈ e M f ( x 0 ) ∫ ∫ a b e − − 1 2 M | f ″ ( x 0 ) | ( x − − x 0 ) 2 d x {\displaystyle \int _{a}^{b}e^{Mf(x)}\,dx\approx e^{Mf(x_{0})}\int _{a}^{b}e^{-{\frac {1}{2}}M|f''(x_{0})|(x-x_{0})^{2}}\,dx} If f ″ ( x 0 ) < 0 {\displaystyle f''(x_{0})<0} this latter integral becomes a Gaussian integral if we replace the limits of integration by − − ∞ ∞ {\displaystyle -\infty } and + ∞ ∞ {\displaystyle +\infty } ; when M {\displaystyle M} is large this creates only a small error because the exponential decays very fast away from x 0 {\displaystyle x_{0}} .  Computing this Gaussian integral we obtain: ∫ ∫ a b e M f ( x ) d x ≈ ≈ 2 π π M | f ″ ( x 0 ) | e M f ( x 0 ) as M → → ∞ ∞ .

{\displaystyle \int _{a}^{b}e^{Mf(x)}\,dx\approx {\sqrt {\frac {2\pi }{M|f''(x_{0})|}}}e^{Mf(x_{0})}{\text{ as }}M\to \infty .} A generalization of this method and extension to arbitrary precision is provided by the book Fog (2008) .

Formal statement and proof [ edit ] Suppose f ( x ) {\displaystyle f(x)} is a twice continuously differentiable function on [ a , b ] , {\displaystyle [a,b],} and there exists a unique point x 0 ∈ ∈ ( a , b ) {\displaystyle x_{0}\in (a,b)} such that: f ( x 0 ) = max x ∈ ∈ [ a , b ] f ( x ) and f ″ ( x 0 ) < 0.

{\displaystyle f(x_{0})=\max _{x\in [a,b]}f(x)\quad {\text{and}}\quad f''(x_{0})<0.} Then: lim n → → ∞ ∞ ∫ ∫ a b e n f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) = 1.

{\displaystyle \lim _{n\to \infty }{\frac {\int _{a}^{b}e^{nf(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n\left(-f''(x_{0})\right)}}}}}=1.} Proof Lower bound: Let ε ε > 0 {\displaystyle \varepsilon >0} . Since f ″ {\displaystyle f''} is continuous there exists δ δ > 0 {\displaystyle \delta >0} such that if | x 0 − − c | < δ δ {\displaystyle |x_{0}-c|<\delta } then f ″ ( c ) ≥ ≥ f ″ ( x 0 ) − − ε ε .

{\displaystyle f''(c)\geq f''(x_{0})-\varepsilon .} By Taylor's Theorem , for any x ∈ ∈ ( x 0 − − δ δ , x 0 + δ δ ) , {\displaystyle x\in (x_{0}-\delta ,x_{0}+\delta ),} f ( x ) ≥ ≥ f ( x 0 ) + 1 2 ( f ″ ( x 0 ) − − ε ε ) ( x − − x 0 ) 2 .

{\displaystyle f(x)\geq f(x_{0})+{\frac {1}{2}}(f''(x_{0})-\varepsilon )(x-x_{0})^{2}.} Then we have the following lower bound: ∫ ∫ a b e n f ( x ) d x ≥ ≥ ∫ ∫ x 0 − − δ δ x 0 + δ δ e n f ( x ) d x ≥ ≥ e n f ( x 0 ) ∫ ∫ x 0 − − δ δ x 0 + δ δ e n 2 ( f ″ ( x 0 ) − − ε ε ) ( x − − x 0 ) 2 d x = e n f ( x 0 ) 1 n ( − − f ″ ( x 0 ) + ε ε ) ∫ ∫ − − δ δ n ( − − f ″ ( x 0 ) + ε ε ) δ δ n ( − − f ″ ( x 0 ) + ε ε ) e − − 1 2 y 2 d y {\displaystyle {\begin{aligned}\int _{a}^{b}e^{nf(x)}\,dx&\geq \int _{x_{0}-\delta }^{x_{0}+\delta }e^{nf(x)}\,dx\\&\geq e^{nf(x_{0})}\int _{x_{0}-\delta }^{x_{0}+\delta }e^{{\frac {n}{2}}(f''(x_{0})-\varepsilon )(x-x_{0})^{2}}\,dx\\&=e^{nf(x_{0})}{\sqrt {\frac {1}{n(-f''(x_{0})+\varepsilon )}}}\int _{-\delta {\sqrt {n(-f''(x_{0})+\varepsilon )}}}^{\delta {\sqrt {n(-f''(x_{0})+\varepsilon )}}}e^{-{\frac {1}{2}}y^{2}}\,dy\end{aligned}}} where the last equality was obtained by a change of variables y = n ( − − f ″ ( x 0 ) + ε ε ) ( x − − x 0 ) .

{\displaystyle y={\sqrt {n(-f''(x_{0})+\varepsilon )}}(x-x_{0}).} Remember f ″ ( x 0 ) < 0 {\displaystyle f''(x_{0})<0} so we can take the square root of its negation.

If we divide both sides of the above inequality by e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) {\displaystyle e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}} and take the limit we get: lim n → → ∞ ∞ ∫ ∫ a b e n f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) ≥ ≥ lim n → → ∞ ∞ 1 2 π π ∫ ∫ − − δ δ n ( − − f ″ ( x 0 ) + ε ε ) δ δ n ( − − f ″ ( x 0 ) + ε ε ) e − − 1 2 y 2 d y ⋅ ⋅ − − f ″ ( x 0 ) − − f ″ ( x 0 ) + ε ε = − − f ″ ( x 0 ) − − f ″ ( x 0 ) + ε ε {\displaystyle \lim _{n\to \infty }{\frac {\int _{a}^{b}e^{nf(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}}}\geq \lim _{n\to \infty }{\frac {1}{\sqrt {2\pi }}}\int _{-\delta {\sqrt {n(-f''(x_{0})+\varepsilon )}}}^{\delta {\sqrt {n(-f''(x_{0})+\varepsilon )}}}e^{-{\frac {1}{2}}y^{2}}\,dy\,\cdot {\sqrt {\frac {-f''(x_{0})}{-f''(x_{0})+\varepsilon }}}={\sqrt {\frac {-f''(x_{0})}{-f''(x_{0})+\varepsilon }}}} since this is true for arbitrary ε ε {\displaystyle \varepsilon } we get the lower bound: lim n → → ∞ ∞ ∫ ∫ a b e n f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) ≥ ≥ 1 {\displaystyle \lim _{n\to \infty }{\frac {\int _{a}^{b}e^{nf(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}}}\geq 1} Note that this proof works also when a = − − ∞ ∞ {\displaystyle a=-\infty } or b = ∞ ∞ {\displaystyle b=\infty } (or both).

Upper bound: The proof is similar to that of the lower bound but there are a few inconveniences. Again we start by picking an ε ε > 0 {\displaystyle \varepsilon >0} but in order for the proof to work we need ε ε {\displaystyle \varepsilon } small enough so that f ″ ( x 0 ) + ε ε < 0.

{\displaystyle f''(x_{0})+\varepsilon <0.} Then, as above, by continuity of f ″ {\displaystyle f''} and Taylor's Theorem we can find δ δ > 0 {\displaystyle \delta >0} so that if | x − − x 0 | < δ δ {\displaystyle |x-x_{0}|<\delta } , then f ( x ) ≤ ≤ f ( x 0 ) + 1 2 ( f ″ ( x 0 ) + ε ε ) ( x − − x 0 ) 2 .

{\displaystyle f(x)\leq f(x_{0})+{\frac {1}{2}}(f''(x_{0})+\varepsilon )(x-x_{0})^{2}.} Lastly, by our assumptions (assuming a , b {\displaystyle a,b} are finite) there exists an η η > 0 {\displaystyle \eta >0} such that if | x − − x 0 | ≥ ≥ δ δ {\displaystyle |x-x_{0}|\geq \delta } , then f ( x ) ≤ ≤ f ( x 0 ) − − η η {\displaystyle f(x)\leq f(x_{0})-\eta } .

Then we can calculate the following upper bound: ∫ ∫ a b e n f ( x ) d x ≤ ≤ ∫ ∫ a x 0 − − δ δ e n f ( x ) d x + ∫ ∫ x 0 − − δ δ x 0 + δ δ e n f ( x ) d x + ∫ ∫ x 0 + δ δ b e n f ( x ) d x ≤ ≤ ( b − − a ) e n ( f ( x 0 ) − − η η ) + ∫ ∫ x 0 − − δ δ x 0 + δ δ e n f ( x ) d x ≤ ≤ ( b − − a ) e n ( f ( x 0 ) − − η η ) + e n f ( x 0 ) ∫ ∫ x 0 − − δ δ x 0 + δ δ e n 2 ( f ″ ( x 0 ) + ε ε ) ( x − − x 0 ) 2 d x ≤ ≤ ( b − − a ) e n ( f ( x 0 ) − − η η ) + e n f ( x 0 ) ∫ ∫ − − ∞ ∞ + ∞ ∞ e n 2 ( f ″ ( x 0 ) + ε ε ) ( x − − x 0 ) 2 d x ≤ ≤ ( b − − a ) e n ( f ( x 0 ) − − η η ) + e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) − − ε ε ) {\displaystyle {\begin{aligned}\int _{a}^{b}e^{nf(x)}\,dx&\leq \int _{a}^{x_{0}-\delta }e^{nf(x)}\,dx+\int _{x_{0}-\delta }^{x_{0}+\delta }e^{nf(x)}\,dx+\int _{x_{0}+\delta }^{b}e^{nf(x)}\,dx\\&\leq (b-a)e^{n(f(x_{0})-\eta )}+\int _{x_{0}-\delta }^{x_{0}+\delta }e^{nf(x)}\,dx\\&\leq (b-a)e^{n(f(x_{0})-\eta )}+e^{nf(x_{0})}\int _{x_{0}-\delta }^{x_{0}+\delta }e^{{\frac {n}{2}}(f''(x_{0})+\varepsilon )(x-x_{0})^{2}}\,dx\\&\leq (b-a)e^{n(f(x_{0})-\eta )}+e^{nf(x_{0})}\int _{-\infty }^{+\infty }e^{{\frac {n}{2}}(f''(x_{0})+\varepsilon )(x-x_{0})^{2}}\,dx\\&\leq (b-a)e^{n(f(x_{0})-\eta )}+e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0})-\varepsilon )}}}\end{aligned}}} If we divide both sides of the above inequality by e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) {\displaystyle e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}} and take the limit we get: lim n → → ∞ ∞ ∫ ∫ a b e n f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) ≤ ≤ lim n → → ∞ ∞ ( b − − a ) e − − η η n n ( − − f ″ ( x 0 ) ) 2 π π + − − f ″ ( x 0 ) − − f ″ ( x 0 ) − − ε ε = − − f ″ ( x 0 ) − − f ″ ( x 0 ) − − ε ε {\displaystyle \lim _{n\to \infty }{\frac {\int _{a}^{b}e^{nf(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}}}\leq \lim _{n\to \infty }(b-a)e^{-\eta n}{\sqrt {\frac {n(-f''(x_{0}))}{2\pi }}}+{\sqrt {\frac {-f''(x_{0})}{-f''(x_{0})-\varepsilon }}}={\sqrt {\frac {-f''(x_{0})}{-f''(x_{0})-\varepsilon }}}} Since ε ε {\displaystyle \varepsilon } is arbitrary we get the upper bound: lim n → → ∞ ∞ ∫ ∫ a b e n f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) ≤ ≤ 1 {\displaystyle \lim _{n\to \infty }{\frac {\int _{a}^{b}e^{nf(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}}}\leq 1} And combining this with the lower bound gives the result.

Note that the above proof obviously fails when a = − − ∞ ∞ {\displaystyle a=-\infty } or b = ∞ ∞ {\displaystyle b=\infty } (or both). To deal with these cases, we need some extra assumptions. A sufficient (not necessary) assumption is that for n = 1 , {\displaystyle n=1,} ∫ ∫ a b e n f ( x ) d x < ∞ ∞ , {\displaystyle \int _{a}^{b}e^{nf(x)}\,dx<\infty ,} and that the number η η {\displaystyle \eta } as above exists (note that this must be an assumption in the case when the interval [ a , b ] {\displaystyle [a,b]} is infinite). The proof proceeds otherwise as above, but with a slightly different approximation of integrals: ∫ ∫ a x 0 − − δ δ e n f ( x ) d x + ∫ ∫ x 0 + δ δ b e n f ( x ) d x ≤ ≤ ∫ ∫ a b e f ( x ) e ( n − − 1 ) ( f ( x 0 ) − − η η ) d x = e ( n − − 1 ) ( f ( x 0 ) − − η η ) ∫ ∫ a b e f ( x ) d x .

{\displaystyle \int _{a}^{x_{0}-\delta }e^{nf(x)}\,dx+\int _{x_{0}+\delta }^{b}e^{nf(x)}\,dx\leq \int _{a}^{b}e^{f(x)}e^{(n-1)(f(x_{0})-\eta )}\,dx=e^{(n-1)(f(x_{0})-\eta )}\int _{a}^{b}e^{f(x)}\,dx.} When we divide by e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) , {\displaystyle e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}},} we get for this term e ( n − − 1 ) ( f ( x 0 ) − − η η ) ∫ ∫ a b e f ( x ) d x e n f ( x 0 ) 2 π π n ( − − f ″ ( x 0 ) ) = e − − ( n − − 1 ) η η n e − − f ( x 0 ) ∫ ∫ a b e f ( x ) d x − − f ″ ( x 0 ) 2 π π {\displaystyle {\frac {e^{(n-1)(f(x_{0})-\eta )}\int _{a}^{b}e^{f(x)}\,dx}{e^{nf(x_{0})}{\sqrt {\frac {2\pi }{n(-f''(x_{0}))}}}}}=e^{-(n-1)\eta }{\sqrt {n}}e^{-f(x_{0})}\int _{a}^{b}e^{f(x)}\,dx{\sqrt {\frac {-f''(x_{0})}{2\pi }}}} whose limit as n → → ∞ ∞ {\displaystyle n\to \infty } is 0 {\displaystyle 0} . The rest of the proof (the analysis of the interesting term) proceeds as above.

The given condition in the infinite interval case is, as said above, sufficient but not necessary. However, the condition is fulfilled in many, if not in most, applications: the condition simply says that the integral we are studying must be well-defined (not infinite) and that the maximum of the function at x 0 {\displaystyle x_{0}} must be a "true" maximum (the number η η > 0 {\displaystyle \eta >0} must exist). There is no need to demand that the integral is finite for n = 1 {\displaystyle n=1} but it is enough to demand that the integral is finite for some n = N .

{\displaystyle n=N.} This method relies on 4 basic concepts such as Concepts 1. Relative error The “approximation” in this method is related to the relative error and not the absolute error . Therefore, if we set s = 2 π π M | f ″ ( x 0 ) | , {\displaystyle s={\sqrt {\frac {2\pi }{M\left|f''(x_{0})\right|}}},} the integral can be written as ∫ ∫ a b e M f ( x ) d x = s e M f ( x 0 ) 1 s ∫ ∫ a b e M ( f ( x ) − − f ( x 0 ) ) d x = s e M f ( x 0 ) ∫ ∫ a − − x 0 s b − − x 0 s e M ( f ( s y + x 0 ) − − f ( x 0 ) ) d y {\displaystyle {\begin{aligned}\int _{a}^{b}e^{Mf(x)}\,dx&=se^{Mf(x_{0})}{\frac {1}{s}}\int _{a}^{b}e^{M(f(x)-f(x_{0}))}\,dx\\&=se^{Mf(x_{0})}\int _{\frac {a-x_{0}}{s}}^{\frac {b-x_{0}}{s}}e^{M(f(sy+x_{0})-f(x_{0}))}\,dy\end{aligned}}} where s {\displaystyle s} is a small number when M {\displaystyle M} is a large number obviously and the relative error will be | ∫ ∫ a − − x 0 s b − − x 0 s e M ( f ( s y + x 0 ) − − f ( x 0 ) ) d y − − 1 | .

{\displaystyle \left|\int _{\frac {a-x_{0}}{s}}^{\frac {b-x_{0}}{s}}e^{M(f(sy+x_{0})-f(x_{0}))}dy-1\right|.} Now, let us separate this integral into two parts: y ∈ ∈ [ − − D y , D y ] {\displaystyle y\in [-D_{y},D_{y}]} region and the rest.

2.

e M ( f ( s y + x 0 ) − − f ( x 0 ) ) → → e − − π π y 2 {\displaystyle e^{M(f(sy+x_{0})-f(x_{0}))}\to e^{-\pi y^{2}}} around the stationary point when M {\displaystyle M} is large enough Let’s look at the Taylor expansion of M ( f ( x ) − − f ( x 0 ) ) {\displaystyle M(f(x)-f(x_{0}))} around x 0 {\displaystyle x_{0}} and translate x {\displaystyle x} to y {\displaystyle y} because we do the comparison in y-space, we will get M ( f ( x ) − − f ( x 0 ) ) = M f ″ ( x 0 ) 2 s 2 y 2 + M f ‴ ( x 0 ) 6 s 3 y 3 + ⋯ ⋯ = − − π π y 2 + O ( 1 M ) .

{\displaystyle M(f(x)-f(x_{0}))={\frac {Mf''(x_{0})}{2}}s^{2}y^{2}+{\frac {Mf'''(x_{0})}{6}}s^{3}y^{3}+\cdots =-\pi y^{2}+O\left({\frac {1}{\sqrt {M}}}\right).} Note that f ′ ( x 0 ) = 0 {\displaystyle f'(x_{0})=0} because x 0 {\displaystyle x_{0}} is a stationary point. From this equation you will find that the terms higher than second derivative in this Taylor expansion is suppressed as the order of 1 M {\displaystyle {\tfrac {1}{\sqrt {M}}}} so that exp ⁡ ⁡ ( M ( f ( x ) − − f ( x 0 ) ) ) {\displaystyle \exp(M(f(x)-f(x_{0})))} will get closer to the Gaussian function as shown in figure. Besides, ∫ ∫ − − ∞ ∞ ∞ ∞ e − − π π y 2 d y = 1.

{\displaystyle \int _{-\infty }^{\infty }e^{-\pi y^{2}}dy=1.} The figure of e M [ f ( s y + x 0 ) − − f ( x 0 ) ] {\displaystyle e^{M[f(sy+x_{0})-f(x_{0})]}} with M {\displaystyle M} equals 1, 2 and 3, and the red line is the curve of function e − − π π y 2 {\displaystyle e^{-\pi y^{2}}} .

3. The larger M {\displaystyle M} is, the smaller range of x {\displaystyle x} is related Because we do the comparison in y-space, y {\displaystyle y} is fixed in y ∈ ∈ [ − − D y , D y ] {\displaystyle y\in [-D_{y},D_{y}]} which will cause x ∈ ∈ [ − − s D y , s D y ] {\displaystyle x\in [-sD_{y},sD_{y}]} ; however, s {\displaystyle s} is inversely proportional to M {\displaystyle {\sqrt {M}}} , the chosen region of x {\displaystyle x} will be smaller when M {\displaystyle M} is increased.

4. If the integral in Laplace's method converges, the contribution of the region which is not around the stationary point of the integration of its relative error will tend to zero as M {\displaystyle M} grows.

Relying on the 3rd concept, even if we choose a very large D y , sD y will finally be a very small number when M {\displaystyle M} is increased to a huge number. Then, how can we guarantee the integral of the rest will tend to 0 when M {\displaystyle M} is large enough?

The basic idea is to find a function m ( x ) {\displaystyle m(x)} such that m ( x ) ≥ ≥ f ( x ) {\displaystyle m(x)\geq f(x)} and the integral of e M m ( x ) {\displaystyle e^{Mm(x)}} will tend to zero when M {\displaystyle M} grows. Because the exponential function of M m ( x ) {\displaystyle Mm(x)} will be always larger than zero as long as m ( x ) {\displaystyle m(x)} is a real number, and this exponential function is proportional to m ( x ) , {\displaystyle m(x),} the integral of e M f ( x ) {\displaystyle e^{Mf(x)}} will tend to zero. For simplicity, choose m ( x ) {\displaystyle m(x)} as a tangent through the point x = s D y {\displaystyle x=sD_{y}} as shown in the figure: m ( x ) {\displaystyle m(x)} is denoted by the two tangent lines passing through x = ± ± s D y + x 0 {\displaystyle x=\pm sD_{y}+x_{0}} . When s D y {\displaystyle sD_{y}} gets smaller, the cover region will be larger.

If the interval of the integration of this method is finite, we will find that no matter f ( x ) {\displaystyle f(x)} is continue in the rest region, it will be always smaller than m ( x ) {\displaystyle m(x)} shown above when M {\displaystyle M} is large enough. By the way, it will be proved later that the integral of e M m ( x ) {\displaystyle e^{Mm(x)}} will tend to zero when M {\displaystyle M} is large enough.

If the interval of the integration of this method is infinite, m ( x ) {\displaystyle m(x)} and f ( x ) {\displaystyle f(x)} might always cross to each other. If so, we cannot guarantee that the integral of e M f ( x ) {\displaystyle e^{Mf(x)}} will tend to zero finally. For example, in the case of f ( x ) = sin ⁡ ⁡ ( x ) x , {\displaystyle f(x)={\tfrac {\sin(x)}{x}},} ∫ ∫ 0 ∞ ∞ e M f ( x ) d x {\displaystyle \int _{0}^{\infty }e^{Mf(x)}dx} will always diverge. Therefore, we need to require that ∫ ∫ d ∞ ∞ e M f ( x ) d x {\displaystyle \int _{d}^{\infty }e^{Mf(x)}dx} can converge for the infinite interval case. If so, this integral will tend to zero when d {\displaystyle d} is large enough and we can choose this d {\displaystyle d} as the cross of m ( x ) {\displaystyle m(x)} and f ( x ) .

{\displaystyle f(x).} You might ask why not choose ∫ ∫ d ∞ ∞ e f ( x ) d x {\displaystyle \int _{d}^{\infty }e^{f(x)}dx} as a convergent integral? Let me use an example to show you the reason. Suppose the rest part of f ( x ) {\displaystyle f(x)} is − − ln ⁡ ⁡ x , {\displaystyle -\ln x,} then e f ( x ) = 1 x {\displaystyle e^{f(x)}={\tfrac {1}{x}}} and its integral will diverge; however, when M = 2 , {\displaystyle M=2,} the integral of e M f ( x ) = 1 x 2 {\displaystyle e^{Mf(x)}={\tfrac {1}{x^{2}}}} converges. So, the integral of some functions will diverge when M {\displaystyle M} is not a large number, but they will converge when M {\displaystyle M} is large enough.

Based on these four concepts, we can derive the relative error of this method.

Other formulations [ edit ] Laplace's approximation is sometimes written as ∫ ∫ a b h ( x ) e M g ( x ) d x ≈ ≈ 2 π π M | g ″ ( x 0 ) | h ( x 0 ) e M g ( x 0 ) as M → → ∞ ∞ {\displaystyle \int _{a}^{b}h(x)e^{Mg(x)}\,dx\approx {\sqrt {\frac {2\pi }{M|g''(x_{0})|}}}h(x_{0})e^{Mg(x_{0})}\ {\text{ as }}M\to \infty } where h {\displaystyle h} is positive.

Importantly, the accuracy of the approximation depends on the variable of integration, that is, on what stays in g ( x ) {\displaystyle g(x)} and what goes into h ( x ) .

{\displaystyle h(x).} [ 3 ] The derivation of its relative error First, use x 0 = 0 {\displaystyle x_{0}=0} to denote the global maximum, which will simplify this derivation. We are interested in the relative error, written as | R | {\displaystyle |R|} , ∫ ∫ a b h ( x ) e M g ( x ) d x = h ( 0 ) e M g ( 0 ) s ∫ ∫ a / s b / s h ( s y ) h ( 0 ) e M [ g ( s y ) − − g ( 0 ) ] d y ⏟ ⏟ 1 + R , {\displaystyle \int _{a}^{b}h(x)e^{Mg(x)}\,dx=h(0)e^{Mg(0)}s\underbrace {\int _{a/s}^{b/s}{\frac {h(sy)}{h(0)}}e^{M\left[g(sy)-g(0)\right]}dy} _{1+R},} where s ≡ ≡ 2 π π M | g ″ ( 0 ) | .

{\displaystyle s\equiv {\sqrt {\frac {2\pi }{M\left|g''(0)\right|}}}.} So, if we let A ≡ ≡ h ( s y ) h ( 0 ) e M [ g ( s y ) − − g ( 0 ) ] {\displaystyle A\equiv {\frac {h(sy)}{h(0)}}e^{M\left[g(sy)-g(0)\right]}} and A 0 ≡ ≡ e − − π π y 2 {\displaystyle A_{0}\equiv e^{-\pi y^{2}}} , we can get | R | = | ∫ ∫ a / s b / s A d y − − ∫ ∫ − − ∞ ∞ ∞ ∞ A 0 d y | {\displaystyle \left|R\right|=\left|\int _{a/s}^{b/s}A\,dy-\int _{-\infty }^{\infty }A_{0}\,dy\right|} since ∫ ∫ − − ∞ ∞ ∞ ∞ A 0 d y = 1 {\displaystyle \int _{-\infty }^{\infty }A_{0}\,dy=1} .

For the upper bound, note that | A + B | ≤ ≤ | A | + | B | , {\displaystyle |A+B|\leq |A|+|B|,} thus we can separate this integration into 5 parts with 3 different types (a), (b) and (c), respectively. Therefore, | R | < | ∫ ∫ D y ∞ ∞ A 0 d y | ⏟ ⏟ ( a 1 ) + | ∫ ∫ D y b / s A d y | ⏟ ⏟ ( b 1 ) + | ∫ ∫ − − D y D y ( A − − A 0 ) d y | ⏟ ⏟ ( c ) + | ∫ ∫ a / s − − D y A d y | ⏟ ⏟ ( b 2 ) + | ∫ ∫ − − ∞ ∞ − − D y A 0 d y | ⏟ ⏟ ( a 2 ) {\displaystyle |R|<\underbrace {\left|\int _{D_{y}}^{\infty }A_{0}dy\right|} _{(a_{1})}+\underbrace {\left|\int _{D_{y}}^{b/s}Ady\right|} _{(b_{1})}+\underbrace {\left|\int _{-D_{y}}^{D_{y}}\left(A-A_{0}\right)dy\right|} _{(c)}+\underbrace {\left|\int _{a/s}^{-D_{y}}Ady\right|} _{(b_{2})}+\underbrace {\left|\int _{-\infty }^{-D_{y}}A_{0}dy\right|} _{(a_{2})}} where ( a 1 ) {\displaystyle (a_{1})} and ( a 2 ) {\displaystyle (a_{2})} are similar, let us just calculate ( a 1 ) {\displaystyle (a_{1})} and ( b 1 ) {\displaystyle (b_{1})} and ( b 2 ) {\displaystyle (b_{2})} are similar, too, I’ll just calculate ( b 1 ) {\displaystyle (b_{1})} .

For ( a 1 ) {\displaystyle (a_{1})} , after the translation of z ≡ ≡ π π y 2 {\displaystyle z\equiv \pi y^{2}} , we can get ( a 1 ) = | 1 2 π π ∫ ∫ π π D y 2 ∞ ∞ e − − z z − − 1 / 2 d z | < e − − π π D y 2 2 π π D y .

{\displaystyle (a_{1})=\left|{\frac {1}{2{\sqrt {\pi }}}}\int _{\pi D_{y}^{2}}^{\infty }e^{-z}z^{-1/2}dz\right|<{\frac {e^{-\pi D_{y}^{2}}}{2\pi D_{y}}}.} This means that as long as D y {\displaystyle D_{y}} is large enough, it will tend to zero.

For ( b 1 ) {\displaystyle (b_{1})} , we can get ( b 1 ) ≤ ≤ | ∫ ∫ D y b / s [ h ( s y ) h ( 0 ) ] max e M m ( s y ) d y | {\displaystyle (b_{1})\leq \left|\int _{D_{y}}^{b/s}\left[{\frac {h(sy)}{h(0)}}\right]_{\text{max}}e^{Mm(sy)}dy\right|} where m ( x ) ≥ ≥ g ( x ) − − g ( 0 ) as x ∈ ∈ [ s D y , b ] {\displaystyle m(x)\geq g(x)-g(0){\text{as}}x\in [sD_{y},b]} and h ( x ) {\displaystyle h(x)} should have the same sign of h ( 0 ) {\displaystyle h(0)} during this region. Let us choose m ( x ) {\displaystyle m(x)} as the tangent across the point at x = s D y {\displaystyle x=sD_{y}} , i.e.

m ( s y ) = g ( s D y ) − − g ( 0 ) + g ′ ( s D y ) ( s y − − s D y ) {\displaystyle m(sy)=g(sD_{y})-g(0)+g'(sD_{y})\left(sy-sD_{y}\right)} which is shown in the figure m ( x ) {\displaystyle m(x)} is the tangent lines across the point at x = s D y {\displaystyle x=sD_{y}} .

From this figure you can find that when s {\displaystyle s} or D y {\displaystyle D_{y}} gets smaller, the region satisfies the above inequality will get larger. Therefore, if we want to find a suitable m ( x ) {\displaystyle m(x)} to cover the whole f ( x ) {\displaystyle f(x)} during the interval of ( b 1 ) {\displaystyle (b_{1})} , D y {\displaystyle D_{y}} will have an upper limit. Besides, because the integration of e − − α α x {\displaystyle e^{-\alpha x}} is simple, let me use it to estimate the relative error contributed by this ( b 1 ) {\displaystyle (b_{1})} .

Based on Taylor expansion, we can get M [ g ( s D y ) − − g ( 0 ) ] = M [ g ″ ( 0 ) 2 s 2 D y 2 + g ‴ ( ξ ξ ) 6 s 3 D y 3 ] as ξ ξ ∈ ∈ [ 0 , s D y ] = − − π π D y 2 + ( 2 π π ) 3 / 2 g ‴ ( ξ ξ ) D y 3 6 M | g ″ ( 0 ) | 3 2 , {\displaystyle {\begin{aligned}M\left[g(sD_{y})-g(0)\right]&=M\left[{\frac {g''(0)}{2}}s^{2}D_{y}^{2}+{\frac {g'''(\xi )}{6}}s^{3}D_{y}^{3}\right]&&{\text{as }}\xi \in [0,sD_{y}]\\&=-\pi D_{y}^{2}+{\frac {(2\pi )^{3/2}g'''(\xi )D_{y}^{3}}{6{\sqrt {M}}|g''(0)|^{\frac {3}{2}}}},\end{aligned}}} and M s g ′ ( s D y ) = M s ( g ″ ( 0 ) s D y + g ‴ ( ζ ζ ) 2 s 2 D y 2 ) as ζ ζ ∈ ∈ [ 0 , s D y ] = − − 2 π π D y + 2 M ( π π | g ″ ( 0 ) | ) 3 2 g ‴ ( ζ ζ ) D y 2 , {\displaystyle {\begin{aligned}Msg'(sD_{y})&=Ms\left(g''(0)sD_{y}+{\frac {g'''(\zeta )}{2}}s^{2}D_{y}^{2}\right)&&{\text{as }}\zeta \in [0,sD_{y}]\\&=-2\pi D_{y}+{\sqrt {\frac {2}{M}}}\left({\frac {\pi }{|g''(0)|}}\right)^{\frac {3}{2}}g'''(\zeta )D_{y}^{2},\end{aligned}}} and then substitute them back into the calculation of ( b 1 ) {\displaystyle (b_{1})} ; however, you can find that the remainders of these two expansions are both inversely proportional to the square root of M {\displaystyle M} , let me drop them out to beautify the calculation. Keeping them is better, but it will make the formula uglier.

( b 1 ) ≤ ≤ | [ h ( s y ) h ( 0 ) ] max e − − π π D y 2 ∫ ∫ 0 b / s − − D y e − − 2 π π D y y d y | ≤ ≤ | [ h ( s y ) h ( 0 ) ] max e − − π π D y 2 1 2 π π D y | .

{\displaystyle {\begin{aligned}(b_{1})&\leq \left|\left[{\frac {h(sy)}{h(0)}}\right]_{\max }e^{-\pi D_{y}^{2}}\int _{0}^{b/s-D_{y}}e^{-2\pi D_{y}y}dy\right|\\&\leq \left|\left[{\frac {h(sy)}{h(0)}}\right]_{\max }e^{-\pi D_{y}^{2}}{\frac {1}{2\pi D_{y}}}\right|.\end{aligned}}} Therefore, it will tend to zero when D y {\displaystyle D_{y}} gets larger, but don't forget that the upper bound of D y {\displaystyle D_{y}} should be considered during this calculation.

About the integration near x = 0 {\displaystyle x=0} , we can also use Taylor's Theorem to calculate it. When h ′ ( 0 ) ≠ ≠ 0 {\displaystyle h'(0)\neq 0} ( c ) ≤ ≤ ∫ ∫ − − D y D y e − − π π y 2 | s h ′ ( ξ ξ ) h ( 0 ) y | d y < 2 π π M | g ″ ( 0 ) | | h ′ ( ξ ξ ) h ( 0 ) | max ( 1 − − e − − π π D y 2 ) {\displaystyle {\begin{aligned}(c)&\leq \int _{-D_{y}}^{D_{y}}e^{-\pi y^{2}}\left|{\frac {sh'(\xi )}{h(0)}}y\right|\,dy\\&<{\sqrt {\frac {2}{\pi M|g''(0)|}}}\left|{\frac {h'(\xi )}{h(0)}}\right|_{\max }\left(1-e^{-\pi D_{y}^{2}}\right)\end{aligned}}} and you can find that it is inversely proportional to the square root of M {\displaystyle M} . In fact, ( c ) {\displaystyle (c)} will have the same behave when h ( x ) {\displaystyle h(x)} is a constant.

Conclusively, the integral near the stationary point will get smaller as M {\displaystyle {\sqrt {M}}} gets larger, and the rest parts will tend to zero as long as D y {\displaystyle D_{y}} is large enough; however, we need to remember that D y {\displaystyle D_{y}} has an upper limit which is decided by whether the function m ( x ) {\displaystyle m(x)} is always larger than g ( x ) − − g ( 0 ) {\displaystyle g(x)-g(0)} in the rest region. However, as long as we can find one m ( x ) {\displaystyle m(x)} satisfying this condition, the upper bound of D y {\displaystyle D_{y}} can be chosen as directly proportional to M {\displaystyle {\sqrt {M}}} since m ( x ) {\displaystyle m(x)} is a tangent across the point of g ( x ) − − g ( 0 ) {\displaystyle g(x)-g(0)} at x = s D y {\displaystyle x=sD_{y}} . So, the bigger M {\displaystyle M} is, the bigger D y {\displaystyle D_{y}} can be.

In the multivariate case, where x {\displaystyle \mathbf {x} } is a d {\displaystyle d} -dimensional vector and f ( x ) {\displaystyle f(\mathbf {x} )} is a scalar function of x {\displaystyle \mathbf {x} } , Laplace's approximation is usually written as: ∫ ∫ h ( x ) e M f ( x ) d d x ≈ ≈ ( 2 π π M ) d / 2 h ( x 0 ) e M f ( x 0 ) | − − H ( f ) ( x 0 ) | 1 / 2 as M → → ∞ ∞ {\displaystyle \int h(\mathbf {x} )e^{Mf(\mathbf {x} )}\,d^{d}x\approx \left({\frac {2\pi }{M}}\right)^{d/2}{\frac {h(\mathbf {x} _{0})e^{Mf(\mathbf {x} _{0})}}{\left|-H(f)(\mathbf {x} _{0})\right|^{1/2}}}{\text{ as }}M\to \infty } where H ( f ) ( x 0 ) {\displaystyle H(f)(\mathbf {x} _{0})} is the Hessian matrix of f {\displaystyle f} evaluated at x 0 {\displaystyle \mathbf {x} _{0}} and where | ⋅ ⋅ | {\displaystyle |\cdot |} denotes its matrix determinant . Analogously to the univariate case, the Hessian is required to be negative-definite .

[ 4 ] Steepest descent extension [ edit ] Main article: Method of steepest descent In extensions of Laplace's method, complex analysis , and in particular Cauchy's integral formula , is used to find a contour of steepest descent for an (asymptotically with large M ) equivalent integral, expressed as a line integral . In particular, if no point x 0 where the derivative of f {\displaystyle f} vanishes exists on the real line, it may be necessary to deform the integration contour to an optimal one, where the above analysis will be possible. Again, the main idea is to reduce, at least asymptotically, the calculation of the given integral to that of a simpler integral that can be explicitly evaluated. See the book of Erdelyi (1956) for a simple discussion (where the method is termed steepest descents ).

The appropriate formulation for the complex z -plane is ∫ ∫ a b e M f ( z ) d z ≈ ≈ 2 π π − − M f ″ ( z 0 ) e M f ( z 0 ) as M → → ∞ ∞ .

{\displaystyle \int _{a}^{b}e^{Mf(z)}\,dz\approx {\sqrt {\frac {2\pi }{-Mf''(z_{0})}}}e^{Mf(z_{0})}{\text{ as }}M\to \infty .} for a path passing through the saddle point at z 0 . Note the explicit appearance of a minus sign to indicate the direction of the second derivative: one must not take the modulus. Also note that if the integrand is meromorphic , one may have to add residues corresponding to poles traversed while deforming the contour (see for example section 3 of Okounkov's paper Symmetric functions and random partitions ).

Further generalizations [ edit ] An extension of the steepest descent method is the so-called nonlinear stationary phase/steepest descent method . Here, instead of integrals, one needs to evaluate asymptotically solutions of Riemann–Hilbert factorization problems .

Given a contour C in the complex sphere , a function f {\displaystyle f} defined on that contour and a special point, such as infinity, a holomorphic function M is sought  away from C , with prescribed jump across C , and with a given normalization at infinity. If f {\displaystyle f} and hence M are matrices rather than scalars this is a problem that in general does not admit an explicit solution.

An asymptotic evaluation is then possible along the lines of the linear stationary phase/steepest descent method. The idea is to reduce asymptotically the solution of the given Riemann–Hilbert problem to that of a simpler, explicitly solvable, Riemann–Hilbert problem. Cauchy's theorem is used to justify deformations of the jump contour.

The nonlinear stationary phase was introduced by Deift and Zhou in 1993, based on earlier work of Its. A (properly speaking) nonlinear steepest descent method was introduced by Kamvissis, K. McLaughlin and P. Miller in 2003, based on previous work of Lax, Levermore, Deift, Venakides and Zhou. As in the linear case, "steepest descent contours" solve a min-max problem. In the nonlinear case they turn out to be "S-curves" (defined in a different context back in the 80s by Stahl, Gonchar and Rakhmanov).

The nonlinear stationary phase/steepest descent method has applications to the theory of soliton equations and integrable models , random matrices and combinatorics .

Median-point approximation generalization [ edit ] In the generalization, evaluation of the integral is considered equivalent to finding the norm of the distribution with density e M f ( x ) .

{\displaystyle e^{Mf(x)}.} Denoting the cumulative distribution F ( x ) {\displaystyle F(x)} , if there is a diffeomorphic Gaussian distribution with density e − − g − − γ γ 2 y 2 {\displaystyle e^{-g-{\frac {\gamma }{2}}y^{2}}} the norm is given by 2 π π γ γ − − 1 e − − g {\displaystyle {\sqrt {2\pi \gamma ^{-1}}}e^{-g}} and the corresponding diffeomorphism is y ( x ) = 1 γ γ Φ Φ − − 1 ( F ( x ) F ( ∞ ∞ ) ) , {\displaystyle y(x)={\frac {1}{\sqrt {\gamma }}}\Phi ^{-1}{\left({\frac {F(x)}{F(\infty )}}\right)},} where Φ Φ {\displaystyle \Phi } denotes cumulative standard normal distribution function.

In general, any distribution diffeomorphic to the Gaussian distribution has density e − − g − − γ γ 2 y 2 ( x ) y ′ ( x ) {\displaystyle e^{-g-{\frac {\gamma }{2}}y^{2}(x)}y'(x)} and the median -point is mapped to the median of the Gaussian distribution. Matching the logarithm of the density functions and their derivatives at the median point up to a given order yields a system of equations that determine the approximate values of γ γ {\displaystyle \gamma } and g {\displaystyle g} .

The approximation was introduced in 2019 by D. Makogon and C. Morais Smith primarily in the context of partition function evaluation for a system of interacting fermions.

[ 5 ] Complex integrals [ edit ] For complex integrals in the form: 1 2 π π i ∫ ∫ c − − i ∞ ∞ c + i ∞ ∞ g ( s ) e s t d s {\displaystyle {\frac {1}{2\pi i}}\int _{c-i\infty }^{c+i\infty }g(s)e^{st}\,ds} with t ≫ ≫ 1 , {\displaystyle t\gg 1,} we make the substitution t = iu and the change of variable s = c + i x {\displaystyle s=c+ix} to get the bilateral Laplace transform: 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ g ( c + i x ) e − − u x e i c u d x .

{\displaystyle {\frac {1}{2\pi }}\int _{-\infty }^{\infty }g(c+ix)e^{-ux}e^{icu}\,dx.} We then split g ( c + ix ) in its real and complex part, after which we recover u = t / i . This is useful for inverse Laplace transforms , the Perron formula and complex integration.

Example: Stirling's approximation [ edit ] Laplace's method can be used to derive Stirling's approximation N !

≈ ≈ 2 π π N ( N e ) N {\displaystyle N!\approx {\sqrt {2\pi N}}\left({\frac {N}{e}}\right)^{N}\,} for a large integer N . From the definition of the Gamma function , we have N !

= Γ Γ ( N + 1 ) = ∫ ∫ 0 ∞ ∞ e − − x x N d x .

{\displaystyle N!=\Gamma (N+1)=\int _{0}^{\infty }e^{-x}x^{N}\,dx.} Now we change variables, letting x = N z {\displaystyle x=Nz} so that d x = N d z .

{\displaystyle dx=Ndz.} Plug these values back in to obtain N !

= ∫ ∫ 0 ∞ ∞ e − − N z ( N z ) N N d z = N N + 1 ∫ ∫ 0 ∞ ∞ e − − N z z N d z = N N + 1 ∫ ∫ 0 ∞ ∞ e − − N z e N ln ⁡ ⁡ z d z = N N + 1 ∫ ∫ 0 ∞ ∞ e N ( ln ⁡ ⁡ z − − z ) d z .

{\displaystyle {\begin{aligned}N!&=\int _{0}^{\infty }e^{-Nz}(Nz)^{N}N\,dz\\&=N^{N+1}\int _{0}^{\infty }e^{-Nz}z^{N}\,dz\\&=N^{N+1}\int _{0}^{\infty }e^{-Nz}e^{N\ln z}\,dz\\&=N^{N+1}\int _{0}^{\infty }e^{N(\ln z-z)}\,dz.\end{aligned}}} This integral has the form necessary for Laplace's method with f ( z ) = ln ⁡ ⁡ z − − z {\displaystyle f(z)=\ln {z}-z} which is twice-differentiable: f ′ ( z ) = 1 z − − 1 , {\displaystyle f'(z)={\frac {1}{z}}-1,} f ″ ( z ) = − − 1 z 2 .

{\displaystyle f''(z)=-{\frac {1}{z^{2}}}.} The maximum of f ( z ) {\displaystyle f(z)} lies at z 0 = 1, and the second derivative of f ( z ) {\displaystyle f(z)} has the value −1 at this point. Therefore, we obtain N !

≈ ≈ N N + 1 2 π π N e − − N = 2 π π N N N e − − N .

{\displaystyle N!\approx N^{N+1}{\sqrt {\frac {2\pi }{N}}}e^{-N}={\sqrt {2\pi N}}N^{N}e^{-N}.} See also [ edit ] Mathematics portal Method of stationary phase Method of steepest descent Large deviations theory Laplace principle (large deviations theory) Laplace's approximation Notes [ edit ] ^ Tierney, Luke; Kadane, Joseph B. (1986). "Accurate Approximations for Posterior Moments and Marginal Densities".

J. Amer. Statist. Assoc .

81 (393): 82– 86.

doi : 10.1080/01621459.1986.10478240 .

^ Amaral Turkman, M. Antónia; Paulino, Carlos Daniel; Müller, Peter (2019). "Methods Based on Analytic Approximations".

Computational Bayesian Statistics: An Introduction . Cambridge University Press. pp.

150– 171.

ISBN 978-1-108-70374-1 .

^ Butler, Ronald W (2007).

Saddlepoint approximations and applications . Cambridge University Press.

ISBN 978-0-521-87250-8 .

^ MacKay, David J. C. (September 2003).

Information Theory, Inference and Learning Algorithms . Cambridge: Cambridge University Press.

ISBN 9780521642989 .

^ Makogon, D.; Morais Smith, C. (2022-05-03).

"Median-point approximation and its application for the study of fermionic systems" .

Physical Review B .

105 (17): 174505.

arXiv : 1909.12553 .

Bibcode : 2022PhRvB.105q4505M .

doi : 10.1103/PhysRevB.105.174505 .

hdl : 1874/423769 .

S2CID 203591796 .

References [ edit ] Azevedo-Filho, A.; Shachter, R. (1994), "Laplace's Method Approximations for Probabilistic Inference in Belief Networks with Continuous Variables", in Mantaras, R.; Poole, D. (eds.), Uncertainty in Artificial Intelligence , San Francisco, CA: Morgan Kaufmann , CiteSeerX 10.1.1.91.2064 .

Deift, P.; Zhou, X. (1993), "A steepest descent method for oscillatory Riemann–Hilbert problems. Asymptotics for the MKdV equation", Ann. of Math.

, vol. 137, no. 2, pp.

295– 368, arXiv : math/9201261 , doi : 10.2307/2946540 , JSTOR 2946540 .

Erdelyi, A. (1956), Asymptotic Expansions , Dover .

Fog, A. (2008), "Calculation Methods for Wallenius' Noncentral Hypergeometric Distribution", Communications in Statistics, Simulation and Computation , vol. 37, no. 2, pp.

258– 273, doi : 10.1080/03610910701790269 , S2CID 9040568 .

Laplace, P S (1774), "Mémoires de Mathématique et de Physique, Tome Sixième" [Memoir on the probability of causes of events.], Statistical Science , 1 (3): 366– 367, JSTOR 2245476 Wang, Xiang-Sheng; Wong, Roderick (2007). "Discrete analogues of Laplace's approximation".

Asymptot. Anal .

54 ( 3– 4): 165– 180.

This article incorporates material from saddle point approximation on PlanetMath , which is licensed under the Creative Commons Attribution/Share-Alike License .

v t e Integrals Types of integrals Riemann integral Lebesgue integral Burkill integral Bochner integral Daniell integral Darboux integral Henstock–Kurzweil integral Haar integral Hellinger integral Khinchin integral Kolmogorov integral Lebesgue–Stieltjes integral Pettis integral Pfeffer integral Riemann–Stieltjes integral Regulated integral Integration techniques Substitution Trigonometric Euler Weierstrass By parts Partial fractions Euler's formula Inverse functions Changing order Reduction formulas Parametric derivatives Differentiation under the integral sign Laplace transform Contour integration Laplace's method Numerical integration Simpson's rule Trapezoidal rule Risch algorithm Improper integrals Gaussian integral Dirichlet integral Fermi–Dirac integral complete incomplete Bose–Einstein integral Frullani integral Common integrals in quantum field theory Stochastic integrals Itô integral Russo–Vallois integral Stratonovich integral Skorokhod integral Miscellaneous Basel problem Euler–Maclaurin formula Gabriel's horn Integration Bee Proof that 22/7 exceeds π Volumes Washers Shells NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐pksgh
Cached time: 20250812035138
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.642 seconds
Real time usage: 0.825 seconds
Preprocessor visited node count: 2654/1000000
Revision size: 32769/2097152 bytes
Post‐expand include size: 42128/2097152 bytes
Template argument size: 1166/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 56236/5000000 bytes
Lua time usage: 0.295/10.000 seconds
Lua memory usage: 7050789/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  476.739      1 -total
 25.96%  123.753      1 Template:Reflist
 20.10%   95.839      1 Template:Short_description
 19.58%   93.369      3 Template:Cite_journal
 14.80%   70.575      1 Template:Integrals
 14.43%   68.806      1 Template:Navbox
 14.24%   67.910      2 Template:Pagetype
  8.21%   39.122      1 Template:Cleanup_tone
  6.84%   32.606      2 Template:Harvtxt
  6.65%   31.723      5 Template:Citation Saved in parser cache with key enwiki:pcache:642006:|#|:idhash:canonical and timestamp 20250812035138 and revision id 1296238852. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Laplace%27s_method&oldid=1296238852 " Categories : Asymptotic analysis Perturbation theory Integral calculus Hidden categories: Articles with short description Short description is different from Wikidata Wikipedia articles with style issues from March 2024 All articles with style issues Wikipedia articles incorporating text from PlanetMath This page was last edited on 18 June 2025, at 19:25 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Laplace's method 11 languages Add topic

