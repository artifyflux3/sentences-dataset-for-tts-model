Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Probability density function 1.2 Support 1.3 Special cases 2 Properties Toggle Properties subsection 2.1 Moments 2.2 Mode 2.3 Marginal distributions 2.4 Conjugate to categorical or multinomial 2.5 Relation to Dirichlet-multinomial distribution 2.6 Entropy 2.7 Kullback–Leibler divergence 2.8 Aggregation 2.9 Neutrality 2.10 Characteristic function 2.11 Inequality 3 Related distributions Toggle Related distributions subsection 3.1 Conjugate prior of the Dirichlet distribution 3.2 Generalization by scaling and translation of log-probabilities 3.2.1 Application 4 Occurrence and applications Toggle Occurrence and applications subsection 4.1 Bayesian models 4.2 Intuitive interpretations of the parameters 4.2.1 The concentration parameter 4.2.2 String cutting 4.2.3 Pólya's urn 5 Random variate generation Toggle Random variate generation subsection 5.1 From gamma distribution 5.2 From marginal beta distributions 5.3 When each alpha is 1 5.4 When each alpha is 1/2 and relationship to the hypersphere 6 See also 7 References 8 External links Toggle the table of contents Dirichlet distribution 16 languages Беларуская Català Deutsch Español فارسی Français 한국어 Italiano עברית Nederlands 日本語 Polski Português Русский Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Dirichlet distribution Probability density function Parameters K ≥ ≥ 2 {\displaystyle K\geq 2} number of categories ( integer ) α α = ( α α 1 , … … , α α K ) {\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{K})} concentration parameters , where α α i > 0 {\displaystyle \alpha _{i}>0} Support x 1 , … … , x K {\displaystyle x_{1},\ldots ,x_{K}} where x i ∈ ∈ [ 0 , 1 ] {\displaystyle x_{i}\in [0,1]} and ∑ ∑ i = 1 K x i = 1 {\displaystyle \sum _{i=1}^{K}x_{i}=1} (i.e. a K − − 1 {\displaystyle K-1} simplex ) PDF 1 B ( α α ) ∏ ∏ i = 1 K x i α α i − − 1 {\displaystyle {\frac {1}{\mathrm {B} ({\boldsymbol {\alpha }})}}\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}} where B ( α α ) = ∏ ∏ i = 1 K Γ Γ ( α α i ) Γ Γ ( α α 0 ) {\displaystyle \mathrm {B} ({\boldsymbol {\alpha }})={\frac {\prod _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma {\bigl (}\alpha _{0}{\bigr )}}}} where α α 0 = ∑ ∑ i = 1 K α α i {\displaystyle \alpha _{0}=\sum _{i=1}^{K}\alpha _{i}} Mean E ⁡ ⁡ [ X i ] = α α i α α 0 {\displaystyle \operatorname {E} [X_{i}]={\frac {\alpha _{i}}{\alpha _{0}}}} E ⁡ ⁡ [ ln ⁡ ⁡ X i ] = ψ ψ ( α α i ) − − ψ ψ ( α α 0 ) {\displaystyle \operatorname {E} [\ln X_{i}]=\psi (\alpha _{i})-\psi (\alpha _{0})} (where ψ ψ {\displaystyle \psi } is the digamma function ) Mode x i = α α i − − 1 α α 0 − − K , α α i > 1.

{\displaystyle x_{i}={\frac {\alpha _{i}-1}{\alpha _{0}-K}},\quad \alpha _{i}>1.} Variance Var ⁡ ⁡ [ X i ] = α α ~ ~ i ( 1 − − α α ~ ~ i ) α α 0 + 1 , {\displaystyle \operatorname {Var} [X_{i}]={\frac {{\tilde {\alpha }}_{i}(1-{\tilde {\alpha }}_{i})}{\alpha _{0}+1}},} Cov ⁡ ⁡ [ X i , X j ] = δ δ i j α α ~ ~ i − − α α ~ ~ i α α ~ ~ j α α 0 + 1 {\displaystyle \operatorname {Cov} [X_{i},X_{j}]={\frac {\delta _{ij}\,{\tilde {\alpha }}_{i}-{\tilde {\alpha }}_{i}{\tilde {\alpha }}_{j}}{\alpha _{0}+1}}} where α α ~ ~ i = α α i α α 0 {\displaystyle {\tilde {\alpha }}_{i}={\frac {\alpha _{i}}{\alpha _{0}}}} , and δ δ i j {\displaystyle \delta _{ij}} is the Kronecker delta Entropy H ( X ) = log ⁡ ⁡ B ( α α ) {\displaystyle H(X)=\log \mathrm {B} ({\boldsymbol {\alpha }})} + ( α α 0 − − K ) ψ ψ ( α α 0 ) − − {\displaystyle +(\alpha _{0}-K)\psi (\alpha _{0})-} ∑ ∑ j = 1 K ( α α j − − 1 ) ψ ψ ( α α j ) {\displaystyle \sum _{j=1}^{K}(\alpha _{j}-1)\psi (\alpha _{j})} with α α 0 {\displaystyle \alpha _{0}} defined as for variance, above; and ψ ψ {\displaystyle \psi } is the digamma function Method of moments α α i = E [ X i ] ( E [ X j ] ( 1 − − E [ X j ] ) V [ X j ] − − 1 ) {\displaystyle \alpha _{i}=E[X_{i}]\left({\frac {E[X_{j}](1-E[X_{j}])}{V[X_{j}]}}-1\right)} where j is any index, possibly i itself In probability and statistics , the Dirichlet distribution (after Peter Gustav Lejeune Dirichlet ), often denoted Dir ⁡ ⁡ ( α α ) {\displaystyle \operatorname {Dir} ({\boldsymbol {\alpha }})} , is a family of continuous multivariate probability distributions parameterized by a vector α of positive reals . It is a multivariate generalization of the beta distribution , [ 1 ] hence its alternative name of multivariate beta distribution ( MBD ).

[ 2 ] Dirichlet distributions are commonly used as prior distributions in Bayesian statistics , and in fact, the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution .

The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process .

Definitions [ edit ] Probability density function [ edit ] Illustrating how the log of the density function changes when K = 3 {\displaystyle K=3} as we change the vector α α {\displaystyle {\boldsymbol {\alpha }}} from α α = ( 0.3 , 0.3 , 0.3 ) {\displaystyle {\boldsymbol {\alpha }}=(0.3,0.3,0.3)} to ( 2.0 , 2.0 , 2.0 ) {\displaystyle (2.0,2.0,2.0)} , keeping all the individual α α i {\displaystyle \alpha _{i}} 's equal to each other.

The Dirichlet distribution of order K ≥ ≥ 2 {\displaystyle K\geq 2} with parameters α α 1 , … … , α α K > 0 {\displaystyle \alpha _{1},\ldots ,\alpha _{K}>0} has a probability density function with respect to Lebesgue measure on the Euclidean space R K − − 1 {\displaystyle \mathbb {R} ^{K-1}} given by f ( x 1 , … … , x K ; α α 1 , … … , α α K ) = 1 B ( α α ) ∏ ∏ i = 1 K x i α α i − − 1 {\displaystyle f\left(x_{1},\ldots ,x_{K};\alpha _{1},\ldots ,\alpha _{K}\right)={\frac {1}{\mathrm {B} ({\boldsymbol {\alpha }})}}\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}} where { x k } k = 1 k = K {\displaystyle \{x_{k}\}_{k=1}^{k=K}} belong to the standard K − − 1 {\displaystyle K-1} simplex , or in other words: ∑ ∑ i = 1 K x i = 1 and x i ∈ ∈ [ 0 , 1 ] for all i ∈ ∈ { 1 , … … , K } .

{\displaystyle \sum _{i=1}^{K}x_{i}=1{\mbox{ and }}x_{i}\in \left[0,1\right]{\mbox{ for all }}i\in \{1,\dots ,K\}\,.} The normalizing constant is the multivariate beta function , which can be expressed in terms of the gamma function : B ( α α ) = ∏ ∏ i = 1 K Γ Γ ( α α i ) Γ Γ ( ∑ ∑ i = 1 K α α i ) , α α = ( α α 1 , … … , α α K ) .

{\displaystyle \mathrm {B} ({\boldsymbol {\alpha }})={\frac {\prod \limits _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma \left(\sum \limits _{i=1}^{K}\alpha _{i}\right)}},\qquad {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{K}).} Support [ edit ] The support of the Dirichlet distribution is the set of K -dimensional vectors x whose entries are real numbers in the interval [0,1] such that ‖ ‖ x ‖ ‖ 1 = 1 {\displaystyle \|{\boldsymbol {x}}\|_{1}=1} , i.e. the sum of the coordinates is equal to 1.  These can be viewed as the probabilities of a K -way categorical event. Another way to express this is that the domain of the Dirichlet distribution is itself a set of probability distributions , specifically the set of K -dimensional discrete distributions . The technical term for the set of points in the support of a K -dimensional Dirichlet distribution is the open standard ( K − 1) -simplex , [ 3 ] which is a generalization of a triangle , embedded in the next-higher dimension.  For example, with K = 3 , the support is an equilateral triangle embedded in a downward-angle fashion in three-dimensional space, with vertices at (1,0,0), (0,1,0) and (0,0,1), i.e. touching each of the coordinate axes at a point 1 unit away from the origin.

Special cases [ edit ] A common special case is the symmetric Dirichlet distribution , where all of the elements making up the parameter vector α have the same value.  The symmetric case might be useful, for example, when a Dirichlet prior over components is called for, but there is no prior knowledge favoring one component over another.  Since all elements of the parameter vector have the same value, the symmetric Dirichlet distribution can be parametrized by a single scalar value α , called the concentration parameter . In terms of α , the density function has the form f ( x 1 , … … , x K ; α α ) = Γ Γ ( α α K ) Γ Γ ( α α ) K ∏ ∏ i = 1 K x i α α − − 1 .

{\displaystyle f(x_{1},\dots ,x_{K};\alpha )={\frac {\Gamma (\alpha K)}{\Gamma (\alpha )^{K}}}\prod _{i=1}^{K}x_{i}^{\alpha -1}.} When α = 1 , [1] the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard ( K −1) -simplex , i.e. it is uniform over all points in its support . This particular distribution is known as the flat Dirichlet distribution . Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other.  Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.

When α = 1/2 , the distribution is the same as would be obtained by choosing a point uniformly at random from the surface of a ( K −1) -dimensional unit hypersphere and squaring each coordinate.  The α = 1/2 distribution is the Jeffreys prior for the Dirichlet distribution.

More generally, the parameter vector is sometimes written as the product α α n {\displaystyle \alpha {\boldsymbol {n}}} of a ( scalar ) concentration parameter α and a ( vector ) base measure n = ( n 1 , … … , n K ) {\displaystyle {\boldsymbol {n}}=(n_{1},\dots ,n_{K})} where n lies within the ( K − 1) -simplex (i.e.: its coordinates n i {\displaystyle n_{i}} sum to one). The concentration parameter in this case is larger by a factor of K than the concentration parameter for a symmetric Dirichlet distribution described above. This construction ties in with concept of a base measure when discussing Dirichlet processes and is often used in the topic modelling literature.

^ If we define the concentration parameter as the sum of the Dirichlet parameters for each dimension, the Dirichlet distribution with concentration parameter K , the dimension of the distribution, is the uniform distribution on the ( K − 1) -simplex.

Properties [ edit ] Moments [ edit ] Let X = ( X 1 , … … , X K ) ∼ ∼ Dir ⁡ ⁡ ( α α ) {\displaystyle X=(X_{1},\ldots ,X_{K})\sim \operatorname {Dir} ({\boldsymbol {\alpha }})} .

Let α α 0 = ∑ ∑ i = 1 K α α i .

{\displaystyle \alpha _{0}=\sum _{i=1}^{K}\alpha _{i}.} Then [ 4 ] [ 5 ] E ⁡ ⁡ [ X i ] = α α i α α 0 , {\displaystyle \operatorname {E} [X_{i}]={\frac {\alpha _{i}}{\alpha _{0}}},} Var ⁡ ⁡ [ X i ] = α α i ( α α 0 − − α α i ) α α 0 2 ( α α 0 + 1 ) .

{\displaystyle \operatorname {Var} [X_{i}]={\frac {\alpha _{i}(\alpha _{0}-\alpha _{i})}{\alpha _{0}^{2}(\alpha _{0}+1)}}.} Furthermore, if i ≠ ≠ j {\displaystyle i\neq j} Cov ⁡ ⁡ [ X i , X j ] = − − α α i α α j α α 0 2 ( α α 0 + 1 ) .

{\displaystyle \operatorname {Cov} [X_{i},X_{j}]={\frac {-\alpha _{i}\alpha _{j}}{\alpha _{0}^{2}(\alpha _{0}+1)}}.} The covariance matrix is singular .

More generally, moments of Dirichlet-distributed random variables can be expressed in the following way. For t = ( t 1 , … … , t K ) ∈ ∈ R K {\displaystyle {\boldsymbol {t}}=(t_{1},\dotsc ,t_{K})\in \mathbb {R} ^{K}} , denote by t ∘ ∘ i = ( t 1 i , … … , t K i ) {\displaystyle {\boldsymbol {t}}^{\circ i}=(t_{1}^{i},\dotsc ,t_{K}^{i})} its i -th Hadamard power . Then, [ 6 ] E ⁡ ⁡ [ ( t ⋅ ⋅ X ) n ] = n !

Γ Γ ( α α 0 ) Γ Γ ( α α 0 + n ) ∑ ∑ t 1 k 1 ⋯ ⋯ t K k K k 1 !

⋯ ⋯ k K !

∏ ∏ i = 1 K Γ Γ ( α α i + k i ) Γ Γ ( α α i ) = n !

Γ Γ ( α α 0 ) Γ Γ ( α α 0 + n ) Z n ( t ∘ ∘ 1 ⋅ ⋅ α α , ⋯ ⋯ , t ∘ ∘ n ⋅ ⋅ α α ) , {\displaystyle \operatorname {E} \left[({\boldsymbol {t}}\cdot {\boldsymbol {X}})^{n}\right]={\frac {n!\,\Gamma (\alpha _{0})}{\Gamma (\alpha _{0}+n)}}\sum {\frac {{t_{1}}^{k_{1}}\cdots {t_{K}}^{k_{K}}}{k_{1}!\cdots k_{K}!}}\prod _{i=1}^{K}{\frac {\Gamma (\alpha _{i}+k_{i})}{\Gamma (\alpha _{i})}}={\frac {n!\,\Gamma (\alpha _{0})}{\Gamma (\alpha _{0}+n)}}Z_{n}({\boldsymbol {t}}^{\circ 1}\cdot {\boldsymbol {\alpha }},\cdots ,{\boldsymbol {t}}^{\circ n}\cdot {\boldsymbol {\alpha }}),} where the sum is over non-negative integers k 1 , … … , k K {\displaystyle k_{1},\ldots ,k_{K}} with n = k 1 + ⋯ ⋯ + k K {\displaystyle n=k_{1}+\cdots +k_{K}} , and Z n {\displaystyle Z_{n}} is the cycle index polynomial of the Symmetric group of degree n .

We have the special case E ⁡ ⁡ [ t ⋅ ⋅ X ] = t ⋅ ⋅ α α α α 0 .

{\displaystyle \operatorname {E} \left[{\boldsymbol {t}}\cdot {\boldsymbol {X}}\right]={\frac {{\boldsymbol {t}}\cdot {\boldsymbol {\alpha }}}{\alpha _{0}}}.} The multivariate analogue E ⁡ ⁡ [ ( t 1 ⋅ ⋅ X ) n 1 ⋯ ⋯ ( t q ⋅ ⋅ X ) n q ] {\textstyle \operatorname {E} \left[({\boldsymbol {t}}_{1}\cdot {\boldsymbol {X}})^{n_{1}}\cdots ({\boldsymbol {t}}_{q}\cdot {\boldsymbol {X}})^{n_{q}}\right]} for vectors t 1 , … … , t q ∈ ∈ R K {\displaystyle {\boldsymbol {t}}_{1},\dotsc ,{\boldsymbol {t}}_{q}\in \mathbb {R} ^{K}} can be expressed [ 7 ] in terms of a color pattern of the exponents n 1 , … … , n q {\displaystyle n_{1},\dotsc ,n_{q}} in the sense of Pólya enumeration theorem .

Particular cases include the simple computation [ 8 ] E ⁡ ⁡ [ ∏ ∏ i = 1 K X i β β i ] = B ( α α + β β ) B ( α α ) = Γ Γ ( ∑ ∑ i = 1 K α α i ) Γ Γ [ ∑ ∑ i = 1 K ( α α i + β β i ) ] × × ∏ ∏ i = 1 K Γ Γ ( α α i + β β i ) Γ Γ ( α α i ) .

{\displaystyle \operatorname {E} \left[\prod _{i=1}^{K}X_{i}^{\beta _{i}}\right]={\frac {B\left({\boldsymbol {\alpha }}+{\boldsymbol {\beta }}\right)}{B\left({\boldsymbol {\alpha }}\right)}}={\frac {\Gamma \left(\sum \limits _{i=1}^{K}\alpha _{i}\right)}{\Gamma \left[\sum \limits _{i=1}^{K}(\alpha _{i}+\beta _{i})\right]}}\times \prod _{i=1}^{K}{\frac {\Gamma (\alpha _{i}+\beta _{i})}{\Gamma (\alpha _{i})}}.} Mode [ edit ] The mode of the distribution is [ 9 ] the vector ( x 1 , ..., x K ) with x i = α α i − − 1 α α 0 − − K , α α i > 1.

{\displaystyle x_{i}={\frac {\alpha _{i}-1}{\alpha _{0}-K}},\qquad \alpha _{i}>1.} Marginal distributions [ edit ] The marginal distributions are beta distributions : [ 10 ] X i ∼ ∼ Beta ⁡ ⁡ ( α α i , α α 0 − − α α i ) .

{\displaystyle X_{i}\sim \operatorname {Beta} (\alpha _{i},\alpha _{0}-\alpha _{i}).} Also see § Related distributions below.

Conjugate to categorical or multinomial [ edit ] The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations).  This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet.  Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one.  This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties.

Formally, this can be expressed as follows.  Given a model α α = ( α α 1 , … … , α α K ) = concentration hyperparameter p ∣ ∣ α α = ( p 1 , … … , p K ) ∼ ∼ Dir ⁡ ⁡ ( K , α α ) X ∣ ∣ p = ( x 1 , … … , x K ) ∼ ∼ Cat ⁡ ⁡ ( K , p ) {\displaystyle {\begin{array}{rcccl}{\boldsymbol {\alpha }}&=&\left(\alpha _{1},\ldots ,\alpha _{K}\right)&=&{\text{concentration hyperparameter}}\\\mathbf {p} \mid {\boldsymbol {\alpha }}&=&\left(p_{1},\ldots ,p_{K}\right)&\sim &\operatorname {Dir} (K,{\boldsymbol {\alpha }})\\\mathbb {X} \mid \mathbf {p} &=&\left(\mathbf {x} _{1},\ldots ,\mathbf {x} _{K}\right)&\sim &\operatorname {Cat} (K,\mathbf {p} )\end{array}}} then the following holds: c = ( c 1 , … … , c K ) = number of occurrences of category i p ∣ ∣ X , α α ∼ ∼ Dir ⁡ ⁡ ( K , c + α α ) = Dir ⁡ ⁡ ( K , c 1 + α α 1 , … … , c K + α α K ) {\displaystyle {\begin{array}{rcccl}\mathbf {c} &=&\left(c_{1},\ldots ,c_{K}\right)&=&{\text{number of occurrences of category }}i\\\mathbf {p} \mid \mathbb {X} ,{\boldsymbol {\alpha }}&\sim &\operatorname {Dir} (K,\mathbf {c} +{\boldsymbol {\alpha }})&=&\operatorname {Dir} \left(K,c_{1}+\alpha _{1},\ldots ,c_{K}+\alpha _{K}\right)\end{array}}} This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector α as pseudocounts , i.e. as representing the number of observations in each category that we have already seen.  Then we simply add in the counts for all the new observations (the vector c ) in order to derive the posterior distribution.

In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models.  See the section on applications below for more information.

Relation to Dirichlet-multinomial distribution [ edit ] In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out ) is a Dirichlet-multinomial distribution .  This distribution plays an important role in hierarchical Bayesian models , because when doing inference over such models using methods such as Gibbs sampling or variational Bayes , Dirichlet prior distributions are often marginalized out.  See the article on this distribution for more details.

Entropy [ edit ] If X is a Dir ⁡ ⁡ ( α α ) {\displaystyle \operatorname {Dir} ({\boldsymbol {\alpha }})} random variable, the differential entropy of X (in nat units ) is [ 11 ] h ( X ) = E ⁡ ⁡ [ − − ln ⁡ ⁡ f ( X ) ] = ln ⁡ ⁡ B ⁡ ⁡ ( α α ) + ( α α 0 − − K ) ψ ψ ( α α 0 ) − − ∑ ∑ j = 1 K ( α α j − − 1 ) ψ ψ ( α α j ) {\displaystyle h({\boldsymbol {X}})=\operatorname {E} [-\ln f({\boldsymbol {X}})]=\ln \operatorname {B} ({\boldsymbol {\alpha }})+(\alpha _{0}-K)\psi (\alpha _{0})-\sum _{j=1}^{K}(\alpha _{j}-1)\psi (\alpha _{j})} where ψ ψ {\displaystyle \psi } is the digamma function .

The following formula for E ⁡ ⁡ [ ln ⁡ ⁡ ( X i ) ] {\displaystyle \operatorname {E} [\ln(X_{i})]} can be used to derive the differential entropy above. Since the functions ln ⁡ ⁡ ( X i ) {\displaystyle \ln(X_{i})} are the sufficient statistics of the Dirichlet distribution, the exponential family differential identities can be used to get an analytic expression for the expectation of ln ⁡ ⁡ ( X i ) {\displaystyle \ln(X_{i})} (see equation (2.62) in [ 12 ] ) and its associated covariance matrix: E ⁡ ⁡ [ ln ⁡ ⁡ ( X i ) ] = ψ ψ ( α α i ) − − ψ ψ ( α α 0 ) {\displaystyle \operatorname {E} [\ln(X_{i})]=\psi (\alpha _{i})-\psi (\alpha _{0})} and Cov ⁡ ⁡ [ ln ⁡ ⁡ ( X i ) , ln ⁡ ⁡ ( X j ) ] = ψ ψ ′ ( α α i ) δ δ i j − − ψ ψ ′ ( α α 0 ) {\displaystyle \operatorname {Cov} [\ln(X_{i}),\ln(X_{j})]=\psi '(\alpha _{i})\delta _{ij}-\psi '(\alpha _{0})} where ψ ψ {\displaystyle \psi } is the digamma function , ψ ψ ′ {\displaystyle \psi '} is the trigamma function , and δ δ i j {\displaystyle \delta _{ij}} is the Kronecker delta .

The spectrum of Rényi information for values other than λ λ = 1 {\displaystyle \lambda =1} is given by [ 13 ] F R ( λ λ ) = ( 1 − − λ λ ) − − 1 ( − − λ λ log ⁡ ⁡ B ( α α ) + ∑ ∑ i = 1 K log ⁡ ⁡ Γ Γ ( λ λ ( α α i − − 1 ) + 1 ) − − log ⁡ ⁡ Γ Γ ( λ λ ( α α 0 − − K ) + K ) ) {\displaystyle F_{R}(\lambda )=(1-\lambda )^{-1}\left(-\lambda \log \mathrm {B} ({\boldsymbol {\alpha }})+\sum _{i=1}^{K}\log \Gamma (\lambda (\alpha _{i}-1)+1)-\log \Gamma (\lambda (\alpha _{0}-K)+K)\right)} and the information entropy is the limit as λ λ {\displaystyle \lambda } goes to 1.

Another related interesting measure is the entropy of a discrete categorical (one-of-K binary) vector Z with probability-mass distribution X , i.e., P ( Z i = 1 , Z j ≠ ≠ i = 0 | X ) = X i {\displaystyle P(Z_{i}=1,Z_{j\neq i}=0|{\boldsymbol {X}})=X_{i}} . The conditional information entropy of Z , given X is S ( X ) = H ( Z | X ) = E Z ⁡ ⁡ [ − − log ⁡ ⁡ P ( Z | X ) ] = ∑ ∑ i = 1 K − − X i log ⁡ ⁡ X i {\displaystyle S({\boldsymbol {X}})=H({\boldsymbol {Z}}|{\boldsymbol {X}})=\operatorname {E} _{\boldsymbol {Z}}[-\log P({\boldsymbol {Z}}|{\boldsymbol {X}})]=\sum _{i=1}^{K}-X_{i}\log X_{i}} This function of X is a scalar random variable. If X has a symmetric Dirichlet distribution with all α α i = α α {\displaystyle \alpha _{i}=\alpha } , the expected value of the entropy (in nat units ) is [ 14 ] E ⁡ ⁡ [ S ( X ) ] = ∑ ∑ i = 1 K E ⁡ ⁡ [ − − X i ln ⁡ ⁡ X i ] = ψ ψ ( K α α + 1 ) − − ψ ψ ( α α + 1 ) {\displaystyle \operatorname {E} [S({\boldsymbol {X}})]=\sum _{i=1}^{K}\operatorname {E} [-X_{i}\ln X_{i}]=\psi (K\alpha +1)-\psi (\alpha +1)} Kullback–Leibler divergence [ edit ] The Kullback–Leibler (KL) divergence between two Dirichlet distributions, Dir ( α α ) {\displaystyle {\text{Dir}}({\boldsymbol {\alpha }})} and Dir ( β β ) {\displaystyle {\text{Dir}}({\boldsymbol {\beta }})} , over the same simplex is: [ 15 ] D K L ( D i r ( α α ) ‖ ‖ D i r ( β β ) ) = log ⁡ ⁡ Γ Γ ( ∑ ∑ i = 1 K α α i ) Γ Γ ( ∑ ∑ i = 1 K β β i ) + ∑ ∑ i = 1 K [ log ⁡ ⁡ Γ Γ ( β β i ) Γ Γ ( α α i ) + ( α α i − − β β i ) ( ψ ψ ( α α i ) − − ψ ψ ( ∑ ∑ j = 1 K α α j ) ) ] {\displaystyle {\begin{aligned}D_{\mathrm {KL} }{\big (}\mathrm {Dir} ({\boldsymbol {\alpha }})\,\|\,\mathrm {Dir} ({\boldsymbol {\beta }}){\big )}&=\log {\frac {\Gamma \left(\sum _{i=1}^{K}\alpha _{i}\right)}{\Gamma \left(\sum _{i=1}^{K}\beta _{i}\right)}}+\sum _{i=1}^{K}\left[\log {\frac {\Gamma (\beta _{i})}{\Gamma (\alpha _{i})}}+(\alpha _{i}-\beta _{i})\left(\psi (\alpha _{i})-\psi \left(\sum _{j=1}^{K}\alpha _{j}\right)\right)\right]\end{aligned}}} Aggregation [ edit ] If X = ( X 1 , … … , X K ) ∼ ∼ Dir ⁡ ⁡ ( α α 1 , … … , α α K ) {\displaystyle X=(X_{1},\ldots ,X_{K})\sim \operatorname {Dir} (\alpha _{1},\ldots ,\alpha _{K})} then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum, X ′ = ( X 1 , … … , X i + X j , … … , X K ) ∼ ∼ Dir ⁡ ⁡ ( α α 1 , … … , α α i + α α j , … … , α α K ) .

{\displaystyle X'=(X_{1},\ldots ,X_{i}+X_{j},\ldots ,X_{K})\sim \operatorname {Dir} (\alpha _{1},\ldots ,\alpha _{i}+\alpha _{j},\ldots ,\alpha _{K}).} This aggregation property may be used to derive the marginal distribution of X i {\displaystyle X_{i}} mentioned above.

Neutrality [ edit ] Main article: Neutral vector If X = ( X 1 , … … , X K ) ∼ ∼ Dir ⁡ ⁡ ( α α ) {\displaystyle X=(X_{1},\ldots ,X_{K})\sim \operatorname {Dir} ({\boldsymbol {\alpha }})} , then the vector X is said to be neutral [ 16 ] in the sense that X K is independent of X ( − − K ) {\displaystyle X^{(-K)}} [ 3 ] where X ( − − K ) = ( X 1 1 − − X K , X 2 1 − − X K , … … , X K − − 1 1 − − X K ) , {\displaystyle X^{(-K)}=\left({\frac {X_{1}}{1-X_{K}}},{\frac {X_{2}}{1-X_{K}}},\ldots ,{\frac {X_{K-1}}{1-X_{K}}}\right),} and similarly for removing any of X 2 , … … , X K − − 1 {\displaystyle X_{2},\ldots ,X_{K-1}} . Observe that any permutation of X is also neutral (a property not possessed by samples drawn from a generalized Dirichlet distribution ).

[ 17 ] Combining this with the property of aggregation it follows that X j + ... + X K is independent of ( X 1 X 1 + ⋯ ⋯ + X j − − 1 , X 2 X 1 + ⋯ ⋯ + X j − − 1 , … … , X j − − 1 X 1 + ⋯ ⋯ + X j − − 1 ) {\displaystyle \left({\frac {X_{1}}{X_{1}+\cdots +X_{j-1}}},{\frac {X_{2}}{X_{1}+\cdots +X_{j-1}}},\ldots ,{\frac {X_{j-1}}{X_{1}+\cdots +X_{j-1}}}\right)} . In fact it is true, further, for the Dirichlet distribution, that for 3 ≤ ≤ j ≤ ≤ K − − 1 {\displaystyle 3\leq j\leq K-1} , the pair ( X 1 + ⋯ ⋯ + X j − − 1 , X j + ⋯ ⋯ + X K ) {\displaystyle \left(X_{1}+\cdots +X_{j-1},X_{j}+\cdots +X_{K}\right)} , and the two vectors ( X 1 X 1 + ⋯ ⋯ + X j − − 1 , X 2 X 1 + ⋯ ⋯ + X j − − 1 , … … , X j − − 1 X 1 + ⋯ ⋯ + X j − − 1 ) {\displaystyle \left({\frac {X_{1}}{X_{1}+\cdots +X_{j-1}}},{\frac {X_{2}}{X_{1}+\cdots +X_{j-1}}},\ldots ,{\frac {X_{j-1}}{X_{1}+\cdots +X_{j-1}}}\right)} and ( X j X j + ⋯ ⋯ + X K , X j + 1 X j + ⋯ ⋯ + X K , … … , X K X j + ⋯ ⋯ + X K ) {\displaystyle \left({\frac {X_{j}}{X_{j}+\cdots +X_{K}}},{\frac {X_{j+1}}{X_{j}+\cdots +X_{K}}},\ldots ,{\frac {X_{K}}{X_{j}+\cdots +X_{K}}}\right)} , viewed as triple of normalised random vectors, are mutually independent . The analogous result is true for partition of the indices {1, 2, ..., K } into any other pair of non-singleton subsets.

Characteristic function [ edit ] The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series .  It is given by Phillips as [ 18 ] C F ( s 1 , … … , s K − − 1 ) = E ⁡ ⁡ ( e i ( s 1 X 1 + ⋯ ⋯ + s K − − 1 X K − − 1 ) ) = Ψ Ψ [ K − − 1 ] ( α α 1 , … … , α α K − − 1 ; α α 0 ; i s 1 , … … , i s K − − 1 ) {\displaystyle CF\left(s_{1},\ldots ,s_{K-1}\right)=\operatorname {E} \left(e^{i\left(s_{1}X_{1}+\cdots +s_{K-1}X_{K-1}\right)}\right)=\Psi ^{\left[K-1\right]}(\alpha _{1},\ldots ,\alpha _{K-1};\alpha _{0};is_{1},\ldots ,is_{K-1})} where Ψ Ψ [ m ] ( a 1 , … … , a m ; c ; z 1 , … … z m ) = ∑ ∑ ( a 1 ) k 1 ⋯ ⋯ ( a m ) k m z 1 k 1 ⋯ ⋯ z m k m ( c ) k k 1 !

⋯ ⋯ k m !

.

{\displaystyle \Psi ^{[m]}(a_{1},\ldots ,a_{m};c;z_{1},\ldots z_{m})=\sum {\frac {(a_{1})_{k_{1}}\cdots (a_{m})_{k_{m}}\,z_{1}^{k_{1}}\cdots z_{m}^{k_{m}}}{(c)_{k}\,k_{1}!\cdots k_{m}!}}.} The sum is over non-negative integers k 1 , … … , k m {\displaystyle k_{1},\ldots ,k_{m}} and k = k 1 + ⋯ ⋯ + k m {\displaystyle k=k_{1}+\cdots +k_{m}} .  Phillips goes on to state that this form is "inconvenient for numerical calculation" and gives an alternative in terms of a complex path integral : Ψ Ψ [ m ] = Γ Γ ( c ) 2 π π i ∫ ∫ L e t t a 1 + ⋯ ⋯ + a m − − c ∏ ∏ j = 1 m ( t − − z j ) − − a j d t {\displaystyle \Psi ^{[m]}={\frac {\Gamma (c)}{2\pi i}}\int _{L}e^{t}\,t^{a_{1}+\cdots +a_{m}-c}\,\prod _{j=1}^{m}(t-z_{j})^{-a_{j}}\,dt} where L denotes any path in the complex plane originating at − − ∞ ∞ {\displaystyle -\infty } , encircling in the positive direction all the singularities of the integrand and returning to − − ∞ ∞ {\displaystyle -\infty } .

Inequality [ edit ] Probability density function f ( x 1 , … … , x K − − 1 ; α α 1 , … … , α α K ) {\displaystyle f\left(x_{1},\ldots ,x_{K-1};\alpha _{1},\ldots ,\alpha _{K}\right)} plays a key role in a multifunctional inequality which implies various bounds for the Dirichlet distribution.

[ 19 ] Another inequality relates the moment-generating function of the Dirichlet distribution to the convex conjugate of the scaled reversed Kullback-Leibler divergence: [ 20 ] log ⁡ ⁡ E ⁡ ⁡ ( exp ⁡ ⁡ ∑ ∑ i = 1 K s i X i ) ≤ ≤ sup p ∑ ∑ i = 1 K ( p i s i − − α α i log ⁡ ⁡ ( α α i α α 0 p i ) ) , {\displaystyle \log \operatorname {E} \left(\exp {\sum _{i=1}^{K}s_{i}X_{i}}\right)\leq \sup _{p}\sum _{i=1}^{K}\left(p_{i}s_{i}-\alpha _{i}\log \left({\frac {\alpha _{i}}{\alpha _{0}p_{i}}}\right)\right),} where the supremum is taken over p spanning the ( K − 1) -simplex.

Related distributions [ edit ] When X = ( X 1 , … … , X K ) ∼ ∼ Dir ⁡ ⁡ ( α α 1 , … … , α α K ) {\displaystyle {\boldsymbol {X}}=(X_{1},\ldots ,X_{K})\sim \operatorname {Dir} \left(\alpha _{1},\ldots ,\alpha _{K}\right)} , the marginal distribution of each component X i ∼ ∼ Beta ⁡ ⁡ ( α α i , α α 0 − − α α i ) {\displaystyle X_{i}\sim \operatorname {Beta} (\alpha _{i},\alpha _{0}-\alpha _{i})} , a Beta distribution . In particular, if K = 2 then X 1 ∼ ∼ Beta ⁡ ⁡ ( α α 1 , α α 2 ) {\displaystyle X_{1}\sim \operatorname {Beta} (\alpha _{1},\alpha _{2})} is equivalent to X = ( X 1 , 1 − − X 1 ) ∼ ∼ Dir ⁡ ⁡ ( α α 1 , α α 2 ) {\displaystyle {\boldsymbol {X}}=(X_{1},1-X_{1})\sim \operatorname {Dir} \left(\alpha _{1},\alpha _{2}\right)} .

For K independently distributed Gamma distributions : Y 1 ∼ ∼ Gamma ⁡ ⁡ ( α α 1 , θ θ ) , … … , Y K ∼ ∼ Gamma ⁡ ⁡ ( α α K , θ θ ) {\displaystyle Y_{1}\sim \operatorname {Gamma} (\alpha _{1},\theta ),\ldots ,Y_{K}\sim \operatorname {Gamma} (\alpha _{K},\theta )} we have: [ 21 ] : 402 V = ∑ ∑ i = 1 K Y i ∼ ∼ Gamma ⁡ ⁡ ( α α 0 , θ θ ) , {\displaystyle V=\sum _{i=1}^{K}Y_{i}\sim \operatorname {Gamma} \left(\alpha _{0},\theta \right),} X = ( X 1 , … … , X K ) = ( Y 1 V , … … , Y K V ) ∼ ∼ Dir ⁡ ⁡ ( α α 1 , … … , α α K ) .

{\displaystyle X=(X_{1},\ldots ,X_{K})=\left({\frac {Y_{1}}{V}},\ldots ,{\frac {Y_{K}}{V}}\right)\sim \operatorname {Dir} \left(\alpha _{1},\ldots ,\alpha _{K}\right).} Although the X i s are not independent from one another, they can be seen to be generated from a set of K independent gamma random variables.

[ 21 ] : 594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X ), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.

Conjugate prior of the Dirichlet distribution [ edit ] Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior.
The conjugate prior is of the form: [ 22 ] CD ⁡ ⁡ ( α α ∣ ∣ v , η η ) ∝ ∝ ( 1 B ⁡ ⁡ ( α α ) ) η η exp ⁡ ⁡ ( − − ∑ ∑ k v k α α k ) .

{\displaystyle \operatorname {CD} ({\boldsymbol {\alpha }}\mid {\boldsymbol {v}},\eta )\propto \left({\frac {1}{\operatorname {B} ({\boldsymbol {\alpha }})}}\right)^{\eta }\exp \left(-\sum _{k}v_{k}\alpha _{k}\right).} Here v {\displaystyle {\boldsymbol {v}}} is a K -dimensional real vector and η η {\displaystyle \eta } is a scalar parameter.  The domain of ( v , η η ) {\displaystyle ({\boldsymbol {v}},\eta )} is restricted to the set of parameters for which the above unnormalized density function can be normalized. The (necessary and sufficient) condition is: [ 23 ] ∀ ∀ k v k > 0 and η η > − − 1 and ( η η ≤ ≤ 0 or ∑ ∑ k exp − − v k η η < 1 ) {\displaystyle \forall k\;\;v_{k}>0\;\;\;\;{\text{ and }}\;\;\;\;\eta >-1\;\;\;\;{\text{ and }}\;\;\;\;(\eta \leq 0\;\;\;\;{\text{ or }}\;\;\;\;\sum _{k}\exp -{\frac {v_{k}}{\eta }}<1)} The conjugation property can be expressed as if [ prior : α α ∼ ∼ CD ⁡ ⁡ ( ⋅ ⋅ ∣ ∣ v , η η ) {\displaystyle {\boldsymbol {\alpha }}\sim \operatorname {CD} (\cdot \mid {\boldsymbol {v}},\eta )} ] and [ observation : x ∣ ∣ α α ∼ ∼ Dirichlet ⁡ ⁡ ( ⋅ ⋅ ∣ ∣ α α ) {\displaystyle {\boldsymbol {x}}\mid {\boldsymbol {\alpha }}\sim \operatorname {Dirichlet} (\cdot \mid {\boldsymbol {\alpha }})} ] then [ posterior : α α ∣ ∣ x ∼ ∼ CD ⁡ ⁡ ( ⋅ ⋅ ∣ ∣ v − − log ⁡ ⁡ x , η η + 1 ) {\displaystyle {\boldsymbol {\alpha }}\mid {\boldsymbol {x}}\sim \operatorname {CD} (\cdot \mid {\boldsymbol {v}}-\log {\boldsymbol {x}},\eta +1)} ].

In the published literature there is no practical algorithm to efficiently generate samples from CD ⁡ ⁡ ( α α ∣ ∣ v , η η ) {\displaystyle \operatorname {CD} ({\boldsymbol {\alpha }}\mid {\boldsymbol {v}},\eta )} .

Generalization by scaling and translation of log-probabilities [ edit ] As noted above, Dirichlet variates can be generated by normalizing independent gamma variates. If instead one normalizes generalized gamma variates, one obtains variates from the simplicial generalized beta distribution (SGB).

[ 24 ] On the other hand, SGB variates can also be obtained by applying the softmax function to scaled and translated logarithms of Dirichlet variates. Specifically, let x = ( x 1 , … … , x K ) ∼ ∼ Dir ⁡ ⁡ ( α α ) {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{K})\sim \operatorname {Dir} ({\boldsymbol {\alpha }})} and let y = ( y 1 , … … , y K ) {\displaystyle \mathbf {y} =(y_{1},\ldots ,y_{K})} , where applying the logarithm elementwise: y = softmax ⁡ ⁡ ( a − − 1 log ⁡ ⁡ x + log ⁡ ⁡ b ) ⟺ ⟺ x = softmax ⁡ ⁡ ( a log ⁡ ⁡ y − − a log ⁡ ⁡ b ) {\displaystyle \mathbf {y} =\operatorname {softmax} (a^{-1}\log \mathbf {x} +\log \mathbf {b} )\;\iff \;\mathbf {x} =\operatorname {softmax} (a\log \mathbf {y} -a\log \mathbf {b} )} or y k = b k x k 1 / a ∑ ∑ i = 1 K b i x i 1 / a ⟺ ⟺ x k = ( y k / b k ) a ∑ ∑ i = 1 K ( y i / b i ) a {\displaystyle y_{k}={\frac {b_{k}x_{k}^{1/a}}{\sum _{i=1}^{K}b_{i}x_{i}^{1/a}}}\;\iff \;x_{k}={\frac {(y_{k}/b_{k})^{a}}{\sum _{i=1}^{K}(y_{i}/b_{i})^{a}}}} where a > 0 {\displaystyle a>0} and b = ( b 1 , … … , b K ) {\displaystyle \mathbf {b} =(b_{1},\ldots ,b_{K})} , with all b k > 0 {\displaystyle b_{k}>0} , then y ∼ ∼ SGB ⁡ ⁡ ( a , b , α α ) {\displaystyle \mathbf {y} \sim \operatorname {SGB} (a,\mathbf {b} ,{\boldsymbol {\alpha }})} . The SGB density function can be derived by noting that the transformation x ↦ ↦ y {\displaystyle \mathbf {x} \mapsto \mathbf {y} } , which is a bijection from the simplex to itself, induces a differential volume change factor [ 25 ] of: R ( y , a , b ) = a 1 − − K ∏ ∏ k = 1 K y k x k {\displaystyle R(\mathbf {y} ,a,\mathbf {b} )=a^{1-K}\prod _{k=1}^{K}{\frac {y_{k}}{x_{k}}}} where it is understood that x {\displaystyle \mathbf {x} } is recovered as a function of y {\displaystyle \mathbf {y} } , as shown above. This facilitates writing the SGB density in terms of the Dirichlet density, as: f SGB ( y ∣ ∣ a , b , α α ) = f Dir ( x ∣ ∣ α α ) R ( y , a , b ) {\displaystyle f_{\text{SGB}}(\mathbf {y} \mid a,\mathbf {b} ,{\boldsymbol {\alpha }})={\frac {f_{\text{Dir}}(\mathbf {x} \mid {\boldsymbol {\alpha }})}{R(\mathbf {y} ,a,\mathbf {b} )}}} This generalization of the Dirichlet density, via a change of variables , is closely related to a normalizing flow , while it must be noted that the differential volume change is not given by the Jacobian determinant of x ↦ ↦ y : R K → → R K {\displaystyle \mathbf {x} \mapsto \mathbf {y} :\mathbb {R} ^{K}\to \mathbb {R} ^{K}} which is zero, but by the Jacobian determinant of ( x 1 , … … , x K − − 1 ) ↦ ↦ ( y 1 , … … , y K − − 1 ) {\displaystyle (x_{1},\ldots ,x_{K-1})\mapsto \mathbf {(} y_{1},\ldots ,y_{K-1})} , as explained in more detail at Normalizing flow § Simplex flow .

For further insight into the interaction between the Dirichlet shape parameters α α {\displaystyle {\boldsymbol {\alpha }}} , and the transformation parameters a , b {\displaystyle a,\mathbf {b} } , it may be helpful to consider the logarithmic marginals, log ⁡ ⁡ x k 1 − − x k {\displaystyle \log {\frac {x_{k}}{1-x_{k}}}} , which follow the logistic-beta distribution , B σ σ ( α α k , ∑ ∑ i ≠ ≠ k α α i ) {\displaystyle B_{\sigma }(\alpha _{k},\sum _{i\neq k}\alpha _{i})} . See in particular the sections on tail behaviour and generalization with location and scale parameters .

Application [ edit ] When b 1 = b 2 = ⋯ ⋯ = b K {\displaystyle b_{1}=b_{2}=\cdots =b_{K}} , then the transformation simplifies to x ↦ ↦ softmax ⁡ ⁡ ( a − − 1 log ⁡ ⁡ x ) {\displaystyle \mathbf {x} \mapsto \operatorname {softmax} (a^{-1}\log \mathbf {x} )} , which is known as temperature scaling in machine learning , where it is used as a calibration transform for multiclass probabilistic classiers.

[ 26 ] Traditionally the temperature parameter ( a {\displaystyle a} here) is learnt discriminatively by minimizing multiclass cross-entropy over a supervised calibration data set with known class labels. But the above PDF transformation mechanism can be used to facilitate also the design of generatively trained calibration models with a temperature scaling component.

Occurrence and applications [ edit ] Bayesian models [ edit ] Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models . (In many fields, such as in natural language processing , categorical variables are often imprecisely called "multinomial variables".  Such a usage is unlikely to cause confusion, just as when Bernoulli distributions and binomial distributions are commonly conflated.) Inference over hierarchical Bayesian models is often done using Gibbs sampling , and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable .  This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution , conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters ).  One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.

Intuitive interpretations of the parameters [ edit ] The concentration parameter [ edit ] Dirichlet distributions are very often used as prior distributions in Bayesian inference .  The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal.  This corresponds to the case where you have no prior information to favor one component over any other.  As described above, the single value α to which all parameters are set is called the concentration parameter .  If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution , then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of the Dirichlet distribution to its center, leading to samples with mass dispersed almost equally among all components, i.e., with a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass, and with a value much greater than 1, the mass will be dispersed almost equally among all the components.  See the article on the concentration parameter for further discussion.

String cutting [ edit ] One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. Recall that α α 0 = ∑ ∑ i = 1 K α α i .

{\displaystyle \alpha _{0}=\sum _{i=1}^{K}\alpha _{i}.} The α α i / α α 0 {\displaystyle \alpha _{i}/\alpha _{0}} values specify the mean lengths of the cut pieces of string resulting from the distribution.  The variance around this mean varies inversely with α α 0 {\displaystyle \alpha _{0}} .

Example of Dirichlet(1/2,1/3,1/6) distribution Pólya's urn [ edit ] Consider an urn containing balls of K different colors. Initially, the urn contains α 1 balls of color 1, α 2 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir( α 1 , ..., α K ) .

[ 27 ] For a formal proof, note that the proportions of the different colored balls form a bounded [0,1] K -valued martingale , hence by the martingale convergence theorem , these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.

Each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.

Random variate generation [ edit ] Further information: Non-uniform random variate generation From gamma distribution [ edit ] With a source of Gamma-distributed random variates, one can easily sample a random vector x = ( x 1 , … … , x K ) {\displaystyle x=(x_{1},\ldots ,x_{K})} from the K -dimensional Dirichlet distribution with parameters ( α α 1 , … … , α α K ) {\displaystyle (\alpha _{1},\ldots ,\alpha _{K})} . First, draw K independent random samples y 1 , … … , y K {\displaystyle y_{1},\ldots ,y_{K}} from Gamma distributions each with density Gamma ⁡ ⁡ ( α α i , 1 ) = y i α α i − − 1 e − − y i Γ Γ ( α α i ) , {\displaystyle \operatorname {Gamma} (\alpha _{i},1)={\frac {y_{i}^{\alpha _{i}-1}\;e^{-y_{i}}}{\Gamma (\alpha _{i})}},\!} and then set x i = y i ∑ ∑ j = 1 K y j .

{\displaystyle x_{i}={\frac {y_{i}}{\sum _{j=1}^{K}y_{j}}}.} [Proof] The joint distribution of the independently sampled gamma variates, { y i } {\displaystyle \{y_{i}\}} , is given by the product: e − − ∑ ∑ i y i ∏ ∏ i = 1 K y i α α i − − 1 Γ Γ ( α α i ) {\displaystyle e^{-\sum _{i}y_{i}}\prod _{i=1}^{K}{\frac {y_{i}^{\alpha _{i}-1}}{\Gamma (\alpha _{i})}}} Next, one uses a change of variables, parametrising { y i } {\displaystyle \{y_{i}\}} in terms of y 1 , y 2 , … … , y K − − 1 {\displaystyle y_{1},y_{2},\ldots ,y_{K-1}} and ∑ ∑ i = 1 K y i {\displaystyle \sum _{i=1}^{K}y_{i}} , and performs a change of variables from y → → x {\displaystyle y\to x} such that x ¯ ¯ = ∑ ∑ i = 1 K y i , x 1 = y 1 x ¯ ¯ , x 2 = y 2 x ¯ ¯ , … … , x K − − 1 = y K − − 1 x ¯ ¯ {\displaystyle {\bar {x}}=\textstyle \sum _{i=1}^{K}y_{i},x_{1}={\frac {y_{1}}{\bar {x}}},x_{2}={\frac {y_{2}}{\bar {x}}},\ldots ,x_{K-1}={\frac {y_{K-1}}{\bar {x}}}} . Each of the variables 0 ≤ ≤ x 1 , x 2 , … … , x k − − 1 ≤ ≤ 1 {\displaystyle 0\leq x_{1},x_{2},\ldots ,x_{k-1}\leq 1} and likewise 0 ≤ ≤ ∑ ∑ i = 1 K − − 1 x i ≤ ≤ 1 {\displaystyle 0\leq \textstyle \sum _{i=1}^{K-1}x_{i}\leq 1} . One must then use the change of variables formula, P ( x ) = P ( y ( x ) ) | ∂ ∂ y ∂ ∂ x | {\displaystyle P(x)=P(y(x)){\bigg |}{\frac {\partial y}{\partial x}}{\bigg |}} in which | ∂ ∂ y ∂ ∂ x | {\displaystyle {\bigg |}{\frac {\partial y}{\partial x}}{\bigg |}} is the transformation Jacobian. Writing y explicitly as a function of x, one obtains y 1 = x ¯ ¯ x 1 , y 2 = x ¯ ¯ x 2 … … y K − − 1 = x ¯ ¯ x K − − 1 , y K = x ¯ ¯ ( 1 − − ∑ ∑ i = 1 K − − 1 x i ) {\displaystyle y_{1}={\bar {x}}x_{1},y_{2}={\bar {x}}x_{2}\ldots y_{K-1}={\bar {x}}x_{K-1},y_{K}={\bar {x}}(1-\textstyle \sum _{i=1}^{K-1}x_{i})} The Jacobian now looks like | x ¯ ¯ 0 … … x 1 0 x ¯ ¯ … … x 2 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ − − x ¯ ¯ − − x ¯ ¯ … … 1 − − ∑ ∑ i = 1 K − − 1 x i | {\displaystyle {\begin{vmatrix}{\bar {x}}&0&\ldots &x_{1}\\0&{\bar {x}}&\ldots &x_{2}\\\vdots &\vdots &\ddots &\vdots \\-{\bar {x}}&-{\bar {x}}&\ldots &1-\sum _{i=1}^{K-1}x_{i}\end{vmatrix}}} The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain | x ¯ ¯ 0 … … x 1 0 x ¯ ¯ … … x 2 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 0 … … 1 | {\displaystyle {\begin{vmatrix}{\bar {x}}&0&\ldots &x_{1}\\0&{\bar {x}}&\ldots &x_{2}\\\vdots &\vdots &\ddots &\vdots \\0&0&\ldots &1\end{vmatrix}}} which can be expanded about the bottom row to obtain the determinant value x ¯ ¯ K − − 1 {\displaystyle {\bar {x}}^{K-1}} . Substituting for x in the joint pdf and including the Jacobian determinant, one obtains: [ ∏ ∏ i = 1 K − − 1 ( x ¯ ¯ x i ) α α i − − 1 ] [ x ¯ ¯ ( 1 − − ∑ ∑ i = 1 K − − 1 x i ) ] α α K − − 1 ∏ ∏ i = 1 K Γ Γ ( α α i ) x ¯ ¯ K − − 1 e − − x ¯ ¯ = Γ Γ ( α α ¯ ¯ ) [ ∏ ∏ i = 1 K − − 1 ( x i ) α α i − − 1 ] [ 1 − − ∑ ∑ i = 1 K − − 1 x i ] α α K − − 1 ∏ ∏ i = 1 K Γ Γ ( α α i ) × × x ¯ ¯ α α ¯ ¯ − − 1 e − − x ¯ ¯ Γ Γ ( α α ¯ ¯ ) {\displaystyle {\begin{aligned}&{\frac {\left[\prod _{i=1}^{K-1}({\bar {x}}x_{i})^{\alpha _{i}-1}\right]\left[{\bar {x}}(1-\sum _{i=1}^{K-1}x_{i})\right]^{\alpha _{K}-1}}{\prod _{i=1}^{K}\Gamma (\alpha _{i})}}{\bar {x}}^{K-1}e^{-{\bar {x}}}\\=&{\frac {\Gamma ({\bar {\alpha }})\left[\prod _{i=1}^{K-1}(x_{i})^{\alpha _{i}-1}\right]\left[1-\sum _{i=1}^{K-1}x_{i}\right]^{\alpha _{K}-1}}{\prod _{i=1}^{K}\Gamma (\alpha _{i})}}\times {\frac {{\bar {x}}^{{\bar {\alpha }}-1}e^{-{\bar {x}}}}{\Gamma ({\bar {\alpha }})}}\end{aligned}}} where α α ¯ ¯ = ∑ ∑ i = 1 K α α i {\displaystyle {\bar {\alpha }}=\textstyle \sum _{i=1}^{K}\alpha _{i}} . The right-hand side can be recognized as the product of a Dirichlet pdf for the x i {\displaystyle x_{i}} and a gamma pdf for x ¯ ¯ {\displaystyle {\bar {x}}} . The product form shows the Dirichlet and gamma variables are independent, so the latter can be integrated out by simply omitting it, to obtain: x 1 , x 2 , … … , x K − − 1 ∼ ∼ ( 1 − − ∑ ∑ i = 1 K − − 1 x i ) α α K − − 1 ∏ ∏ i = 1 K − − 1 x i α α i − − 1 B ( α α ) {\displaystyle x_{1},x_{2},\ldots ,x_{K-1}\sim {\frac {(1-\sum _{i=1}^{K-1}x_{i})^{\alpha _{K}-1}\prod _{i=1}^{K-1}x_{i}^{\alpha _{i}-1}}{B({\boldsymbol {\alpha }})}}} Which is equivalent to ∏ ∏ i = 1 K x i α α i − − 1 B ( α α ) {\displaystyle {\frac {\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}}{B({\boldsymbol {\alpha }})}}} with support ∑ ∑ i = 1 K x i = 1 {\displaystyle \sum _{i=1}^{K}x_{i}=1} Below is example Python code to draw the sample: params = [ a1 , a2 , ...

, ak ] sample = [ random .

gammavariate ( a , 1 ) for a in params ] sample = [ v / sum ( sample ) for v in sample ] This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.

From marginal beta distributions [ edit ] A less efficient algorithm [ 28 ] relies on the univariate marginal and conditional distributions being beta and proceeds as follows.  Simulate x 1 {\displaystyle x_{1}} from Beta ( α α 1 , ∑ ∑ i = 2 K α α i ) {\displaystyle {\textrm {Beta}}\left(\alpha _{1},\sum _{i=2}^{K}\alpha _{i}\right)} Then simulate x 2 , … … , x K − − 1 {\displaystyle x_{2},\ldots ,x_{K-1}} in order, as follows.  For j = 2 , … … , K − − 1 {\displaystyle j=2,\ldots ,K-1} , simulate ϕ ϕ j {\displaystyle \phi _{j}} from Beta ( α α j , ∑ ∑ i = j + 1 K α α i ) , {\displaystyle {\textrm {Beta}}\left(\alpha _{j},\sum _{i=j+1}^{K}\alpha _{i}\right),} and let x j = ( 1 − − ∑ ∑ i = 1 j − − 1 x i ) ϕ ϕ j .

{\displaystyle x_{j}=\left(1-\sum _{i=1}^{j-1}x_{i}\right)\phi _{j}.} Finally, set x K = 1 − − ∑ ∑ i = 1 K − − 1 x i .

{\displaystyle x_{K}=1-\sum _{i=1}^{K-1}x_{i}.} This iterative procedure corresponds closely to the "string cutting" intuition described above.

Below is example Python code to draw the sample: params = [ a1 , a2 , ...

, ak ] xs = [ random .

betavariate ( params [ 0 ], sum ( params [ 1 :]))] for j in range ( 1 , len ( params ) - 1 ): phi = random .

betavariate ( params [ j ], sum ( params [ j + 1 :])) xs .

append (( 1 - sum ( xs )) * phi ) xs .

append ( 1 - sum ( xs )) When each alpha is 1 [ edit ] When α 1 = ... = α K = 1 , a sample from the distribution can be found by randomly drawing a set of K − 1 values independently and uniformly from the interval [0, 1] , adding the values 0 and 1 to the set to make it have K + 1 values, sorting the set, and computing the difference between each pair of order-adjacent values, to give x 1 , ..., x K .

When each alpha is 1/2 and relationship to the hypersphere [ edit ] When α 1 = ... = α K = 1/2 , a sample from the distribution can be found by randomly drawing K values independently from the standard normal distribution, squaring these values, and normalizing them by dividing by their sum, to give x 1 , ..., x K .

A point ( x 1 , ..., x K ) can be drawn uniformly at random from the ( K −1 )-dimensional unit hypersphere (which is the surface of a K -dimensional hyperball ) via a similar procedure.  Randomly draw K values independently from the standard normal distribution and normalize these coordinate values by dividing each by the constant that is the square root of the sum of their squares.

See also [ edit ] Generalized Dirichlet distribution Grouped Dirichlet distribution Inverted Dirichlet distribution Latent Dirichlet allocation Dirichlet process Matrix variate Dirichlet distribution References [ edit ] ^ S. Kotz; N. Balakrishnan; N. L. Johnson (2000).

Continuous Multivariate Distributions. Volume 1: Models and Applications . New York: Wiley.

ISBN 978-0-471-18387-7 .

(Chapter 49: Dirichlet and Inverted Dirichlet Distributions) ^ Olkin, Ingram; Rubin, Herman (1964).

"Multivariate Beta Distributions and Independence Properties of the Wishart Distribution" .

The Annals of Mathematical Statistics .

35 (1): 261– 269.

doi : 10.1214/aoms/1177703748 .

JSTOR 2238036 .

^ a b Bela A. Frigyik; Amol Kapila; Maya R. Gupta (2010).

"Introduction to the Dirichlet Distribution and Related Processes" (PDF) . University of Washington Department of Electrical Engineering. Archived from the original (Technical Report UWEETR-2010-006) on 2015-02-19.

^ Eq. (49.9) on page 488 of Kotz, Balakrishnan & Johnson (2000). Continuous Multivariate Distributions. Volume 1: Models and Applications. New York: Wiley.

^ BalakrishV. B. (2005).

" "Chapter 27. Dirichlet Distribution" " .

A Primer on Statistical Distributions . Hoboken, NJ: John Wiley & Sons, Inc. p.

274 .

ISBN 978-0-471-42798-8 .

^ Dello Schiavo, Lorenzo (2019).

"Characteristic functionals of Dirichlet measures" .

Electron. J. Probab .

24 : 1– 38.

arXiv : 1810.09790 .

doi : 10.1214/19-EJP371 .

^ Dello Schiavo, Lorenzo; Quattrocchi, Filippo (2023). "Multivariate Dirichlet Moments and a Polychromatic Ewens Sampling Formula".

arXiv : 2309.11292 [ math.PR ].

^ Hoffmann, Till.

"Moments of the Dirichlet distribution" . Archived from the original on 2016-02-14 . Retrieved 14 February 2016 .

^ Christopher M. Bishop (17 August 2006).

Pattern Recognition and Machine Learning . Springer.

ISBN 978-0-387-31073-2 .

^ Farrow, Malcolm.

"MAS3301 Bayesian Statistics" (PDF) .

Newcastle University . Retrieved 10 April 2013 .

^ Lin, Jiayu (2016).

On The Dirichlet Distribution (PDF) . Kingston, Canada: Queen's University. pp. § 2.4.9.

^ Nguyen, Duy (15 August 2023).

"AN IN DEPTH INTRODUCTION TO VARIATIONAL BAYES NOTE" .

SSRN 4541076 . Retrieved 15 August 2023 .

^ Song, Kai-Sheng (2001). "Rényi information, loglikelihood, and an intrinsic distribution measure".

Journal of Statistical Planning and Inference .

93 (325). Elsevier: 51– 69.

doi : 10.1016/S0378-3758(00)00169-5 .

^ Nemenman, Ilya; Shafee, Fariel; Bialek, William (2002).

Entropy and Inference, revisited (PDF) . NIPS 14.

, eq. 8 ^ Joram Soch (2020-05-10).

"Kullback–Leibler divergence for the Dirichlet distribution" .

The Book of Statistical Proofs . StatProofBook . Retrieved 2025-06-23 .

^ Connor, Robert J.; Mosimann, James E (1969). "Concepts of Independence for Proportions with a Generalization of the Dirichlet Distribution".

Journal of the American Statistical Association .

64 (325). American Statistical Association: 194– 206.

doi : 10.2307/2283728 .

JSTOR 2283728 .

^ See Kotz, Balakrishnan & Johnson (2000), Section 8.5, "Connor and Mosimann's Generalization", pp. 519–521.

^ Phillips, P. C. B. (1988).

"The characteristic function of the Dirichlet and multivariate F distribution" (PDF) .

Cowles Foundation Discussion Paper 865 .

^ Grinshpan, A. Z. (2017).

"An inequality for multiple convolutions with respect to Dirichlet probability measure" .

Advances in Applied Mathematics .

82 (1): 102– 119.

doi : 10.1016/j.aam.2016.08.001 .

^ Perrault, P. (2024). "A New Bound on the Cumulant Generating Function of Dirichlet Processes".

arXiv : 2409.18621 [ math.PR ].

Theorem 3.3 ^ a b Devroye, Luc (1986).

Non-Uniform Random Variate Generation . Springer-Verlag.

ISBN 0-387-96305-7 .

^ Lefkimmiatis, Stamatios; Maragos, Petros; Papandreou, George (2009). "Bayesian Inference on Multiscale Models for Poisson Intensity Estimation: Applications to Photon-Limited Image Denoising".

IEEE Transactions on Image Processing .

18 (8): 1724– 1741.

Bibcode : 2009ITIP...18.1724L .

doi : 10.1109/TIP.2009.2022008 .

PMID 19414285 .

S2CID 859561 .

^ Andreoli, Jean-Marc (2018). "A conjugate prior for the Dirichlet distribution".

arXiv : 1811.05266 [ cs.LG ].

^ Graf, Monique (2019).

"The Simplicial Generalized Beta distribution - R-package SGB and applications" .

Libra . Retrieved 26 May 2025 .

{{ cite web }} :  CS1 maint: numeric names: authors list ( link ) ^ Sorrenson, Peter; et al. (2024).

"Learning Distributions on Manifolds with Free-Form Flows" .

arXiv .

{{ cite web }} :  CS1 maint: numeric names: authors list ( link ) ^ Ferrer, Luciana; Ramos, Daniel (2025).

"Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration" .

Transactions on Machine Learning Research .

^ Blackwell, David; MacQueen, James B. (1973).

"Ferguson distributions via Polya urn schemes" .

Ann. Stat .

1 (2): 353– 355.

doi : 10.1214/aos/1176342372 .

^ A. Gelman; J. B. Carlin; H. S. Stern; D. B. Rubin (2003).

Bayesian Data Analysis (2nd ed.). Chapman & Hall/CRC. pp.

582 .

ISBN 1-58488-388-X .

External links [ edit ] "Dirichlet distribution" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Dirichlet Distribution How to estimate the parameters of the compound Dirichlet distribution (Pólya distribution) using expectation-maximization (EM) Luc Devroye.

"Non-Uniform Random Variate Generation" . Retrieved 19 October 2019 .

Dirichlet Random Measures, Method of Construction via Compound Poisson Random Variables, and Exchangeability Properties of the resulting Gamma Distribution SciencesPo : R package that contains functions for simulating parameters of the Dirichlet distribution.

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons v t e Peter Gustav Lejeune Dirichlet Dirichlet distribution Dirichlet character Dirichlet process Dirichlet-multinomial distribution Dirichlet series Dirichlet's theorem on arithmetic progressions Dirichlet convolution Dirichlet problem Dirichlet integral NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐prbxs
Cached time: 20250817043830
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.946 seconds
Real time usage: 1.421 seconds
Preprocessor visited node count: 6538/1000000
Revision size: 50359/2097152 bytes
Post‐expand include size: 144898/2097152 bytes
Template argument size: 7637/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 130840/5000000 bytes
Lua time usage: 0.406/10.000 seconds
Lua memory usage: 6554366/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  824.746      1 -total
 31.48%  259.590      1 Template:Reflist
 13.18%  108.687      6 Template:Cite_book
 13.11%  108.161      1 Template:Short_description
 12.70%  104.735      5 Template:Navbox
 12.14%  100.090      1 Template:ProbDistributions
  8.77%   72.297      2 Template:Pagetype
  7.35%   60.632     45 Template:Math
  6.36%   52.437      9 Template:Cite_journal
  5.93%   48.909      8 Template:Cite_web Saved in parser cache with key enwiki:pcache:1117833:|#|:idhash:canonical and timestamp 20250817043830 and revision id 1302632005. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Dirichlet_distribution&oldid=1302632005 " Categories : Multivariate continuous distributions Conjugate prior distributions Exponential family distributions Continuous distributions Hidden categories: CS1 maint: numeric names: authors list Articles with short description Short description matches Wikidata This page was last edited on 26 July 2025, at 16:43 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Dirichlet distribution 16 languages Add topic

