Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Informal description 2 Formal description 3 Difference from evidential decision theory 4 Thought experiments Toggle Thought experiments subsection 4.1 Newcomb's paradox 5 Criticism Toggle Criticism subsection 5.1 Vagueness 5.2 Counterexamples 6 See also 7 References 8 External links Toggle the table of contents Causal decision theory 1 language Español Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia School of thought within decision theory Causal decision theory (CDT) is a school of thought within decision theory which states that, when a rational agent is confronted with a set of possible actions, one should select the action which causes the best outcome in expectation . CDT contrasts with evidential decision theory (EDT), which recommends the action which would be indicative of the best outcome if one received the "news" that it had been taken.

[ 1 ] : 7 Informal description [ edit ] Informally, causal decision theory recommends the agent to make the decision with the best expected causal consequences. For example: if eating an apple will cause you to be happy and eating an orange will cause you to be sad then you would be rational to eat the apple.

One complication is the notion of expected causal consequences. Imagine that eating a good apple will cause you to be happy and eating a bad apple will cause you to be sad but you aren't sure if the apple is good or bad. In this case you don't know the causal effects of eating the apple.

Instead, then, you work from the expected causal effects, where these will depend on three things: how likely you think the apple is to be good or bad; how happy eating a good apple makes you; and how sad eating a bad apple makes you.

In informal terms, causal decision theory advises the agent to make the decision with the best expected causal effects.

Formal description [ edit ] In a 1981 article, Allan Gibbard and William Harper explained causal decision theory as maximization of the expected utility U {\displaystyle U} of an action A {\displaystyle A} "calculated from probabilities of counterfactuals ": [ 2 ] U ( A ) = ∑ ∑ j P ( A > O j ) D ( O j ) , {\displaystyle U(A)=\sum \limits _{j}P(A>O_{j})D(O_{j}),} where D ( O j ) {\displaystyle D(O_{j})} is the desirability of outcome O j {\displaystyle O_{j}} and P ( A > O j ) {\displaystyle P(A>O_{j})} is the counterfactual probability that, if A {\displaystyle A} were done, then O j {\displaystyle O_{j}} would hold.

Difference from evidential decision theory [ edit ] David Lewis proved [ 3 ] that the probability of a conditional P ( A > O j ) {\displaystyle P(A>O_{j})} does not always equal the conditional probability P ( O j | A ) {\displaystyle P(O_{j}|A)} .

[ 4 ] (see also Lewis's triviality result ) If that were the case, causal decision theory would be equivalent to evidential decision theory, which uses conditional probabilities.

Gibbard and Harper showed that if we accept two axioms (one related to the controversial principle of the conditional excluded middle [ 5 ] ), then the statistical independence of A {\displaystyle A} and A > O j {\displaystyle A>O_{j}} suffices to guarantee that P ( A > O j ) = P ( O j | A ) {\displaystyle P(A>O_{j})=P(O_{j}|A)} . However, there are cases in which actions and conditionals are not independent. Gibbard and Harper give an example in which King David wants Bathsheba but fears that summoning her would provoke a revolt.

Further, David has studied works on psychology and political science which teach him the following: Kings have two personality types, charismatic and uncharismatic. A king's degree of charisma depends on his genetic make-up and early childhood experiences, and cannot be changed in adulthood. Now, charismatic kings tend to act justly and uncharismatic kings unjustly. Successful revolts against charismatic kings are rare, whereas successful revolts against uncharismatic kings are frequent. Unjust acts themselves, though, do not cause successful revolts; the reason uncharismatic kings are prone to successful revolts is that they have a sneaky, ignoble bearing. David does not know whether or not he is charismatic; he does know that it is unjust to send for another man's wife. (p. 164) In this case, evidential decision theory recommends that David abstain from Bathsheba, while causal decision theory—noting that whether David is charismatic or uncharismatic cannot be changed—recommends sending for her.

When required to choose between causal decision theory and evidential decision theory, philosophers usually prefer causal decision theory.

[ 6 ] Thought experiments [ edit ] Different decision theories are often examined in their recommendations for action in different thought experiments .

Newcomb's paradox [ edit ] Main article: Newcomb's paradox In Newcomb's paradox, there is a predictor, a player, and two boxes designated A and B. The predictor is able to reliably predict the player's choices— say, with 99% accuracy. The player is given a choice between taking only box B, or taking both boxes A and B. The player knows the following: [ 7 ] Box A is transparent and always contains a visible $1,000.

Box B is opaque, and its content has already been set by the predictor: If the predictor has predicted the player will take both boxes A and B, then box B contains nothing.

If the predictor has predicted that the player will take only box B, then box B contains $1,000,000.

The player does not know what the predictor predicted or what box B contains while making the choice. Should the player take both boxes, or only box B?

Causal decision theory recommends taking both boxes in this scenario, because at the moment when the player must make a decision, the predictor has already made a prediction (therefore, the action of the player will not affect the outcome).

[ citation needed ] Conversely, evidential decision theory (EDT) would have recommended that the player takes only box B because taking only box B is strong evidence that the predictor anticipated that the player would only take box B, and therefore it is very likely that box B contains $1,000,000. Conversely, choosing to take both boxes is strong evidence that the predictor knew that the player would take both boxes; therefore we should expect that box B contains nothing.

[ 1 ] : 22 Criticism [ edit ] Vagueness [ edit ] The theory of causal decision theory (CDT) does not itself specify what algorithm to use to calculate the counterfactual probabilities.

[ 5 ] One proposal is the "imaging" technique suggested by Lewis: [ 8 ] To evaluate P ( A > O j ) {\displaystyle P(A>O_{j})} , move probability mass from each possible world w {\displaystyle w} to the closest possible world w A {\displaystyle w_{A}} in which A {\displaystyle A} holds, assuming A {\displaystyle A} is possible. However, this procedure requires that we know what we would believe if we were certain of A {\displaystyle A} ; this is itself a conditional to which we might assign probability less than 1, leading to regress.

[ 5 ] Counterexamples [ edit ] There are innumerable "counterexamples" where, it is argued, a straightforward application of CDT fails to produce a defensibly "sane" decision. Philosopher Andy Egan argues this is due to a fundamental disconnect between the intuitive rational rule, "do what you expect will bring about the best results", and CDT's algorithm of "do whatever has the best expected outcome, holding fixed our initial views about the likely causal structure of the world." In this view, it is CDT's requirement to "hold fixed the agent’s unconditional credences in dependency hypotheses" that leads to irrational decisions.

[ 9 ] An early alleged counterexample is Newcomb's problem .

[ citation needed ] Because your choice of one or two boxes can't causally affect the Predictor's guess, causal decision theory recommends the two-boxing strategy.

[ 2 ] However, this results in getting only $1,000, not $1,000,000. Philosophers disagree whether one-boxing or two-boxing is the "rational" strategy.

[ 10 ] Similar concerns may arise even in seemingly-straightforward problems like the prisoner's dilemma , [ 11 ] especially when playing opposite your "twin" whose choice to cooperate or defect correlates strongly, but is not caused by, your own choice.

[ 12 ] In the "Death in Damascus" scenario, an anthropomorphic "Death" predicts where you will be tomorrow, and goes to wait for you there. As in Newcomb's problem, we postulate that Death is a reliable predictor. A CDT agent would be unable to process the correlation, and may as a consequence make irrational decisions: [ 9 ] [ 13 ] [ 14 ] Recently, a few variants of Death in Damascus have been proposed in which following CDT’s recommendations voluntarily loses money or, relatedly, forgoes a guaranteed payoff.

[ 15 ] [ 16 ] [ 17 ] One example is the Adversarial Offer: [ 16 ] "Two boxes are on offer. A buyer may purchase one or none of the boxes but not both. Each of the two boxes costs $1. Yesterday, the seller put $3 in each box that she predicted the buyer not to acquire. Both the seller and the buyer believe the seller’s prediction to be accurate with probability 0.75." Adopting the buyer's perspective, CDT reasons that at least one box contains $3. Therefore, the average box contains at least $1.50 in causal expected value, which is more than the cost. Hence, CDT requires buying one of the two boxes. However, this is profitable for the seller.

Another recent counterexample is the "Psychopath Button": [ 9 ] [ 18 ] Paul is debating whether to press the ‘kill all psychopaths’ button. It would, he thinks, be much better to live in a world with no psychopaths. Unfortunately, Paul is quite confident that only a psychopath would press such a button. Paul very strongly prefers living in a world with psychopaths to dying. Should Paul press the button?

According to Egan, "pretty much everyone" agrees that Paul should not press the button, yet CDT endorses pressing the button.

[ 9 ] Philosopher Jim Joyce , perhaps the most prominent modern defender of CDT, [ 19 ] argues that CDT naturally is capable of taking into account any "information about what one is inclined or likely to do as evidence".

[ 6 ] [ 20 ] Caspar Oesterheld and Vincent Conitzer have argued against the use of causal decision theory when applied to Newcomb's paradox . They argue that there exist multiple scenarios where the use of causal decision theory cause the agent to voluntarily lose money, and that causal decision theorists are subject to diachronic Dutch books .

[ 21 ] See also [ edit ] Economics portal Philosophy portal Decision making Evidential decision theory Expected utility hypothesis Game theory References [ edit ] ^ a b Ahmed, Arif (2021).

Evidential Decision Theory . Cambridge University Press.

ISBN 9781108607865 .

^ a b Gibbard, A.; Harper, W.L. (1981), "Counterfactuals and two kinds of expected utility", Ifs: Conditionals, Beliefs, Decision, Chance, and Time : 153– 190 ^ Lewis, D. (1976), "Probabilities of conditionals and conditional probabilities", The Philosophical Review , 85 (3): 297– 315, doi : 10.2307/2184045 , JSTOR 2184045 ^ In fact, Lewis proved a stronger result: "if a class of probability functions is closed under conditionalizing, then there can be no probability conditional for that class unless the class consists entirely of trivial probability functions," where a trivial probability function is one that "never assigns positive probability to more than two incompatible alternatives, and hence is at most four-valued [...]." ^ a b c Shaffer, Michael John (2009), "Decision Theory, Intelligent Planning and Counterfactuals" , Minds and Machines , 19 (1): 61– 92, doi : 10.1007/s11023-008-9126-2 , S2CID 13484119 ^ a b Weirich, Paul.

"Causal Decision Theory" . In Zalta, Edward N.

(ed.).

Stanford Encyclopedia of Philosophy (Winter 2016 ed.).

^ Wolpert, D. H.; Benford, G. (June 2013). "The lesson of Newcomb's paradox".

Synthese .

190 (9): 1637– 1646.

doi : 10.1007/s11229-011-9899-3 .

JSTOR 41931515 .

S2CID 113227 .

^ Lewis, D. (1981), "Causal decision theory" (PDF) , Australasian Journal of Philosophy , 59 (1): 5– 30, doi : 10.1080/00048408112340011 , retrieved 2009-05-29 ^ a b c d Egan, A. (2007), "Some counterexamples to causal decision theory" (PDF) , The Philosophical Review , 116 (1): 93– 114, CiteSeerX 10.1.1.642.5936 , doi : 10.1215/00318108-2006-023 , archived from the original (PDF) on 2017-03-11 , retrieved 2017-07-27 ^ Bellos, Alex (28 November 2016).

"Newcomb's problem divides philosophers. Which side are you on?" .

The Guardian . Retrieved 27 July 2017 .

^ Lewis, D. (1979), "Prisoners' dilemma is a Newcomb problem", Philosophy & Public Affairs , 8 (3): 235– 240, JSTOR 2265034 ^ Howard, J. V. (May 1988). "Cooperation in the Prisoner's Dilemma".

Theory and Decision .

24 (3): 203– 213.

doi : 10.1007/BF00148954 .

S2CID 121119727 .

^ Meacham, Christopher J. G. (2010). "Binding and its consequences".

Philosophical Studies .

149 (1): 49– 71.

doi : 10.1007/s11098-010-9539-7 .

^ Harper, William (January 1984). "Ratifiability and Causal Decision Theory: Comments on Eells and Seidenfeld".

PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association .

1984 (2): 213– 228.

doi : 10.1086/psaprocbienmeetp.1984.2.192506 .

S2CID 123368403 .

^ Spencer, J. (2020).

"An argument against causal decision theory" (PDF) .

Analysis .

81 : 52– 61.

doi : 10.1093/analys/anaa037 . Retrieved 2021-04-23 .

^ a b Oesterheld, C.; Conitzer, V. (2021), "Extracting Money from Causal Decision Theorists", The Philosophical Quarterly , 71 (4), doi : 10.1093/pq/pqaa086 ^ Joyce, James M.

"Yet Another Refutation of Causal Decision Theory?" (PDF) . Retrieved 2021-04-23 .

^ Greaves, Hilary (2013). "Epistemic decision theory".

Mind .

122 (488): 915– 952.

doi : 10.1093/mind/fzt090 .

^ Wedgwood, Ralph (2013). "Gandalf's solution to the Newcomb problem".

Synthese .

190 (14): 2643– 2675.

doi : 10.1007/s11229-011-9900-1 .

^ Joyce, James M. (2012). "Regret and instability in causal decision theory".

Synthese .

187 (1): 123– 145.

doi : 10.1007/s11229-011-0022-6 .

^ Oesterheld, Caspar; Conitzer, Vincent (23 January 2021).

"Extracting Money from Causal Decision Theorists" .

The Philosophical Quarterly .

71 (4). Oxford University Press.

doi : 10.1093/pq/pqaa086 . Retrieved 8 July 2025 .

External links [ edit ] Causal Decision Theory at the Stanford Encyclopedia of Philosophy The Logic of Conditionals at the Stanford Encyclopedia of Philosophy v t e Decision theory Core concepts Ambiguity aversion Bounded rationality Choice architecture Expected utility Expected value Hyperbolic discounting Leximin Loss aversion Multi-attribute utility Path dependence Principle of indifference Prospect theory Rational choice theory Risk aversion Risk-seeking Satisficing Strategic dominance Subjective expected utility Sure-thing Utility theorem Decision models Anscombe-Aumann framework Causal decision Decision field theory Emotional choice Evidential decision Fuzzy-trace theory Intertemporal choice Naturalistic decision Normative model Quantum cognition Recognition-primed decision Rubicon model Savage's subjective expected utility model Decision analysis tools Analytic hierarchy process Analytic network process Cost–benefit analysis Cost-effectiveness analysis Cost–utility analysis Decision conferencing Decision curve analysis Decision rule Decision support system Decision table Decision tree Decision matrix Decisional balance sheet Gittins index Influence diagram Minimax MCDA Scoring rule Value of information perfect sample uncertainty Paradoxes and biases Allais paradox Certainty effect Cognitive bias Decoy effect Disposition effect Ellsberg paradox Endowment effect Framing effect Heuristics Newcomb's paradox Pseudocertainty effect Reference dependence Regret St. Petersburg paradox Status quo bias Sunk cost Uncertainty and risk Deep uncertainty Exploration–exploitation Info-gap Pignistic probability Robust decision-making Related fields Behavioral economics Game theory Operations research Social choice theory Utility theory Key people David Blackwell Bruno de Finetti Morris H. DeGroot Peter C. Fishburn Gerd Gigerenzer Itzhak Gilboa Daniel Kahneman R. Duncan Luce Oskar Morgenstern Howard Raiffa Leonard J. Savage David Schmeidler Herbert Simon Amos Tversky John von Neumann Category Retrieved from " https://en.wikipedia.org/w/index.php?title=Causal_decision_theory&oldid=1306529004 " Category : Decision theory Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from July 2025 Articles with unsourced statements from July 2024 This page was last edited on 18 August 2025, at 06:55 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Causal decision theory 1 language Add topic

