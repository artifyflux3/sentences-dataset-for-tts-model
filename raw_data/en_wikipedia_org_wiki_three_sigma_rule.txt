Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Proof 2 Cumulative distribution function 3 Normality tests 4 Table of numerical values 5 See also 6 References 7 External links Toggle the table of contents 68–95–99.7 rule 18 languages العربية Čeština Eesti Español فارسی Français 한국어 हिन्दी Italiano עברית Lombard Magyar 日本語 Polski Svenska ไทย Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Three-sigma rule ) Shorthand used in statistics This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "68–95–99.7 rule" – news · newspapers · books · scholar · JSTOR ( September 2023 ) ( Learn how and when to remove this message ) For an approximately normal data set , the values within one standard deviation of the mean account for about 68% of the set; while within two standard deviations account for about 95%; and within three standard deviations account for about 99.7%. Shown percentages are rounded theoretical probabilities intended only to approximate the empirical data derived from a normal population.

Prediction interval (on the y-axis ) given from the standard score (on the x-axis ). The y-axis is scaled as the negative logarithm of the complement of the probability to 1, i.e. -log(1-p), and labeled with the values of p.

In statistics , the 68–95–99.7 rule , also known as the empirical rule , and sometimes abbreviated 3sr or 3 σ , is a shorthand used to remember the percentage of values that lie within an interval estimate in a normal distribution : approximately 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean , respectively.

In mathematical notation, these facts can be expressed as follows, where Pr() is the probability function , [ 1 ] Χ is an observation from a normally distributed random variable , μ (mu) is the mean of the distribution, and σ (sigma) is its standard deviation: Pr ( μ μ − − 1 σ σ ≤ ≤ X ≤ ≤ μ μ + 1 σ σ ) ≈ ≈ 68.27 % % Pr ( μ μ − − 2 σ σ ≤ ≤ X ≤ ≤ μ μ + 2 σ σ ) ≈ ≈ 95.45 % % Pr ( μ μ − − 3 σ σ ≤ ≤ X ≤ ≤ μ μ + 3 σ σ ) ≈ ≈ 99.73 % % {\displaystyle {\begin{aligned}\Pr(\mu -1\sigma \leq X\leq \mu +1\sigma )&\approx 68.27\%\\\Pr(\mu -2\sigma \leq X\leq \mu +2\sigma )&\approx 95.45\%\\\Pr(\mu -3\sigma \leq X\leq \mu +3\sigma )&\approx 99.73\%\end{aligned}}} The usefulness of this heuristic especially depends on the question under consideration.

In the empirical sciences , the so-called three-sigma rule of thumb (or 3 σ rule ) expresses a conventional heuristic that nearly all values are taken to lie within three standard deviations of the mean, and thus it is empirically useful to treat 99.7% probability as near certainty.

[ 2 ] In the social sciences , a result may be considered statistically significant if its confidence level is of the order of a two-sigma effect (95%), while in particle physics , there is a convention of requiring statistical significance of a five-sigma effect (99.99994% confidence) to qualify as a discovery .

[ 3 ] A weaker three-sigma rule can be derived from Chebyshev's inequality , stating that even for non-normally distributed variables, at least 88.8% of cases should fall within properly calculated three-sigma intervals. For unimodal distributions , the probability of being within the interval is at least 95% by the Vysochanskij–Petunin inequality . There may be certain assumptions for a distribution that force this probability to be at least 98%.

[ 4 ] Proof [ edit ] We have that Pr ( μ μ − − n σ σ ≤ ≤ X ≤ ≤ μ μ + n σ σ ) = ∫ ∫ μ μ − − n σ σ μ μ + n σ σ 1 2 π π σ σ e − − 1 2 ( x − − μ μ σ σ ) 2 d x , {\displaystyle {\begin{aligned}\Pr(\mu -n\sigma \leq X\leq \mu +n\sigma )=\int _{\mu -n\sigma }^{\mu +n\sigma }{\frac {1}{{\sqrt {2\pi }}\sigma }}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}dx,\end{aligned}}} doing the change of variable in terms of the standard score z = x − − μ μ σ σ {\displaystyle z={\frac {x-\mu }{\sigma }}} , we have 1 2 π π ∫ ∫ − − n n e − − z 2 2 d z , {\displaystyle {\begin{aligned}{\frac {1}{\sqrt {2\pi }}}\int _{-n}^{n}e^{-{\frac {z^{2}}{2}}}dz\end{aligned}},} and this integral is independent of μ μ {\displaystyle \mu } and σ σ {\displaystyle \sigma } . We only need to calculate each integral for the cases n = 1 , 2 , 3 {\displaystyle n=1,2,3} .

Pr ( μ μ − − 1 σ σ ≤ ≤ X ≤ ≤ μ μ + 1 σ σ ) = 1 2 π π ∫ ∫ − − 1 1 e − − z 2 2 d z ≈ ≈ 0.6826894921 Pr ( μ μ − − 2 σ σ ≤ ≤ X ≤ ≤ μ μ + 2 σ σ ) = 1 2 π π ∫ ∫ − − 2 2 e − − z 2 2 d z ≈ ≈ 0.9544997361 Pr ( μ μ − − 3 σ σ ≤ ≤ X ≤ ≤ μ μ + 3 σ σ ) = 1 2 π π ∫ ∫ − − 3 3 e − − z 2 2 d z ≈ ≈ 0.9973002039.

{\displaystyle {\begin{aligned}\Pr(\mu -1\sigma \leq X\leq \mu +1\sigma )&={\frac {1}{\sqrt {2\pi }}}\int _{-1}^{1}e^{-{\frac {z^{2}}{2}}}dz\approx 0.6826894921\\\Pr(\mu -2\sigma \leq X\leq \mu +2\sigma )&={\frac {1}{\sqrt {2\pi }}}\int _{-2}^{2}e^{-{\frac {z^{2}}{2}}}dz\approx 0.9544997361\\\Pr(\mu -3\sigma \leq X\leq \mu +3\sigma )&={\frac {1}{\sqrt {2\pi }}}\int _{-3}^{3}e^{-{\frac {z^{2}}{2}}}dz\approx 0.9973002039.\end{aligned}}} Cumulative distribution function [ edit ] Main article: Prediction interval § Known mean, known variance Diagram showing the cumulative distribution function for the normal distribution with mean ( μ ) 0 and variance ( σ 2 ) 1 These numerical values "68%, 95%, 99.7%" come from the cumulative distribution function of the normal distribution .

The prediction interval for any standard score z corresponds numerically to (1 − (1 − Φ μ , σ 2 (z)) · 2) .

For example, Φ (2) ≈ 0.9772 , or Pr( X ≤ μ + 2 σ ) ≈ 0.9772 , corresponding to a prediction interval of (1 − (1 − 0.97725)·2) = 0.9545 = 95.45% .
This is not a symmetrical interval – this is merely the probability that an observation is less than μ + 2 σ . To compute the probability that an observation is within two standard deviations of the mean (small differences due to rounding): Pr ( μ μ − − 2 σ σ ≤ ≤ X ≤ ≤ μ μ + 2 σ σ ) = Φ Φ ( 2 ) − − Φ Φ ( − − 2 ) ≈ ≈ 0.9772 − − ( 1 − − 0.9772 ) ≈ ≈ 0.9545 {\displaystyle \Pr(\mu -2\sigma \leq X\leq \mu +2\sigma )=\Phi (2)-\Phi (-2)\approx 0.9772-(1-0.9772)\approx 0.9545} This is related to confidence interval as used in statistics: X ¯ ¯ ± ± 2 σ σ n {\displaystyle {\bar {X}}\pm 2{\frac {\sigma }{\sqrt {n}}}} is approximately a 95% confidence interval when X ¯ ¯ {\displaystyle {\bar {X}}} is the average of a sample of size n {\displaystyle n} .

Normality tests [ edit ] Main article: Normality test The "68–95–99.7 rule" is often used to quickly get a rough probability estimate of something, given its standard deviation, if the population is assumed to be normal.  It is also used as a simple test for outliers if the population is assumed normal, and as a normality test if the population is potentially not normal.

To pass from a sample to a number of standard deviations, one first computes the deviation , either the error or residual depending on whether one knows the population mean or only estimates it.  The next step is standardizing (dividing by the population standard deviation), if the population parameters are known, or studentizing (dividing by an estimate of the standard deviation), if the parameters are unknown and only estimated.

To use as a test for outliers or a normality test, one computes the size of deviations in terms of standard deviations, and compares this to expected frequency. Given a sample set, one can compute the studentized residuals and compare these to the expected frequency: points that fall more than 3 standard deviations from the norm are likely outliers (unless the sample size is significantly large, by which point one expects a sample this extreme), and if there are many points more than 3 standard deviations from the norm, one likely has reason to question the assumed normality of the distribution. This holds ever more strongly for moves of 4 or more standard deviations.

One can compute more precisely, approximating the number of extreme moves of a given magnitude or greater by a Poisson distribution , but simply, if one has multiple 4 standard deviation moves in a sample of size 1,000, one has strong reason to consider these outliers or question the assumed normality of the distribution.

For example, a 6 σ event corresponds to a chance of about two parts per billion . For illustration, if events are taken to occur daily, this would correspond to an event expected every 1.4 million years.  This gives a simple normality test : if one witnesses a 6 σ in daily data and significantly fewer than 1 million years have passed, then a normal distribution most likely does not provide a good model for the magnitude or frequency of large deviations in this respect.

In The Black Swan , Nassim Nicholas Taleb gives the example of risk models according to which the Black Monday crash would correspond to a 36- σ event:
the occurrence of such an event should instantly suggest that the model is flawed, i.e. that the process under consideration is not satisfactorily modeled by a normal distribution. Refined models should then be considered, e.g. by the introduction of stochastic volatility .  In such discussions it is important to be aware of the problem of the gambler's fallacy , which states that a single observation of a rare event does not contradict that the event is in fact rare. It is the observation of a plurality of purportedly rare events that increasingly undermines the hypothesis that they are rare, i.e. the validity of the assumed model. A proper modelling of this process of gradual loss of confidence in a hypothesis would involve the designation of prior probability not just to the hypothesis itself but to all possible alternative hypotheses. For this reason, statistical hypothesis testing works not so much by confirming a hypothesis considered to be likely, but by refuting hypotheses considered unlikely .

Table of numerical values [ edit ] Because of the exponentially decreasing tails of the normal distribution, odds of higher deviations decrease very quickly. From the rules for normally distributed data for a daily event: Range Expected fraction of population inside range Expected fraction of population outside range Approx. expected frequency outside range Approx. frequency outside range for daily event μ ± 0.5 σ 0.382 924 922 548 026 0.6171 = 61.71 % 3 in 5 Four or five times a week μ ± σ 0.682 689 492 137 086 [ 5 ] 0.3173 = 31.73 % 1 in 3 Twice or thrice a week μ ± 1.5 σ 0.866 385 597 462 284 0.1336 = 13.36 % 2 in 15 Weekly μ ± 2 σ 0.954 499 736 103 642 [ 6 ] 0.045 50 = 4.550 % 1 in 22 Every three weeks μ ± 2.5 σ 0.987 580 669 348 448 0.012 42 = 1.242 % 1 in 81 Quarterly μ ± 3 σ 0.997 300 203 936 740 [ 7 ] 0.002 700 = 0.270 % = 2.700 ‰ 1 in 370 Yearly μ ± 3.5 σ 0.999 534 741 841 929 0.000 4653 = 0.04653 % = 465.3 ppm 1 in 2149 Every 6 years μ ± 4 σ 0.999 936 657 516 334 6.334 × 10 −5 = 63.34 ppm 1 in 15 787 Every 43 years (twice in a lifetime) μ ± 4.5 σ 0.999 993 204 653 751 6.795 × 10 −6 = 6.795 ppm 1 in 147 160 Every 403 years (once in the modern era ) μ ± 5 σ 0.999 999 426 696 856 5.733 × 10 −7 = 0.5733 ppm = 573.3 ppb 1 in 1 744 278 Every 4776 years (once in recorded history ) μ ± 5.5 σ 0.999 999 962 020 875 3.798 × 10 −8 = 37.98 ppb 1 in 26 330 254 Every 72 090 years (thrice in history of modern humankind ) μ ± 6 σ 0.999 999 998 026 825 1.973 × 10 −9 = 1.973 ppb 1 in 506 797 346 Every 1.38 million years (twice in history of humankind ) μ ± 6.5 σ 0.999 999 999 919 680 8.032 × 10 −11 = 0.080 32 ppb = 80.32 ppt 1 in 12 450 197 393 Every 34 million years (twice since the extinction of dinosaurs ) μ ± 7 σ 0.999 999 999 997 440 2.560 × 10 −12 = 2.560 ppt 1 in 390 682 215 445 Every 1.07 billion years (four occurrences in history of Earth ) μ ± 7.5 σ 0.999 999 999 999 936 6.382 × 10 −14 = 63.82 ppq 1 in 15 669 601 204 101 Once every 43 billion years (never in the history of the Universe , twice in the future of the Local Group before its merger) μ ± 8 σ 0.999 999 999 999 999 1.244 × 10 −15 = 1.244 ppq 1 in 803 734 397 655 348 Once every 2.2 trillion years (never in the history of the Universe , once during the life of a red dwarf ) μ ± x σ erf ⁡ ⁡ ( x 2 ) {\displaystyle \operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)} 1 − − erf ⁡ ⁡ ( x 2 ) {\displaystyle 1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)} 1 in 1 1 − − erf ⁡ ⁡ ( x 2 ) {\displaystyle {\tfrac {1}{1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}}} Every 1 1 − − erf ⁡ ⁡ ( x 2 ) {\displaystyle {\tfrac {1}{1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}}} days See also [ edit ] p -value Six Sigma § Sigma levels Standard score t -statistic References [ edit ] ^ Huber, Franz (2018).

A Logical Introduction to Probability and Induction . New York: Oxford University Press . p. 80.

ISBN 9780190845414 .

^ This usage of "three-sigma rule" entered common usage in the 2000s, e.g. cited in Schaum's Outline of Business Statistics . McGraw Hill Professional. 2003. p. 359.

ISBN 9780071398763 Grafarend, Erik W. (2006).

Linear and Nonlinear Models: Fixed Effects, Random Effects, and Mixed Models . Walter de Gruyter. p.

553 .

ISBN 9783110162165 .

^ Lyons, Louis (October 7, 2013). "DISCOVERING THE SIGIFICANCE OF 5σ".

arXiv : 1310.1284 [ physics.data-an ].

^ See: Wheeler, D. J.; Chambers, D. S. (1992).

Understanding Statistical Process Control . SPC Press.

ISBN 9780945320135 .

Czitrom, Veronica ; Spagon, Patrick D. (1997).

Statistical Case Studies for Industrial Process Improvement . SIAM. p. 342.

ISBN 9780898713947 .

Pukelsheim, F. (1994). "The Three Sigma Rule".

American Statistician .

48 (2): 88– 91.

doi : 10.2307/2684253 .

JSTOR 2684253 .

^ Sloane, N. J. A.

(ed.).

"Sequence A178647" .

The On-Line Encyclopedia of Integer Sequences . OEIS Foundation.

^ Sloane, N. J. A.

(ed.).

"Sequence A110894" .

The On-Line Encyclopedia of Integer Sequences . OEIS Foundation.

^ Sloane, N. J. A.

(ed.).

"Sequence A270712" .

The On-Line Encyclopedia of Integer Sequences . OEIS Foundation.

External links [ edit ] " Calculate percentage proportion within x sigmas at WolframAlpha v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐s7t9m
Cached time: 20250812014922
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.544 seconds
Real time usage: 0.737 seconds
Preprocessor visited node count: 2629/1000000
Revision size: 20841/2097152 bytes
Post‐expand include size: 117631/2097152 bytes
Template argument size: 2459/2097152 bytes
Highest expansion depth: 11/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 46633/5000000 bytes
Lua time usage: 0.329/10.000 seconds
Lua memory usage: 8767108/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  552.549      1 -total
 29.36%  162.225      1 Template:Reflist
 18.91%  104.503      4 Template:Navbox
 18.68%  103.217      1 Template:ProbDistributions
 18.50%  102.206      5 Template:Cite_book
 14.44%   79.773     55 Template:Val
 12.36%   68.313      1 Template:Short_description
 10.83%   59.860      1 Template:More_citations_needed
 10.07%   55.622      1 Template:Ambox
  7.87%   43.497      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:9606881:|#|:idhash:canonical and timestamp 20250812014922 and revision id 1303210293. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=68–95–99.7_rule&oldid=1303210293#Three-sigma_rule " Categories : Normal distribution Statistical approximations Rules of thumb Hidden categories: Articles with short description Short description matches Wikidata Articles needing additional references from September 2023 All articles needing additional references This page was last edited on 29 July 2025, at 17:37 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents 68–95–99.7 rule 18 languages Add topic

