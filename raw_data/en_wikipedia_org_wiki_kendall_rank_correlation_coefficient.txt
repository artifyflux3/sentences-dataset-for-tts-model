Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Properties 2 Hypothesis test 3 Case of standard normal distributions 4 Accounting for ties Toggle Accounting for ties subsection 4.1 Tau-a 4.2 Tau-b 4.3 Tau-c 5 Significance tests 6 Algorithms 7 Approximating Kendall rank correlation from a stream 8 Software implementations 9 See also 10 References 11 Further reading 12 External links Toggle the table of contents Kendall rank correlation coefficient 13 languages العربية Deutsch Español Euskara فارسی Français Nederlands 日本語 Polski Português Suomi Українська 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistic for rank correlation "Tau-a" redirects here. For the astronomical radio source, see Taurus A .

"Tau coefficient" redirects here; not to be confused with Tau distribution .

In statistics , the Kendall rank correlation coefficient , commonly referred to as Kendall's τ coefficient (after the Greek letter τ , tau), is a statistic used to measure the ordinal association between two measured quantities. A τ test is a non-parametric hypothesis test for statistical dependence based on the τ coefficient. It is a measure of rank correlation : the similarity of the orderings of the data when ranked by each of the quantities. It is named after Maurice Kendall , who developed it in 1938, [ 1 ] though Gustav Fechner had proposed a similar measure in the context of time series in 1897.

[ 2 ] Intuitively, the Kendall correlation between two variables will be high when observations have a similar or identical rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar or fully reversed rank between the two variables.

Both Kendall's τ τ {\displaystyle \tau } and Spearman's ρ ρ {\displaystyle \rho } can be formulated as special cases of a more general correlation coefficient . Its notions of concordance and discordance also appear in other areas of statistics, like the Rand index in cluster analysis .

Definition [ edit ] All points in the gray area are concordant and all points in the white area are discordant with respect to point ( X 1 , Y 1 ) {\displaystyle (X_{1},Y_{1})} . With n = 30 {\displaystyle n=30} points, there are a total of ( 30 2 ) = 435 {\displaystyle {\binom {30}{2}}=435} possible point pairs. In this example there are 395 concordant point pairs and 40 discordant point pairs, leading to a Kendall rank correlation coefficient of 0.816.

Let ( x 1 , y 1 ) , .

.

.

, ( x n , y n ) {\displaystyle (x_{1},y_{1}),...,(x_{n},y_{n})} be a set of observations of the joint random variables X and Y , such that all the values of ( x i {\displaystyle x_{i}} ) and ( y i {\displaystyle y_{i}} ) are unique. (See the section #Accounting for ties for ways of handling non-unique values.) Any pair of  observations ( x i , y i ) {\displaystyle (x_{i},y_{i})} and ( x j , y j ) {\displaystyle (x_{j},y_{j})} , where i < j {\displaystyle i<j} , are said to be concordant if the sort order of ( x i , x j ) {\displaystyle (x_{i},x_{j})} and ( y i , y j ) {\displaystyle (y_{i},y_{j})} agrees: that is, if either both x i > x j {\displaystyle x_{i}>x_{j}} and y i > y j {\displaystyle y_{i}>y_{j}} holds or  both x i < x j {\displaystyle x_{i}<x_{j}} and y i < y j {\displaystyle y_{i}<y_{j}} ; otherwise they are said to be discordant .

In the absence of ties, the Kendall τ coefficient is defined as: τ τ = ( number of concordant pairs ) − − ( number of discordant pairs ) ( number of pairs ) = 1 − − 2 ( number of discordant pairs ) ( n 2 ) .

{\displaystyle \tau ={\frac {({\text{number of concordant pairs}})-({\text{number of discordant pairs}})}{({\text{number of pairs}})}}=1-{\frac {2({\text{number of discordant pairs}})}{n \choose 2}}.} [ 3 ] for i < j < n {\displaystyle i<j<n} where ( n 2 ) = n ( n − − 1 ) 2 {\displaystyle {n \choose 2}={n(n-1) \over 2}} is the binomial coefficient for the number of ways to choose two items from n items.

The number of discordant pairs is equal to the inversion number that permutes the y-sequence into the same order as the x-sequence.

Properties [ edit ] The denominator is the total number of pair combinations, so the coefficient must be in the range −1 ≤ τ ≤ 1.

If the agreement between the two rankings is perfect (i.e., the two rankings are the same) the coefficient has value 1.

If the disagreement between the two rankings is perfect (i.e., one ranking is the reverse of the other) the coefficient has value −1.

If X and Y are independent random variables and not constant, then the expectation of the coefficient is zero.

An explicit expression for Kendall's rank coefficient is τ τ = 2 n ( n − − 1 ) ∑ ∑ i < j sgn ⁡ ⁡ ( x i − − x j ) sgn ⁡ ⁡ ( y i − − y j ) {\displaystyle \tau ={\frac {2}{n(n-1)}}\sum _{i<j}\operatorname {sgn}(x_{i}-x_{j})\operatorname {sgn}(y_{i}-y_{j})} .

Hypothesis test [ edit ] The Kendall rank coefficient is often used as a test statistic in a statistical hypothesis test to establish whether two variables may be regarded as statistically dependent. This test is non-parametric , as it does not rely on any assumptions on the distributions of X or Y or the distribution of ( X , Y ).

Under the null hypothesis of independence of X and Y , the sampling distribution of τ has an expected value of zero. The precise distribution cannot be characterized in terms of common distributions, but may be calculated exactly for small samples; for larger samples, it is common to use an approximation to the normal distribution , with mean zero and variance 2 ( 2 n + 5 ) / 9 n ( n − − 1 ) {\textstyle 2(2n+5)/9n(n-1)} .

[ 4 ] Theorem.

If the samples are independent, then the variance of τ τ A {\textstyle \tau _{A}} is given by V a r [ τ τ A ] = 2 ( 2 n + 5 ) / 9 n ( n − − 1 ) {\textstyle Var[\tau _{A}]=2(2n+5)/9n(n-1)} .

Proof Proof Valz & McLeod (1990; [ 5 ] 1995 [ 6 ] ) WLOG, we reorder the data pairs, so that x 1 < x 2 < ⋯ ⋯ < x n {\textstyle x_{1}<x_{2}<\cdots <x_{n}} . By assumption of independence, the order of y 1 , .

.

.

, y n {\textstyle y_{1},...,y_{n}} is a permutation sampled uniformly at random from S n {\textstyle S_{n}} , the permutation group on 1 : n {\textstyle 1:n} .

For each permutation, its unique l {\textstyle l} inversion code is l 0 l 1 ⋯ ⋯ l n − − 1 {\textstyle l_{0}l_{1}\cdots l_{n-1}} such that each l i {\textstyle l_{i}} is in the range 0 : i {\textstyle 0:i} . Sampling a permutation uniformly is equivalent to sampling a l {\textstyle l} -inversion code uniformly, which is equivalent to sampling each l i {\textstyle l_{i}} uniformly and independently.

Then we have E [ τ τ A 2 ] = E [ ( 1 − − 4 ∑ ∑ i l i n ( n − − 1 ) ) 2 ] = 1 − − 8 n ( n − − 1 ) ∑ ∑ i E [ l i ] + 16 n 2 ( n − − 1 ) 2 ∑ ∑ i j E [ l i l j ] = 1 − − 8 n ( n − − 1 ) ∑ ∑ i E [ l i ] + 16 n 2 ( n − − 1 ) 2 ( ∑ ∑ i j E [ l i ] E [ l j ] + ∑ ∑ i V [ l i ] ) = 1 − − 8 n ( n − − 1 ) ∑ ∑ i E [ l i ] + 16 n 2 ( n − − 1 ) 2 ∑ ∑ i j E [ l i ] E [ l j ] + 16 n 2 ( n − − 1 ) 2 ( ∑ ∑ i V [ l i ] ) = ( 1 − − 4 ∑ ∑ i E [ l i ] n ( n − − 1 ) ) 2 + 16 n 2 ( n − − 1 ) 2 ( ∑ ∑ i V [ l i ] ) {\displaystyle {\begin{aligned}E[\tau _{A}^{2}]&=E\left[\left(1-{\frac {4\sum _{i}l_{i}}{n(n-1)}}\right)^{2}\right]\\&=1-{\frac {8}{n(n-1)}}\sum _{i}E[l_{i}]+{\frac {16}{n^{2}(n-1)^{2}}}\sum _{ij}E[l_{i}l_{j}]\\&=1-{\frac {8}{n(n-1)}}\sum _{i}E[l_{i}]+{\frac {16}{n^{2}(n-1)^{2}}}\left(\sum _{ij}E[l_{i}]E[l_{j}]+\sum _{i}V[l_{i}]\right)\\&=1-{\frac {8}{n(n-1)}}\sum _{i}E[l_{i}]+{\frac {16}{n^{2}(n-1)^{2}}}\sum _{ij}E[l_{i}]E[l_{j}]+{\frac {16}{n^{2}(n-1)^{2}}}\left(\sum _{i}V[l_{i}]\right)\\&=\left(1-{\frac {4\sum _{i}E[l_{i}]}{n(n-1)}}\right)^{2}+{\frac {16}{n^{2}(n-1)^{2}}}\left(\sum _{i}V[l_{i}]\right)\end{aligned}}} The first term is just E [ τ τ A ] 2 = 0 {\textstyle E[\tau _{A}]^{2}=0} . The second term can be calculated by noting that l i {\textstyle l_{i}} is a uniform random variable on 0 : i {\textstyle 0:i} , so E [ l i ] = i 2 {\textstyle E[l_{i}]={\frac {i}{2}}} and E [ l i 2 ] = 0 2 + ⋯ ⋯ + i 2 i + 1 = i ( 2 i + 1 ) 6 {\textstyle E[l_{i}^{2}]={\frac {0^{2}+\cdots +i^{2}}{i+1}}={\frac {i(2i+1)}{6}}} , then using the sum of squares formula again.

Asymptotic normality — At the n → → ∞ ∞ {\textstyle n\to \infty } limit, z A = τ τ A V a r [ τ τ A ] = n C − − n D n ( n − − 1 ) ( 2 n + 5 ) / 18 {\textstyle z_{A}={\frac {\tau _{A}}{\sqrt {Var[\tau _{A}]}}}={n_{C}-n_{D} \over {\sqrt {n(n-1)(2n+5)/18}}}} converges in distribution to the standard normal distribution.

Proof Use a result from A class of statistics with asymptotically normal distribution Hoeffding (1948).

[ 7 ] Case of standard normal distributions [ edit ] If ( x 1 , y 1 ) , ( x 2 , y 2 ) , .

.

.

, ( x n , y n ) {\textstyle (x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})} are independent and identically distributed samples from the same jointly normal distribution with a known Pearson correlation coefficient r {\textstyle r} , then the expectation of Kendall rank correlation has a closed-form formula.

[ 8 ] Greiner's equality — If X , Y {\textstyle X,Y} are jointly normal, with correlation r {\textstyle r} , then r = sin ⁡ ⁡ ( π π 2 E [ τ τ A ] ) {\displaystyle r=\sin {\left({\frac {\pi }{2}}E[\tau _{A}]\right)}} The name is credited to Richard Greiner (1909) [ 9 ] by P. A. P. Moran .

[ 10 ] Proof Proof [ 11 ] Define the following quantities.

A + := { ( Δ Δ x , Δ Δ y ) : Δ Δ x Δ Δ y > 0 } {\textstyle A^{+}:=\{(\Delta x,\Delta y):\Delta x\Delta y>0\}} Δ Δ i , j := ( x i − − x j , y i − − y j ) {\textstyle \Delta _{i,j}:=(x_{i}-x_{j},y_{i}-y_{j})} is a point in R 2 {\textstyle \mathbb {R} ^{2}} .

In the notation, we see that the number of concordant pairs, n C {\textstyle n_{C}} , is equal to the number of Δ Δ i , j {\textstyle \Delta _{i,j}} that fall in the subset A + {\textstyle A^{+}} . That is, n C = ∑ ∑ 1 ≤ ≤ i < j ≤ ≤ n 1 Δ Δ i , j ∈ ∈ A + {\textstyle n_{C}=\sum _{1\leq i<j\leq n}1_{\Delta _{i,j}\in A^{+}}} .

Thus, E [ τ τ A ] = 4 n ( n − − 1 ) E [ n C ] − − 1 = 4 n ( n − − 1 ) ∑ ∑ 1 ≤ ≤ i < j ≤ ≤ n P r ( Δ Δ i , j ∈ ∈ A + ) − − 1 {\displaystyle E[\tau _{A}]={\frac {4}{n(n-1)}}E[n_{C}]-1={\frac {4}{n(n-1)}}\sum _{1\leq i<j\leq n}Pr(\Delta _{i,j}\in A^{+})-1} Since each ( x i , y i ) {\textstyle (x_{i},y_{i})} is an independent and identically distributed sample of the jointly normal distribution, the pairing does not matter, so each term in the summation is exactly the same, and so E [ τ τ A ] = 2 P r ( Δ Δ 1 , 2 ∈ ∈ A + ) − − 1 {\displaystyle E[\tau _{A}]=2Pr(\Delta _{1,2}\in A^{+})-1} and it remains to calculate the probability. We perform this by repeated affine transforms.

First normalize X , Y {\textstyle X,Y} by subtracting the mean and dividing the standard deviation. This does not change τ τ A {\textstyle \tau _{A}} . This gives us [ x y ] = [ 1 r r 1 ] 1 / 2 [ z w ] {\displaystyle {\begin{bmatrix}x\\y\end{bmatrix}}={\begin{bmatrix}1&r\\r&1\end{bmatrix}}^{1/2}{\begin{bmatrix}z\\w\end{bmatrix}}} where ( Z , W ) {\textstyle (Z,W)} is sampled from the standard normal distribution on R 2 {\textstyle \mathbb {R} ^{2}} .

Thus, Δ Δ 1 , 2 = 2 [ 1 r r 1 ] 1 / 2 [ ( z 1 − − z 2 ) / 2 ( w 1 − − w 2 ) / 2 ] {\displaystyle \Delta _{1,2}={\sqrt {2}}{\begin{bmatrix}1&r\\r&1\end{bmatrix}}^{1/2}{\begin{bmatrix}(z_{1}-z_{2})/{\sqrt {2}}\\(w_{1}-w_{2})/{\sqrt {2}}\end{bmatrix}}} where the vector [ ( z 1 − − z 2 ) / 2 ( w 1 − − w 2 ) / 2 ] {\textstyle {\begin{bmatrix}(z_{1}-z_{2})/{\sqrt {2}}\\(w_{1}-w_{2})/{\sqrt {2}}\end{bmatrix}}} is still distributed as the standard normal distribution on R 2 {\textstyle \mathbb {R} ^{2}} . It remains to perform some unenlightening tedious matrix exponentiations and trigonometry, which can be skipped over.

Thus, Δ Δ 1 , 2 ∈ ∈ A + {\textstyle \Delta _{1,2}\in A^{+}} iff [ ( z 1 − − z 2 ) / 2 ( w 1 − − w 2 ) / 2 ] ∈ ∈ 1 2 [ 1 r r 1 ] − − 1 / 2 A + = 1 2 2 [ 1 1 + r + 1 1 − − r 1 1 + r − − 1 1 − − r 1 1 + r − − 1 1 − − r 1 1 + r + 1 1 − − r ] A + {\displaystyle {\begin{bmatrix}(z_{1}-z_{2})/{\sqrt {2}}\\(w_{1}-w_{2})/{\sqrt {2}}\end{bmatrix}}\in {\frac {1}{\sqrt {2}}}{\begin{bmatrix}1&r\\r&1\end{bmatrix}}^{-1/2}A^{+}={\frac {1}{2{\sqrt {2}}}}{\begin{bmatrix}{\frac {1}{\sqrt {1+r}}}+{\frac {1}{\sqrt {1-r}}}&{\frac {1}{\sqrt {1+r}}}-{\frac {1}{\sqrt {1-r}}}\\{\frac {1}{\sqrt {1+r}}}-{\frac {1}{\sqrt {1-r}}}&{\frac {1}{\sqrt {1+r}}}+{\frac {1}{\sqrt {1-r}}}\end{bmatrix}}A^{+}} where the subset on the right is a “squashed” version of two quadrants. Since the standard normal distribution is rotationally symmetric, we need only calculate the angle spanned by each squashed quadrant.

The first quadrant is the sector bounded by the two rays ( 1 , 0 ) , ( 0 , 1 ) {\textstyle (1,0),(0,1)} . It is transformed to the sector bounded by the two rays ( 1 1 + r + 1 1 − − r , 1 1 + r − − 1 1 − − r ) {\textstyle ({\frac {1}{\sqrt {1+r}}}+{\frac {1}{\sqrt {1-r}}},{\frac {1}{\sqrt {1+r}}}-{\frac {1}{\sqrt {1-r}}})} and ( 1 1 + r − − 1 1 − − r , 1 1 + r + 1 1 − − r ) {\textstyle ({\frac {1}{\sqrt {1+r}}}-{\frac {1}{\sqrt {1-r}}},{\frac {1}{\sqrt {1+r}}}+{\frac {1}{\sqrt {1-r}}})} . They respectively make angle θ θ {\textstyle \theta } with the horizontal and vertical axis, where θ θ = arctan ⁡ ⁡ 1 1 + r − − 1 1 − − r 1 1 + r + 1 1 − − r {\displaystyle \theta =\arctan {\frac {{\frac {1}{\sqrt {1+r}}}-{\frac {1}{\sqrt {1-r}}}}{{\frac {1}{\sqrt {1+r}}}+{\frac {1}{\sqrt {1-r}}}}}} Together, the two transformed quadrants span an angle of π π + 4 θ θ {\textstyle \pi +4\theta } , so P r ( Δ Δ 1 , 2 ∈ ∈ A + ) = π π + 4 θ θ 2 π π {\displaystyle Pr(\Delta _{1,2}\in A^{+})={\frac {\pi +4\theta }{2\pi }}} and therefore sin ⁡ ⁡ ( π π 2 E [ τ τ A ] ) = sin ⁡ ⁡ ( 2 θ θ ) = r {\displaystyle \sin {\left({\frac {\pi }{2}}E[\tau _{A}]\right)}=\sin(2\theta )=r} Accounting for ties [ edit ] A pair { ( x i , y i ) , ( x j , y j ) } {\displaystyle \{(x_{i},y_{i}),(x_{j},y_{j})\}} is said to be tied if and only if x i = x j {\displaystyle x_{i}=x_{j}} or y i = y j {\displaystyle y_{i}=y_{j}} ; a tied pair is neither concordant nor discordant. When tied pairs arise in the data, the coefficient may be modified in a number of ways to keep it in the range [−1, 1]: Tau-a [ edit ] The Tau statistic defined by Kendall in 1938 [ 1 ] was retrospectively renamed Tau-a. It represents the strength of positive or negative association of two quantitative or ordinal variables without any adjustment for ties. It is defined as: τ τ A = n c − − n d n 0 {\displaystyle \tau _{A}={\frac {n_{c}-n_{d}}{n_{0}}}} where n c , n d and n 0 are defined as in the next section.

When ties are present, n c + n d < n 0 {\displaystyle n_{c}+n_{d}<n_{0}} and, the coefficient can never be equal to +1 or −1. Even a perfect equality of the two variables (X=Y) leads to a Tau-a < 1.

Tau-b [ edit ] The Tau-b statistic, unlike Tau-a, makes adjustments for ties.
This Tau-b was first described by Kendall in 1945 under the name Tau-w [ 12 ] as an extension of the original Tau statistic supporting ties.
Values of Tau-b range from −1 (100% negative association, or perfect disagreement) to +1 (100% positive association, or perfect agreement). In case of the absence of association, Tau-b is equal to zero.

The Kendall Tau-b coefficient is defined as : τ τ B = n c − − n d ( n 0 − − n 1 ) ( n 0 − − n 2 ) {\displaystyle \tau _{B}={\frac {n_{c}-n_{d}}{\sqrt {(n_{0}-n_{1})(n_{0}-n_{2})}}}} where n 0 = n ( n − − 1 ) / 2 n 1 = ∑ ∑ i t i ( t i − − 1 ) / 2 n 2 = ∑ ∑ j u j ( u j − − 1 ) / 2 n c = Number of concordant pairs, i.e. pairs with 0 < i < j < n where x i < x j and y i < y j or x i > x j and y i > y j n d = Number of discordant, i.e. pairs where 0 < i < j < n where x i < x j and y i > y j or x i < x j and y i > y j t i = Number of tied values in the i th group of ties for the empirical distribution of X u j = Number of tied values in the j th group of ties for the empirical distribution of Y {\displaystyle {\begin{aligned}n_{0}&=n(n-1)/2\\n_{1}&=\sum _{i}t_{i}(t_{i}-1)/2\\n_{2}&=\sum _{j}u_{j}(u_{j}-1)/2\\n_{c}&={\text{Number of concordant pairs, i.e. pairs with }}0<i<j<n{\text{ where }}x_{i}<x_{j}{\text{ and }}y_{i}<y_{j}{\text{ or }}x_{i}>x_{j}{\text{ and }}y_{i}>y_{j}\\n_{d}&={\text{Number of discordant, i.e. pairs where }}0<i<j<n{\text{ where }}x_{i}<x_{j}{\text{ and }}y_{i}>y_{j}{\text{ or }}x_{i}<x_{j}{\text{ and }}y_{i}>y_{j}\\t_{i}&={\text{Number of tied values in the }}i^{\text{th}}{\text{ group of ties for the empirical distribution of X}}\\u_{j}&={\text{Number of tied values in the }}j^{\text{th}}{\text{ group of ties for the empirical distribution of Y}}\end{aligned}}} A simple algorithm developed in BASIC computes Tau-b coefficient using an alternative formula.

[ 13 ] Be aware that some statistical packages, e.g. SPSS, use alternative formulas for computational efficiency, with double the 'usual' number of concordant and discordant pairs.

[ 14 ] Tau-c [ edit ] Tau-c (also called Stuart-Kendall Tau-c) [ 15 ] was first defined by Stuart in 1953.

[ 16 ] Contrary to Tau-b, Tau-c can be equal to +1 or −1 for non-square (i.e. rectangular) contingency tables , [ 15 ] [ 16 ] i.e. when the underlying scales of both variables have different number of possible values. For instance, if the variable X has a continuous uniform distribution between 0 and 100 and Y is a dichotomous variable equal to 1 if X ≥ 50 and 0 if X < 50, the Tau-c statistic of X and Y is equal to 1 while Tau-b is equal to 0.707. A Tau-c equal to 1 can be interpreted as the best possible positive correlation conditional to marginal distributions while a Tau-b equal to 1 can be interpreted as the perfect positive monotonic correlation where the distribution of X conditional to Y has zero variance and the distribution of Y conditional to X has zero variance so that a bijective function f with f(X)=Y exists.

The Stuart-Kendall Tau-c coefficient is defined as: [ 16 ] τ τ C = 2 ( n c − − n d ) n 2 ( m − − 1 ) m = τ τ A n − − 1 n m m − − 1 {\displaystyle \tau _{C}={\frac {2(n_{c}-n_{d})}{n^{2}{\frac {(m-1)}{m}}}}=\tau _{A}{\frac {n-1}{n}}{\frac {m}{m-1}}} where n c = Number of concordant pairs n d = Number of discordant pairs r = Number of rows of the contingency table (i.e. number of distinct x i ) c = Number of columns of the contingency table (i.e. number of distinct y i ) m = min ( r , c ) {\displaystyle {\begin{aligned}n_{c}&={\text{Number of concordant pairs}}\\n_{d}&={\text{Number of discordant pairs}}\\r&={\text{Number of rows of the contingency table (i.e. number of distinct }}x_{i}{\text{)}}\\c&={\text{Number of columns of the contingency table (i.e. number of distinct }}y_{i}{\text{)}}\\m&=\min(r,c)\end{aligned}}} Significance tests [ edit ] When two quantities are statistically dependent, the distribution of τ τ {\displaystyle \tau } is not easily characterizable in terms of known distributions. However, for τ τ A {\displaystyle \tau _{A}} the following statistic, z A {\displaystyle z_{A}} , is approximately distributed as a standard normal when the variables are statistically independent: z A = n c − − n d 1 18 v 0 {\displaystyle z_{A}={n_{c}-n_{d} \over {\sqrt {{\frac {1}{18}}v_{0}}}}} where v 0 = n ( n − − 1 ) ( 2 n + 5 ) {\displaystyle v_{0}=n(n-1)(2n+5)} .

Thus, to test whether two variables are statistically dependent, one computes z A {\displaystyle z_{A}} , and finds the cumulative probability for a standard normal distribution at − − | z A | {\displaystyle -|z_{A}|} . For a 2-tailed test, multiply that number by two to obtain the p -value. If the p -value is below a given significance level, one rejects the null hypothesis (at that significance level) that the quantities are statistically independent.

Numerous adjustments should be added to z A {\displaystyle z_{A}} when accounting for ties. The following statistic, z B {\displaystyle z_{B}} , has the same distribution as the τ τ B {\displaystyle \tau _{B}} distribution, and is again approximately equal to a standard normal distribution when the quantities are statistically independent: z B = n c − − n d v {\displaystyle z_{B}={n_{c}-n_{d} \over {\sqrt {v}}}} where v = 1 18 v 0 − − ( v t + v u ) / 18 + ( v 1 + v 2 ) v 0 = n ( n − − 1 ) ( 2 n + 5 ) v t = ∑ ∑ i t i ( t i − − 1 ) ( 2 t i + 5 ) v u = ∑ ∑ j u j ( u j − − 1 ) ( 2 u j + 5 ) v 1 = ∑ ∑ i t i ( t i − − 1 ) ∑ ∑ j u j ( u j − − 1 ) / ( 2 n ( n − − 1 ) ) v 2 = ∑ ∑ i t i ( t i − − 1 ) ( t i − − 2 ) ∑ ∑ j u j ( u j − − 1 ) ( u j − − 2 ) / ( 9 n ( n − − 1 ) ( n − − 2 ) ) {\displaystyle {\begin{array}{ccl}v&=&{\frac {1}{18}}v_{0}-(v_{t}+v_{u})/18+(v_{1}+v_{2})\\v_{0}&=&n(n-1)(2n+5)\\v_{t}&=&\sum _{i}t_{i}(t_{i}-1)(2t_{i}+5)\\v_{u}&=&\sum _{j}u_{j}(u_{j}-1)(2u_{j}+5)\\v_{1}&=&\sum _{i}t_{i}(t_{i}-1)\sum _{j}u_{j}(u_{j}-1)/(2n(n-1))\\v_{2}&=&\sum _{i}t_{i}(t_{i}-1)(t_{i}-2)\sum _{j}u_{j}(u_{j}-1)(u_{j}-2)/(9n(n-1)(n-2))\end{array}}} This is sometimes referred to as the Mann-Kendall test.

[ 17 ] Algorithms [ edit ] The direct computation of the numerator n c − − n d {\displaystyle n_{c}-n_{d}} , involves two nested iterations, as characterized by the following pseudocode: numer := 0 for i := 2..N do for j := 1..(i − 1) do numer := numer + sign(x[i] − x[j]) × sign(y[i] − y[j]) return numer Although quick to implement, this algorithm is O ( n 2 ) {\displaystyle O(n^{2})} in complexity and becomes very slow on large samples. A more sophisticated algorithm [ 18 ] built upon the Merge Sort algorithm can be used to compute the numerator in O ( n ⋅ ⋅ log ⁡ ⁡ n ) {\displaystyle O(n\cdot \log {n})} time.

Begin by ordering your data points sorting by the first quantity, x {\displaystyle x} , and secondarily (among ties in x {\displaystyle x} ) by the second quantity, y {\displaystyle y} . With this initial ordering, y {\displaystyle y} is not sorted, and the core of the algorithm consists of computing how many steps a Bubble Sort would take to sort this initial y {\displaystyle y} . An enhanced Merge Sort algorithm, with O ( n log ⁡ ⁡ n ) {\displaystyle O(n\log n)} complexity, can be applied to compute the number of swaps, S ( y ) {\displaystyle S(y)} , that would be required by a Bubble Sort to sort y i {\displaystyle y_{i}} . Then the numerator for τ τ {\displaystyle \tau } is computed as: n c − − n d = n 0 − − n 1 − − n 2 + n 3 − − 2 S ( y ) , {\displaystyle n_{c}-n_{d}=n_{0}-n_{1}-n_{2}+n_{3}-2S(y),} where n 3 {\displaystyle n_{3}} is computed like n 1 {\displaystyle n_{1}} and n 2 {\displaystyle n_{2}} , but with respect to the joint ties in x {\displaystyle x} and y {\displaystyle y} .

A Merge Sort partitions the data to be sorted, y {\displaystyle y} into two roughly equal halves, y l e f t {\displaystyle y_{\mathrm {left} }} and y r i g h t {\displaystyle y_{\mathrm {right} }} , then sorts each half recursively, and then merges the two sorted halves into a fully sorted vector. The number of Bubble Sort swaps is equal to: S ( y ) = S ( y l e f t ) + S ( y r i g h t ) + M ( Y l e f t , Y r i g h t ) {\displaystyle S(y)=S(y_{\mathrm {left} })+S(y_{\mathrm {right} })+M(Y_{\mathrm {left} },Y_{\mathrm {right} })} where Y l e f t {\displaystyle Y_{\mathrm {left} }} and Y r i g h t {\displaystyle Y_{\mathrm {right} }} are the sorted versions of y l e f t {\displaystyle y_{\mathrm {left} }} and y r i g h t {\displaystyle y_{\mathrm {right} }} , and M ( ⋅ ⋅ , ⋅ ⋅ ) {\displaystyle M(\cdot ,\cdot )} characterizes the Bubble Sort swap-equivalent for a merge operation.

M ( ⋅ ⋅ , ⋅ ⋅ ) {\displaystyle M(\cdot ,\cdot )} is computed as depicted in the following pseudo-code: function M(L[1..n], R[1..m]) is i := 1
    j := 1
    nSwaps := 0 while i ≤ n and j ≤ m do if R[j] < L[i] then nSwaps := nSwaps + n − i + 1
            j := j + 1 else i := i + 1 return nSwaps A side effect of the above steps is that you end up with both a sorted version of x {\displaystyle x} and a sorted version of y {\displaystyle y} . With these, the factors t i {\displaystyle t_{i}} and u j {\displaystyle u_{j}} used to compute τ τ B {\displaystyle \tau _{B}} are easily obtained in a single linear-time pass through the sorted arrays.

Approximating Kendall rank correlation from a stream [ edit ] Efficient algorithms for calculating the Kendall rank correlation coefficient as per the standard estimator have O ( n ⋅ ⋅ log ⁡ ⁡ n ) {\displaystyle O(n\cdot \log {n})} time complexity. However, these algorithms necessitate the availability of all data to determine observation ranks, posing a challenge in sequential data settings where observations are revealed incrementally. Fortunately, algorithms do exist to estimate the Kendall rank correlation coefficient in sequential settings.

[ 19 ] [ 20 ] These algorithms have O ( 1 ) {\displaystyle O(1)} update time and space complexity, scaling efficiently with the number of observations. Consequently, when processing a batch of n {\displaystyle n} observations, the time complexity becomes O ( n ) {\displaystyle O(n)} , while space complexity remains a constant O ( 1 ) {\displaystyle O(1)} .

The first such algorithm [ 19 ] presents an approximation to the Kendall rank correlation coefficient based on coarsening the joint distribution of the random variables. Non-stationary data is treated via a moving window approach. This algorithm [ 19 ] is simple and is able to handle discrete random variables along with continuous random variables without modification.

The second algorithm [ 20 ] is based on Hermite series estimators and utilizes an alternative estimator for the exact Kendall rank correlation coefficient i.e. for the probability of concordance minus the probability of discordance of pairs of bivariate observations. This alternative estimator also serves as an approximation to the standard estimator. This algorithm [ 20 ] is only applicable to continuous random variables, but it has demonstrated superior accuracy and potential speed gains compared to the first algorithm described, [ 19 ] along with the capability to handle non-stationary data without relying on sliding windows. An efficient implementation of the Hermite series based  approach is contained in the R package package hermiter .

[ 20 ] Software implementations [ edit ] R implements the test for τ τ B {\displaystyle \tau _{B}} cor.test(x, y, method = "kendall") in its "stats" package (also cor(x, y, method = "kendall") will work, but the latter does not return the p-value). All three versions of the coefficient are available in the "DescTools" package along with the confidence intervals: KendallTauA(x,y,conf.level=0.95) for τ τ A {\displaystyle \tau _{A}} , KendallTauB(x,y,conf.level=0.95) for τ τ B {\displaystyle \tau _{B}} , StuartTauC(x,y,conf.level=0.95) for τ τ C {\displaystyle \tau _{C}} . Fast batch estimates of the Kendall rank correlation coefficient along with sequential estimates are provided for in the package hermiter .

[ 20 ] For Python , the SciPy library implements the computation of τ τ B {\displaystyle \tau _{B}} in scipy.stats.kendalltau In Stata is implemented as ktau varlist .

See also [ edit ] Mathematics portal Correlation Kendall tau distance Kendall's W Spearman's rank correlation coefficient Goodman and Kruskal's gamma Theil–Sen estimator Mann–Whitney U test - it is equivalent to Kendall's tau correlation coefficient if one of the variables is binary.

References [ edit ] ^ a b Kendall, M. G. (1938). "A New Measure of Rank Correlation".

Biometrika .

30 ( 1– 2): 81– 89.

doi : 10.1093/biomet/30.1-2.81 .

JSTOR 2332226 .

^ Kruskal, W. H.

(1958). "Ordinal Measures of Association".

Journal of the American Statistical Association .

53 (284): 814– 861.

doi : 10.2307/2281954 .

JSTOR 2281954 .

MR 0100941 .

^ Nelsen, R.B. (2001) [1994], "Kendall tau metric" , Encyclopedia of Mathematics , EMS Press ^ Prokhorov, A.V. (2001) [1994], "Kendall coefficient of rank correlation" , Encyclopedia of Mathematics , EMS Press ^ Valz, Paul D.; McLeod, A. Ian (February 1990).

"A Simplified Derivation of the Variance of Kendall's Rank Correlation Coefficient" .

The American Statistician .

44 (1): 39– 40.

doi : 10.1080/00031305.1990.10475691 .

ISSN 0003-1305 .

^ Valz, Paul D.; McLeod, A. Ian; Thompson, Mary E. (February 1995).

"Cumulant Generating Function and Tail Probability Approximations for Kendall's Score with Tied Rankings" .

The Annals of Statistics .

23 (1): 144– 160.

doi : 10.1214/aos/1176324460 .

ISSN 0090-5364 .

^ Hoeffding, Wassily (1992), Kotz, Samuel; Johnson, Norman L. (eds.), "A Class of Statistics with Asymptotically Normal Distribution" , Breakthroughs in Statistics: Foundations and Basic Theory , Springer Series in Statistics, New York, NY: Springer, pp.

308– 334, doi : 10.1007/978-1-4612-0919-5_20 , ISBN 978-1-4612-0919-5 , retrieved 2024-01-19 ^ Kendall, M. G. (1949).

"Rank and Product-Moment Correlation" .

Biometrika .

36 (1/2): 177– 193.

doi : 10.2307/2332540 .

ISSN 0006-3444 .

JSTOR 2332540 .

PMID 18132091 .

^ Richard Greiner, (1909), Ueber das Fehlersystem der Kollektiv-maßlehre , Zeitschrift für Mathematik und Physik, Band 57, B. G. Teubner, Leipzig, pages 121-158, 225-260, 337-373.

^ Moran, P. A. P. (1948).

"Rank Correlation and Product-Moment Correlation" .

Biometrika .

35 (1/2): 203– 206.

doi : 10.2307/2332641 .

ISSN 0006-3444 .

JSTOR 2332641 .

PMID 18867425 .

^ Berger, Daniel (2016).

"A Proof of Greiner's Equality" .

SSRN Electronic Journal .

doi : 10.2139/ssrn.2830471 .

ISSN 1556-5068 .

^ Kendall, M. G. (1945).

"The Treatment of Ties in Ranking Problems" .

Biometrika .

33 (3): 239– 251.

doi : 10.2307/2332303 .

PMID 21006841 . Retrieved 12 November 2024 .

^ Alfred Brophy (1986).

"An algorithm and program for calculation of Kendall's rank correlation coefficient" (PDF) .

Behavior Research Methods, Instruments, & Computers .

18 : 45– 46.

doi : 10.3758/BF03200993 .

S2CID 62601552 .

^ IBM (2016).

IBM SPSS Statistics 24 Algorithms . IBM. p. 168 . Retrieved 31 August 2017 .

^ a b Berry, K. J.; Johnston, J. E.; Zahran, S.; Mielke, P. W. (2009).

"Stuart's tau measure of effect size for ordinal variables: Some methodological considerations" .

Behavior Research Methods .

41 (4): 1144– 1148.

doi : 10.3758/brm.41.4.1144 .

PMID 19897822 .

^ a b c Stuart, A. (1953). "The Estimation and Comparison of Strengths of Association in Contingency Tables".

Biometrika .

40 ( 1– 2): 105– 110.

doi : 10.2307/2333101 .

JSTOR 2333101 .

^ Valz, Paul D.; McLeod, A. Ian; Thompson, Mary E. (February 1995).

"Cumulant Generating Function and Tail Probability Approximations for Kendall's Score with Tied Rankings" .

The Annals of Statistics .

23 (1): 144– 160.

doi : 10.1214/aos/1176324460 .

ISSN 0090-5364 .

^ Knight, W. (1966). "A Computer Method for Calculating Kendall's Tau with Ungrouped Data".

Journal of the American Statistical Association .

61 (314): 436– 439.

doi : 10.2307/2282833 .

JSTOR 2282833 .

^ a b c d Xiao, W. (2019). "Novel Online Algorithms for Nonparametric Correlations with Application to Analyze Sensor Data".

2019 IEEE International Conference on Big Data (Big Data) . pp.

404– 412.

doi : 10.1109/BigData47090.2019.9006483 .

ISBN 978-1-7281-0858-2 .

S2CID 211298570 .

^ a b c d e Stephanou, M. and Varughese, M (2023). "Hermiter: R package for sequential nonparametric estimation".

Computational Statistics .

arXiv : 2111.14091 .

doi : 10.1007/s00180-023-01382-0 .

S2CID 244715035 .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) Further reading [ edit ] Abdi, H. (2007).

"Kendall rank correlation" (PDF) . In Salkind, N.J. (ed.).

Encyclopedia of Measurement and Statistics . Thousand Oaks (CA): Sage.

Daniel, Wayne W. (1990).

"Kendall's tau" .

Applied Nonparametric Statistics (2nd ed.). Boston: PWS-Kent. pp.

365– 377.

ISBN 978-0-534-91976-4 .

Kendall, Maurice; Gibbons, Jean Dickinson (1990) [First published 1948].

Rank Correlation Methods . Charles Griffin Book Series (5th ed.). Oxford: Oxford University Press.

ISBN 978-0195208375 .

Bonett, Douglas G.; Wright, Thomas A. (2000). "Sample size requirements for estimating Pearson, Kendall, and Spearman correlations".

Psychometrika .

65 (1): 23– 28.

doi : 10.1007/BF02294183 .

S2CID 120558581 .

External links [ edit ] Tied rank calculation Software for computing Kendall's tau on very large datasets Online software: computes Kendall's tau rank correlation v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐n29jf
Cached time: 20250812010228
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.691 seconds
Real time usage: 0.906 seconds
Preprocessor visited node count: 3460/1000000
Revision size: 32354/2097152 bytes
Post‐expand include size: 207540/2097152 bytes
Template argument size: 9702/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 105145/5000000 bytes
Lua time usage: 0.344/10.000 seconds
Lua memory usage: 6002625/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  554.334      1 -total
 41.15%  228.091      1 Template:Reflist
 30.29%  167.904     15 Template:Cite_journal
 25.40%  140.824      1 Template:Statistics
 24.77%  137.294      1 Template:Navbox_with_collapsible_groups
 15.34%   85.034      1 Template:Short_description
 10.36%   57.438     11 Template:Navbox
  9.57%   53.039      2 Template:Pagetype
  5.34%   29.627      1 Template:Hlist
  5.16%   28.582      5 Template:Cite_book Saved in parser cache with key enwiki:pcache:7287830:|#|:idhash:canonical and timestamp 20250812010228 and revision id 1298561604. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Kendall_rank_correlation_coefficient&oldid=1298561604 " Categories : Covariance and correlation Nonparametric statistics Statistical tests Independence (probability theory) Hidden categories: CS1 maint: multiple names: authors list Articles with short description Short description is different from Wikidata This page was last edited on 3 July 2025, at 07:33 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Kendall rank correlation coefficient 13 languages Add topic

