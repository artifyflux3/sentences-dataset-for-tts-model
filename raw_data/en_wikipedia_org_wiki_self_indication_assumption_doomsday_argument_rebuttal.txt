Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 The Bayesian inference of N from n under the SIA Toggle The Bayesian inference of N from n under the SIA subsection 2.1 Effect of the "unborn" on the Bayesian inference 2.2 The probabilistic bounds on N with the SIA 3 Significance of omega 4 Remarks 5 SIA intuition: the lost-property metaphor 6 Problems with the SIA 7 SIA's own doomsday argument 8 Notes 9 External links Toggle the table of contents Self-indication assumption doomsday argument rebuttal Add languages Add links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Objection to the doomsday argument This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( October 2013 ) ( Learn how and when to remove this message ) The self-indication assumption (SIA) represents one of the major objections to the doomsday argument (DA). The doomsday argument states that humanity is unlikely to survive for long, as it would imply that current humans are implausibly early in the history of humanity. The SIA states that the chance of being born is not one, but an increasing function of the number of people who will be born. Under the SIA, it is more likely to be born in a world that with a larger total population, which counteracts the DA.

History [ edit ] This objection to the doomsday argument (DA), originally by Dennis Dieks (1992), developed by Bartha and Hitchcock (1999), and expanded by Ken Olum (2001), is that the possibility of an individual existing all depends on how many humans will ever exist ( N ). If N is big, then the chance of said individual existing is higher than if only a few humans will ever exist. If the individual does exist, this is evidence that N is high. The argument is sometimes expressed in an alternative way by having the posterior marginal distribution of n based on N without explicitly invoking a non-zero chance of existing. The Bayesian inference mathematics are identical.

The name for this attack within the DA community is the "self-indication assumption" (SIA), proposed by one of its opponents, the DA-advocate Nick Bostrom . His (2000) definition reads: SIA: Given the fact that people exist, people should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.

A development of Dieks's original paper by Kopf, Krtous and Page (1994), showed that the SIA precisely cancels out the effect of the doomsday argument, and therefore, one's birth position ( n ) gives no information about the total number of humans that will exist ( N ). This conclusion of SIA is uncontroversial with modern DA-proponents, who instead question the validity of the assumption itself, not the conclusion which would follow, if the SIA were true.

The SIA has been defended by Matthew Adelstein, arguing that all alternatives to the SIA imply the soundness of the doomsday argument, and other even stranger conclusions.

[ 1 ] The Bayesian inference of N from n under the SIA [ edit ] The SIA-mathematics considers the chance of being the n th human as being conditioned on the joint probability of two separate events, both of which must be true: Being born: With marginal probability P( b ).

Being n th in line: With marginal probability (1/ N ), under the principle of indifference .

This means that the probability density function for n is concentrated at P( n = 0) = 1 - P( b ), and that for P( n > 0) the marginal distribution can be calculated from the conditional: P ( n ∣ ∣ N ) = P ( b ∣ ∣ N ) N {\displaystyle P(n\mid N)={\frac {P(b\mid N)}{N}}} Where n > 0 J. Richard Gott 's DA could be formulated similarly up to this point, where it has P( b | N ) = P( b ) = 1, producing Gott's inference of n from N . However, Dennis Dieks argues that P( b ) < 1, and that P( b | N ) rises proportionally in N (which is a SIA). This can be expressed mathematically: P ( b ∣ ∣ N ) = N c {\displaystyle P(b\mid N)={\frac {N}{c}}} Where c is a constant .

The SIA 's effect was expressed by Page et al. as Assumption 2 for the prior probability distribution , P(N): "The probability for the observer to exist somewhere in a history of length N is proportional to the probability for that history and to the number of people in that history ." [ 2 ] They note that similar assumptions had been dismissed by Leslie on the grounds that: "it seems wrong to treat ourselves as if we were once immaterial souls harbouring hopes of becoming embodied, hopes that would have been greater, the greater the number of bodies to be created." (1992) One argument given for P( b | N ) rising in N that does not create Leslie's "immaterial souls" is the possibility of being born into any of a large number of universes within a multiverse . A person can only be born into one, so the indifference principle within this (humans-across-universes) reference class would mean that the chance of being born into a particular universe is proportional to its weight in humans, N . (Echoing the weak anthropic principle .) In this framework, the chance of 'not being born' is zero, but the chance of 'not being born into this universe ' is non-zero.

Whatever the reasoning, the essential idea of the self-indication assumption is that the prior probability of birth into this universe is rising in N , and is generally considered to be proportional to N . (The following discussion assumes they are proportional so P( b | N ) = 2 P( b | 2 N ), since other functions increasing in N produce similar results.)
Therefore: P ( n ∣ ∣ N ) = 1 c {\displaystyle P(n\mid N)={\frac {1}{c}}} Where n > 0 Effect of the "unborn" on the Bayesian inference [ edit ] To clarify the exposition, Gott's vague prior N distribution is 'capped' at some "universal carrying capacity", Ω Ω {\displaystyle \Omega } . (This prevents N 's distribution being an improper prior .) Ω Ω {\displaystyle \Omega } is the largest possible value for N if all living space in the 'universe' is consumed. The Ω Ω {\displaystyle \Omega } limit has no specified upper bounds (to habitable planets in the Galaxy , say) but makes N 's posterior distribution more tractable : P ( N ) = { 1 N ln ⁡ ⁡ ( Ω Ω ) , N ≤ ≤ Ω Ω 0 , N > Ω Ω {\displaystyle P(N)=\left\{{\begin{matrix}{\frac {1}{N\ln(\Omega )}},\;\;N\leq \Omega \\0,\;\;N>\Omega \end{matrix}}\right.} The ln ⁡ ⁡ ( Ω Ω ) {\displaystyle \ln(\Omega )} factor normalizes the N 's probability, allowing calculation of the marginal P( n > 0) by integration of P( b | N ) across the [1, Ω Ω {\displaystyle \Omega } ] range of possible N: P ( n ) = ∫ ∫ N = n Ω Ω P ( n ∣ ∣ N ) P ( N ) d N = ∫ ∫ n Ω Ω [ 1 c ] 1 N ln ⁡ ⁡ ( Ω Ω ) d N {\displaystyle P(n)=\int _{N=n}^{\Omega }P(n\mid N)P(N)\,dN=\int _{n}^{\Omega }\left[{\frac {1}{c}}\right]{\frac {1}{N\ln(\Omega )}}\,dN} This range starts at n rather than 1, because n cannot be greater than N . It uses the calculation above for n 's distribution given N , and implies: P ( n ) = ln ⁡ ⁡ ( Ω Ω n ) 1 ln ⁡ ⁡ ( Ω Ω ) c {\displaystyle P(n)=\ln \left({\frac {\Omega }{n}}\right){\frac {1}{\ln(\Omega )c}}} Substituting these marginals into the conditional equation (assuming N below Ω Ω {\displaystyle \Omega } ) gives: P ( N ∣ ∣ n ) = P ( n ∣ ∣ N ) P ( N ) P ( n ) = 1 c 1 N ln ⁡ ⁡ ( Ω Ω ) ln ⁡ ⁡ ( Ω Ω ) c ln ⁡ ⁡ ( Ω Ω n ) = 1 N ln ⁡ ⁡ ( Ω Ω n ) {\displaystyle P(N\mid n)={\frac {P(n\mid N)P(N)}{P(n)}}={\frac {1}{c}}{\frac {1}{N\ln(\Omega )}}{\frac {\ln(\Omega )c}{\ln({\frac {\Omega }{n}})}}={\frac {1}{N\ln({\frac {\Omega }{n}})}}} The probabilistic bounds on N with the SIA [ edit ] The chance of doomsday before an arbitrary factor of the current population, x , is born can be inferred, by integrating the chance of N having any value above xn . (Normally x = 20.) P ( ( N > x n ) ∣ ∣ n ) = ∫ ∫ x n Ω Ω 1 N ln ⁡ ⁡ ( Ω Ω n ) d N = ln ⁡ ⁡ ( Ω Ω ) − − ln ⁡ ⁡ ( x n ) ln ⁡ ⁡ ( Ω Ω ) − − ln ⁡ ⁡ ( n ) {\displaystyle P((N>xn)\mid n)=\int _{xn}^{\Omega }{\frac {1}{N\ln({\frac {\Omega }{n}})}}\,dN={\frac {\ln(\Omega )-\ln(xn)}{\ln(\Omega )-\ln(n)}}} Therefore, given the posterior information that we have been born and that we are n th in line: For any factor, x << ( Ω Ω {\displaystyle \Omega } / n ), of the current population: lim Ω Ω → → ∞ ∞ P ( N <= x n ) = 0 {\displaystyle \lim _{\Omega \to \infty }P(N<=xn)=0} Conclusion : n provides no information about N , in an unbounded vague prior SIA universe.

Significance of omega [ edit ] Figure A : The probability of reaching one trillion people from the current (60 billion) count under the SIA , against different theoretical maxima for N . (The x-axis is the finite maximum number of people that can ever be born into the universe - the upper limit of N - plotted on a log scale.) Note that the carrying capacity is the maximum number of people in any potential universe, not just this one, but even on Earth the maximum number of people that sunlight can simultaneously support has been estimated (e.g. by Isaac Asimov ) at over a trillion; extending this for a million generations gives an upper limit over a quintillion on just this planet, and a septillion if humanity colonizes fewer than 0.001% of the galaxy's stars with similar efficiency. Under the SIA, as the potential size of N increases, the significance of the current number of births ( n ) decreases, to the point that this posterior information does not constrain the actual value of N at all.

The finite Ω Ω {\displaystyle \Omega } is essential to this solution in order to produce finite integrals. In a bounded universe, Ω Ω {\displaystyle \Omega } actually must be finite, although this is not usually an argument used by those proposing the SIA rebuttal. However, other proponents of indefinite survival of human (and posthuman ) intelligence have postulated a finite endpoint, as the (extremely high) " Omega ".

Specifying any finite upper limit, Ω Ω {\displaystyle \Omega } , was not a part of Dieks's argument, and critics of the SIA have argued that an infinite upper bound on N creates an Improper integral (or summation) in the bayesian inference on N , which is a challenge to the logic of the critique. (For example Eastmond, and Bostrom , who argues that if the SIA cannot rule out an infinite number of potential humans, it is fatally flawed.) The unbounded vague prior is scale invariant , in that the mean is arbitrary. Therefore no finite value can be selected with more than a 50% chance of being above N (the marginal distribution of N ). Olum's critique depends on such a limit existing; without this his critique is technically not applicable. Therefore it must be cautioned that the simplification here (to bound N 's distribution at Ω Ω {\displaystyle \Omega } ) omits a significant hurdle to the credibility of the self-indication assumption doomsday argument rebuttal .

Remarks [ edit ] Nick Bostrom believes that the leading candidate for doomsday argument refutation is a self-indication assumption of some kind. It is popular partly because it is a purely Bayesian argument which accepts some of the DA's premises (such as the indifference and Copernican principles). Other observations: The joint prior distribution, P( n | N ), can be manipulated to produce a wide range of links between n and N by defining various birth probabilities given N . Since this distribution must be assumed prior to evidence, any particular choice of P( b | N ) is faith -based. Many writers feel a joint distribution with no link N to n is more natural than the strong link given by the vague prior, making the DA "Irrelevant" (Page et al.) Others, such as Gott feel the opposite, and are more comfortable using the pure vague prior as the prior joint probability, with P( b | N ) = 1 at all N .

The SIA rebuttal is a very special form of the "a priori" rebuttal of the DA , and differs from that approach in being purely statistical .

If the SIA is true then the mere fact of existence leads credence to "any" theory that postulates a "high" number of conscious beings in the universe, and controversially implies that a theory which does not is unlikely to be true. (For instance, the SIA implies that N is likely to be very high, so the probability of an upcoming Armageddon is correspondingly low, which makes the Doomsday Clock 's warning of relatively imminent destruction a mistake.) Under the self-indication assumption the 'reference class' of which we are part includes a potentially vast number of the unborn (at least into this universe). In order to overturn the conventional DA calculation so completely the reservoir of souls (potential births) in the reference class must be astoundingly large. For instance, the certain-birth DA estimates the chance of reaching the trillionth ( 10 12 {\displaystyle 10^{12}} th) birth at around 5%; to shift this probability above 90% the SIA requires a potential number of humans ( Ω Ω {\displaystyle \Omega } ) in the order of 10 24 {\displaystyle 10^{24}} (a septillion births). This might be feasible physically, and is also possible within the conventional DA model (though staggeringly unlikely). However, the SIA differs from the normal DA in having the reference class include all septillion unborn potential-humans at this point in history, when only sixty billion have been born. Including unborn people in the reference class we sample from means including in the reference class things for which we can never have any evidence . This puts the SIA at odds with philosophical approaches requiring strictly falsifiable constructs, such as logical positivism .

SIA intuition: the lost-property metaphor [ edit ] It can be hard to visualize how the self-indication assumption changes the distribution because everyday cases where a null result can be returned do not change the statistics significantly. The following two examples of estimating the size of a darkened space show how the probability shift can occur: Cloak-room case : Consider a situation where someone is looking for their coat in a dark room and finding it one foot from the door; the Bayesian inference from a vague prior is that the room is less than 20 feet long (with 95% confidence).

Lost-property case : A person's lost coat has been filed somewhere in a huge lost property warehouse , and as they search through its many aisles they see that they are all filled to capacity with belongings, and are various lengths. The aisle lengths are distributed according to the vague prior, except that none are more than 100 feet long. Finally, they find their coat one foot into a dark aisle, and wonder whether that aisle is more than twenty feet long.

The Bayesian inference shifts from the cloak-room case to the lost-property case, because of the chance that the coat would not be found in the aisle it was found in, and some estimate of the aisle's dimensions. Using the SIA Bayesian inference equation with Ω Ω {\displaystyle \Omega } = 100, n = 1, x = 20 gives the chance that the aisle is above 20 feet long in the Lost-property case: Cloak-room case: The confidence that the room is shorter than 20 feet long given the position of the coat = 95% Lost-property case: The confidence that the aisle is less than 20 feet long given exactly the same information about the coat's position in it = 65% The confidence that the unseen space is longer than 20 feet is directly analogous to the confidence that the human race will become more than 20 times as numerous as it has been. Using an Ω Ω {\displaystyle \Omega } of one hundred times the current value only increases the subjective chance seven times (from 5% to 35%), but this is a very small limit for the purposes of exposition.

Problems with the SIA [ edit ] The SIA is not an assumption or axiom of Dieks' system.  In fact, as stated, the negation of the SIA is a theorem of Dieks' system.  A proposition similar to the SIA can be derived from Dieks' system, but it is necessary to revise the SIA to limit it to situations where a person does not know the date or their birth order number.  Even this related proposition is not an axiom of Dieks.  It is a theorem, derived from other fundamental assumptions.  In Dieks, they may never have been born and the end of the human race is independent of their birth order number.  A proposition related to the SIA, but not the SIA itself, can be derived from these assumptions.  Hence, no one assumes the SIA. It should be called the self-indication corollary, perhaps.

One of the most prominent objections to SIA concerns Bostrom's "presumptuous philosopher" scenario. In this example, the non-anthropic evidence has led scientists to place equal credence on two rival cosmological theories T 1 {\displaystyle T_{1}} and T 2 {\displaystyle T_{2}} . "According to T 1 {\displaystyle T_{1}} the world is very, very big but finite and there are a total of a trillion trillion observers in the cosmos. According to T 2 {\displaystyle T_{2}} , the world is very, very, very big but finite and there are a trillion trillion trillion observers." Just as the scientists are preparing to run a cheap experiment that will definitively rule out one of the two theories, they are interrupted by a philosopher who believes SIA. He argues that the experiment is a waste of effort as T 2 {\displaystyle T_{2}} is a trillion times as likely to be true as T 1 {\displaystyle T_{1}} . Bostrom finds this conclusion absurd and believes that it should thoroughly discredit SIA. An open scientific question such as the size of the universe cannot be settled, he writes, "simply by leaning back in your armchair and registering the fact that you exist." [ 3 ] In his 2022 dissertation, Joe Carlsmith replies that the problem of presumptuousness is not unique to SIA. He considers a case in which both T 1 {\displaystyle T_{1}} and T 2 {\displaystyle T_{2}} claim that the Earth sits at the center of the (finite) universe. The numbers of observers predicted by the two theories are as they were before, but this time SIA remains agnostic between the theories while SSA bets on T 1 {\displaystyle T_{1}} a trillion to one. SIA says that the number of observers in our unique position at the center of the universe is the same on either theory, so no anthropic update is needed. On the other hand, SSA reasons that we would be much less likely to find ourselves at the center of the universe if a trillion times more observers, none of whom are at the center, were added to our reference class, so it updates hard in favor of T 1 {\displaystyle T_{1}} . Carlsmith argues that SSA's presumptuousness in this case is just as bad as SIA's presumptuousness in the original scenario. Presumptuous philosopher cases therefore give us no reason to prefer SSA over SIA.

[ 4 ] SIA's own doomsday argument [ edit ] Katja Grace argues that while SIA overcomes the standard doomsday argument, when combined with an assumption of a Great Filter , SIA leads to another kind of doomsday prediction. The reasoning is as follows. In some worlds, the filter may be early—some time before the advent of a technological civilization like ours. In other worlds, the filter may be late—between the advent of technological civilization and galactic colonization. Collectively, the worlds with mostly late filters have many more instances of life at the human level of development, so SIA, together with the knowledge that we are at the human-level stage, implies we are probably in one of the worlds with a late filter. In other words, the risk of extinction is higher than we would have naively supposed.

[ 5 ] [ 6 ] [ 7 ] Notes [ edit ] ^ Adelstein, Matthew (2024).

"Alternatives to the self-indication assumption are doomed" .

Synthese .

204 : 1--17.

doi : 10.1007/s11229-024-04686-w .

^ Kopf, Tomas; Krtous, Pavel; Page, Don N. (1994), Too Soon for Doom Gloom?

, arXiv, doi : 10.48550/arXiv.gr-qc/9407002 , arXiv:gr-qc/9407002 , retrieved 2025-08-10 ^ Bostrom, Nick (2002).

Anthropic Bias: Observational Selection Effects in Science and Philosophy . New York & London: Routledge. pp.

124– 126.

ISBN 0-415-93858-9 .

^ Carlsmith, Joe (2022).

"A Stranger Priority?: Topics at the Outer Reaches of Effective Altruism" (PDF) . Retrieved March 7, 2023 .

^ Grace, Katja (Oct 2010).

Anthropic Reasoning in the Great Filter .

^ Grace, Katja (23 Mar 2010).

"SIA doomsday: The filter is ahead" .

Meteuphoric . Retrieved 13 June 2014 .

^ Hanson, Robin (22 Mar 2010).

" Very Bad News" .

Overcoming Bias . Retrieved 13 June 2014 .

External links [ edit ] 2005 version of Dennis Dieks's refutation (Requires a PostScript viewer.) Analysis of the SIA by Milan M. Ćirković (Finds no compelling reason to accept the self-indication assumption , and suggests that some of its consequences are implausible.) v t e Global catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Human extinction risk estimates Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Ransomware Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeover Existential risk from artificial intelligence Technological singularity Transhumanism Sociological Anthropogenic hazard Collapsology Doomsday argument Self-indication assumption doomsday argument rebuttal Self-referencing doomsday argument rebuttal Economic collapse Malthusian catastrophe New World Order (conspiracy theory) Nuclear holocaust cobalt famine winter Riots Social crisis Societal collapse State collapse World War III Ecological Climate change Anoxic event Biodiversity loss Mass mortality event Cascade effect Cataclysmic pole shift hypothesis Deforestation Desertification Plant or animal species extinctions Civilizational collapse Tipping points Climate sensitivity Flood basalt Global dimming Global terrestrial stilling Global warming Hypercane Ice age Ecocide Ecological collapse Environmental degradation Habitat destruction Human impact on the environment coral reefs on marine life Land degradation Land consumption Land surface effects on climate Ocean acidification Ozone depletion Resource depletion Sea level rise Supervolcano winter Verneshot Water pollution Water scarcity Earth Overshoot Day Overexploitation Overpopulation Human overpopulation Biological Extinction Extinction event Holocene extinction Human extinction List of extinction events Genetic erosion Genetic pollution Others Biodiversity loss Decline in amphibian populations Decline in insect populations Biotechnology risk Biological agent Biological warfare Bioterrorism Colony collapse disorder Defaunation Dysgenics Interplanetary contamination Pandemic Pollinator decline Overfishing Astronomical Big Crunch Big Rip Coronal mass ejection Cosmological phase transition Geomagnetic storm False vacuum decay Gamma-ray burst Heat death of the universe Proton decay Virtual black hole Impact event Asteroid impact avoidance Asteroid impact prediction Potentially hazardous object Near-Earth object winter Rogue planet Rogue star Near-Earth supernova Hypernova Micronova Solar flare Stellar collision Eschatological Buddhist Maitreya Three Ages Hindu Kalki Kali Yuga Last Judgement Second Coming 1 Enoch Daniel Abomination of desolation Prophecy of Seventy Weeks Messiah Christian Futurism Historicism Interpretations of Revelation Idealism Preterism 2 Esdras 2 Thessalonians Man of sin Katechon Antichrist Book of Revelation Events Four Horsemen of the Apocalypse Seven bowls Seven seals The Beast Two witnesses War in Heaven Whore of Babylon Great Apostasy New Earth New Jerusalem Olivet Discourse Great Tribulation Son of perdition Sheep and Goats Islamic Al-Qa'im Beast of the Earth Dhu al-Qarnayn Dhul-Suwayqatayn Dajjal Israfil Mahdi Sufyani Jewish Messiah War of Gog and Magog Third Temple Norse Zoroastrian Saoshyant Others 2011 end times prediction 2012 phenomenon Apocalypse Apocalyptic literature Apocalypticism Armageddon Blood moon prophecy Earth Changes End time Gog and Magog List of dates predicted for apocalyptic events Messianism Messianic Age Millenarianism Millennialism Premillennialism Amillennialism Postmillennialism Nemesis (hypothetical star) Nibiru cataclysm Rapture Prewrath Posttribulation rapture Resurrection of the dead Vulnerable world hypothesis World to come Fictional Alien invasion Apocalyptic and post-apocalyptic fiction List of apocalyptic and post-apocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films Zombie apocalypse Zombie Organizations Centre for the Study of Existential Risk Future of Humanity Institute Future of Life Institute Nuclear Threat Initiative General Disaster Depression Financial crisis Survivalism World portal Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios Retrieved from " https://en.wikipedia.org/w/index.php?title=Self-indication_assumption_doomsday_argument_rebuttal&oldid=1305204406 " Categories : Anthropic principle Astronomical hypotheses Doomsday scenarios Existential risk Physical cosmology Probabilistic arguments Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from October 2013 All articles lacking in-text citations This page was last edited on 10 August 2025, at 17:29 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Self-indication assumption doomsday argument rebuttal Add languages Add topic

