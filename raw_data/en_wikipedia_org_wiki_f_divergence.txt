Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Definition Toggle Definition subsection 2.1 Non-singular case 2.2 Extension to singular measures 3 Properties Toggle Properties subsection 3.1 Basic relations between f-divergences 3.2 Basic properties of f-divergences 3.3 Analytic properties 3.4 Basic variational representation 3.4.1 Example applications 3.5 Improved variational representation 3.5.1 Example applications 4 Common examples of f -divergences 5 Relations to other statistical divergences Toggle Relations to other statistical divergences subsection 5.1 Rényi divergence 5.2 Bregman divergence 5.3 Integral probability metrics 6 Financial interpretation 7 See also 8 References Toggle the table of contents f -divergence 1 language Русский Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Function that measures dissimilarity between two probability distributions This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( September 2015 ) ( Learn how and when to remove this message ) In probability theory , an f {\displaystyle f} -divergence is a certain type of function D f ( P ‖ ‖ Q ) {\displaystyle D_{f}(P\|Q)} that measures the difference between two probability distributions P {\displaystyle P} and Q {\displaystyle Q} . Many common divergences, such as KL-divergence , Hellinger distance , and total variation distance , are special cases of f {\displaystyle f} -divergence.

History [ edit ] These divergences were introduced by Alfréd Rényi [ 1 ] in the same paper where he introduced the well-known Rényi entropy . He proved that these divergences decrease in Markov processes .

f -divergences were studied further independently by Csiszár (1963) , Morimoto (1963) and Ali & Silvey (1966) and are sometimes known as Csiszár f {\displaystyle f} -divergences, Csiszár–Morimoto divergences, or Ali–Silvey distances.

Definition [ edit ] Non-singular case [ edit ] Let P {\displaystyle P} and Q {\displaystyle Q} be two probability distributions over a space Ω Ω {\displaystyle \Omega } , such that P ≪ ≪ Q {\displaystyle P\ll Q} , that is, P {\displaystyle P} is absolutely continuous with respect to Q {\displaystyle Q} (meaning Q > 0 {\displaystyle Q>0} wherever P > 0 {\displaystyle P>0} ). Then, for a convex function f : [ 0 , + ∞ ∞ ) → → ( − − ∞ ∞ , + ∞ ∞ ] {\displaystyle f:[0,+\infty )\to (-\infty ,+\infty ]} such that f ( x ) {\displaystyle f(x)} is finite for all x > 0 {\displaystyle x>0} , f ( 1 ) = 0 {\displaystyle f(1)=0} , and f ( 0 ) = lim t → → 0 + f ( t ) {\displaystyle f(0)=\lim _{t\to 0^{+}}f(t)} (which could be infinite), the f {\displaystyle f} -divergence of P {\displaystyle P} from Q {\displaystyle Q} is defined as D f ( P ∥ ∥ Q ) ≡ ≡ ∫ ∫ Ω Ω f ( d P d Q ) d Q .

{\displaystyle D_{f}(P\parallel Q)\equiv \int _{\Omega }f\left({\frac {dP}{dQ}}\right)\,dQ.} We call f {\displaystyle f} the generator of D f {\displaystyle D_{f}} .

In concrete applications, there is usually a reference distribution μ μ {\displaystyle \mu } on Ω Ω {\displaystyle \Omega } (for example, when Ω Ω = R n {\displaystyle \Omega =\mathbb {R} ^{n}} , the reference distribution is the Lebesgue measure ), such that P , Q ≪ ≪ μ μ {\displaystyle P,Q\ll \mu } , then we can use Radon–Nikodym theorem to take their probability densities p {\displaystyle p} and q {\displaystyle q} , giving D f ( P ∥ ∥ Q ) = ∫ ∫ Ω Ω f ( p ( x ) q ( x ) ) q ( x ) d μ μ ( x ) .

{\displaystyle D_{f}(P\parallel Q)=\int _{\Omega }f\left({\frac {p(x)}{q(x)}}\right)q(x)\,d\mu (x).} When there is no such reference distribution ready at hand, we can simply define μ μ = P + Q {\displaystyle \mu =P+Q} , and proceed as above. This is a useful technique in more abstract proofs.

Extension to singular measures [ edit ] The above definition can be extended to cases where P ≪ ≪ Q {\displaystyle P\ll Q} is no longer satisfied (Definition 7.1 of [ 2 ] ).

Since f {\displaystyle f} is convex, and f ( 1 ) = 0 {\displaystyle f(1)=0} , the function f ( x ) x − − 1 {\displaystyle {\frac {f(x)}{x-1}}} must be nondecreasing, so there exists f ′ ( ∞ ∞ ) := lim x → → ∞ ∞ f ( x ) / x {\displaystyle f'(\infty ):=\lim _{x\to \infty }f(x)/x} , taking value in ( − − ∞ ∞ , + ∞ ∞ ] {\displaystyle (-\infty ,+\infty ]} .

Since for any p ( x ) > 0 {\displaystyle p(x)>0} , we have lim q ( x ) → → 0 q ( x ) f ( p ( x ) q ( x ) ) = p ( x ) f ′ ( ∞ ∞ ) {\displaystyle \lim _{q(x)\to 0}q(x)f\left({\frac {p(x)}{q(x)}}\right)=p(x)f'(\infty )} , we can extend f-divergence to the P ≪ ≪ ̸ Q {\displaystyle P\not \ll Q} .

Properties [ edit ] Basic relations between f-divergences [ edit ] Linearity: D ∑ ∑ i a i f i = ∑ ∑ i a i D f i {\displaystyle D_{\sum _{i}a_{i}f_{i}}=\sum _{i}a_{i}D_{f_{i}}} given a finite sequence of nonnegative real numbers a i {\displaystyle a_{i}} and generators f i {\displaystyle f_{i}} .

D f = D g {\displaystyle D_{f}=D_{g}} iff f ( x ) = g ( x ) + c ( x − − 1 ) {\displaystyle f(x)=g(x)+c(x-1)} for some c ∈ ∈ R {\displaystyle c\in \mathbb {R} } .

Proof If f ( x ) = g ( x ) + c ( x − − 1 ) {\displaystyle f(x)=g(x)+c(x-1)} , then D f = D g {\displaystyle D_{f}=D_{g}} by definition.

Conversely, if D f − − D g = 0 {\displaystyle D_{f}-D_{g}=0} , then let h = f − − g {\displaystyle h=f-g} . For any two probability measures P , Q {\displaystyle P,Q} on the set { 0 , 1 } {\displaystyle \{0,1\}} , since D f ( P ‖ ‖ Q ) − − D g ( P ‖ ‖ Q ) = 0 {\displaystyle D_{f}(P\|Q)-D_{g}(P\|Q)=0} , we get h ( P 1 / Q 1 ) = − − Q 0 Q 1 h ( P 0 / Q 0 ) {\displaystyle h(P_{1}/Q_{1})=-{\frac {Q_{0}}{Q_{1}}}h(P_{0}/Q_{0})} Since each probability measure P , Q {\displaystyle P,Q} has one degree of freedom, we can solve P 0 Q 0 = a , P 1 Q 1 = x {\displaystyle {\frac {P_{0}}{Q_{0}}}=a,{\frac {P_{1}}{Q_{1}}}=x} for every choice of 0 < a < 1 < x {\displaystyle 0<a<1<x} .

Linear algebra yields Q 0 = x − − 1 x − − a , Q 1 = 1 − − a x − − a {\displaystyle Q_{0}={\frac {x-1}{x-a}},Q_{1}={\frac {1-a}{x-a}}} , which is a valid probability measure. Then we obtain h ( x ) = h ( a ) a − − 1 ( x − − 1 ) , h ( a ) = h ( x ) x − − 1 ( a − − 1 ) {\displaystyle h(x)={\frac {h(a)}{a-1}}(x-1),h(a)={\frac {h(x)}{x-1}}(a-1)} .

Thus h ( x ) = { c 1 ( x − − 1 ) if x > 1 , c 0 ( x − − 1 ) if 0 < x < 1 , {\displaystyle h(x)={\begin{cases}c_{1}(x-1)\quad {\text{if }}x>1,\\c_{0}(x-1)\quad {\text{if }}0<x<1,\\\end{cases}}} for some constants c 0 , c 1 {\displaystyle c_{0},c_{1}} . Plugging the formula into h ( x ) = h ( a ) a − − 1 ( x − − 1 ) {\displaystyle h(x)={\frac {h(a)}{a-1}}(x-1)} yields c 0 = c 1 {\displaystyle c_{0}=c_{1}} .

Basic properties of f-divergences [ edit ] Non-negativity : the ƒ -divergence is always positive; it is zero if the measures P and Q coincide. This follows immediately from Jensen’s inequality : D f ( P ∥ ∥ Q ) = ∫ ∫ f ( d P d Q ) d Q ≥ ≥ f ( ∫ ∫ d P d Q d Q ) = f ( 1 ) = 0.

{\displaystyle D_{f}(P\!\parallel \!Q)=\int \!f{\bigg (}{\frac {dP}{dQ}}{\bigg )}dQ\geq f{\bigg (}\int {\frac {dP}{dQ}}dQ{\bigg )}=f(1)=0.} Data-processing inequality : if κ is an arbitrary transition probability that transforms measures P and Q into P κ and Q κ correspondingly, then D f ( P ∥ ∥ Q ) ≥ ≥ D f ( P κ κ ∥ ∥ Q κ κ ) .

{\displaystyle D_{f}(P\!\parallel \!Q)\geq D_{f}(P_{\kappa }\!\parallel \!Q_{\kappa }).} The equality here holds if and only if the transition is induced from a sufficient statistic with respect to { P , Q }.

Joint convexity : for any 0 ≤ λ ≤ 1 , D f ( λ λ P 1 + ( 1 − − λ λ ) P 2 ∥ ∥ λ λ Q 1 + ( 1 − − λ λ ) Q 2 ) ≤ ≤ λ λ D f ( P 1 ∥ ∥ Q 1 ) + ( 1 − − λ λ ) D f ( P 2 ∥ ∥ Q 2 ) .

{\displaystyle D_{f}{\Big (}\lambda P_{1}+(1-\lambda )P_{2}\parallel \lambda Q_{1}+(1-\lambda )Q_{2}{\Big )}\leq \lambda D_{f}(P_{1}\!\parallel \!Q_{1})+(1-\lambda )D_{f}(P_{2}\!\parallel \!Q_{2}).} This follows from the convexity of the mapping ( p , q ) ↦ ↦ q f ( p / q ) {\displaystyle (p,q)\mapsto qf(p/q)} on R + 2 {\displaystyle \mathbb {R} _{+}^{2}} .

Reversal by convex inversion : for any function f {\displaystyle f} , its convex inversion is defined as g ( t ) := t f ( 1 / t ) {\displaystyle g(t):=tf(1/t)} . When f {\displaystyle f} satisfies the defining features of a f-divergence generator ( f ( x ) {\displaystyle f(x)} is finite for all x > 0 {\displaystyle x>0} , f ( 1 ) = 0 {\displaystyle f(1)=0} , and f ( 0 ) = lim t → → 0 + f ( t ) {\displaystyle f(0)=\lim _{t\to 0^{+}}f(t)} ), then g {\displaystyle g} satisfies the same features, and thus defines a f-divergence D g {\displaystyle D_{g}} . This is the "reverse" of D f {\displaystyle D_{f}} , in the sense that D g ( P ‖ ‖ Q ) = D f ( Q ‖ ‖ P ) {\displaystyle D_{g}(P\|Q)=D_{f}(Q\|P)} for all P , Q {\displaystyle P,Q} that are absolutely continuous with respect to each other.

In this way, every f-divergence D f {\displaystyle D_{f}} can be turned symmetric by D 1 2 ( f + g ) {\displaystyle D_{{\frac {1}{2}}(f+g)}} . For example, performing this symmetrization turns KL-divergence into Jeffreys divergence.

In particular, the monotonicity implies that if a Markov process has a positive equilibrium probability distribution P ∗ ∗ {\displaystyle P^{*}} then D f ( P ( t ) ∥ ∥ P ∗ ∗ ) {\displaystyle D_{f}(P(t)\parallel P^{*})} is a monotonic (non-increasing) function of time, where the probability distribution P ( t ) {\displaystyle P(t)} is a solution of the Kolmogorov forward equations (or Master equation ), used to describe the time evolution of the probability distribution in the Markov process. This means that all f -divergences D f ( P ( t ) ∥ ∥ P ∗ ∗ ) {\displaystyle D_{f}(P(t)\parallel P^{*})} are the Lyapunov functions of the Kolmogorov forward equations. The converse statement is also true:  If H ( P ) {\displaystyle H(P)} is a Lyapunov function   for all   Markov chains with positive equilibrium P ∗ ∗ {\displaystyle P^{*}} and is of the trace-form
( H ( P ) = ∑ ∑ i f ( P i , P i ∗ ∗ ) {\displaystyle H(P)=\sum _{i}f(P_{i},P_{i}^{*})} ) then H ( P ) = D f ( P ( t ) ∥ ∥ P ∗ ∗ ) {\displaystyle H(P)=D_{f}(P(t)\parallel P^{*})} , for some convex function f .

[ 3 ] [ 4 ] For example, Bregman divergences in general do not have such property and can increase in Markov processes.

[ 5 ] Analytic properties [ edit ] The f-divergences can be expressed using Taylor series and rewritten using a weighted sum of chi-type distances ( Nielsen & Nock (2013) ).

Basic variational representation [ edit ] Let f ∗ ∗ {\displaystyle f^{*}} be the convex conjugate of f {\displaystyle f} . Let e f f d o m ( f ∗ ∗ ) {\displaystyle \mathrm {effdom} (f^{*})} be the effective domain of f ∗ ∗ {\displaystyle f^{*}} , that is, e f f d o m ( f ∗ ∗ ) = { y : f ∗ ∗ ( y ) < ∞ ∞ } {\displaystyle \mathrm {effdom} (f^{*})=\{y:f^{*}(y)<\infty \}} . Then we have two variational representations of D f {\displaystyle D_{f}} , which we describe below.

Under the above setup, Theorem — D f ( P ; Q ) = sup g : Ω Ω → → e f f d o m ( f ∗ ∗ ) E P [ g ] − − E Q [ f ∗ ∗ ∘ ∘ g ] {\displaystyle D_{f}(P;Q)=\sup _{g:\Omega \to \mathrm {effdom} (f^{*})}E_{P}[g]-E_{Q}[f^{*}\circ g]} .

This is Theorem 7.24 in.

[ 2 ] Example applications [ edit ] Using this theorem on total variation distance, with generator f ( x ) = 1 2 | x − − 1 | , {\displaystyle f(x)={\frac {1}{2}}|x-1|,} its convex conjugate is f ∗ ∗ ( x ∗ ∗ ) = { x ∗ ∗ on [ − − 1 / 2 , 1 / 2 ] , + ∞ ∞ else.

{\displaystyle f^{*}(x^{*})={\begin{cases}x^{*}{\text{ on }}[-1/2,1/2],\\+\infty {\text{ else.}}\end{cases}}} , and we obtain T V ( P ‖ ‖ Q ) = sup | g | ≤ ≤ 1 / 2 E P [ g ( X ) ] − − E Q [ g ( X ) ] .

{\displaystyle TV(P\|Q)=\sup _{|g|\leq 1/2}E_{P}[g(X)]-E_{Q}[g(X)].} For chi-squared divergence, defined by f ( x ) = ( x − − 1 ) 2 , f ∗ ∗ ( y ) = y 2 / 4 + y {\displaystyle f(x)=(x-1)^{2},f^{*}(y)=y^{2}/4+y} , we obtain χ χ 2 ( P ; Q ) = sup g E P [ g ( X ) ] − − E Q [ g ( X ) 2 / 4 + g ( X ) ] .

{\displaystyle \chi ^{2}(P;Q)=\sup _{g}E_{P}[g(X)]-E_{Q}[g(X)^{2}/4+g(X)].} Since the variation term is not affine-invariant in g {\displaystyle g} , even though the domain over which g {\displaystyle g} varies is affine-invariant, we can use up the affine-invariance to obtain a leaner expression.

Replacing g {\displaystyle g} by a g + b {\displaystyle ag+b} and taking the maximum over a , b ∈ ∈ R {\displaystyle a,b\in \mathbb {R} } , we obtain χ χ 2 ( P ; Q ) = sup g ( E P [ g ( X ) ] − − E Q [ g ( X ) ] ) 2 V a r Q [ g ( X ) ] , {\displaystyle \chi ^{2}(P;Q)=\sup _{g}{\frac {(E_{P}[g(X)]-E_{Q}[g(X)])^{2}}{Var_{Q}[g(X)]}},} which is just a few steps away from the Hammersley–Chapman–Robbins bound and the Cramér–Rao bound (Theorem 29.1 and its corollary in [ 2 ] ).

For α α {\displaystyle \alpha } -divergence with α α ∈ ∈ ( − − ∞ ∞ , 0 ) ∪ ∪ ( 0 , 1 ) {\displaystyle \alpha \in (-\infty ,0)\cup (0,1)} , we have f α α ( x ) = x α α − − α α x − − ( 1 − − α α ) α α ( α α − − 1 ) {\displaystyle f_{\alpha }(x)={\frac {x^{\alpha }-\alpha x-(1-\alpha )}{\alpha (\alpha -1)}}} , with range x ∈ ∈ [ 0 , ∞ ∞ ) {\displaystyle x\in [0,\infty )} . Its convex conjugate is f α α ∗ ∗ ( y ) = 1 α α ( x ( y ) α α − − 1 ) {\displaystyle f_{\alpha }^{*}(y)={\frac {1}{\alpha }}(x(y)^{\alpha }-1)} with range y ∈ ∈ ( − − ∞ ∞ , ( 1 − − α α ) − − 1 ) {\displaystyle y\in (-\infty ,(1-\alpha )^{-1})} , where x ( y ) = ( ( α α − − 1 ) y + 1 ) 1 α α − − 1 {\displaystyle x(y)=((\alpha -1)y+1)^{\frac {1}{\alpha -1}}} .

Applying this theorem yields, after substitution with h = ( ( α α − − 1 ) g + 1 ) 1 α α − − 1 {\displaystyle h=((\alpha -1)g+1)^{\frac {1}{\alpha -1}}} , D α α ( P ‖ ‖ Q ) = 1 α α ( 1 − − α α ) − − inf h : Ω Ω → → ( 0 , ∞ ∞ ) ( E Q [ h α α α α ] + E P [ h α α − − 1 1 − − α α ] ) , {\displaystyle D_{\alpha }(P\|Q)={\frac {1}{\alpha (1-\alpha )}}-\inf _{h:\Omega \to (0,\infty )}\left(E_{Q}\left[{\frac {h^{\alpha }}{\alpha }}\right]+E_{P}\left[{\frac {h^{\alpha -1}}{1-\alpha }}\right]\right),} or, releasing the constraint on h {\displaystyle h} , D α α ( P ‖ ‖ Q ) = 1 α α ( 1 − − α α ) − − inf h : Ω Ω → → R ( E Q [ | h | α α α α ] + E P [ | h | α α − − 1 1 − − α α ] ) .

{\displaystyle D_{\alpha }(P\|Q)={\frac {1}{\alpha (1-\alpha )}}-\inf _{h:\Omega \to \mathbb {R} }\left(E_{Q}\left[{\frac {|h|^{\alpha }}{\alpha }}\right]+E_{P}\left[{\frac {|h|^{\alpha -1}}{1-\alpha }}\right]\right).} Setting α α = − − 1 {\displaystyle \alpha =-1} yields the variational representation of χ χ 2 {\displaystyle \chi ^{2}} -divergence obtained above.

The domain over which h {\displaystyle h} varies is not affine-invariant in general, unlike the χ χ 2 {\displaystyle \chi ^{2}} -divergence case. The χ χ 2 {\displaystyle \chi ^{2}} -divergence is special, since in that case, we can remove the | ⋅ ⋅ | {\displaystyle |\cdot |} from | h | {\displaystyle |h|} .

For general α α ∈ ∈ ( − − ∞ ∞ , 0 ) ∪ ∪ ( 0 , 1 ) {\displaystyle \alpha \in (-\infty ,0)\cup (0,1)} , the domain over which h {\displaystyle h} varies is merely scale invariant. Similar to above, we can replace h {\displaystyle h} by a h {\displaystyle ah} , and take minimum over a > 0 {\displaystyle a>0} to obtain D α α ( P ‖ ‖ Q ) = sup h > 0 [ 1 α α ( 1 − − α α ) ( 1 − − E P [ h α α − − 1 ] α α E Q [ h α α ] α α − − 1 ) ] .

{\displaystyle D_{\alpha }(P\|Q)=\sup _{h>0}\left[{\frac {1}{\alpha (1-\alpha )}}\left(1-{\frac {E_{P}[h^{\alpha -1}]^{\alpha }}{E_{Q}[h^{\alpha }]^{\alpha -1}}}\right)\right].} Setting α α = 1 2 {\displaystyle \alpha ={\frac {1}{2}}} , and performing another substitution by g = h {\displaystyle g={\sqrt {h}}} , yields two variational representations of the squared Hellinger distance: H 2 ( P ‖ ‖ Q ) = 1 2 D 1 / 2 ( P ‖ ‖ Q ) = 2 − − inf h > 0 ( E Q [ h ( X ) ] + E P [ h ( X ) − − 1 ] ) , {\displaystyle H^{2}(P\|Q)={\frac {1}{2}}D_{1/2}(P\|Q)=2-\inf _{h>0}\left(E_{Q}\left[h(X)\right]+E_{P}\left[h(X)^{-1}\right]\right),} H 2 ( P ‖ ‖ Q ) = 2 sup h > 0 ( 1 − − E P [ h − − 1 ] E Q [ h ] ) .

{\displaystyle H^{2}(P\|Q)=2\sup _{h>0}\left(1-{\sqrt {E_{P}[h^{-1}]E_{Q}[h]}}\right).} Applying this theorem to the KL-divergence, defined by f ( x ) = x ln ⁡ ⁡ x , f ∗ ∗ ( y ) = e y − − 1 {\displaystyle f(x)=x\ln x,f^{*}(y)=e^{y-1}} , yields D K L ( P ; Q ) = sup g E P [ g ( X ) ] − − e − − 1 E Q [ e g ( X ) ] .

{\displaystyle D_{KL}(P;Q)=\sup _{g}E_{P}[g(X)]-e^{-1}E_{Q}[e^{g(X)}].} This is strictly less efficient than the Donsker–Varadhan representation D K L ( P ; Q ) = sup g E P [ g ( X ) ] − − ln ⁡ ⁡ E Q [ e g ( X ) ] .

{\displaystyle D_{KL}(P;Q)=\sup _{g}E_{P}[g(X)]-\ln E_{Q}[e^{g(X)}].} This defect is fixed by the next theorem.

Improved variational representation [ edit ] Assume the setup in the beginning of this section ("Variational representations").

Theorem — If f ( x ) = + ∞ ∞ {\displaystyle f(x)=+\infty } on x < 0 {\displaystyle x<0} (redefine f {\displaystyle f} if necessary), then D f ( P ‖ ‖ Q ) = f ′ ′ ( ∞ ∞ ) P [ S c ] + sup g E P [ g 1 S ] − − Ψ Ψ Q , P ∗ ∗ ( g ) {\displaystyle D_{f}(P\|Q)=f^{\prime }(\infty )P\left[S^{c}\right]+\sup _{g}\mathbb {E} _{P}\left[g1_{S}\right]-\Psi _{Q,P}^{*}(g)} , where Ψ Ψ Q , P ∗ ∗ ( g ) := inf a ∈ ∈ R E Q [ f ∗ ∗ ( g ( X ) − − a ) ] + a P [ S ] {\displaystyle \Psi _{Q,P}^{*}(g):=\inf _{a\in \mathbb {R} }\mathbb {E} _{Q}\left[f^{*}(g(X)-a)\right]+aP[S]} and S := { q > 0 } {\displaystyle S:=\{q>0\}} , where q {\displaystyle q} is the probability density function of Q {\displaystyle Q} with respect to some underlying measure.

In the special case of f ′ ′ ( ∞ ∞ ) = + ∞ ∞ {\displaystyle f^{\prime }(\infty )=+\infty } , we have D f ( P ‖ ‖ Q ) = sup g E P [ g ] − − Ψ Ψ Q ∗ ∗ ( g ) , Ψ Ψ Q ∗ ∗ ( g ) := inf a ∈ ∈ R E Q [ f ∗ ∗ ( g ( X ) − − a ) ] + a {\displaystyle D_{f}(P\|Q)=\sup _{g}\mathbb {E} _{P}[g]-\Psi _{Q}^{*}(g),\quad \Psi _{Q}^{*}(g):=\inf _{a\in \mathbb {R} }\mathbb {E} _{Q}\left[f^{*}(g(X)-a)\right]+a} .

This is Theorem 7.25 in.

[ 2 ] Example applications [ edit ] Applying this theorem to KL-divergence yields the Donsker–Varadhan representation.

Attempting to apply this theorem to the general α α {\displaystyle \alpha } -divergence with α α ∈ ∈ ( − − ∞ ∞ , 0 ) ∪ ∪ ( 0 , 1 ) {\displaystyle \alpha \in (-\infty ,0)\cup (0,1)} does not yield a closed-form solution.

Common examples of f -divergences [ edit ] The following table lists many of the common divergences between probability distributions and the possible generating functions to which they correspond. Notably, except for total variation distance, all others are special cases of α α {\displaystyle \alpha } -divergence, or linear sums of α α {\displaystyle \alpha } -divergences.

For each f-divergence D f {\displaystyle D_{f}} , its generating function is not uniquely defined, but only up to c ⋅ ⋅ ( t − − 1 ) {\displaystyle c\cdot (t-1)} , where c {\displaystyle c} is any real constant. That is, for any f {\displaystyle f} that generates an f-divergence, we have D f ( t ) = D f ( t ) + c ⋅ ⋅ ( t − − 1 ) {\displaystyle D_{f(t)}=D_{f(t)+c\cdot (t-1)}} . This freedom is not only convenient, but actually necessary.

Divergence Corresponding f(t) Discrete Form χ χ α α {\displaystyle \chi ^{\alpha }} -divergence, α α ≥ ≥ 1 {\displaystyle \alpha \geq 1\,} 1 2 | t − − 1 | α α {\displaystyle {\frac {1}{2}}|t-1|^{\alpha }\,} 1 2 ∑ ∑ i | p i − − q i q i | α α q i {\displaystyle {\frac {1}{2}}\sum _{i}\left|{\frac {p_{i}-q_{i}}{q_{i}}}\right|^{\alpha }q_{i}\,} Total variation distance ( α α = 1 {\displaystyle \alpha =1\,} ) 1 2 | t − − 1 | {\displaystyle {\frac {1}{2}}|t-1|\,} 1 2 ∑ ∑ i | p i − − q i | {\displaystyle {\frac {1}{2}}\sum _{i}|p_{i}-q_{i}|\,} α-divergence { t α α − − α α t − − ( 1 − − α α ) α α ( α α − − 1 ) if α α ≠ ≠ 0 , α α ≠ ≠ 1 , t ln ⁡ ⁡ t − − t + 1 , if α α = 1 , − − ln ⁡ ⁡ t + t − − 1 , if α α = 0 {\displaystyle {\begin{cases}{\frac {t^{\alpha }-\alpha t-\left(1-\alpha \right)}{\alpha \left(\alpha -1\right)}}&{\text{if}}\ \alpha \neq 0,\,\alpha \neq 1,\\t\ln t-t+1,&{\text{if}}\ \alpha =1,\\-\ln t+t-1,&{\text{if}}\ \alpha =0\end{cases}}} KL-divergence ( α α = 1 {\displaystyle \alpha =1} ) t ln ⁡ ⁡ t {\displaystyle t\ln t} ∑ ∑ i p i ln ⁡ ⁡ p i q i {\displaystyle \sum _{i}p_{i}\ln {\frac {p_{i}}{q_{i}}}} reverse KL-divergence ( α α = 0 {\displaystyle \alpha =0} ) − − ln ⁡ ⁡ t {\displaystyle -\ln t} ∑ ∑ i q i ln ⁡ ⁡ q i p i {\displaystyle \sum _{i}q_{i}\ln {\frac {q_{i}}{p_{i}}}} Jensen–Shannon divergence 1 2 ( t ln ⁡ ⁡ t − − ( t + 1 ) ln ⁡ ⁡ ( t + 1 2 ) ) {\displaystyle {\frac {1}{2}}\left(t\ln t-(t+1)\ln \left({\frac {t+1}{2}}\right)\right)} 1 2 ∑ ∑ i ( p i ln ⁡ ⁡ p i ( p i + q i ) / 2 + q i ln ⁡ ⁡ q i ( p i + q i ) / 2 ) {\displaystyle {\frac {1}{2}}\sum _{i}\left(p_{i}\ln {\frac {p_{i}}{(p_{i}+q_{i})/2}}+q_{i}\ln {\frac {q_{i}}{(p_{i}+q_{i})/2}}\right)} Jeffreys divergence (KL + reverse KL) ( t − − 1 ) ln ⁡ ⁡ ( t ) {\displaystyle (t-1)\ln(t)} ∑ ∑ i ( p i − − q i ) ln ⁡ ⁡ p i q i {\displaystyle \sum _{i}(p_{i}-q_{i})\ln {\frac {p_{i}}{q_{i}}}} squared Hellinger distance ( α α = 1 2 {\displaystyle \alpha ={\frac {1}{2}}} ) 1 2 ( t − − 1 ) 2 , 1 − − t {\displaystyle {\frac {1}{2}}({\sqrt {t}}-1)^{2},\,1-{\sqrt {t}}} 1 2 ∑ ∑ i ( p i − − q i ) 2 ; 1 − − ∑ ∑ i p i q i {\displaystyle {\frac {1}{2}}\sum _{i}({\sqrt {p_{i}}}-{\sqrt {q_{i}}})^{2};\;1-\sum _{i}{\sqrt {p_{i}q_{i}}}} Neyman χ χ 2 {\displaystyle \chi ^{2}} -divergence ( t − − 1 ) 2 {\displaystyle (t-1)^{2}} ∑ ∑ i ( p i − − q i ) 2 q i {\displaystyle \sum _{i}{\frac {(p_{i}-q_{i})^{2}}{q_{i}}}} Pearson χ χ 2 {\displaystyle \chi ^{2}} -divergence ( t − − 1 ) 2 t {\displaystyle {\frac {(t-1)^{2}}{t}}} ∑ ∑ i ( p i − − q i ) 2 p i {\displaystyle \sum _{i}{\frac {(p_{i}-q_{i})^{2}}{p_{i}}}} Comparison between the generators of alpha-divergences, as alpha varies from -1 to 2.

Let f α α {\displaystyle f_{\alpha }} be the generator of α α {\displaystyle \alpha } -divergence, then f α α {\displaystyle f_{\alpha }} and f 1 − − α α {\displaystyle f_{1-\alpha }} are convex inversions of each other, so D α α ( P ‖ ‖ Q ) = D 1 − − α α ( Q ‖ ‖ P ) {\displaystyle D_{\alpha }(P\|Q)=D_{1-\alpha }(Q\|P)} . In particular, this shows that the squared Hellinger distance and Jensen-Shannon divergence are symmetric.

In the literature, the α α {\displaystyle \alpha } -divergences are sometimes parametrized as { 4 1 − − α α 2 ( 1 − − t ( 1 + α α ) / 2 ) , if α α ≠ ≠ ± ± 1 , t ln ⁡ ⁡ t , if α α = 1 , − − ln ⁡ ⁡ t , if α α = − − 1 {\displaystyle {\begin{cases}{\frac {4}{1-\alpha ^{2}}}{\big (}1-t^{(1+\alpha )/2}{\big )},&{\text{if}}\ \alpha \neq \pm 1,\\t\ln t,&{\text{if}}\ \alpha =1,\\-\ln t,&{\text{if}}\ \alpha =-1\end{cases}}} which is equivalent to the parametrization in this page by substituting α α ← ← α α + 1 2 {\displaystyle \alpha \leftarrow {\frac {\alpha +1}{2}}} .

Relations to other statistical divergences [ edit ] Here, we compare f -divergences with other statistical divergences .

Rényi divergence [ edit ] The Rényi divergences is a family of divergences defined by R α α ( P ‖ ‖ Q ) = 1 α α − − 1 log ⁡ ⁡ ( E Q [ ( d P d Q ) α α ] ) {\displaystyle R_{\alpha }(P\|Q)={\frac {1}{\alpha -1}}\log {\Bigg (}E_{Q}\left[\left({\frac {dP}{dQ}}\right)^{\alpha }\right]{\Bigg )}\,} when α α ∈ ∈ ( 0 , 1 ) ∪ ∪ ( 1 , + ∞ ∞ ) {\displaystyle \alpha \in (0,1)\cup (1,+\infty )} . It is extended to the cases of α α = 0 , 1 , + ∞ ∞ {\displaystyle \alpha =0,1,+\infty } by taking the limit.

Simple algebra shows that R α α ( P ‖ ‖ Q ) = 1 α α − − 1 ln ⁡ ⁡ ( 1 + α α ( α α − − 1 ) D α α ( P ‖ ‖ Q ) ) {\displaystyle R_{\alpha }(P\|Q)={\frac {1}{\alpha -1}}\ln(1+\alpha (\alpha -1)D_{\alpha }(P\|Q))} , where D α α {\displaystyle D_{\alpha }} is the α α {\displaystyle \alpha } -divergence defined above.

Bregman divergence [ edit ] The only f-divergence that is also a Bregman divergence is the KL divergence.

[ 6 ] Integral probability metrics [ edit ] The only f-divergence that is also an integral probability metric is the total variation.

[ 7 ] Financial interpretation [ edit ] A pair of probability distributions can be viewed as a game of chance in which one of the distributions defines the official odds and the other contains the actual probabilities. Knowledge of the actual probabilities allows a player to profit from the game. For a large class of rational players the expected profit rate has the same general form as the ƒ -divergence.

[ 8 ] See also [ edit ] Kullback–Leibler divergence Bregman divergence References [ edit ] ^ Rényi, Alfréd (1961).

On measures of entropy and information (PDF) . The 4th Berkeley Symposium on Mathematics, Statistics and Probability, 1960. Berkeley, CA: University of California Press. pp.

547– 561.

Eq. (4.20) ^ a b c d Polyanskiy, Yury; Yihong, Wu (2022).

Information Theory: From Coding to Learning (draft of October 20, 2022) (PDF) . Cambridge University Press. Archived from the original (PDF) on 2023-02-01.

^ Gorban, Pavel A. (15 October 2003). "Monotonically equivalent entropies and solution of additivity equation".

Physica A .

328 ( 3– 4): 380– 390.

arXiv : cond-mat/0304131 .

Bibcode : 2003PhyA..328..380G .

doi : 10.1016/S0378-4371(03)00578-8 .

S2CID 14975501 .

^ Amari, Shun'ichi (2009). Leung, C.S.; Lee, M.; Chan, J.H. (eds.).

Divergence, Optimization, Geometry . The 16th International Conference on Neural Information Processing (ICONIP 20009), Bangkok, Thailand, 1--5 December 2009. Lecture Notes in Computer Science, vol 5863. Berlin, Heidelberg: Springer. pp.

185– 193.

doi : 10.1007/978-3-642-10677-4_21 .

^ Gorban, Alexander N. (29 April 2014).

"General H-theorem and Entropies that Violate the Second Law" .

Entropy .

16 (5): 2408– 2432.

arXiv : 1212.6767 .

Bibcode : 2014Entrp..16.2408G .

doi : 10.3390/e16052408 .

^ Jiao, Jiantao; Courtade, Thomas; No, Albert; Venkat, Kartik; Weissman, Tsachy (December 2014). "Information Measures: the Curious Case of the Binary Alphabet".

IEEE Transactions on Information Theory .

60 (12): 7616– 7626.

arXiv : 1404.6810 .

doi : 10.1109/TIT.2014.2360184 .

ISSN 0018-9448 .

S2CID 13108908 .

^ Sriperumbudur, Bharath K.; Fukumizu, Kenji; Gretton, Arthur; Schölkopf, Bernhard ; Lanckriet, Gert R. G. (2009). "On integral probability metrics, φ-divergences and binary classification".

arXiv : 0901.2698 [ cs.IT ].

^ Soklakov, Andrei N. (2020).

"Economics of Disagreement—Financial Intuition for the Rényi Divergence" .

Entropy .

22 (8): 860.

arXiv : 1811.08308 .

Bibcode : 2020Entrp..22..860S .

doi : 10.3390/e22080860 .

PMC 7517462 .

PMID 33286632 .

Csiszár, I. (1963). "Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis der Ergodizitat von Markoffschen Ketten".

Magyar. Tud. Akad. Mat. Kutato Int. Kozl .

8 : 85– 108.

Morimoto, T. (1963). "Markov processes and the H-theorem".

J. Phys. Soc. Jpn .

18 (3): 328– 331.

Bibcode : 1963JPSJ...18..328M .

doi : 10.1143/JPSJ.18.328 .

Ali, S. M.; Silvey, S. D. (1966). "A general class of coefficients of divergence of one distribution from another".

Journal of the Royal Statistical Society, Series B .

28 (1): 131– 142.

JSTOR 2984279 .

MR 0196777 .

Csiszár, I. (1967). "Information-type measures of difference of probability distributions and indirect observation".

Studia Scientiarum Mathematicarum Hungarica .

2 : 229– 318.

Csiszár, I.

; Shields, P. (2004).

"Information Theory and Statistics: A Tutorial" (PDF) .

Foundations and Trends in Communications and Information Theory .

1 (4): 417– 528.

doi : 10.1561/0100000004 . Retrieved 2009-04-08 .

Liese, F.; Vajda, I. (2006). "On divergences and informations in statistics and information theory".

IEEE Transactions on Information Theory .

52 (10): 4394– 4412.

doi : 10.1109/TIT.2006.881731 .

S2CID 2720215 .

Nielsen, F.; Nock, R. (2013). "On the Chi square and higher-order Chi distances for approximating f-divergences".

IEEE Signal Processing Letters .

21 (1): 10– 13.

arXiv : 1309.3029 .

Bibcode : 2014ISPL...21...10N .

doi : 10.1109/LSP.2013.2288355 .

S2CID 4152365 .

Coeurjolly, J-F.; Drouilhet, R. (2006). "Normalized information-based divergences".

arXiv : math/0604246 .

NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐qxx58
Cached time: 20250812023548
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.457 seconds
Real time usage: 0.632 seconds
Preprocessor visited node count: 2374/1000000
Revision size: 24062/2097152 bytes
Post‐expand include size: 50850/2097152 bytes
Template argument size: 4011/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 61071/5000000 bytes
Lua time usage: 0.209/10.000 seconds
Lua memory usage: 7322527/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  342.972      1 -total
 35.33%  121.165      1 Template:Reflist
 20.43%   70.069      2 Template:Cite_conference
 18.90%   64.810      1 Template:Short_description
 16.86%   57.831     11 Template:Cite_journal
 15.01%   51.466      1 Template:More_footnotes
 13.47%   46.192      1 Template:Ambox
 12.79%   43.861      2 Template:Pagetype
  8.18%   28.070      4 Template:Harvtxt
  4.59%   15.756      2 Template:Cite_arXiv Saved in parser cache with key enwiki:pcache:15224289:|#|:idhash:canonical and timestamp 20250812023548 and revision id 1285175501. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=F-divergence&oldid=1285175501 " Category : F-divergences Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from September 2015 All articles lacking in-text citations This page was last edited on 12 April 2025, at 03:25 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents f -divergence 1 language Add topic

