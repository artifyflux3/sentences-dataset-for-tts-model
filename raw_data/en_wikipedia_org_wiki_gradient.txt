Title: Gradient

URL Source: https://en.wikipedia.org/wiki/Gradient

Published Time: 2001-08-17T21:10:16Z

Markdown Content:
This article is about a generalized derivative of a multivariate function. For another use in mathematics, see [Slope](https://en.wikipedia.org/wiki/Slope "Slope"). For a similarly spelled unit of angle, see [Gradian](https://en.wikipedia.org/wiki/Gradian "Gradian"). For other uses, see [Gradient (disambiguation)](https://en.wikipedia.org/wiki/Gradient_(disambiguation) "Gradient (disambiguation)").

[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Gradient2.svg/330px-Gradient2.svg.png)](https://en.wikipedia.org/wiki/File:Gradient2.svg)

The gradient, represented by the blue arrows, denotes the direction of greatest change of a scalar function. The values of the function are represented in greyscale and increase in value from white (low) to dark (high).

In [vector calculus](https://en.wikipedia.org/wiki/Vector_calculus "Vector calculus"), the **gradient** of a [scalar-valued](https://en.wikipedia.org/wiki/Scalar-valued_function "Scalar-valued function")[differentiable function](https://en.wikipedia.org/wiki/Differentiable_function "Differentiable function")![Image 2: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) of [several variables](https://en.wikipedia.org/wiki/Multivalued_function "Multivalued function") is the [vector field](https://en.wikipedia.org/wiki/Vector_field "Vector field") (or [vector-valued function](https://en.wikipedia.org/wiki/Vector-valued_function "Vector-valued function")) ![Image 3: {\displaystyle \nabla f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b) whose value at a point ![Image 4: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36) gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of ![Image 5: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61). If the gradient of a function is non-zero at a point ![Image 6: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36), the direction of the gradient is the direction in which the function increases most quickly from ![Image 7: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36), and the [magnitude](https://en.wikipedia.org/wiki/Magnitude_(mathematics) "Magnitude (mathematics)") of the gradient is the rate of increase in that direction, the greatest [absolute](https://en.wikipedia.org/wiki/Absolute_value "Absolute value") directional derivative.[[1]](https://en.wikipedia.org/wiki/Gradient#cite_note-1) Further, a point where the gradient is the zero vector is known as a [stationary point](https://en.wikipedia.org/wiki/Stationary_point "Stationary point"). The gradient thus plays a fundamental role in [optimization theory](https://en.wikipedia.org/wiki/Optimization_theory "Optimization theory"), where it is used to minimize a function by [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent "Gradient descent"). In coordinate-free terms, the gradient of a function ![Image 8: {\displaystyle f(\mathbf {r} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bb64b30ae67dec8ef9cb06c1d3537f00b9a7efed) may be defined by:

![Image 9: {\displaystyle df=\nabla f\cdot d\mathbf {r} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/0a3a80fcf642620d60ec5d82e0618fac56a13d0a)

where ![Image 10: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f) is the total infinitesimal change in ![Image 11: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) for an infinitesimal displacement ![Image 12: {\displaystyle d\mathbf {r} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/454281f527ea3224487aa645577f0d78a97d4c88), and is seen to be maximal when ![Image 13: {\displaystyle d\mathbf {r} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/454281f527ea3224487aa645577f0d78a97d4c88) is in the direction of the gradient ![Image 14: {\displaystyle \nabla f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b). The [nabla symbol](https://en.wikipedia.org/wiki/Nabla_symbol "Nabla symbol")![Image 15: {\displaystyle \nabla }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a3d0e93b78c50237f9ea83d027e4ebbdaef354b2), written as an upside-down triangle and pronounced "del", denotes the [vector differential operator](https://en.wikipedia.org/wiki/Del "Del").

When a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics) "Vector (mathematics and physics)")[[a]](https://en.wikipedia.org/wiki/Gradient#cite_note-row-column-2) whose components are the [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative "Partial derivative") of ![Image 16: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at ![Image 17: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36).[[2]](https://en.wikipedia.org/wiki/Gradient#cite_note-3) That is, for ![Image 18: {\displaystyle f\colon \mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6991fd928a18a993d719e93137af6240a9b6545a), its gradient ![Image 19: {\displaystyle \nabla f\colon \mathbb {R} ^{n}\to \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6ed4bb04e6fb7487b22344c31edb39f2e4a67718) is defined at the point ![Image 20: {\displaystyle p=(x_{1},\ldots ,x_{n})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4a13838da0a205f7cbc141ec11c055eeefccaa38) in _n_-dimensional space as the vector[[b]](https://en.wikipedia.org/wiki/Gradient#cite_note-4)

![Image 21: {\displaystyle \nabla f(p)={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d632a346cd0677aef80d9fa32f476a5b5bf4dc58)

Note that the above definition for gradient is defined for the function ![Image 22: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) only if ![Image 23: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) is differentiable at ![Image 24: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36). There can be functions for which partial derivatives exist in every direction but fail to be differentiable. Furthermore, this definition as the vector of partial derivatives is only valid when the basis of the coordinate system is [orthonormal](https://en.wikipedia.org/wiki/Orthonormal_basis "Orthonormal basis"). For any other basis, the [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor "Metric tensor") at that point needs to be taken into account.

For example, the function ![Image 25: {\displaystyle f(x,y)={\frac {x^{2}y}{x^{2}+y^{2}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/14e8b8c6b45727ce0de3b5f10c8e12ee2379eca5) unless at origin where ![Image 26: {\displaystyle f(0,0)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/73eda9d9d0146876d534fe2d9d228990825ec759), is not differentiable at the origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin.[[3]](https://en.wikipedia.org/wiki/Gradient#cite_note-5) In this particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the 'steepest ascent' in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always point towards the fastest increase.

The gradient is dual to the [total derivative](https://en.wikipedia.org/wiki/Total_derivative "Total derivative")![Image 27: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f): the value of the gradient at a point is a [tangent vector](https://en.wikipedia.org/wiki/Tangent_vector "Tangent vector") – a vector at each point; while the value of the derivative at a point is a [_co_ tangent vector](https://en.wikipedia.org/wiki/Cotangent_vector "Cotangent vector") – a linear functional on vectors.[[c]](https://en.wikipedia.org/wiki/Gradient#cite_note-6) They are related in that the [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product") of the gradient of ![Image 28: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at a point ![Image 29: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36) with another tangent vector ![Image 30: {\displaystyle \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c1866e359fbfd2e0f606c725ba5cc37a5195d6) equals the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative "Directional derivative") of ![Image 31: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at ![Image 32: {\displaystyle p}](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36) of the function along ![Image 33: {\displaystyle \mathbf {v} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c1866e359fbfd2e0f606c725ba5cc37a5195d6); that is, ![Image 34: {\textstyle \nabla f(p)\cdot \mathbf {v} ={\frac {\partial f}{\partial \mathbf {v} }}(p)=df_{p}(\mathbf {v} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ce190299853cdc35c183f101928d3fc2ad7f469). The gradient admits multiple generalizations to more general functions on [manifolds](https://en.wikipedia.org/wiki/Manifold "Manifold"); see [§Generalizations](https://en.wikipedia.org/wiki/Gradient#Generalizations).

[![Image 35](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Vector_Field_of_a_Function%27s_Gradient_imposed_over_a_Color_Plot_of_that_Function.svg/500px-Vector_Field_of_a_Function%27s_Gradient_imposed_over_a_Color_Plot_of_that_Function.svg.png)](https://en.wikipedia.org/wiki/File:Vector_Field_of_a_Function%27s_Gradient_imposed_over_a_Color_Plot_of_that_Function.svg)

Gradient of the 2D function _f_(_x_, _y_) = _xe_−(_x_ 2 + _y_ 2) is plotted as arrows over the pseudocolor plot of the function.

Consider a room where the temperature is given by a [scalar field](https://en.wikipedia.org/wiki/Scalar_field "Scalar field"), _T_, so at each point (_x_, _y_, _z_) the temperature is _T_(_x_, _y_, _z_), independent of time. At each point in the room, the gradient of _T_ at that point will show the direction in which the temperature rises most quickly, moving away from (_x_, _y_, _z_). The magnitude of the gradient will determine how fast the temperature rises in that direction.

Consider a surface whose height above sea level at point (_x_, _y_) is _H_(_x_, _y_). The gradient of _H_ at a point is a plane vector pointing in the direction of the steepest slope or [grade](https://en.wikipedia.org/wiki/Grade_(slope) "Grade (slope)") at that point. The steepness of the slope at that point is given by the magnitude of the gradient vector.

The gradient can also be used to measure how a scalar field changes in other directions, rather than just the direction of greatest change, by taking a [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product"). Suppose that the steepest slope on a hill is 40%. A road going directly uphill has slope 40%, but a road going around the hill at an angle will have a shallower slope. For example, if the road is at a 60° angle from the uphill direction (when both directions are projected onto the horizontal plane), then the slope along the road will be the dot product between the gradient vector and a [unit vector](https://en.wikipedia.org/wiki/Unit_vector "Unit vector") along the road, as the dot product measures how much the unit vector along the road aligns with the steepest slope,[[d]](https://en.wikipedia.org/wiki/Gradient#cite_note-7) which is 40% times the [cosine](https://en.wikipedia.org/wiki/Cosine "Cosine") of 60°, or 20%.

More generally, if the hill height function _H_ is [differentiable](https://en.wikipedia.org/wiki/Differentiable_function "Differentiable function"), then the gradient of _H_[dotted](https://en.wikipedia.org/wiki/Dot_product "Dot product") with a [unit vector](https://en.wikipedia.org/wiki/Unit_vector "Unit vector") gives the slope of the hill in the direction of the vector, the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative "Directional derivative") of _H_ along the unit vector.

The gradient of a function ![Image 36: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at point ![Image 37: {\displaystyle a}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc) is usually written as ![Image 38: {\displaystyle \nabla f(a)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3bc7a6a083301ee0a3cd9d3147c1d705ab48067a). It may also be denoted by any of the following:

[![Image 39](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/3d-gradient-cos.svg/500px-3d-gradient-cos.svg.png)](https://en.wikipedia.org/wiki/File:3d-gradient-cos.svg)

The gradient of the function _f_(_x_,_y_) = −(cos 2 _x_ + cos 2 _y_)2 depicted as a projected [vector field](https://en.wikipedia.org/wiki/Vector_field "Vector field") on the bottom plane.

The gradient (or gradient vector field) of a scalar function _f_(_x_ 1, _x_ 2, _x_ 3, …, _x n_) is denoted ∇_f_ or ∇→_f_ where ∇ ([nabla](https://en.wikipedia.org/wiki/Nabla_symbol "Nabla symbol")) denotes the vector [differential operator](https://en.wikipedia.org/wiki/Differential_operator "Differential operator"), [del](https://en.wikipedia.org/wiki/Del "Del"). The notation grad _f_ is also commonly used to represent the gradient. The gradient of _f_ is defined as the unique vector field whose dot product with any [vector](https://en.wikipedia.org/wiki/Euclidean_vector "Euclidean vector")**v** at each point _x_ is the directional derivative of _f_ along **v**. That is,

![Image 40: {\displaystyle {\big (}\nabla f(x){\big )}\cdot \mathbf {v} =D_{\mathbf {v} }f(x)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bc755512ce431aba7900e6efb14fd3d24ddf3ee4)

where the right-hand side is the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative "Directional derivative") and there are many ways to represent it. Formally, the derivative is _dual_ to the gradient; see [relationship with derivative](https://en.wikipedia.org/wiki/Gradient#Derivative).

When a function also depends on a parameter such as time, the gradient often refers simply to the vector of its spatial derivatives only (see [Spatial gradient](https://en.wikipedia.org/wiki/Spatial_gradient "Spatial gradient")).

The magnitude and direction of the gradient vector are [independent](https://en.wikipedia.org/wiki/Invariant_(mathematics) "Invariant (mathematics)") of the particular [coordinate representation](https://en.wikipedia.org/wiki/Coordinate_system "Coordinate system").[[4]](https://en.wikipedia.org/wiki/Gradient#cite_note-8)[[5]](https://en.wikipedia.org/wiki/Gradient#cite_note-9)

### Cartesian coordinates

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=4 "Edit section: Cartesian coordinates")]

In the three-dimensional [Cartesian coordinate system](https://en.wikipedia.org/wiki/Cartesian_coordinate_system "Cartesian coordinate system") with a [Euclidean metric](https://en.wikipedia.org/wiki/Euclidean_metric "Euclidean metric"), the gradient, if it exists, is given by

![Image 41: {\displaystyle \nabla f={\frac {\partial f}{\partial x}}\mathbf {i} +{\frac {\partial f}{\partial y}}\mathbf {j} +{\frac {\partial f}{\partial z}}\mathbf {k} ,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7a948b9d1165bf36e01ff46716000a06b34dd801)

where **i**, **j**, **k** are the [standard](https://en.wikipedia.org/wiki/Standard_basis "Standard basis") unit vectors in the directions of the _x_, _y_ and _z_ coordinates, respectively. For example, the gradient of the function ![Image 42: {\displaystyle f(x,y,z)=2x+3y^{2}-\sin(z)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ea1b6ce6a59fbd9485ea984d0e6a8fce07aaa48a) is ![Image 43: {\displaystyle \nabla f(x,y,z)=2\mathbf {i} +6y\mathbf {j} -\cos(z)\mathbf {k} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6242e1286984b3b7334d09b30453765685bd235a) or ![Image 44: {\displaystyle \nabla f(x,y,z)={\begin{bmatrix}2\\6y\\-\cos z\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0bdff4be7360758831b69b4260379becfee9d3ab)

In some applications it is customary to represent the gradient as a [row vector](https://en.wikipedia.org/wiki/Row_vector "Row vector") or [column vector](https://en.wikipedia.org/wiki/Column_vector "Column vector") of its components in a rectangular coordinate system; this article follows the convention of the gradient being a column vector, while the derivative is a row vector.

### Cylindrical and spherical coordinates

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=5 "Edit section: Cylindrical and spherical coordinates")]

In [cylindrical coordinates](https://en.wikipedia.org/wiki/Cylindrical_coordinate_system#Definition "Cylindrical coordinate system"), the gradient is given by:[[6]](https://en.wikipedia.org/wiki/Gradient#cite_note-Schey-1992-10)

![Image 45: {\displaystyle \nabla f(\rho ,\varphi ,z)={\frac {\partial f}{\partial \rho }}\mathbf {e} _{\rho }+{\frac {1}{\rho }}{\frac {\partial f}{\partial \varphi }}\mathbf {e} _{\varphi }+{\frac {\partial f}{\partial z}}\mathbf {e} _{z},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0bb569d0b33fee08a15753941010b4e7284b83a2)

where _ρ_ is the axial distance, _φ_ is the azimuthal or azimuth angle, _z_ is the axial coordinate, and **e**_ρ_, **e**_φ_ and **e**_z_ are unit vectors pointing along the coordinate directions.

In [spherical coordinates](https://en.wikipedia.org/wiki/Spherical_coordinate_system#Definition "Spherical coordinate system") with a Euclidean metric, the gradient is given by:[[6]](https://en.wikipedia.org/wiki/Gradient#cite_note-Schey-1992-10)

![Image 46: {\displaystyle \nabla f(r,\theta ,\varphi )={\frac {\partial f}{\partial r}}\mathbf {e} _{r}+{\frac {1}{r}}{\frac {\partial f}{\partial \theta }}\mathbf {e} _{\theta }+{\frac {1}{r\sin \theta }}{\frac {\partial f}{\partial \varphi }}\mathbf {e} _{\varphi },}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4b9a4289d5a458f0146919f8d793ac92ba5b7e62)

where _r_ is the radial distance, _φ_ is the azimuthal angle and _θ_ is the polar angle, and **e**_r_, **e**_θ_ and **e**_φ_ are again local unit vectors pointing in the coordinate directions (that is, the normalized [covariant basis](https://en.wikipedia.org/wiki/Curvilinear_coordinates#Covariant_and_contravariant_bases "Curvilinear coordinates")).

For the gradient in other [orthogonal coordinate systems](https://en.wikipedia.org/wiki/Orthogonal_coordinate_system "Orthogonal coordinate system"), see [Orthogonal coordinates (Differential operators in three dimensions)](https://en.wikipedia.org/wiki/Orthogonal_coordinates#Differential_operators_in_three_dimensions "Orthogonal coordinates").

### General coordinates

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=6 "Edit section: General coordinates")]

We consider [general coordinates](https://en.wikipedia.org/wiki/Curvilinear_coordinates "Curvilinear coordinates"), which we write as _x_ 1, …, _x_ _i_, …, _x_ _n_, where n is the number of dimensions of the domain. Here, the upper index refers to the position in the list of the coordinate or component, so _x_ 2 refers to the second component—not the quantity _x_ squared. The index variable _i_ refers to an arbitrary element _x_ _i_. Using [Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation "Einstein notation"), the gradient can then be written as:

![Image 47: {\displaystyle \nabla f={\frac {\partial f}{\partial x^{i}}}g^{ij}\mathbf {e} _{j}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c233e388c7aba9c6b0c6df898bc8987cb74ceb15) (Note that its [dual](https://en.wikipedia.org/wiki/Dual_space "Dual space") is ![Image 48: {\textstyle \mathrm {d} f={\frac {\partial f}{\partial x^{i}}}\mathbf {e} ^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0fb9f5934aecdf855d79d48d060bb38f4a249a7e)),

where ![Image 49: {\displaystyle \mathbf {e} ^{i}=\mathrm {d} x^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0043a0cc57140284f3ee9b17cadd078703204f26) and ![Image 50: {\displaystyle \mathbf {e} _{i}=\partial \mathbf {x} /\partial x^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6a7a7022bce33a29a17e9d6e61226f48fc6f8e06) refer to the unnormalized local [covariant and contravariant bases](https://en.wikipedia.org/wiki/Curvilinear_coordinates#Covariant_and_contravariant_bases "Curvilinear coordinates") respectively, ![Image 51: {\displaystyle g^{ij}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b14a4aa3b277a89268fd9026b8f16a749199cb10) is the [inverse metric tensor](https://en.wikipedia.org/wiki/Metric_tensor#Inverse_metric "Metric tensor"), and the Einstein summation convention implies summation over _i_ and _j_.

If the coordinates are orthogonal we can easily express the gradient (and the [differential](https://en.wikipedia.org/wiki/Differential_form "Differential form")) in terms of the normalized bases, which we refer to as ![Image 52: {\displaystyle {\hat {\mathbf {e} }}_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/38435968475058bd50296f545cc5a8ae9c22b883) and ![Image 53: {\displaystyle {\hat {\mathbf {e} }}^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2043a2fb3af0de0e1113ee9ca94a49cf6451934c), using the scale factors (also known as [Lamé coefficients](https://en.wikipedia.org/wiki/Lam%C3%A9_coefficients "Lamé coefficients")) ![Image 54: {\displaystyle h_{i}=\lVert \mathbf {e} _{i}\rVert ={\sqrt {g_{ii}}}=1\,/\lVert \mathbf {e} ^{i}\rVert }](https://wikimedia.org/api/rest_v1/media/math/render/svg/528656876249f3fe2e0f5630d218b4423388cd86):

![Image 55: {\displaystyle \nabla f={\frac {\partial f}{\partial x^{i}}}g^{ij}{\hat {\mathbf {e} }}_{j}{\sqrt {g_{jj}}}=\sum _{i=1}^{n}\,{\frac {\partial f}{\partial x^{i}}}{\frac {1}{h_{i}}}\mathbf {\hat {e}} _{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/99135bf8cd0edbeb308dd2e58831efe70af23c85) (and ![Image 56: {\textstyle \mathrm {d} f=\sum _{i=1}^{n}\,{\frac {\partial f}{\partial x^{i}}}{\frac {1}{h_{i}}}\mathbf {\hat {e}} ^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/da2437bfcc3e0da20de6e7694195cafa0580f9fd)),

where we cannot use Einstein notation, since it is impossible to avoid the repetition of more than two indices. Despite the use of upper and lower indices, ![Image 57: {\displaystyle \mathbf {\hat {e}} _{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f01cfee86a741d3d25c6510aea329ab584019060), ![Image 58: {\displaystyle \mathbf {\hat {e}} ^{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/69a1e1ef2476e6b38b316009b5f386ceb8e9d238), and ![Image 59: {\displaystyle h_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d535f210cbd9b9fe6689e61427b3e213e5b2d547) are neither contravariant nor covariant.

The latter expression evaluates to the expressions given above for cylindrical and spherical coordinates.

Relationship with derivative
----------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=7 "Edit section: Relationship with derivative")]

### Relationship with total derivative

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=8 "Edit section: Relationship with total derivative")]

The gradient is closely related to the [total derivative](https://en.wikipedia.org/wiki/Total_derivative "Total derivative") ([total differential](https://en.wikipedia.org/wiki/Total_differential "Total differential")) ![Image 60: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f): they are [transpose](https://en.wikipedia.org/wiki/Transpose "Transpose") ([dual](https://en.wikipedia.org/wiki/Transpose_of_a_linear_map "Transpose of a linear map")) to each other. Using the convention that vectors in ![Image 61: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) are represented by [column vectors](https://en.wikipedia.org/wiki/Column_vector "Column vector"), and that covectors (linear maps ![Image 62: {\displaystyle \mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/bc9261e2dc56fc66565c77dfb0539c69ec62c78d)) are represented by [row vectors](https://en.wikipedia.org/wiki/Row_vector "Row vector"),[[a]](https://en.wikipedia.org/wiki/Gradient#cite_note-row-column-2) the gradient ![Image 63: {\displaystyle \nabla f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b) and the derivative ![Image 64: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f) are expressed as a column and row vector, respectively, with the same components, but transpose of each other:

![Image 65: {\displaystyle \nabla f(p)={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}};}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1b3d363e5847bda190156e986d6a85b19c24588e)![Image 66: {\displaystyle df_{p}={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)&\cdots &{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d437472e1221ac4c79d2be1c058b8b6b04528f76)

While these both have the same components, they differ in what kind of mathematical object they represent: at each point, the derivative is a [cotangent vector](https://en.wikipedia.org/wiki/Cotangent_vector "Cotangent vector"), a [linear form](https://en.wikipedia.org/wiki/Linear_form "Linear form") (or covector) which expresses how much the (scalar) output changes for a given infinitesimal change in (vector) input, while at each point, the gradient is a [tangent vector](https://en.wikipedia.org/wiki/Tangent_vector "Tangent vector"), which represents an infinitesimal change in (vector) input. In symbols, the gradient is an element of the tangent space at a point, ![Image 67: {\displaystyle \nabla f(p)\in T_{p}\mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2219dfffd170b12078fd30252bad77627b7902f4), while the derivative is a map from the tangent space to the real numbers, ![Image 68: {\displaystyle df_{p}\colon T_{p}\mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/95008e03a2d7756da7fc265771c63b67222a423e). The tangent spaces at each point of ![Image 69: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) can be "naturally" identified[[e]](https://en.wikipedia.org/wiki/Gradient#cite_note-11) with the vector space ![Image 70: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) itself, and similarly the cotangent space at each point can be naturally identified with the [dual vector space](https://en.wikipedia.org/wiki/Dual_vector_space "Dual vector space")![Image 71: {\displaystyle (\mathbb {R} ^{n})^{*}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f8cb411b58bcbb00284f91947ff06d480a7d8339) of covectors; thus the value of the gradient at a point can be thought of a vector in the original ![Image 72: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d), not just as a tangent vector.

Computationally, given a tangent vector, the vector can be _multiplied_ by the derivative (as matrices), which is equal to taking the [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product") with the gradient: ![Image 73: {\displaystyle (df_{p})(v)={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)&\cdots &{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}{\begin{bmatrix}v_{1}\\\vdots \\v_{n}\end{bmatrix}}=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i}}}(p)v_{i}={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}\cdot {\begin{bmatrix}v_{1}\\\vdots \\v_{n}\end{bmatrix}}=\nabla f(p)\cdot v}](https://wikimedia.org/api/rest_v1/media/math/render/svg/badf9c913d236f7b3cdd3a474830c5da8012d8e2)

#### Differential or (exterior) derivative

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=9 "Edit section: Differential or (exterior) derivative")]

The best linear approximation to a differentiable function ![Image 74: {\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/306c097f43c91dce633d12cde024948d39e73752) at a point ![Image 75: {\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) in ![Image 76: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) is a linear map from ![Image 77: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) to ![Image 78: {\displaystyle \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/786849c765da7a84dbc3cce43e96aad58a5868dc) which is often denoted by ![Image 79: {\displaystyle df_{x}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/585da1938f3fb781e4c8912b7d18c22c313725fb) or ![Image 80: {\displaystyle Df(x)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e9a890517132ee5973bf3fd826c31e1355adf036) and called the [differential](https://en.wikipedia.org/wiki/Differential_(calculus) "Differential (calculus)") or [total derivative](https://en.wikipedia.org/wiki/Total_derivative "Total derivative") of ![Image 81: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at ![Image 82: {\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4). The function ![Image 83: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f), which maps ![Image 84: {\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) to ![Image 85: {\displaystyle df_{x}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/585da1938f3fb781e4c8912b7d18c22c313725fb), is called the [total differential](https://en.wikipedia.org/wiki/Total_differential "Total differential") or [exterior derivative](https://en.wikipedia.org/wiki/Exterior_derivative "Exterior derivative") of ![Image 86: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) and is an example of a [differential 1-form](https://en.wikipedia.org/wiki/Differential_1-form "Differential 1-form").

Much as the derivative of a function of a single variable represents the [slope](https://en.wikipedia.org/wiki/Slope "Slope") of the [tangent](https://en.wikipedia.org/wiki/Tangent "Tangent") to the [graph](https://en.wikipedia.org/wiki/Graph_of_a_function "Graph of a function") of the function,[[7]](https://en.wikipedia.org/wiki/Gradient#cite_note-12) the directional derivative of a function in several variables represents the slope of the tangent [hyperplane](https://en.wikipedia.org/wiki/Hyperplane "Hyperplane") in the direction of the vector.

The gradient is related to the differential by the formula ![Image 87: {\displaystyle (\nabla f)_{x}\cdot v=df_{x}(v)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/78105f22c1f0463a9edcac38bf4fcc06a99cd13a) for any ![Image 88: {\displaystyle v\in \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/37576c57a5c8ee1c052cb82a6b88aaa4b41764f4), where ![Image 89: {\displaystyle \cdot }](https://wikimedia.org/api/rest_v1/media/math/render/svg/ba2c023bad1bd39ed49080f729cbf26bc448c9ba) is the [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product"): taking the dot product of a vector with the gradient is the same as taking the directional derivative along the vector.

If ![Image 90: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) is viewed as the space of (dimension ![Image 91: {\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b)) column vectors (of real numbers), then one can regard ![Image 92: {\displaystyle df}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53181e2067a93b6bbf150042723cb059d9d2d26f) as the row vector with components ![Image 93: {\displaystyle \left({\frac {\partial f}{\partial x_{1}}},\dots ,{\frac {\partial f}{\partial x_{n}}}\right),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b21662ead4d5087362cb4cd8765e66454a7b5ac1) so that ![Image 94: {\displaystyle df_{x}(v)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b5cd88405a4b9a01d77c289e6c7fc59831d30e6b) is given by [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication"). Assuming the standard Euclidean metric on ![Image 95: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d), the gradient is then the corresponding column vector, that is, ![Image 96: {\displaystyle (\nabla f)_{i}=df_{i}^{\mathsf {T}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/dcd278acfb405cde74552c1dd935e80f61bd836e)

#### Linear approximation to a function

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=10 "Edit section: Linear approximation to a function")]

The best [linear approximation](https://en.wikipedia.org/wiki/Linear_approximation "Linear approximation") to a function can be expressed in terms of the gradient, rather than the derivative. The gradient of a [function](https://en.wikipedia.org/wiki/Function_(mathematics) "Function (mathematics)")![Image 97: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) from the Euclidean space ![Image 98: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) to ![Image 99: {\displaystyle \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/786849c765da7a84dbc3cce43e96aad58a5868dc) at any particular point ![Image 100: {\displaystyle x_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf) in ![Image 101: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d) characterizes the best [linear approximation](https://en.wikipedia.org/wiki/Linear_approximation "Linear approximation") to ![Image 102: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at ![Image 103: {\displaystyle x_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf). The approximation is as follows:

![Image 104: {\displaystyle f(x)\approx f(x_{0})+(\nabla f)_{x_{0}}\cdot (x-x_{0})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9dada2b2ce21a5e4a72ba77af6f4b59028d6d989)

for ![Image 105: {\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) close to ![Image 106: {\displaystyle x_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf), where ![Image 107: {\displaystyle (\nabla f)_{x_{0}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/78ad19929b28ee2477d7e8a043e398866a937f49) is the gradient of ![Image 108: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) computed at ![Image 109: {\displaystyle x_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf), and the dot denotes the dot product on ![Image 110: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d). This equation is equivalent to the first two terms in the [multivariable Taylor series](https://en.wikipedia.org/wiki/Taylor_series#Taylor_series_in_several_variables "Taylor series") expansion of ![Image 111: {\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61) at ![Image 112: {\displaystyle x_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf).

### Relationship with Fréchet derivative

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=11 "Edit section: Relationship with Fréchet derivative")]

Let _U_ be an [open set](https://en.wikipedia.org/wiki/Open_set "Open set") in **R**_n_. If the function _f_: _U_ → **R** is differentiable, then the differential of _f_ is the [Fréchet derivative](https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative "Fréchet derivative") of _f_. Thus ∇_f_ is a function from _U_ to the space **R**_n_ such that ![Image 113: {\displaystyle \lim _{h\to 0}{\frac {|f(x+h)-f(x)-\nabla f(x)\cdot h|}{\|h\|}}=0,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e77dd0be916e297e7a42ab6e08b1702ec1868907) where · is the dot product.

As a consequence, the usual properties of the derivative hold for the gradient, though the gradient is not a derivative itself, but rather dual to the derivative:

[Linearity](https://en.wikipedia.org/wiki/Linearity "Linearity")The gradient is linear in the sense that if _f_ and _g_ are two real-valued functions differentiable at the point _a_ ∈ **R**_n_, and α and β are two constants, then _αf_ + _βg_ is differentiable at _a_, and moreover ![Image 114: {\displaystyle \nabla \left(\alpha f+\beta g\right)(a)=\alpha \nabla f(a)+\beta \nabla g(a).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b5f5908b588bc0b5c2b2152a66333a6f71fca89a)[Product rule](https://en.wikipedia.org/wiki/Product_rule "Product rule")If _f_ and _g_ are real-valued functions differentiable at a point _a_ ∈ **R**_n_, then the product rule asserts that the product _fg_ is differentiable at _a_, and ![Image 115: {\displaystyle \nabla (fg)(a)=f(a)\nabla g(a)+g(a)\nabla f(a).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/46c11dbb365540ac00844730030c020a641ba44f)[Chain rule](https://en.wikipedia.org/wiki/Chain_rule "Chain rule")Suppose that _f_: _A_ → **R** is a real-valued function defined on a subset _A_ of **R**_n_, and that _f_ is differentiable at a point _a_. There are two forms of the chain rule applying to the gradient. First, suppose that the function _g_ is a [parametric curve](https://en.wikipedia.org/wiki/Parametric_curve "Parametric curve"); that is, a function _g_: _I_ → **R**_n_ maps a subset _I_ ⊂ **R** into **R**_n_. If _g_ is differentiable at a point _c_ ∈ _I_ such that _g_(_c_) = _a_, then ![Image 116: {\displaystyle (f\circ g)'(c)=\nabla f(a)\cdot g'(c),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ea0fcf147eade8ba0756d2e98deba8dc75728e67) where ∘ is the [composition operator](https://en.wikipedia.org/wiki/Composition_operator "Composition operator"): (_f_ ∘ _g_)(_x_) = _f_(_g_(_x_)).
More generally, if instead _I_ ⊂ **R**_k_, then the following holds: ![Image 117: {\displaystyle \nabla (f\circ g)(c)={\big (}Dg(c){\big )}^{\mathsf {T}}{\big (}\nabla f(a){\big )},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e9274d3a0cc8d6531aa172b4a7a4b661dcb182c3) where (_Dg_)T denotes the transpose [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix "Jacobian matrix").

For the second form of the chain rule, suppose that _h_: _I_ → **R** is a real valued function on a subset _I_ of **R**, and that _h_ is differentiable at the point _f_(_a_) ∈ _I_. Then ![Image 118: {\displaystyle \nabla (h\circ f)(a)=h'{\big (}f(a){\big )}\nabla f(a).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/095d52d8bab29b0777d55f3ea527dbb5d60a4f41)

Further properties and applications
-----------------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=12 "Edit section: Further properties and applications")]

A level surface, or [isosurface](https://en.wikipedia.org/wiki/Isosurface "Isosurface"), is the set of all points where some function has a given value.

If _f_ is differentiable, then the dot product (∇_f_ )_x_ ⋅ _v_ of the gradient at a point _x_ with a vector _v_ gives the directional derivative of _f_ at _x_ in the direction _v_. It follows that in this case the gradient of _f_ is [orthogonal](https://en.wikipedia.org/wiki/Orthogonal "Orthogonal") to the [level sets](https://en.wikipedia.org/wiki/Level_set "Level set") of _f_. For example, a level surface in three-dimensional space is defined by an equation of the form _F_(_x_, _y_, _z_) = _c_. The gradient of _F_ is then normal to the surface.

More generally, any [embedded](https://en.wikipedia.org/wiki/Embedded_submanifold "Embedded submanifold")[hypersurface](https://en.wikipedia.org/wiki/Hypersurface "Hypersurface") in a [Riemannian manifold](https://en.wikipedia.org/wiki/Riemannian_manifold "Riemannian manifold") can be cut out by an equation of the form _F_(_P_) = 0 such that _dF_ is nowhere zero. The gradient of _F_ is then normal to the hypersurface.

Similarly, an [affine algebraic hypersurface](https://en.wikipedia.org/wiki/Affine_algebraic_variety "Affine algebraic variety") may be defined by an equation _F_(_x_ 1, ..., _x_ _n_) = 0, where _F_ is a polynomial. The gradient of _F_ is zero at a singular point of the hypersurface (this is the definition of a singular point). At a non-singular point, it is a nonzero normal vector.

### Conservative vector fields and the gradient theorem

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=14 "Edit section: Conservative vector fields and the gradient theorem")]

The gradient of a function is called a gradient field. A (continuous) gradient field is always a [conservative vector field](https://en.wikipedia.org/wiki/Conservative_vector_field "Conservative vector field"): its [line integral](https://en.wikipedia.org/wiki/Line_integral "Line integral") along any path depends only on the endpoints of the path, and can be evaluated by the gradient theorem (the fundamental theorem of calculus for line integrals). Conversely, a (continuous) conservative vector field is always the gradient of a function.

### Gradient is direction of steepest ascent

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=15 "Edit section: Gradient is direction of steepest ascent")]

The gradient of a function ![Image 119: {\displaystyle f\colon \mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6991fd928a18a993d719e93137af6240a9b6545a) at point _x_ is also the direction of its steepest ascent, i.e. it maximizes its [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative "Directional derivative"):

Let ![Image 120: {\displaystyle v\in \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/37576c57a5c8ee1c052cb82a6b88aaa4b41764f4) be an arbitrary unit vector. With the directional derivative defined as

![Image 121: {\displaystyle \nabla _{v}f(x)=\lim _{h\rightarrow 0}{\frac {f(x+vh)-f(x)}{h}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/82073fc99f5522efd32a6cf06227e837646e15b7)

we get, by substituting the function ![Image 122: {\displaystyle f(x+vh)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c79d11617bfebc188f6c35538f8f719daf2d03a) with its [Taylor series](https://en.wikipedia.org/wiki/Taylor_series "Taylor series"),

![Image 123: {\displaystyle \nabla _{v}f(x)=\lim _{h\rightarrow 0}{\frac {(f(x)+\nabla f\cdot vh+R)-f(x)}{h}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f10aaba337d4bf2ebed1dda2b82c83ab79ebf8b3)

where ![Image 124: {\displaystyle R}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4b0bfb3769bf24d80e15374dc37b0441e2616e33) denotes higher order terms in ![Image 125: {\displaystyle vh}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0fedce0df6848691b2ed0e8df7619b5436a22764).

Dividing by ![Image 126: {\displaystyle h}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a), and taking the limit yields a term which is bounded from above by the [Cauchy–Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality "Cauchy–Schwarz inequality")[[8]](https://en.wikipedia.org/wiki/Gradient#cite_note-13)

![Image 127: {\displaystyle |\nabla _{v}f(x)|=|\nabla f\cdot v|\leq |\nabla f||v|=|\nabla f|.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/721c4630282dc726bf8061bc5a76c405613415e6)

Choosing ![Image 128: {\displaystyle v^{*}=\nabla f/|\nabla f|}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3c24b85b4f62948a177939a0bc833ce48f721508) maximizes the directional derivative, and equals the upper bound

![Image 129: {\displaystyle |\nabla _{v^{*}}f(x)|=|(\nabla f)^{2}/|\nabla f||=|\nabla f|.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/388fb723cad8eaddb1ac8599bb03b0d1f743309e)

The [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix "Jacobian matrix") is the generalization of the gradient for vector-valued functions of several variables and [differentiable maps](https://en.wikipedia.org/wiki/Differentiable_map "Differentiable map") between [Euclidean spaces](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space") or, more generally, [manifolds](https://en.wikipedia.org/wiki/Manifold "Manifold").[[9]](https://en.wikipedia.org/wiki/Gradient#cite_note-14)[[10]](https://en.wikipedia.org/wiki/Gradient#cite_note-15) A further generalization for a function between [Banach spaces](https://en.wikipedia.org/wiki/Banach_space "Banach space") is the [Fréchet derivative](https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative "Fréchet derivative").

Suppose **f**: **R**_n_ → **R**_m_ is a function such that each of its first-order partial derivatives exist on ℝ _n_. Then the Jacobian matrix of **f** is defined to be an _m_×_n_ matrix, denoted by ![Image 130: {\displaystyle \mathbf {J} _{\mathbb {f} }(\mathbb {x} )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d778216067ac9d59d505b39841727bab0b4cf1d6) or simply ![Image 131: {\displaystyle \mathbf {J} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/7686846b1a6b756cb514954000004ab5e7b2a5ba). The (_i_,_j_)th entry is ![Image 132: {\textstyle \mathbf {J} _{ij}={\partial f_{i}}/{\partial x_{j}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d8cdb67646a538495c0ead54385cc3021397c0b8). Explicitly ![Image 133: {\displaystyle \mathbf {J} ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x_{1}}}&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x_{n}}}\end{bmatrix}}={\begin{bmatrix}\nabla ^{\mathsf {T}}f_{1}\\\vdots \\\nabla ^{\mathsf {T}}f_{m}\end{bmatrix}}={\begin{bmatrix}{\dfrac {\partial f_{1}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{1}}{\partial x_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{m}}{\partial x_{n}}}\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/017805d50ea8eab512b39f9aa822c5ca165e910e)

### Gradient of a vector field

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=18 "Edit section: Gradient of a vector field")]

Since the [total derivative](https://en.wikipedia.org/wiki/Total_derivative "Total derivative") of a vector field is a [linear mapping](https://en.wikipedia.org/wiki/Linear_mapping "Linear mapping") from vectors to vectors, it is a [tensor](https://en.wikipedia.org/wiki/Tensor "Tensor") quantity.

In rectangular coordinates, the gradient of a vector field **f** = ( _f_ 1, _f_ 2, _f_ 3) is defined by:

![Image 134: {\displaystyle \nabla \mathbf {f} =g^{jk}{\frac {\partial f^{i}}{\partial x^{j}}}\mathbf {e} _{i}\otimes \mathbf {e} _{k},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/27bdc5c9d3fe5e1ec927fb93e5b05a8c9402a045)

(where the [Einstein summation notation](https://en.wikipedia.org/wiki/Einstein_summation_notation "Einstein summation notation") is used and the [tensor product](https://en.wikipedia.org/wiki/Tensor_product "Tensor product") of the vectors **e**_i_ and **e**_k_ is a [dyadic tensor](https://en.wikipedia.org/wiki/Dyadic_tensor "Dyadic tensor") of type (2,0)). Overall, this expression equals the transpose of the Jacobian matrix:

![Image 135: {\displaystyle {\frac {\partial f^{i}}{\partial x^{j}}}={\frac {\partial (f^{1},f^{2},f^{3})}{\partial (x^{1},x^{2},x^{3})}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8abd95986428a331d1ac01b1a729830f3a6c4158)

In curvilinear coordinates, or more generally on a curved [manifold](https://en.wikipedia.org/wiki/Riemannian_manifold "Riemannian manifold"), the gradient involves [Christoffel symbols](https://en.wikipedia.org/wiki/Christoffel_symbols "Christoffel symbols"):

![Image 136: {\displaystyle \nabla \mathbf {f} =g^{jk}\left({\frac {\partial f^{i}}{\partial x^{j}}}+{\Gamma ^{i}}_{jl}f^{l}\right)\mathbf {e} _{i}\otimes \mathbf {e} _{k},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/88a51a5cf3ac7da66fe15df26ff6fea2e0a15c87)

where _g_ _jk_ are the components of the inverse [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor "Metric tensor") and the **e**_i_ are the coordinate basis vectors.

Expressed more invariantly, the gradient of a vector field **f** can be defined by the [Levi-Civita connection](https://en.wikipedia.org/wiki/Levi-Civita_connection "Levi-Civita connection") and metric tensor:[[11]](https://en.wikipedia.org/wiki/Gradient#cite_note-16)

![Image 137: {\displaystyle \nabla ^{a}f^{b}=g^{ac}\nabla _{c}f^{b},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/321ccec9b721069aa6171a25e1070dbe4b86378c)

where ∇_c_ is the connection.

### Riemannian manifolds

[[edit](https://en.wikipedia.org/w/index.php?title=Gradient&action=edit&section=19 "Edit section: Riemannian manifolds")]

For any [smooth function](https://en.wikipedia.org/wiki/Smooth_function "Smooth function")f on a Riemannian manifold (_M_, _g_), the gradient of _f_ is the vector field ∇_f_ such that for any vector field _X_, ![Image 138: {\displaystyle g(\nabla f,X)=\partial _{X}f,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/016ba65c6ad393de95fce69303333e29fc5368b5) that is, ![Image 139: {\displaystyle g_{x}{\big (}(\nabla f)_{x},X_{x}{\big )}=(\partial _{X}f)(x),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e3c202458f676cadf4047f28ce4e4df039d4fb58) where _g_ _x_( , ) denotes the [inner product](https://en.wikipedia.org/wiki/Inner_product "Inner product") of tangent vectors at _x_ defined by the metric _g_ and ∂_X_ _f_ is the function that takes any point _x_ ∈ _M_ to the directional derivative of _f_ in the direction _X_, evaluated at _x_. In other words, in a [coordinate chart](https://en.wikipedia.org/wiki/Coordinate_chart "Coordinate chart")_φ_ from an open subset of _M_ to an open subset of **R**_n_, (∂_X_ _f_ )(_x_) is given by: ![Image 140: {\displaystyle \sum _{j=1}^{n}X^{j}{\big (}\varphi (x){\big )}{\frac {\partial }{\partial x_{j}}}(f\circ \varphi ^{-1}){\Bigg |}_{\varphi (x)},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/78dcb98b7081519ff2d497a76f8918a2934de446) where _X_ _j_ denotes the _j_ th component of _X_ in this coordinate chart.

So, the local form of the gradient takes the form:

![Image 141: {\displaystyle \nabla f=g^{ik}{\frac {\partial f}{\partial x^{k}}}{\textbf {e}}_{i}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/da3f6be5bfdc620a2910ae011dae491d5d5e73a8)

Generalizing the case _M_ = **R**_n_, the gradient of a function is related to its exterior derivative, since ![Image 142: {\displaystyle (\partial _{X}f)(x)=(df)_{x}(X_{x}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d03472524b7b7bb39f84e867a2e7f2fe29fa405) More precisely, the gradient ∇_f_ is the vector field associated to the differential 1-form _df_ using the [musical isomorphism](https://en.wikipedia.org/wiki/Musical_isomorphism "Musical isomorphism")![Image 143: {\displaystyle \sharp =\sharp ^{g}\colon T^{*}M\to TM}](https://wikimedia.org/api/rest_v1/media/math/render/svg/37af56f4726d43f9423825c41e5df20927250f1b) (called "sharp") defined by the metric _g_. The relation between the exterior derivative and the gradient of a function on **R**_n_ is a special case of this in which the metric is the flat metric given by the dot product.

*   [Curl](https://en.wikipedia.org/wiki/Curl_(mathematics) "Curl (mathematics)")– Circulation density in a vector field
*   [Divergence](https://en.wikipedia.org/wiki/Divergence "Divergence")– Vector operator in vector calculus
*   [Four-gradient](https://en.wikipedia.org/wiki/Four-gradient "Four-gradient")– Four-vector analogue of the gradient operation
*   [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix "Hessian matrix")– Matrix of second derivatives
*   [Skew gradient](https://en.wikipedia.org/wiki/Skew_gradient "Skew gradient")
*   [Spatial gradient](https://en.wikipedia.org/wiki/Spatial_gradient "Spatial gradient")– Gradient whose components are spatial derivatives

1.   ^ [_**a**_](https://en.wikipedia.org/wiki/Gradient#cite_ref-row-column_2-0)[_**b**_](https://en.wikipedia.org/wiki/Gradient#cite_ref-row-column_2-1)This article uses the convention that [column vectors](https://en.wikipedia.org/wiki/Column_vector "Column vector") represent vectors, and [row vectors](https://en.wikipedia.org/wiki/Row_vector "Row vector") represent covectors, but the opposite convention is also common.
2.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-4)**Strictly speaking, the gradient is a [vector field](https://en.wikipedia.org/wiki/Vector_field "Vector field")![Image 144: {\displaystyle f\colon \mathbb {R} ^{n}\to T\mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b3d4f77c35af7e853230f719673d89f99290f396), and the value of the gradient at a point is a [tangent vector](https://en.wikipedia.org/wiki/Tangent_vector "Tangent vector") in the [tangent space](https://en.wikipedia.org/wiki/Tangent_space "Tangent space") at that point, ![Image 145: {\displaystyle T_{p}\mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b1ff0cee923f9ef0490325b09a0450dc77b3e8d8), not a vector in the original space ![Image 146: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d). However, all the tangent spaces can be naturally identified with the original space ![Image 147: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d), so these do not need to be distinguished; see [§Definition](https://en.wikipedia.org/wiki/Gradient#Definition) and [relationship with the derivative](https://en.wikipedia.org/wiki/Gradient#Derivative).
3.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-6)**The value of the gradient at a point can be thought of as a vector in the original space ![Image 148: {\displaystyle \mathbb {R} ^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d), while the value of the derivative at a point can be thought of as a covector on the original space: a linear map ![Image 149: {\displaystyle \mathbb {R} ^{n}\to \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/bc9261e2dc56fc66565c77dfb0539c69ec62c78d).
4.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-7)**the dot product (the slope of the road around the hill) would be 40% if the degree between the road and the steepest slope is 0°, i.e. when they are completely aligned, and flat when the degree is 90°, i.e. when the road is perpendicular to the steepest slope.
5.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-11)**Informally, "naturally" identified means that this can be done without making any arbitrary choices. This can be formalized with a [natural transformation](https://en.wikipedia.org/wiki/Natural_transformation "Natural transformation").

1.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-1)**
    *   [Bachman (2007](https://en.wikipedia.org/wiki/Gradient#CITEREFBachman2007), p.77)
    *   [Downing (2010](https://en.wikipedia.org/wiki/Gradient#CITEREFDowning2010), pp.316–317)
    *   [Kreyszig (1972](https://en.wikipedia.org/wiki/Gradient#CITEREFKreyszig1972), p.309)
    *   [McGraw-Hill (2007](https://en.wikipedia.org/wiki/Gradient#CITEREFMcGraw-Hill2007), p.196)
    *   [Moise (1967](https://en.wikipedia.org/wiki/Gradient#CITEREFMoise1967), p.684)
    *   [Protter & Morrey (1970](https://en.wikipedia.org/wiki/Gradient#CITEREFProtterMorrey1970), p.715)
    *   [Swokowski et al. (1994](https://en.wikipedia.org/wiki/Gradient#CITEREFSwokowski_et_al.1994), pp.1036, 1038–1039)

2.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-3)**
    *   [Bachman (2007](https://en.wikipedia.org/wiki/Gradient#CITEREFBachman2007), p.76)
    *   [Beauregard & Fraleigh (1973](https://en.wikipedia.org/wiki/Gradient#CITEREFBeauregardFraleigh1973), p.84)
    *   [Downing (2010](https://en.wikipedia.org/wiki/Gradient#CITEREFDowning2010), p.316)
    *   [Harper (1976](https://en.wikipedia.org/wiki/Gradient#CITEREFHarper1976), p.15)
    *   [Kreyszig (1972](https://en.wikipedia.org/wiki/Gradient#CITEREFKreyszig1972), p.307)
    *   [McGraw-Hill (2007](https://en.wikipedia.org/wiki/Gradient#CITEREFMcGraw-Hill2007), p.196)
    *   [Moise (1967](https://en.wikipedia.org/wiki/Gradient#CITEREFMoise1967), p.683)
    *   [Protter & Morrey (1970](https://en.wikipedia.org/wiki/Gradient#CITEREFProtterMorrey1970), p.714)
    *   [Swokowski et al. (1994](https://en.wikipedia.org/wiki/Gradient#CITEREFSwokowski_et_al.1994), p.1038)

3.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-5)**["Non-differentiable functions must have discontinuous partial derivatives - Math Insight"](https://mathinsight.org/nondifferentiable_discontinuous_partial_derivatives). _mathinsight.org_. Retrieved 2023-10-21.
4.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-8)**[Kreyszig (1972](https://en.wikipedia.org/wiki/Gradient#CITEREFKreyszig1972), pp.308–309)
5.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-9)**[Stoker (1969](https://en.wikipedia.org/wiki/Gradient#CITEREFStoker1969), p.292)
6.   ^ [_**a**_](https://en.wikipedia.org/wiki/Gradient#cite_ref-Schey-1992_10-0)[_**b**_](https://en.wikipedia.org/wiki/Gradient#cite_ref-Schey-1992_10-1)[Schey 1992](https://en.wikipedia.org/wiki/Gradient#CITEREFSchey1992), pp.139–142.
7.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-12)**[Protter & Morrey (1970](https://en.wikipedia.org/wiki/Gradient#CITEREFProtterMorrey1970), pp.21, 88)
8.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-13)**T. Arens (2022). [_Mathematik_](https://doi.org/10.1007/978-3-662-64389-1) (5th ed.). Springer Spektrum Berlin. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-3-662-64389-1](https://doi.org/10.1007%2F978-3-662-64389-1). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-662-64388-4](https://en.wikipedia.org/wiki/Special:BookSources/978-3-662-64388-4 "Special:BookSources/978-3-662-64388-4").
9.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-14)**[Beauregard & Fraleigh (1973](https://en.wikipedia.org/wiki/Gradient#CITEREFBeauregardFraleigh1973), pp.87, 248)
10.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-15)**[Kreyszig (1972](https://en.wikipedia.org/wiki/Gradient#CITEREFKreyszig1972), pp.333, 353, 496)
11.   **[^](https://en.wikipedia.org/wiki/Gradient#cite_ref-16)**[Dubrovin, Fomenko & Novikov 1991](https://en.wikipedia.org/wiki/Gradient#CITEREFDubrovinFomenkoNovikov1991), pp.348–349.

*   Bachman, David (2007), _Advanced Calculus Demystified_, New York: [McGraw-Hill](https://en.wikipedia.org/wiki/McGraw-Hill "McGraw-Hill"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-07-148121-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-148121-2 "Special:BookSources/978-0-07-148121-2")
*   Beauregard, Raymond A.; Fraleigh, John B. (1973), [_A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields_](https://archive.org/details/firstcourseinlin0000beau), Boston: [Houghton Mifflin Company](https://en.wikipedia.org/wiki/Houghton_Mifflin_Company "Houghton Mifflin Company"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-395-14017-X](https://en.wikipedia.org/wiki/Special:BookSources/0-395-14017-X "Special:BookSources/0-395-14017-X")
*   Downing, Douglas, Ph.D. (2010), _Barron's E-Z Calculus_, New York: [Barron's](https://en.wikipedia.org/wiki/B.E.S._Publishing "B.E.S. Publishing"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-7641-4461-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-7641-4461-5 "Special:BookSources/978-0-7641-4461-5")`{{citation}}`: CS1 maint: multiple names: authors list ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list "Category:CS1 maint: multiple names: authors list"))
*   Dubrovin, B. A.; Fomenko, A. T.; Novikov, S. P. (1991). _Modern Geometry—Methods and Applications: Part I: The Geometry of Surfaces, Transformation Groups, and Fields_. [Graduate Texts in Mathematics](https://en.wikipedia.org/wiki/Graduate_Texts_in_Mathematics "Graduate Texts in Mathematics") (2nd ed.). Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-97663-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-97663-1 "Special:BookSources/978-0-387-97663-1").
*   Harper, Charlie (1976), _Introduction to Mathematical Physics_, New Jersey: [Prentice-Hall](https://en.wikipedia.org/wiki/Prentice-Hall "Prentice-Hall"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-13-487538-9](https://en.wikipedia.org/wiki/Special:BookSources/0-13-487538-9 "Special:BookSources/0-13-487538-9")
*   [Kreyszig, Erwin](https://en.wikipedia.org/wiki/Erwin_Kreyszig "Erwin Kreyszig") (1972), [_Advanced Engineering Mathematics_](https://archive.org/details/advancedengineer00krey) (3rd ed.), New York: [Wiley](https://en.wikipedia.org/wiki/John_Wiley_%26_Sons "John Wiley & Sons"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-471-50728-8](https://en.wikipedia.org/wiki/Special:BookSources/0-471-50728-8 "Special:BookSources/0-471-50728-8")
*   "McGraw Hill Encyclopedia of Science & Technology". _McGraw-Hill Encyclopedia of Science & Technology_ (10th ed.). New York: [McGraw-Hill](https://en.wikipedia.org/wiki/McGraw-Hill "McGraw-Hill"). 2007. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-07-144143-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-07-144143-8 "Special:BookSources/978-0-07-144143-8").
*   Moise, Edwin E. (1967), _Calculus: Complete_, Reading: [Addison-Wesley](https://en.wikipedia.org/wiki/Addison-Wesley "Addison-Wesley")
*   Protter, Murray H.; Morrey, Charles B. Jr. (1970), _College Calculus with Analytic Geometry_ (2nd ed.), Reading: [Addison-Wesley](https://en.wikipedia.org/wiki/Addison-Wesley "Addison-Wesley"), [LCCN](https://en.wikipedia.org/wiki/LCCN_(identifier) "LCCN (identifier)")[76087042](https://lccn.loc.gov/76087042)
*   Schey, H. M. (1992). [_Div, Grad, Curl, and All That_](https://archive.org/details/divgradcurlall00sche) (2nd ed.). W. W. Norton. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-393-96251-2](https://en.wikipedia.org/wiki/Special:BookSources/0-393-96251-2 "Special:BookSources/0-393-96251-2"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[25048561](https://search.worldcat.org/oclc/25048561).
*   Stoker, J. J. (1969), _Differential Geometry_, New York: [Wiley](https://en.wikipedia.org/wiki/John_Wiley_%26_Sons "John Wiley & Sons"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-471-82825-4](https://en.wikipedia.org/wiki/Special:BookSources/0-471-82825-4 "Special:BookSources/0-471-82825-4")
*   Swokowski, Earl W.; Olinick, Michael; Pence, Dennis; Cole, Jeffery A. (1994), [_Calculus_](https://archive.org/details/calculus00swok) (6th ed.), Boston: PWS Publishing Company, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-534-93624-5](https://en.wikipedia.org/wiki/Special:BookSources/0-534-93624-5 "Special:BookSources/0-534-93624-5")
*   Arens, T.; Hettlich, F.; Karpfinger, C.; Kockelkorn, U.; Lichtenegger, K.; Stachel, H. (2022), [_Mathematik_](https://doi.org/10.1007/978-3-662-64389-1) (5th ed.), Springer Spektrum Berlin, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-3-662-64389-1](https://doi.org/10.1007%2F978-3-662-64389-1), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-662-64388-4](https://en.wikipedia.org/wiki/Special:BookSources/978-3-662-64388-4 "Special:BookSources/978-3-662-64388-4")

*   [Korn, Theresa M.](https://en.wikipedia.org/wiki/Theresa_M._Korn "Theresa M. Korn"); Korn, Granino Arthur (2000). _Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review_. Dover Publications. pp.157–160. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-486-41147-8](https://en.wikipedia.org/wiki/Special:BookSources/0-486-41147-8 "Special:BookSources/0-486-41147-8"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[43864234](https://search.worldcat.org/oclc/43864234).

[![Image 150](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Wiktionary-logo-en-v2.svg/40px-Wiktionary-logo-en-v2.svg.png)](https://en.wikipedia.org/wiki/File:Wiktionary-logo-en-v2.svg)

Look up _**[gradient](https://en.wiktionary.org/wiki/Special:Search/gradient "wiktionary:Special:Search/gradient")**_ in Wiktionary, the free dictionary.

*   ["Gradient"](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient). [Khan Academy](https://en.wikipedia.org/wiki/Khan_Academy "Khan Academy").
*   Kuptsov, L.P. (2001) [1994], ["Gradient"](https://www.encyclopediaofmath.org/index.php?title=Gradient), _[Encyclopedia of Mathematics](https://en.wikipedia.org/wiki/Encyclopedia\_of\_Mathematics "Encyclopedia of Mathematics")_, [EMS Press](https://en.wikipedia.org/wiki/European_Mathematical_Society "European Mathematical Society").
*   [Weisstein, Eric W.](https://en.wikipedia.org/wiki/Eric_W._Weisstein "Eric W. Weisstein")["Gradient"](https://mathworld.wolfram.com/Gradient.html). _[MathWorld](https://en.wikipedia.org/wiki/MathWorld "MathWorld")_.
