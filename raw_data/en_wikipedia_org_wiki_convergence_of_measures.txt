Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Informal descriptions 2 Total variation convergence of measures 3 Setwise convergence of measures 4 Weak convergence of measures Toggle Weak convergence of measures subsection 4.1 Weak convergence of random variables 4.2 Comparison with vague convergence 4.2.1 Vague Convergence 4.2.2 Weak Convergence 4.3 Weak convergence of measures as an example of weak-* convergence 5 See also 6 Notes and references 7 Further reading Toggle the table of contents Convergence of measures 2 languages Français 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Mathematical concept Not to be confused with Convergence in measure .

In mathematics , more specifically measure theory , there are various notions of the convergence of measures . For an intuitive general sense of what is meant by convergence of measures , consider a sequence of measures μ n on a space, sharing a common collection of measurable sets. Such a sequence might represent an attempt to construct 'better and better' approximations to a desired measure μ that is difficult to obtain directly. The meaning of 'better and better' is subject to all the usual caveats for taking limits ; for any error tolerance ε > 0 we require there be N sufficiently large for n ≥ N to ensure the 'difference' between μ n and μ is smaller than ε . Various notions of convergence specify precisely what the word 'difference' should mean in that description; these notions are not equivalent to one another, and vary in strength.

Three of the most common notions of convergence are described below.

Informal descriptions [ edit ] This section attempts to provide a rough intuitive description of three notions of convergence, using terminology developed in calculus courses; this section is necessarily imprecise as well as inexact, and the reader should refer to the formal clarifications in subsequent sections. In particular, the descriptions here do not address the possibility that the measure of some sets could be infinite, or that the underlying space could exhibit pathological behavior, and additional technical assumptions are needed for some of the statements. The statements in this section are however all correct if μ n is a sequence of probability measures on a Polish space .

The various notions of convergence formalize the assertion that the 'average value' of each 'sufficiently nice' function should converge: ∫ ∫ f d μ μ n → → ∫ ∫ f d μ μ {\displaystyle \int f\,d\mu _{n}\to \int f\,d\mu } To formalize this requires a careful specification of the set of functions under consideration and how uniform the convergence should be.

The notion of weak convergence requires this convergence to take place for every continuous bounded function f . 
This notion treats convergence for different functions f independently of one another, i.e., different functions f may require different values of N ≤ n to be approximated equally well (thus, convergence is non-uniform in f ).

The notion of setwise convergence formalizes the assertion that the measure of each measurable set should converge: μ μ n ( A ) → → μ μ ( A ) {\displaystyle \mu _{n}(A)\to \mu (A)} Again, no uniformity over the set A is required.
Intuitively, considering integrals of 'nice' functions, this notion provides more uniformity than weak convergence. As a matter of fact, when considering sequences of measures with uniformly bounded
variation on a Polish space , setwise convergence implies the convergence ∫ ∫ f d μ μ n → → ∫ ∫ f d μ μ {\textstyle \int f\,d\mu _{n}\to \int f\,d\mu } for any bounded measurable function f [ citation needed ] .
As before, this convergence is non-uniform in f .

The notion of total variation convergence formalizes the assertion that the measure of all measurable sets should converge uniformly , i.e. for every ε > 0 there exists N such that | μ μ n ( A ) − − μ μ ( A ) | < ε ε {\displaystyle |\mu _{n}(A)-\mu (A)|<\varepsilon } for every n > N and for every measurable set A . As before, this implies convergence of integrals against bounded measurable functions, but this time convergence is uniform over all functions bounded by any fixed constant.

Total variation convergence of measures [ edit ] This is the strongest notion of convergence shown on this page and is defined as follows. Let ( X , F ) {\displaystyle (X,{\mathcal {F}})} be a measurable space . The total variation distance between two (positive) measures μ and ν is then given by ‖ μ μ − − ν ν ‖ TV = sup f { ∫ ∫ X f d μ μ − − ∫ ∫ X f d ν ν } .

{\displaystyle \left\|\mu -\nu \right\|_{\text{TV}}=\sup _{f}\left\{\int _{X}f\,d\mu -\int _{X}f\,d\nu \right\}.} Here the supremum is taken over f ranging over the set of all measurable functions from X to [−1, 1] . This is in contrast, for example, to the Wasserstein metric , where the definition is of the same form, but the supremum is taken over f ranging over the set of those measurable functions from X to [−1, 1] which have Lipschitz constant at most 1; and also in contrast to the Radon metric , where the supremum is taken over f ranging over the set of continuous functions from X to [−1, 1] . In the case where X is a Polish space , the total variation metric coincides with the Radon metric.

If μ and ν are both probability measures , then the total variation distance is also given by ‖ μ μ − − ν ν ‖ TV = 2 ⋅ ⋅ sup A ∈ ∈ F | μ μ ( A ) − − ν ν ( A ) | .

{\displaystyle \left\|\mu -\nu \right\|_{\text{TV}}=2\cdot \sup _{A\in {\mathcal {F}}}|\mu (A)-\nu (A)|.} The equivalence between these two definitions can be seen as a particular case of the Monge–Kantorovich duality .  From the two definitions above, it is clear that the total variation distance between probability measures is always between 0 and 2.

To illustrate the meaning of the total variation distance, consider the following thought experiment. Assume that we are given two probability measures μ and ν , as well as a random variable X . We know that X has law either μ or ν but we do not know which one of the two. Assume that these two measures have prior probabilities 0.5 each of being the true law of X . Assume now that we are given one single sample distributed according to the law of X and that we are then asked to guess which one of the two distributions describes that law. The quantity 2 + ‖ ‖ μ μ − − ν ν ‖ ‖ TV 4 {\displaystyle {2+\|\mu -\nu \|_{\text{TV}} \over 4}} then provides a sharp upper bound on the prior probability that our guess will be correct.

Given the above definition of total variation distance, a sequence μ n of measures defined on the same measure space is said to converge to a measure μ in total variation distance if for every ε > 0 , there exists an N such that for all n > N , one has that [ 1 ] ‖ ‖ μ μ n − − μ μ ‖ ‖ TV < ε ε .

{\displaystyle \|\mu _{n}-\mu \|_{\text{TV}}<\varepsilon .} Setwise convergence of measures [ edit ] For ( X , F ) {\displaystyle (X,{\mathcal {F}})} a measurable space , a sequence μ n is said to converge setwise to a limit μ if lim n → → ∞ ∞ μ μ n ( A ) = μ μ ( A ) {\displaystyle \lim _{n\to \infty }\mu _{n}(A)=\mu (A)} for every set A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} .

Typical arrow notations are μ μ n → s w μ μ {\displaystyle \mu _{n}\xrightarrow {sw} \mu } and μ μ n → s μ μ {\displaystyle \mu _{n}\xrightarrow {s} \mu } .

For example, as a consequence of the Riemann–Lebesgue lemma , the sequence μ n of measures on the interval [−1, 1] given by μ n ( dx ) = (1 + sin( nx )) dx converges setwise to Lebesgue measure, but it does not converge in total variation.

In a measure theoretical or probabilistic context setwise convergence is often referred to as strong convergence (as opposed to weak convergence). This can lead to some ambiguity because in functional analysis , strong convergence usually refers to convergence with respect to a norm.

Weak convergence of measures [ edit ] In mathematics and statistics , weak convergence is one of many types of convergence relating to the convergence of measures . It depends on a topology on the underlying space and thus is not a purely measure-theoretic notion.

There are several equivalent definitions of weak convergence of a sequence of measures, some of which are (apparently) more general than others. The equivalence of these conditions is sometimes known as the Portmanteau theorem .

[ 2 ] Definition.

Let S {\displaystyle S} be a metric space with its Borel σ σ {\displaystyle \sigma } -algebra Σ Σ {\displaystyle \Sigma } . A bounded sequence of positive probability measures P n ( n = 1 , 2 , … … ) {\displaystyle P_{n}\,(n=1,2,\dots )} on ( S , Σ Σ ) {\displaystyle (S,\Sigma )} is said to converge weakly to a probability measure P {\displaystyle P} (denoted P n ⇒ ⇒ P {\displaystyle P_{n}\Rightarrow P} ) if any of the following equivalent conditions is true (here E n {\displaystyle \operatorname {E} _{n}} denotes expectation or the integral with respect to P n {\displaystyle P_{n}} , while E {\displaystyle \operatorname {E} } denotes expectation or the integral with respect to P {\displaystyle P} ): E n ⁡ ⁡ [ f ] → → E ⁡ ⁡ [ f ] {\displaystyle \operatorname {E} _{n}[f]\to \operatorname {E} [f]} for all bounded , continuous functions f {\displaystyle f} ; E n ⁡ ⁡ [ f ] → → E ⁡ ⁡ [ f ] {\displaystyle \operatorname {E} _{n}[f]\to \operatorname {E} [f]} for all bounded and Lipschitz functions f {\displaystyle f} ; lim sup E n ⁡ ⁡ [ f ] ≤ ≤ E ⁡ ⁡ [ f ] {\displaystyle \limsup \operatorname {E} _{n}[f]\leq \operatorname {E} [f]} for every upper semi-continuous function f {\displaystyle f} bounded from above; lim inf E n ⁡ ⁡ [ f ] ≥ ≥ E ⁡ ⁡ [ f ] {\displaystyle \liminf \operatorname {E} _{n}[f]\geq \operatorname {E} [f]} for every lower semi-continuous function f {\displaystyle f} bounded from below; lim sup P n ( C ) ≤ ≤ P ( C ) {\displaystyle \limsup P_{n}(C)\leq P(C)} for all closed sets C {\displaystyle C} of space S {\displaystyle S} ; lim inf P n ( U ) ≥ ≥ P ( U ) {\displaystyle \liminf P_{n}(U)\geq P(U)} for all open sets U {\displaystyle U} of space S {\displaystyle S} ; lim P n ( A ) = P ( A ) {\displaystyle \lim P_{n}(A)=P(A)} for all continuity sets A {\displaystyle A} of measure P {\displaystyle P} .

In the case S {\displaystyle S} and R {\displaystyle \mathbf {R} } (with its usual topology) are homeomorphic , if F n {\displaystyle F_{n}} and F {\displaystyle F} denote the cumulative distribution functions of the measures P n {\displaystyle P_{n}} and P {\displaystyle P} , respectively, then P n {\displaystyle P_{n}} converges weakly to P {\displaystyle P} if and only if lim n → → ∞ ∞ F n ( x ) = F ( x ) {\displaystyle \lim _{n\to \infty }F_{n}(x)=F(x)} for all points x ∈ ∈ R {\displaystyle x\in \mathbf {R} } at which F {\displaystyle F} is continuous.

For example, the sequence where P n {\displaystyle P_{n}} is the Dirac measure located at 1 / n {\displaystyle 1/n} converges weakly to the Dirac measure located at 0 (if we view these as measures on R {\displaystyle \mathbf {R} } with the usual topology), but it does not converge setwise. This is intuitively clear: we only know that 1 / n {\displaystyle 1/n} is "close" to 0 {\displaystyle 0} because of the topology of R {\displaystyle \mathbf {R} } .

This definition of weak convergence can be extended for S {\displaystyle S} any metrizable topological space . It also defines a weak topology on P ( S ) {\displaystyle {\mathcal {P}}(S)} , the set of all probability measures defined on ( S , Σ Σ ) {\displaystyle (S,\Sigma )} .  The weak topology is generated by the following basis of open sets: { U φ φ , x , δ δ | φ φ : S → → R is bounded and continuous, x ∈ ∈ R and δ δ > 0 } , {\displaystyle \left\{\ U_{\varphi ,x,\delta }\ \left|\quad \varphi :S\to \mathbf {R} {\text{ is bounded and continuous, }}x\in \mathbf {R} {\text{ and }}\delta >0\ \right.\right\},} where U φ φ , x , δ δ := { μ μ ∈ ∈ P ( S ) | | ∫ ∫ S φ φ d μ μ − − x | < δ δ } .

{\displaystyle U_{\varphi ,x,\delta }:=\left\{\ \mu \in {\mathcal {P}}(S)\ \left|\quad \left|\int _{S}\varphi \,\mathrm {d} \mu -x\right|<\delta \ \right.\right\}.} If S {\displaystyle S} is also separable , then P ( S ) {\displaystyle {\mathcal {P}}(S)} is metrizable and separable, for example by the Lévy–Prokhorov metric . If S {\displaystyle S} is also compact or Polish , so is P ( S ) {\displaystyle {\mathcal {P}}(S)} .

If S {\displaystyle S} is separable, it naturally embeds into P ( S ) {\displaystyle {\mathcal {P}}(S)} as the (closed) set of Dirac measures , and its convex hull is dense .

There are many "arrow notations" for this kind of convergence: the most frequently used are P n ⇒ ⇒ P {\displaystyle P_{n}\Rightarrow P} , P n ⇀ ⇀ P {\displaystyle P_{n}\rightharpoonup P} , P n → w P {\displaystyle P_{n}\xrightarrow {w} P} and P n → D P {\displaystyle P_{n}\xrightarrow {\mathcal {D}} P} .

Weak convergence of random variables [ edit ] Main article: Convergence of random variables Let ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},\mathbb {P} )} be a probability space and X be a metric space. If X n : Ω → X is a sequence of random variables then X n is said to converge weakly (or in distribution or in law ) to the random variable X : Ω → X as n → ∞ if the sequence of pushforward measures ( X n ) ∗ ( P ) converges weakly to X ∗ ( P ) in the sense of weak convergence of measures on X , as defined above.

Comparison with vague convergence [ edit ] Let X {\displaystyle X} be a metric space (for example R {\displaystyle \mathbb {R} } or [ 0 , 1 ] {\displaystyle [0,1]} ). The following spaces of test functions are commonly used in the convergence of probability measures.

[ 3 ] C c ( X ) {\displaystyle C_{c}(X)} the class of continuous functions f {\displaystyle f} each vanishing outside a compact set.

C 0 ( X ) {\displaystyle C_{0}(X)} the class of continuous functions f {\displaystyle f} such that lim | x | → → ∞ ∞ f ( x ) = 0 {\displaystyle \lim _{|x|\rightarrow \infty }f(x)=0} C B ( X ) {\displaystyle C_{B}(X)} the class of continuous bounded functions We have C c ⊂ ⊂ C 0 ⊂ ⊂ C B ⊂ ⊂ C {\displaystyle C_{c}\subset C_{0}\subset C_{B}\subset C} . Moreover, C 0 {\displaystyle C_{0}} is the closure of C c {\displaystyle C_{c}} with respect to uniform convergence.

[ 3 ] Vague Convergence [ edit ] A sequence of measures ( μ μ n ) n ∈ ∈ N {\displaystyle \left(\mu _{n}\right)_{n\in \mathbb {N} }} converges vaguely to a measure μ μ {\displaystyle \mu } if for all f ∈ ∈ C c ( X ) {\displaystyle f\in C_{c}(X)} , ∫ ∫ X f d μ μ n → → ∫ ∫ X f d μ μ {\displaystyle \int _{X}f\,d\mu _{n}\rightarrow \int _{X}f\,d\mu } .

Weak Convergence [ edit ] A sequence of measures ( μ μ n ) n ∈ ∈ N {\displaystyle \left(\mu _{n}\right)_{n\in \mathbb {N} }} converges weakly to a measure μ μ {\displaystyle \mu } if for all f ∈ ∈ C B ( X ) {\displaystyle f\in C_{B}(X)} , ∫ ∫ X f d μ μ n → → ∫ ∫ X f d μ μ {\displaystyle \int _{X}f\,d\mu _{n}\rightarrow \int _{X}f\,d\mu } .

In general, these two convergence notions are not equivalent.

In a probability setting, vague convergence and weak convergence of probability measures are equivalent assuming tightness . That is, a tight sequence of probability measures ( μ μ n ) n ∈ ∈ N {\displaystyle (\mu _{n})_{n\in \mathbb {N} }} converges vaguely to a probability measure μ μ {\displaystyle \mu } if and only if ( μ μ n ) n ∈ ∈ N {\displaystyle (\mu _{n})_{n\in \mathbb {N} }} converges weakly to μ μ {\displaystyle \mu } .

The weak limit of a sequence of probability measures, provided it exists, is a probability measure. In general, if tightness is not assumed, a sequence of probability (or sub-probability) measures may not necessarily converge vaguely to a true probability measure, but rather to a sub-probability measure (a measure such that μ μ ( X ) ≤ ≤ 1 {\displaystyle \mu (X)\leq 1} ).

[ 3 ] Thus, a sequence of probability measures ( μ μ n ) n ∈ ∈ N {\displaystyle (\mu _{n})_{n\in \mathbb {N} }} such that μ μ n → → v μ μ {\displaystyle \mu _{n}{\overset {v}{\to }}\mu } where μ μ {\displaystyle \mu } is not specified to be a probability measure is not guaranteed to imply weak convergence.

Weak convergence of measures as an example of weak-* convergence [ edit ] Despite having the same name as weak convergence in the context of functional analysis, weak convergence of measures is actually an example of weak-* convergence. The definitions of weak and weak-* convergences used in functional analysis are as follows: Let V {\displaystyle V} be a topological vector space or Banach space.

A sequence x n {\displaystyle x_{n}} in V {\displaystyle V} converges weakly to x {\displaystyle x} if φ φ ( x n ) → → φ φ ( x ) {\displaystyle \varphi \left(x_{n}\right)\rightarrow \varphi (x)} as n → → ∞ ∞ {\displaystyle n\to \infty } for all φ φ ∈ ∈ V ∗ ∗ {\displaystyle \varphi \in V^{*}} . One writes x n → → w x {\displaystyle x_{n}\mathrel {\stackrel {w}{\rightarrow }} x} as n → → ∞ ∞ {\displaystyle n\to \infty } .

A sequence of φ φ n ∈ ∈ V ∗ ∗ {\displaystyle \varphi _{n}\in V^{*}} converges in the weak-* topology to φ φ {\displaystyle \varphi } provided that φ φ n ( x ) → → φ φ ( x ) {\displaystyle \varphi _{n}(x)\rightarrow \varphi (x)} for all x ∈ ∈ V {\displaystyle x\in V} . That is, convergence occurs in the point-wise sense. In this case, one writes φ φ n → → w ∗ ∗ φ φ {\displaystyle \varphi _{n}\mathrel {\stackrel {w^{*}}{\rightarrow }} \varphi } as n → → ∞ ∞ {\displaystyle n\to \infty } .

To illustrate how weak convergence of measures is an example of weak-* convergence, we give an example in terms of vague convergence (see above). Let X {\displaystyle X} be a locally compact Hausdorff space. By the Riesz-Representation theorem , the space M ( X ) {\displaystyle M(X)} of Radon measures is isomorphic to a subspace of the space of continuous linear functionals on C 0 ( X ) {\displaystyle C_{0}(X)} . Therefore, for each Radon measure μ μ n ∈ ∈ M ( X ) {\displaystyle \mu _{n}\in M(X)} , there is a linear functional φ φ n ∈ ∈ C 0 ( X ) ∗ ∗ {\displaystyle \varphi _{n}\in C_{0}(X)^{*}} such that φ φ n ( f ) = ∫ ∫ X f d μ μ n {\displaystyle \varphi _{n}(f)=\int _{X}f\,d\mu _{n}} for all f ∈ ∈ C 0 ( X ) {\displaystyle f\in C_{0}(X)} . Applying the definition of weak-* convergence in terms of linear functionals, the characterization of vague convergence of measures is obtained. For compact X {\displaystyle X} , C 0 ( X ) = C B ( X ) {\displaystyle C_{0}(X)=C_{B}(X)} , so in this case weak convergence of measures is a special case of weak-* convergence.

See also [ edit ] Convergence of random variables Lévy–Prokhorov metric Prokhorov's theorem Tightness of measures Notes and references [ edit ] ^ Madras, Neil; Sezer, Deniz (25 Feb 2011). "Quantitative bounds for Markov chain convergence: Wasserstein and total variation distances".

Bernoulli .

16 (3): 882– 908.

arXiv : 1102.5245 .

doi : 10.3150/09-BEJ238 .

S2CID 88518773 .

^ Klenke, Achim (2006).

Probability Theory . Springer-Verlag.

ISBN 978-1-84800-047-6 .

^ a b c Chung, Kai Lai (1974).

A course in probability theory . Internet Archive. New York, Academic Press. pp.

84– 99.

ISBN 978-0-12-174151-8 .

Further reading [ edit ] Ambrosio, L., Gigli, N. & Savaré, G. (2005).

Gradient Flows in Metric Spaces and in the Space of Probability Measures . Basel: ETH Zürich, Birkhäuser Verlag.

ISBN 3-7643-2428-7 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) Billingsley, Patrick (1995).

Probability and Measure . New York, NY: John Wiley & Sons, Inc.

ISBN 0-471-00710-2 .

Billingsley, Patrick (1999).

Convergence of Probability Measures . New York, NY: John Wiley & Sons, Inc.

ISBN 0-471-19745-9 .

v t e Measure theory Basic concepts Absolute continuity of measures Lebesgue integration L p spaces Measure Measure space Probability space Measurable space / function Sets Almost everywhere Atom Baire set Borel set equivalence relation Borel space Carathéodory's criterion Cylindrical σ-algebra Cylinder set 𝜆-system Essential range infimum/supremum Locally measurable π -system σ-algebra Non-measurable set Vitali set Null set Support Transverse measure Universally measurable Types of measures Atomic Baire Banach Besov Borel Brown Complex Complete Content ( Logarithmically ) Convex Decomposable Discrete Equivalent Finite Inner ( Quasi- ) Invariant Locally finite Maximising Metric outer Outer Perfect Pre-measure ( Sub- ) Probability Projection-valued Radon Random Regular Borel regular Inner regular Outer regular Saturated Set function σ-finite s-finite Signed Singular Spectral Strictly positive Tight Vector Particular measures Counting Dirac Euler Gaussian Haar Harmonic Hausdorff Intensity Lebesgue Infinite-dimensional Logarithmic Product Projections Pushforward Spherical measure Tangent Trivial Young Maps Measurable function Bochner Strongly Weakly Convergence: almost everywhere of measures in measure of random variables in distribution in probability Cylinder set measure Random: compact set element measure process variable vector Projection-valued measure Main results Carathéodory's extension theorem Convergence theorems Dominated Monotone Vitali Decomposition theorems Hahn Jordan Maharam's Egorov's Fatou's lemma Fubini's Fubini–Tonelli Hölder's inequality Minkowski inequality Radon–Nikodym Riesz–Markov–Kakutani representation theorem Other results Disintegration theorem Lifting theory Lebesgue's density theorem Lebesgue differentiation theorem Sard's theorem Vitali–Hahn–Saks theorem For Lebesgue measure Isoperimetric inequality Brunn–Minkowski theorem Milman's reverse Minkowski–Steiner formula Prékopa–Leindler inequality Vitale's random Brunn–Minkowski inequality Applications & related Convex analysis Descriptive set theory Probability theory Real analysis Spectral theory This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( February 2010 ) ( Learn how and when to remove this message ) NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐m2fdx
Cached time: 20250812015058
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.431 seconds
Real time usage: 0.747 seconds
Preprocessor visited node count: 2817/1000000
Revision size: 18885/2097152 bytes
Post‐expand include size: 52097/2097152 bytes
Template argument size: 2771/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 33878/5000000 bytes
Lua time usage: 0.206/10.000 seconds
Lua memory usage: 5996946/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  371.331      1 -total
 24.46%   90.821      1 Template:Reflist
 20.24%   75.153      2 Template:Navbox
 20.02%   74.344      1 Template:Measure_theory
 18.40%   68.307      1 Template:Short_description
 18.25%   67.772      1 Template:Cite_journal
 11.81%   43.847      2 Template:Pagetype
 10.14%   37.671      1 Template:More_footnotes
  8.55%   31.739      1 Template:Ambox
  8.27%   30.706      1 Template:Citation_needed Saved in parser cache with key enwiki:pcache:6811795:|#|:idhash:canonical and timestamp 20250812015058 and revision id 1284455581. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Convergence_of_measures&oldid=1284455581 " Categories : Measure theory Convergence (mathematics) Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from February 2024 CS1 maint: multiple names: authors list Articles lacking in-text citations from February 2010 All articles lacking in-text citations This page was last edited on 7 April 2025, at 18:10 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Convergence of measures 2 languages Add topic

