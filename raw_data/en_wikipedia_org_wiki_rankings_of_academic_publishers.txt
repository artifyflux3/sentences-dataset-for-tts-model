Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Ranking challenges 2 Australian Political Science rankings 3 SENSE rankings 4 Spanish National Research Council rankings 5 Granada rankings 6 Libcitation rankings 7 See also 8 References 9 Further reading 10 External links Toggle the table of contents Rankings of academic publishers 4 languages Español 日本語 Русский Slovenščina Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia There are a number of approaches to ranking academic publishing groups and publishers .

[ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] Rankings rely on subjective impressions by the scholarly community, on analyses of prize winners of scientific associations, discipline, a publisher's reputation, and its impact factor (particularly in the sciences).

Ranking challenges [ edit ] Publications are often judged by venue, rather than merit.

[ 7 ] This has been criticized in the Leiden Manifesto [ 8 ] and the San Francisco Declaration on Research Assessment . According to the manifesto, "Science and technology indicators are prone to conceptual ambiguity and uncertainty and require strong assumptions that are not universally accepted. The meaning of citation counts, for example, has long been debated. Thus, best practice uses multiple indicators to provide a more robust and pluralistic picture." [ 8 ] Moreover, studies of methodological quality and reliability have found that "reliability of published research works in several fields may be decreasing with increasing journal rank", [ 9 ] contrary to widespread expectations.

[ 10 ] In a study assessing an increasingly-diversified array of publishers and their service to the academic community, Janice S. Lewis concluded that college and university librarians ranked university presses higher and commercial publishers lower than did members of the American Political Science Association .

[ 4 ] According to Colin Steele, a librarian at the Australian National Library in Canberra , "Listings of publishers by title also fail to take into account that some university presses are strong in certain disciplines, but not across the whole spectrum." [ 11 ] Rankings can vary widely by discipline.

Australian Political Science rankings [ edit ] The Australian Political Studies Association (APSA)  ranked academic publishers in 2007, taking into consideration both book and journal publication.

[ 12 ] By 2022 this was replaced by a ranking of journal titles only.

[ 13 ] In 2007, their top-ranked (A+) publishers were: Cambridge University Press University of Chicago Press Columbia University Press Harvard University Press MIT Press Oxford University Press /Clarendon (UK/US) Princeton University Press Stanford University Press University of California Press Yale University Press In 2007, their second-ranked (A) publishers were: Alfred A. Knopf Allen & Unwin Cornell University Press Duke University Press Edward Elgar Elsevier Science Ltd IPA , Warsaw Johns Hopkins University Press Kluwer Manchester University Press Melbourne University Press New York University Press Palgrave MacMillan (UK and Australia, St. Martin's Press in US) Politico's Polity Press Routledge ( Taylor and Francis ) Sage Publishing Science Publishers Univ. of Pennsylvania Press University of Michigan Press University of Minnesota Press University of New South Wales Press University of Toronto Press WHO/EDM, Geneva Wiley-Blackwell AP, London Basic Books , New York Blackwell , Oxford Clarendon Press , Gloucestershire, UK CRC , Ghent, Belgium CRC, New York Harper & Row , New York John Wiley & Sons , West Sussex, UK Pergamon Press , Oxford/Amsterdam Prentice Hall , Eaglewood Cliffs (NJ), US Random House , New York Springer , London/Berlin SENSE rankings [ edit ] The Research School for Socio-Economic and Natural Sciences of the Environment (SENSE Research School) has ranked scientific publishers every year from 2006 until 2022.

[ 14 ] This ranking was intended for internal use only and is not anymore available.

Spanish National Research Council rankings [ edit ] In 2012 and 2014, the Spanish National Research Council asked 11,864 Spanish academics to name the 10 most prestigious academic publishers from over 600 international and 500 Spanish-language publishers. It received 2,731 responses, a response rate of 23.05 percent. Results were compiled using a weighted average .

[ 15 ] The results were: Cambridge University Press Oxford University Press Springer Nature Routledge Elsevier Peter Lang Thomson Reuters Blackwell De Gruyter McGraw Hill [ 15 ] IGI Global Granada rankings [ edit ] To quantitatively assess the output of a publishing company, in 2014 a research group associated with the University of Granada created a methodology based on the Thomson-Reuters Book Citation Index .

[ 16 ] The quantitative weight of the publishers is based on output data, impact (citations) and publisher profile. According to the Granada study, the 10 leading companies were: [ 16 ] Springer Palgrave Macmillan Routledge Cambridge University Press Elsevier Nova Science Publishers Edward Elgar Information Age Publishing Princeton University Press University of California Press IGI Global Libcitation rankings [ edit ] The Research Impact Measurement Service (RIMS) at the University of New South Wales presented a quantitative methodology of bibliometric comparisons of book publishers.

[ 17 ] [ 18 ] [ 19 ] In a Journal of the American Society for Information Science and Technology article, Howard D. White et al. wrote: "Bibliometric measures for evaluating research units in the book-oriented humanities and social sciences are underdeveloped relative to those available for journal-oriented science and technology". The RIMS proposed what they called a "libcitation count", counting the libraries holding a given book as reported in a national (or international) union catalog . In the follow-up literature, comparing research units or even the output of publishing companies became the target of research.

[ 17 ] [ 20 ] White et al. wrote, Libcitation counts reflect judgments by librarians on the usefulness of publications for their various audiences of readers. The Libcitation measure thus resembles a citation impact measure in discriminating values of publications on a defined ground. It rewards authors whose books (or other publications) are seen by librarians as having relatively wide appeal. A book's absolute appeal can be determined simply by counting how many libraries hold it, but it can also be gauged in relation to other books in its subject class.

[ 17 ] Libcitations , according to the RIMS, reflect what librarians know about the prestige of publishers, the opinions of reviewers, and the reputations of authors.

[ 17 ] See also [ edit ] Academic publishing Bibliometrics Citation impact Journal ranking Informetrics Publishing References [ edit ] ^ Goodson, Larry P.; Dillman, Bradford; Hira, Anil (1999).

"Ranking the Presses: Political Scientists' Evaluations of Publisher Quality" .

PS: Political Science and Politics .

32 (2): 257– 262.

doi : 10.1017/S1049096500049416 .

JSTOR 420561 .

^ Steele, Colin (2008).

"Scholarly Monograph Publishing in the 21st Century: The Future More Than Ever Should be an Open Book" .

The Journal of Electronic Publishing .

11 (2).

doi : 10.3998/3336451.0011.201 .

^ Garand, James C.; Giles, Micheal W. (2011).

"Ranking Scholarly Publishers in Political Science: An Alternative Approach" .

PS: Political Science and Politics .

44 (2): 375– 383.

doi : 10.1017/S1049096511000229 .

JSTOR 41319924 .

^ a b Lewis, Janice S. (2000).

"An Assessment of Publisher Quality by Political Science Librarians" .

College & Research Libraries .

61 (4): 313– 323.

doi : 10.5860/crl.61.4.313 .

^ Samuels, David (2013).

"Book Citations Count" .

PS: Political Science & Politics .

46 (4): 785– 790.

doi : 10.1017/S1049096513001054 .

^ Rhodes, R. A. W.; Hamilton, Margaret (2007).

"Australian Political Science: Journal and Publisher Rankings" (PDF) .

^ Lee, Icy (2014). "Publish or perish: The myth and reality of academic publishing".

Language Teaching .

47 (2): 250– 261.

doi : 10.1017/S0261444811000504 .

S2CID 146536290 .

^ a b Hicks D, Wouters P, Waltman L, de Rijcke S, Rafols I (23 April 2015).

"The Leiden Manifesto for research metrics" (PDF) .

Nature .

520 (7548): 429– 431.

doi : 10.1038/520429a .

PMID 25903611 .

S2CID 4462115 . Retrieved 18 October 2017 .

^ Brembs, Björn (2018).

"Prestigious Science Journals Struggle to Reach Even Average Reliability" .

Frontiers in Human Neuroscience .

12 : 37.

doi : 10.3389/fnhum.2018.00037 .

PMC 5826185 .

PMID 29515380 .

^ Triggle, Chris R; MacDonald, Ross; Triggle, David J.; Grierson, Donald (3 April 2022).

"Requiem for impact factors and high publication charges" .

Accountability in Research .

29 (3): 133– 164.

doi : 10.1080/08989621.2021.1909481 .

PMID 33787413 .

One might expect, therefore, that a high JIF factor indicates a higher standard of interest, accuracy and reliability of papers published therein. This is sometimes true but unfortunately is certainly not always the case (Brembs 2018, 2019). Thus, Björn Brembs (2019) concluded: "There is a growing body of evidence against our subjective notion of more prestigious journals publishing 'better' science. In fact, the most prestigious journals may be publishing the least reliable science." ^ Steele, Colin (2008).

"Scholarly Monograph Publishing in the 21st Century: The Future More Than Ever Should be an Open Book" .

The Journal of Electronic Publishing .

11 (2).

doi : 10.3998/3336451.0011.201 .

^ "Ranking List of Academic Book Publishers" (PDF) . Archived from the original (PDF) on 18 October 2017.

^ "APSA Preferred Journal List Review 2022" . 10 October 2022.

^ "SENSE – Quality & Criteria" .

www.sense.nl .

^ a b http://ilia.cchs.csic.es/SPI/metodologia_2014.html Archived 7 March 2022 at the Wayback Machine and http://ilia.cchs.csic.es/SPI/prestigio_expertos_2014.php Archived 23 June 2022 at the Wayback Machine ^ a b Torres-Salinas, Daniel; Robinson-Garcia, Nicolas; Miguel Campanario, Juan; Delgado López-Cózar, Emilio (2014). "Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index".

Online Information Review .

38 : 24– 42.

arXiv : 1312.2791 .

doi : 10.1108/OIR-10-2012-0169 .

S2CID 3794376 .

^ a b c d White, Howard D.; Boell, Sebastian K.; Yu, Hairong; Davis, Mari; Wilson, Concepción S.; Cole, Fletcher T.H. (2009).

"Libcitations: A measure for comparative assessment of book publications in the humanities and social sciences" .

Journal of the American Society for Information Science and Technology .

60 (6): 1083– 1096.

doi : 10.1002/asi.21045 .

hdl : 1959.4/44715 .

S2CID 33661687 .

^ Drummond, Robyn; Wartho, Richard (2009). "RIMS: The Research Impact Measurement Service at the University of New South Wales".

Australian Academic & Research Libraries .

40 (2): 76– 87.

doi : 10.1080/00048623.2009.10721387 .

^ For a recent summary of the literature see Tausch, Arno (2017), Die Buchpublikationen der Nobelpreis-Ökonomen und die führenden Buchverlage der Disziplin. Eine bibliometrische Analyse Bibliotheksdienst, March 2017: 339 – 374.

SSRN 2674502 ^ Zuccala, A., Guns, R., Cornacchia, R., & Bod, R. (2014). Can we rank scholarly book publishers? A bibliometric experiment with the field of history. Journal of the Association for Information Science and Technology Further reading [ edit ] This " further reading " section may need cleanup .

Please read the editing guide and help improve the section.

( January 2025 ) ( Learn how and when to remove this message ) Amsler, S. S., & Bolsmann, C. (2012). University ranking as social exclusion. British journal of sociology of education, 33(2), 283–301.

Andrés, A. (2009). Measuring academic research: how to undertake a bibliometric study. Oxford: Chandos Publishing.

Bornmann, L., Mutz, R., & Daniel, H. D. (2013). Multilevel‐statistical reformulation of citation‐based university rankings: The Leiden ranking 2011/2012. Journal of the American Society for Information Science and Technology, 64(8), 1649–1658.

Braun, Tibor et al. (1985). Scientometric indicators: a 32 country comparative evaluation of publishing performance and citation impact. Singapore; Philadelphia: World Scientific.

Dill, D. D., & Soo, M. (2005). Academic quality, league tables, and public policy: A cross-national analysis of university ranking systems. Higher Education, 49(4), 495–533.

Donohue, Joseph C. (1974). Understanding scientific literatures: a bibliometric approach. Cambridge, MIT Press.

Drummond, R., & Wartho, R. (2009). RIMS: the research impact measurement service at the University of New South Wales. Australian Academic & Research Libraries, Herb, U., Kranz, E., Leidinger, T., & Mittelsdorf, B. (2010). How to assess the impact of an electronic document? And what does impact mean anyway? Reliable usage statistics in heterogeneous repository communities. OCLC Systems & Services: International digital library perspectives, 26(2), 133–145.

Hug, Sven E.; Ochsner, Michael; Daniel, Hans-Dieter. (2013). Criteria for assessing research quality in the humanities: a Delphi study among scholars of English literature, German literature and art history. Research Evaluation. Dec2013, Vol. 22 Issue 5, p369-383.

Kousha, K., Thelwall, M., & Rezaie, S. (2011). Assessing the citation impact of books: The role of Google Books, Google Scholar, and Scopus. Journal of the American Society for Information Science and Technology, 62(11), 2147–2164.

Oltersdorf, J. (2013). Publikationen: Funktion und Repräsentation (Doctoral dissertation, Humboldt-Universität zu Berlin, Philosophische Fakultät I).

Rostaing, H., Boutin, E., & Mannina, B. (1999). Evaluation of internet resources: bibliometric techniques applications. cybermetrics, 99.

Sadlak, J., & Liu, N. C. (2007). The world-class university and ranking: Aiming beyond status. Bucharest, Romania/Shanghai, China/Cluj-Napoca, Romania: Unesco-Cepes.

Sahel, J. A. (2011). Quality versus quantity: assessing individual research performance. Science translational medicine, 3(84) Sieber, J., & Gradmann, S. (2011). How to best assess monographs?. Humboldt University Berlin.

Tausch, A. (2011). On the Global Impact of Selected Social-Policy Publishers in More Than 100 Countries. Journal of Scholarly Publishing, 42(4), 476–513.

Tausch, A. (2018). The Market Power of Global Scientific Publishing Companies in the Age of Globalization: An Analysis Based on the OCLC Worldcat (June 16, 2018). Journal of Globalization Studies, 9(2), 63–91. Also Available at SSRN: https://ssrn.com/abstract=3197632 or http://dx.doi.org/10.2139/ssrn.3197632 .

Tausch, A. (2022). Beyond 'Channel Registers' Ways and Aberrations of Ranking International Academic Book Publishers (September 18, 2022). Available at SSRN: https://ssrn.com/abstract=4222481 or http://dx.doi.org/10.2139/ssrn.4222481 .

Tausch, Arno (2023), Bibliometry from a global perspective : library and classroom outreach and the future ranking of political scientists and publishers , Nova Science Publishers, ISBN 9798886978865 Taylor, P., & Braddock, R. (2007). International university ranking systems and the idea of university excellence. Journal of Higher Education Policy and Management, 29(3), Thelwall, M., Klitkou, A., Verbeek, A., Stuart, D., & Vincent, C. (2010). Policy‐ relevant Webometrics for individual scientific fields. Journal of the American Society for Information Science and Technology, 61(7), 1464–1475.

Torres-Salinas, D., Robinson-García, N., & López-Cózar, E. D. (2012). Towards a Book Publishers Citation Reports. First approach using the Book Citation Index. arXiv preprint arXiv:1207.7067.

Torres-Salinas, D., Robinson-García, N., Cabezas-Clavijo, Á., & Jiménez-Contreras, E. (2014). Analyzing the citation characteristics of books: edited books, book series and publisher types in the book citation index. Scientometrics, 98(3), 2113–2127.

Torres-Salinas, D., Robinson-Garcia, N., Miguel Campanario, J., & Delgado López- Cózar, E. (2014). Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index. Online Information Review, 38(1), 24–42.

Torres-Salinas, D., Rodríguez-Sánchez, R., Robinson-García, N., Fdez-Valdivia, J., & García, J. A. (2013). Mapping citation patterns of book chapters in the Book Citation Index. Journal of Informetrics, 7(2), 412–424.

Usher, A., & Savino, M. (2007). A global survey of university ranking and league tables. Higher Education in Europe, 32(1), 5–15.

Vinkler, Peter (2010). The evaluation of research by scientometric indicators. Oxford [England] : Chandos Publishing.

Waltman, L., & Schreiber, M. (2013). On the calculation of percentile‐based bibliometric indicators. Journal of the American Society for Information Science and Technology, 64(2), 372–379.

White, H. D.; Boell, Sebastian K.; Yu, H.; Davis, M.; Wilson, C. S.; Cole, Fletcher T.H. J. (2009) Libcitations: A measure for comparative assessment of book publications in the humanities and social sciences. Journal of the American Society for Information Science & Technology. Jun2009, Vol. 60 Issue 6, p1083-1096.

Zuccala, A. A., & White, H. D. (2015). Correlating Libcitations and Citations in the Humanities with WorldCat and Scopus Data. In A. A. Salah, Y. Tonta, A. A. Akdag Salah, C. Sugimoto, & U. Al (Eds.), Proceedings of the 15th International Society for Scientometrics and Informetrics (ISSI), Istanbul, Turkey, 29 June to 4 July 2015. (pp. 305–316). Bogazici University.

Zuccala, A., & Guns, R. (2013). Comparing book citations in humanities journals to library holdings: Scholarly use versus perceived cultural benefit. In 14th international conference of the international society for scientometrics and informetrics (pp. 353–360).

Zuccala, A., Guns, R., Cornacchia, R., & Bod, R. (2014). Can we rank scholarly book publishers? A bibliometric experiment with the field of history. Journal of the Association for Information Science and Technology.

Zuccala, A., Someren, M., & Bellen, M. (2014). A machine‐learning approach to coding book reviews as quality indicators: Toward a theory of megacitation. Journal of the Association for Information Science and Technology, 65(11), 2248–2260.

External links [ edit ] Scilit rankings of journals and publishers, automated by MDPI v t e Academic publishing Journals Academic journal Public health Papers Paper Abstract Review article Position paper Literature review Grey literature Working paper White paper Technical report Annual report Pamphlet Essay Lab notes Other publication types Thesis Collection of articles Patent Biological Chemical Book Monograph Chapter Treatise Poster session Proceedings Impact and ranking Acknowledgment index Altmetrics Article-level metrics Author-level metrics Bibliometrics C-score Journal ranking Eigenfactor g-index h -index Impact factor Rankings of academic publishers Science-wide author databases of standardized citation indicators Scientometrics SCImago Journal Rank Citation cartel Reform and access Academic journal publishing reform Open access Citation advantage Serials crisis Sci-Hub #ICanHazPDF Versioning Preprint Postprint Version of record Erratum Retraction Indexes and search engines Google Scholar AMiner BASE CORE Semantic Scholar Scopus Web of Science Paperity OpenAlex Index Copernicus ERIH PLUS Sherpa Romeo OpenAIRE Related topics Imprint Scientific writing Peer review Scholarly communication Scientific literature Learned society Open research Open scientific data ORCID Electronic publishing Ingelfinger rule Least publishable unit " Publish or perish " Lists Academic databases and search engines Academic journals Copyright policies Highly Cited Researchers Open-access journals Preprint policies Scientific journals Style/formatting guides University presses v t e Science and technology studies Economics Economics of science Economics of scientific knowledge History History and philosophy of science History of science and technology History of technology Languages of science Scientific language Philosophy Anthropocene Antipositivism Empiricism Fuzzy logic Neo-Luddism Philosophy of science Philosophy of social science Philosophy of technology Positivism Postpositivism Religion and science Scientism Social constructivism Social epistemology Transhumanism Sociology Actor–network theory Social construction of technology shaping of technology Sociology of knowledge scientific Sociology of scientific ignorance Sociology of the history of science Sociotechnology Strong programme Science studies Academic bias Antiscience Bibliometrics Boundary-work Consilience Criticism of science Demarcation problem Double hermeneutic Logology Mapping controversies Metascience Paradigm shift black swan events Pseudoscience Psychology of science Science citizen communication education normal Neo-colonial post-normal rhetoric wars Scientific community consensus controversy dissent enterprise literacy method misconduct priority skepticism Scientocracy Scientometrics Team science Traditional knowledge ecological Unity of science Women in science STEM Technology studies Co-production Cyborg anthropology Design studies Dematerialization Digital anthropology Digital media use and mental health Early adopter Engineering studies Financial technology Hype cycle Innovation diffusion disruptive linear model system user Leapfrogging Normalization process theory Media studies Reverse salient Skunkworks project Sociotechnical system Technical change Technocracy Technoscience feminist Technological change convergence determinism revolution transitions Technology and society criticism of dynamics theories of transfer Women in engineering Policy Academic freedom Digital divide Evidence-based policy Factor 10 Funding of science Horizon scanning Politicization of science Regulation of science Research ethics Right to science Science policy history of science of Technology assessment Technology policy Transition management Portals Science History of science Technology Category Associations Journals Scholars Retrieved from " https://en.wikipedia.org/w/index.php?title=Rankings_of_academic_publishers&oldid=1301251710 " Categories : Academic publishing companies Bibliometrics Library science University and college rankings Hidden categories: Webarchive template wayback links Articles with short description Short description with empty Wikidata description Use dmy dates from August 2019 Wikipedia spam cleanup from January 2025 Wikipedia further reading cleanup This page was last edited on 18 July 2025, at 22:05 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Rankings of academic publishers 4 languages Add topic

