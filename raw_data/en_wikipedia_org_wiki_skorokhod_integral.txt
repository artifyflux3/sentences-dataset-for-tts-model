Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Preliminaries: the Malliavin derivative 1.2 The Skorokhod integral 2 Properties 3 Alternatives 4 References Toggle the table of contents Skorokhod integral 2 languages Deutsch Português Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In mathematics , the Skorokhod integral , also named Hitsuda–Skorokhod integral , often denoted δ δ {\displaystyle \delta } , is an operator of great importance in the theory of stochastic processes .  It is named after the Ukrainian mathematician Anatoliy Skorokhod and Japanese mathematician Masuyuki Hitsuda .  Part of its importance is that it unifies several concepts: δ δ {\displaystyle \delta } is an extension of the Itô integral to non- adapted processes ; δ δ {\displaystyle \delta } is the adjoint of the Malliavin derivative , which is fundamental to the stochastic calculus of variations ( Malliavin calculus ); δ δ {\displaystyle \delta } is an infinite-dimensional generalization of the divergence operator from classical vector calculus .

The integral was introduced by Hitsuda in 1972 [ 1 ] and by Skorokhod in 1975.

[ 2 ] Definition [ edit ] Preliminaries: the Malliavin derivative [ edit ] Consider a fixed probability space ( Ω Ω , Σ Σ , P ) {\displaystyle (\Omega ,\Sigma ,\mathbf {P} )} and a Hilbert space H {\displaystyle H} ; E {\displaystyle \mathbf {E} } denotes expectation with respect to P {\displaystyle \mathbf {P} } E [ X ] := ∫ ∫ Ω Ω X ( ω ω ) d P ( ω ω ) .

{\displaystyle \mathbf {E} [X]:=\int _{\Omega }X(\omega )\,\mathrm {d} \mathbf {P} (\omega ).} Intuitively speaking, the Malliavin derivative of a random variable F {\displaystyle F} in L p ( Ω Ω ) {\displaystyle L^{p}(\Omega )} is defined by expanding it in terms of Gaussian random variables that are parametrized by the elements of H {\displaystyle H} and differentiating the expansion formally; the Skorokhod integral is the adjoint operation to the Malliavin derivative.

Consider a family of R {\displaystyle \mathbb {R} } -valued random variables W ( h ) {\displaystyle W(h)} , indexed by the elements h {\displaystyle h} of the Hilbert space H {\displaystyle H} .  Assume further that each W ( h ) {\displaystyle W(h)} is a Gaussian ( normal ) random variable, that the map taking h {\displaystyle h} to W ( h ) {\displaystyle W(h)} is a linear map , and that the mean and covariance structure is given by E [ W ( h ) ] = 0 , {\displaystyle \mathbf {E} [W(h)]=0,} E [ W ( g ) W ( h ) ] = ⟨ ⟨ g , h ⟩ ⟩ H , {\displaystyle \mathbf {E} [W(g)W(h)]=\langle g,h\rangle _{H},} for all g {\displaystyle g} and h {\displaystyle h} in H {\displaystyle H} .  It can be shown that, given H {\displaystyle H} , there always exists a probability space ( Ω Ω , Σ Σ , P ) {\displaystyle (\Omega ,\Sigma ,\mathbf {P} )} and a family of random variables with the above properties.  The Malliavin derivative is essentially defined by formally setting the derivative of the random variable W ( h ) {\displaystyle W(h)} to be h {\displaystyle h} , and then extending this definition to " smooth enough " random variables.  For a random variable F {\displaystyle F} of the form F = f ( W ( h 1 ) , … … , W ( h n ) ) , {\displaystyle F=f(W(h_{1}),\ldots ,W(h_{n})),} where f : R n → → R {\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} } is smooth, the Malliavin derivative is defined using the earlier "formal definition" and the chain rule: D F := ∑ ∑ i = 1 n ∂ ∂ f ∂ ∂ x i ( W ( h 1 ) , … … , W ( h n ) ) h i .

{\displaystyle \mathrm {D} F:=\sum _{i=1}^{n}{\frac {\partial f}{\partial x_{i}}}(W(h_{1}),\ldots ,W(h_{n}))h_{i}.} In other words, whereas F {\displaystyle F} was a real-valued random variable, its derivative D F {\displaystyle \mathrm {D} F} is an H {\displaystyle H} -valued random variable, an element of the space L p ( Ω Ω ; H ) {\displaystyle L^{p}(\Omega ;H)} .  Of course, this procedure only defines D F {\displaystyle \mathrm {D} F} for "smooth" random variables, but an approximation procedure can be employed to define D F {\displaystyle \mathrm {D} F} for F {\displaystyle F} in a large subspace of L p ( Ω Ω ) {\displaystyle L^{p}(\Omega )} ;  the domain of D {\displaystyle \mathrm {D} } is the closure of the smooth random variables in the seminorm : ‖ ‖ F ‖ ‖ 1 , p := ( E [ | F | p ] + E [ ‖ ‖ D F ‖ ‖ H p ] ) 1 / p .

{\displaystyle \|F\|_{1,p}:={\big (}\mathbf {E} [|F|^{p}]+\mathbf {E} [\|\mathrm {D} F\|_{H}^{p}]{\big )}^{1/p}.} This space is denoted by D 1 , p {\displaystyle \mathbf {D} ^{1,p}} and is called the Watanabe–Sobolev space .

The Skorokhod integral [ edit ] For simplicity, consider now just the case p = 2 {\displaystyle p=2} .  The Skorokhod integral δ δ {\displaystyle \delta } is defined to be the L 2 {\displaystyle L^{2}} -adjoint of the Malliavin derivative D {\displaystyle \mathrm {D} } .  Just as D {\displaystyle \mathrm {D} } was not defined on the whole of L 2 ( Ω Ω ) {\displaystyle L^{2}(\Omega )} , δ δ {\displaystyle \delta } is not defined on the whole of L 2 ( Ω Ω ; H ) {\displaystyle L^{2}(\Omega ;H)} :  the domain of δ δ {\displaystyle \delta } consists of those processes u {\displaystyle u} in L 2 ( Ω Ω ; H ) {\displaystyle L^{2}(\Omega ;H)} for which there exists a constant C ( u ) {\displaystyle C(u)} such that, for all F {\displaystyle F} in D 1 , 2 {\displaystyle \mathbf {D} ^{1,2}} , | E [ ⟨ ⟨ D F , u ⟩ ⟩ H ] | ≤ ≤ C ( u ) ‖ ‖ F ‖ ‖ L 2 ( Ω Ω ) .

{\displaystyle {\big |}\mathbf {E} [\langle \mathrm {D} F,u\rangle _{H}]{\big |}\leq C(u)\|F\|_{L^{2}(\Omega )}.} The Skorokhod integral of a process u {\displaystyle u} in L 2 ( Ω Ω ; H ) {\displaystyle L^{2}(\Omega ;H)} is a real-valued random variable δ δ u {\displaystyle \delta u} in L 2 ( Ω Ω ) {\displaystyle L^{2}(\Omega )} ; if u {\displaystyle u} lies in the domain of δ δ {\displaystyle \delta } , then δ δ u {\displaystyle \delta u} is defined by the relation that, for all F ∈ ∈ D 1 , 2 {\displaystyle F\in \mathbf {D} ^{1,2}} , E [ F δ δ u ] = E [ ⟨ ⟨ D F , u ⟩ ⟩ H ] .

{\displaystyle \mathbf {E} [F\,\delta u]=\mathbf {E} [\langle \mathrm {D} F,u\rangle _{H}].} Just as the Malliavin derivative D {\displaystyle \mathrm {D} } was first defined on simple, smooth random variables, the Skorokhod integral has a simple expression for "simple processes":  if u {\displaystyle u} is given by u = ∑ ∑ j = 1 n F j h j {\displaystyle u=\sum _{j=1}^{n}F_{j}h_{j}} with F j {\displaystyle F_{j}} smooth and h j {\displaystyle h_{j}} in H {\displaystyle H} , then δ δ u = ∑ ∑ j = 1 n ( F j W ( h j ) − − ⟨ ⟨ D F j , h j ⟩ ⟩ H ) .

{\displaystyle \delta u=\sum _{j=1}^{n}\left(F_{j}W(h_{j})-\langle \mathrm {D} F_{j},h_{j}\rangle _{H}\right).} Properties [ edit ] The isometry property:  for any process u {\displaystyle u} in D 1 , p {\displaystyle \mathbf {D} ^{1,p}} that lies in the domain of δ δ {\displaystyle \delta } , E [ ( δ δ u ) 2 ] = E ∫ ∫ | u t | 2 d t + E ∫ ∫ D s u t D t u s d s d t .

{\displaystyle \mathbf {E} {\big [}(\delta u)^{2}{\big ]}=\mathbf {E} \int |u_{t}|^{2}dt+\mathbf {E} \int D_{s}u_{t}\,D_{t}u_{s}\,ds\,dt.} If u {\displaystyle u} is an adapted process, then D s u t = 0 {\displaystyle D_{s}u_{t}=0} for s > t {\displaystyle s>t} , so the second term on the right-hand side vanishes. The Skorokhod and Itô integrals coincide in that case, and the above equation becomes the Itô isometry .

The derivative of a Skorokhod integral is given by the formula D h ( δ δ u ) = ⟨ ⟨ u , h ⟩ ⟩ H + δ δ ( D h u ) , {\displaystyle \mathrm {D} _{h}(\delta u)=\langle u,h\rangle _{H}+\delta (\mathrm {D} _{h}u),} where D h X {\displaystyle \mathrm {D} _{h}X} stands for ( D X ) ( h ) {\displaystyle (\mathrm {D} X)(h)} , the random variable that is the value of the process D X {\displaystyle \mathrm {D} X} at "time" h {\displaystyle h} in H {\displaystyle H} .

The Skorokhod integral of the product of a random variable F {\displaystyle F} in D 1 , 2 {\displaystyle \mathbf {D} ^{1,2}} and a process u {\displaystyle u} in dom ⁡ ⁡ ( δ δ ) {\displaystyle \operatorname {dom} (\delta )} is given by the formula δ δ ( F u ) = F δ δ u − − ⟨ ⟨ D F , u ⟩ ⟩ H .

{\displaystyle \delta (Fu)=F\,\delta u-\langle \mathrm {D} F,u\rangle _{H}.} Alternatives [ edit ] An alternative to the Skorokhod integral is the Ogawa integral .

References [ edit ] ^ Hitsuda, Masuyuki (1972). "Formula for Brownian partial derivatives".

Second Japan-USSR Symp. Probab. Th.2.

: 111– 114.

^ Kuo, Hui-Hsiung (2014).

"The Itô calculus and white noise theory: a brief survey toward general stochastic integration" .

Communications on Stochastic Analysis .

8 (1).

doi : 10.31390/cosa.8.1.07 .

"Skorokhod integral" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Ocone, Daniel L. (1988). "A guide to the stochastic calculus of variations".

Stochastic analysis and related topics (Silivri, 1986) . Lecture Notes in Math. 1316. Berlin: Springer. pp.

1– 79.

MR 0953793 Sanz-Solé, Marta (2008).

"Applications of Malliavin Calculus to Stochastic Partial Differential Equations (Lectures given at Imperial College London, 7–11 July 2008)" (PDF) . Retrieved 2008-07-09 .

v t e Integrals Types of integrals Riemann integral Lebesgue integral Burkill integral Bochner integral Daniell integral Darboux integral Henstock–Kurzweil integral Haar integral Hellinger integral Khinchin integral Kolmogorov integral Lebesgue–Stieltjes integral Pettis integral Pfeffer integral Riemann–Stieltjes integral Regulated integral Integration techniques Substitution Trigonometric Euler Weierstrass By parts Partial fractions Euler's formula Inverse functions Changing order Reduction formulas Parametric derivatives Differentiation under the integral sign Laplace transform Contour integration Laplace's method Numerical integration Simpson's rule Trapezoidal rule Risch algorithm Improper integrals Gaussian integral Dirichlet integral Fermi–Dirac integral complete incomplete Bose–Einstein integral Frullani integral Common integrals in quantum field theory Stochastic integrals Itô integral Russo–Vallois integral Stratonovich integral Skorokhod integral Miscellaneous Basel problem Euler–Maclaurin formula Gabriel's horn Integration Bee Proof that 22/7 exceeds π Volumes Washers Shells v t e Stochastic processes Discrete time Bernoulli process Branching process Chinese restaurant process Galton–Watson process Independent and identically distributed random variables Markov chain Moran process Random walk Loop-erased Self-avoiding Biased Maximal entropy Continuous time Additive process Airy process Bessel process Birth–death process pure birth Brownian motion Bridge Dyson Excursion Fractional Geometric Meander Cauchy process Contact process Continuous-time random walk Cox process Diffusion process Empirical process Feller process Fleming–Viot process Gamma process Geometric process Hawkes process Hunt process Interacting particle systems Itô diffusion Itô process Jump diffusion Jump process Lévy process Local time Markov additive process McKean–Vlasov process Ornstein–Uhlenbeck process Poisson process Compound Non-homogeneous Quasimartingale Schramm–Loewner evolution Semimartingale Sigma-martingale Stable process Superprocess Telegraph process Variance gamma process Wiener process Wiener sausage Both Branching process Gaussian process Hidden Markov model (HMM) Markov process Martingale Differences Local Sub- Super- Random dynamical system Regenerative process Renewal process Stochastic chains with memory of variable length White noise Fields and other Dirichlet process Gaussian random field Gibbs measure Hopfield model Ising model Potts model Boolean network Markov random field Percolation Pitman–Yor process Point process Cox Determinantal Poisson Random field Random graph Time series models Autoregressive conditional heteroskedasticity (ARCH) model Autoregressive integrated moving average (ARIMA) model Autoregressive (AR) model Autoregressive–moving-average (ARMA) model Generalized autoregressive conditional heteroskedasticity (GARCH) model Moving-average (MA) model Financial models Binomial options pricing model Black–Derman–Toy Black–Karasinski Black–Scholes Chan–Karolyi–Longstaff–Sanders (CKLS) Chen Constant elasticity of variance (CEV) Cox–Ingersoll–Ross (CIR) Garman–Kohlhagen Heath–Jarrow–Morton (HJM) Heston Ho–Lee Hull–White Korn-Kreer-Lenssen LIBOR market Rendleman–Bartter SABR volatility Vašíček Wilkie Actuarial models Bühlmann Cramér–Lundberg Risk process Sparre–Anderson Queueing models Bulk Fluid Generalized queueing network M/G/1 M/M/1 M/M/c Properties Càdlàg paths Continuous Continuous paths Ergodic Exchangeable Feller-continuous Gauss–Markov Markov Mixing Piecewise-deterministic Predictable Progressively measurable Self-similar Stationary Time-reversible Limit theorems Central limit theorem Donsker's theorem Doob's martingale convergence theorems Ergodic theorem Fisher–Tippett–Gnedenko theorem Large deviation principle Law of large numbers (weak/strong) Law of the iterated logarithm Maximal ergodic theorem Sanov's theorem Zero–one laws ( Blumenthal , Borel–Cantelli , Engelbert–Schmidt , Hewitt–Savage , Kolmogorov , Lévy ) Inequalities Burkholder–Davis–Gundy Doob's martingale Doob's upcrossing Kunita–Watanabe Marcinkiewicz–Zygmund Tools Cameron–Martin formula Convergence of random variables Doléans-Dade exponential Doob decomposition theorem Doob–Meyer decomposition theorem Doob's optional stopping theorem Dynkin's formula Feynman–Kac formula Filtration Girsanov theorem Infinitesimal generator Itô integral Itô's lemma Karhunen–Loève theorem Kolmogorov continuity theorem Kolmogorov extension theorem Lévy–Prokhorov metric Malliavin calculus Martingale representation theorem Optional stopping theorem Prokhorov's theorem Quadratic variation Reflection principle Skorokhod integral Skorokhod's representation theorem Skorokhod space Snell envelope Stochastic differential equation Tanaka Stopping time Stratonovich integral Uniform integrability Usual hypotheses Wiener space Classical Abstract Disciplines Actuarial mathematics Control theory Econometrics Ergodic theory Extreme value theory (EVT) Large deviations theory Mathematical finance Mathematical statistics Probability theory Queueing theory Renewal theory Ruin theory Signal processing Statistics Stochastic analysis Time series analysis Machine learning List of topics Category Retrieved from " https://en.wikipedia.org/w/index.php?title=Skorokhod_integral&oldid=1213781016 " Categories : Definitions of mathematical integration Stochastic calculus This page was last edited on 15 March 2024, at 02:51 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Skorokhod integral 2 languages Add topic

