Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 The distribution 2 Generalized Boltzmann distribution 3 In statistical mechanics 4 In mathematics 5 In economics 6 See also 7 References Toggle the table of contents Boltzmann distribution 28 languages العربية Català Čeština Dansk Deutsch Español فارسی Français 한국어 Bahasa Indonesia Italiano עברית Magyar Nederlands 日本語 Norsk bokmål Oʻzbekcha / ўзбекча Polski Português Русский Slovenčina Srpskohrvatski / српскохрватски Suomi Svenska Türkçe Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Boltzmann factor ) Probability distribution of energy states of a system This article is about system energy states. For particle energy levels and velocities, see Maxwell–Boltzmann distribution .

Boltzmann's distribution is an exponential distribution.

Boltzmann factor ⁠ p i p j {\displaystyle {\tfrac {p_{i}}{p_{j}}}} ⁠ (vertical axis) as a function of temperature T for several energy differences ε i − ε j .

In statistical mechanics and mathematics , a Boltzmann distribution (also called Gibbs distribution [ 1 ] ) is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system. The distribution is expressed in the form: p i ∝ ∝ exp ⁡ ⁡ ( − − ε ε i k T ) {\displaystyle p_{i}\propto \exp \left(-{\frac {\varepsilon _{i}}{kT}}\right)} where p i is the probability of the system being in state i , exp is the exponential function , ε i is the energy of that state, and a constant kT of the distribution is the product of the Boltzmann constant k and thermodynamic temperature T . The symbol ∝ ∝ {\textstyle \propto } denotes proportionality (see § The distribution for the proportionality constant).

The term system here has a wide meaning; it can range from a collection of 'sufficient number' of atoms or a single atom [ 1 ] to a macroscopic system such as a natural gas storage tank . Therefore, the Boltzmann distribution can be used to solve a wide variety of problems. The distribution shows that states with lower energy will always have a higher probability of being occupied.

The ratio of probabilities of two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference: p i p j = exp ⁡ ⁡ ( ε ε j − − ε ε i k T ) {\displaystyle {\frac {p_{i}}{p_{j}}}=\exp \left({\frac {\varepsilon _{j}-\varepsilon _{i}}{kT}}\right)} The Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium .

[ 2 ] Boltzmann's statistical work is borne out in his paper “On the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium" [ 3 ] The distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.

[ 4 ] The Boltzmann distribution should not be confused with the Maxwell–Boltzmann distribution or Maxwell–Boltzmann statistics . The Boltzmann distribution gives the probability that a system will be in a certain state as a function of that state's energy, [ 5 ] while the Maxwell–Boltzmann distributions give the probabilities of particle speeds or energies in ideal gases. The distribution of energies in a one-dimensional gas however, does follow the Boltzmann distribution.

The distribution [ edit ] The Boltzmann distribution is a probability distribution that gives the probability of a certain state as a function of that state's energy and  temperature of the system to which the distribution is applied.

[ 6 ] It is given as p i = 1 Q exp ⁡ ⁡ ( − − ε ε i k T ) = exp ⁡ ⁡ ( − − ε ε i k T ) ∑ ∑ j = 1 M exp ⁡ ⁡ ( − − ε ε j k T ) {\displaystyle p_{i}={\frac {1}{Q}}\exp \left(-{\frac {\varepsilon _{i}}{kT}}\right)={\frac {\exp \left(-{\tfrac {\varepsilon _{i}}{kT}}\right)}{\displaystyle \sum _{j=1}^{M}\exp \left(-{\tfrac {\varepsilon _{j}}{kT}}\right)}}} where: exp() is the exponential function , p i is the probability of state i , ε i is the energy of state i , k is the Boltzmann constant , T is the absolute temperature of the system, M is the number of all states accessible to the system of interest, [ 6 ] [ 5 ] Q (denoted by some authors by Z ) is the normalization denominator, which is the canonical partition function Q = ∑ ∑ j = 1 M exp ⁡ ⁡ ( − − ε ε j k T ) {\displaystyle Q=\sum _{j=1}^{M}\exp \left(-{\tfrac {\varepsilon _{j}}{kT}}\right)} It results from the constraint that the probabilities of all accessible states must add up to 1.

Using Lagrange multipliers , one can prove that the Boltzmann distribution is the distribution that maximizes the entropy S ( p 1 , p 2 , ⋯ ⋯ , p M ) = − − ∑ ∑ i = 1 M p i log 2 ⁡ ⁡ p i {\displaystyle S(p_{1},p_{2},\cdots ,p_{M})=-\sum _{i=1}^{M}p_{i}\log _{2}p_{i}} subject to the normalization constraint that ∑ ∑ p i = 1 {\textstyle \sum p_{i}=1} and the constraint that ∑ ∑ p i ε ε i {\textstyle \sum {p_{i}{\varepsilon }_{i}}} equals a particular mean energy value, except for two special cases.  (These special cases occur when the mean value is either the minimum or maximum of the energies ε i .  In these cases, the entropy maximizing distribution is a limit of Boltzmann distributions where T approaches zero from above or below, respectively.) The partition function can be calculated if we know the energies of the states accessible to the system of interest. For atoms the partition function values can be found in the NIST Atomic Spectra Database.

[ 7 ] The distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy. It also gives the quantitative relationship between the probabilities of the two states being occupied. The ratio of probabilities for states i and j is given as p i p j = exp ⁡ ⁡ ( ε ε j − − ε ε i k T ) {\displaystyle {\frac {p_{i}}{p_{j}}}=\exp \left({\frac {\varepsilon _{j}-\varepsilon _{i}}{kT}}\right)} where: p i is the probability of state i , p j the probability of state j , ε i is the energy of state i , ε j is the energy of state j .

The corresponding ratio of populations of energy levels must also take their degeneracies into account.

The Boltzmann distribution is often used to describe the distribution of particles, such as atoms or molecules, over bound states accessible to them. If we have a system consisting of many particles, the probability of a particle being in state i is practically the probability that, if we pick a random particle from that system and check what state it is in, we will find it is in state i . This probability is equal to the number of particles in state i divided by the total number of particles in the system, that is the fraction of particles that occupy state i .

p i = N i N {\displaystyle p_{i}={\frac {N_{i}}{N}}} where N i is the number of particles in state i and N is the total number of particles in the system. We may use the Boltzmann distribution to find this probability that is, as we have seen, equal to the fraction of particles that are in state i. So the equation that gives the fraction of particles in state i as a function of the energy of that state is [ 5 ] N i N = exp ⁡ ⁡ ( − − ε ε i k T ) ∑ ∑ j = 1 M exp ⁡ ⁡ ( − − ε ε j k T ) {\displaystyle {\frac {N_{i}}{N}}={\frac {\exp \left(-{\frac {\varepsilon _{i}}{kT}}\right)}{\displaystyle \sum _{j=1}^{M}\exp \left(-{\tfrac {\varepsilon _{j}}{kT}}\right)}}} This equation is of great importance to spectroscopy . In spectroscopy we observe a spectral line of atoms or molecules undergoing transitions from one state to another.

[ 5 ] [ 8 ] In order for this to be possible, there must be some particles in the first state to undergo the transition. We may find that this condition is fulfilled by finding the fraction of particles in the first state. If it is negligible, the transition is very likely not observed at the temperature for which the calculation was done. In general, a larger fraction of molecules in the first state means a higher number of transitions to the second state.

[ 9 ] This gives a stronger spectral line. However, there are other factors that influence the intensity of a spectral line, such as whether it is caused by an allowed or a forbidden transition .

The softmax function commonly used in machine learning is related to the Boltzmann distribution: ( p 1 , … … , p M ) = softmax ⁡ ⁡ [ − − ε ε 1 k T , … … , − − ε ε M k T ] {\displaystyle (p_{1},\ldots ,p_{M})=\operatorname {softmax} \left[-{\frac {\varepsilon _{1}}{kT}},\ldots ,-{\frac {\varepsilon _{M}}{kT}}\right]} Generalized Boltzmann distribution [ edit ] A distribution of the form Pr ( ω ω ) ∝ ∝ exp ⁡ ⁡ [ ∑ ∑ η η = 1 n X η η x η η ( ω ω ) k B T − − E ( ω ω ) k B T ] {\displaystyle \Pr \left(\omega \right)\propto \exp \left[\sum _{\eta =1}^{n}{\frac {X_{\eta }x_{\eta }^{\left(\omega \right)}}{k_{B}T}}-{\frac {E^{\left(\omega \right)}}{k_{B}T}}\right]} is called generalized Boltzmann distribution by some authors.

[ 10 ] The Boltzmann distribution is a special case of the generalized Boltzmann distribution. The generalized Boltzmann distribution is used in statistical mechanics to describe canonical ensemble , grand canonical ensemble and isothermal–isobaric ensemble . The generalized Boltzmann distribution is usually derived from the principle of maximum entropy , but there are other derivations.

[ 10 ] [ 11 ] The generalized Boltzmann distribution has the following properties: It is the only distribution for which the entropy as defined by Gibbs entropy formula matches with the entropy as defined in classical thermodynamics .

[ 10 ] It is the only distribution that is mathematically consistent with the fundamental thermodynamic relation where state functions are described by ensemble average.

[ 11 ] In statistical mechanics [ edit ] Main articles: Canonical ensemble and Maxwell–Boltzmann statistics The Boltzmann distribution appears in statistical mechanics when considering closed systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble. Some special cases (derivable from the canonical ensemble) show the Boltzmann distribution in different aspects: Canonical ensemble (general case) The canonical ensemble gives the probabilities of the various possible states of a closed system of fixed volume, in thermal equilibrium with a heat bath . The canonical ensemble has a state probability distribution with the Boltzmann form.

Statistical frequencies of subsystems' states (in a non-interacting collection) When the system of interest is a collection of many non-interacting copies of a smaller subsystem, it is sometimes useful to find the statistical frequency of a given subsystem state, among the collection. The canonical ensemble has the property of separability when applied to such a collection: as long as the non-interacting subsystems have fixed composition, then each subsystem's state is independent of the others and is also characterized by a canonical ensemble. As a result, the expected statistical frequency distribution of subsystem states has the Boltzmann form.

Maxwell–Boltzmann statistics of classical gases (systems of non-interacting particles) In particle systems, many particles share the same space and regularly change places with each other; the single-particle state space they occupy is a shared space.

Maxwell–Boltzmann statistics give the expected number of particles found in a given single-particle state, in a classical gas of non-interacting particles at equilibrium. This expected number distribution has the Boltzmann form.

Although these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed: When a system is in thermodynamic equilibrium with respect to both energy exchange and particle exchange , the requirement of fixed composition is relaxed and a grand canonical ensemble is obtained rather than canonical ensemble. On the other hand, if both composition and energy are fixed, then a microcanonical ensemble applies instead.

If the subsystems within a collection do interact with each other, then the expected frequencies of subsystem states no longer follow a Boltzmann distribution, and even may not have an analytical solution .

[ 12 ] The canonical ensemble can however still be applied to the collective states of the entire system considered as a whole, provided the entire system is in thermal equilibrium.

With quantum gases of non-interacting particles in equilibrium, the number of particles found in a given single-particle state does not follow Maxwell–Boltzmann statistics, and there is no simple closed form expression for quantum gases in the canonical ensemble. In the grand canonical ensemble the state-filling statistics of quantum gases are described by Fermi–Dirac statistics or Bose–Einstein statistics , depending on whether the particles are fermions or bosons , respectively.

In mathematics [ edit ] Main articles: Gibbs measure , Log-linear model , and Boltzmann machine In more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure .

In statistics and machine learning , it is called a log-linear model .

In deep learning , the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine , restricted Boltzmann machine , energy-based models and deep Boltzmann machine . In deep learning, the Boltzmann machine is considered to be one of the unsupervised learning models. In the design of Boltzmann machine in deep learning, as the number of nodes are increased the difficulty of implementing in real time applications becomes critical, so a different type of architecture named Restricted Boltzmann machine is introduced.

In economics [ edit ] The Boltzmann distribution can be introduced to allocate permits in emissions trading .

[ 13 ] [ 14 ] A new allocation method, known as the Boltzmann Fair Division , uses the Boltzmann distribution to describe the most probable, natural, and unbiased distribution of emission permits among multiple countries.

[ 15 ] This framework has been further extended to address general problems of distributive justice, including cake-cutting and resource allocation, by allowing flexibility in how factors such as contribution, need, or preference are weighted. The Boltzmann fair division is recognized for providing a simple yet powerful probabilistic model that can be adapted to various social, political, and economic contexts.

[ 15 ] The Boltzmann distribution has the same form as the multinomial logit model. As a discrete choice model, this is very well known in economics since Daniel McFadden made the connection to random utility maximization.

[ 16 ] See also [ edit ] Boltzmann Fair Division Bose–Einstein statistics Fermi–Dirac statistics Negative temperature Softmax function References [ edit ] ^ a b Landau, Lev Davidovich & Lifshitz, Evgeny Mikhailovich (1980) [1976].

Statistical Physics . Course of Theoretical Physics. Vol. 5 (3 ed.). Oxford: Pergamon Press.

ISBN 0-7506-3372-7 .

Translated by J.B. Sykes and M.J. Kearsley. See section 28 ^ Boltzmann, Ludwig (1868). "Studien über das Gleichgewicht der lebendigen Kraft zwischen bewegten materiellen Punkten" [Studies on the balance of living force between moving material points].

Wiener Berichte .

58 : 517– 560.

^ "Translation of Ludwig Boltzmann's Paper "On the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium" " (PDF) . Archived from the original (PDF) on 2020-10-21 . Retrieved 2017-05-11 .

^ Gibbs, Josiah Willard (1902).

Elementary Principles in Statistical Mechanics . New York: Charles Scribner's Sons .

^ a b c d Atkins, P. W. (2010) Quanta, W. H. Freeman and Company, New York ^ a b McQuarrie, A. (2000).

Statistical Mechanics . Sausalito, CA: University Science Books.

ISBN 1-891389-15-7 .

^ NIST Atomic Spectra Database Levels Form at nist.gov ^ Atkins, P. W.; de Paula, J. (2009).

Physical Chemistry (9th ed.). Oxford: Oxford University Press.

ISBN 978-0-19-954337-3 .

^ Skoog, D. A.; Holler, F. J.; Crouch, S. R. (2006).

Principles of Instrumental Analysis . Boston, MA: Brooks/Cole.

ISBN 978-0-495-12570-9 .

^ a b c Gao, Xiang; Gallicchio, Emilio; Roitberg, Adrian (2019).

"The generalized Boltzmann distribution is the only distribution in which the Gibbs-Shannon entropy equals the thermodynamic entropy" .

The Journal of Chemical Physics .

151 (3): 034113.

arXiv : 1903.02121 .

Bibcode : 2019JChPh.151c4113G .

doi : 10.1063/1.5111333 .

PMID 31325924 .

S2CID 118981017 .

^ a b Gao, Xiang (March 2022).

"The Mathematics of the Ensemble Theory" .

Results in Physics .

34 : 105230.

arXiv : 2006.00485 .

Bibcode : 2022ResPh..3405230G .

doi : 10.1016/j.rinp.2022.105230 .

S2CID 221978379 .

^ A classic example of this is magnetic ordering . Systems of non-interacting spins show paramagnetic behaviour that can be understood with a single-particle canonical ensemble (resulting in the Brillouin function ). Systems of interacting spins can show much more complex behaviour such as ferromagnetism or antiferromagnetism .

^ Park, J.-W., Kim, C. U., & Isard, W. (2012). Permit allocation in emissions trading using the Boltzmann distribution. Physica A, 391, 4883–4890.

^ The Thorny Problem Of Fair Allocation .

Technology Review blog. August 17, 2011. Cites and summarizes Park, Kim, and Isard (2012).

^ a b Park, J.-W., Kim, J.U., Ghim, C.-M., & Kim, C.U. (2022). The Boltzmann fair division for distributive justice.

Scientific Reports , 12, 16179.

doi:10.1038/s41598-022-16179-1 ^ Amemiya, Takeshi (1985).

"Multinomial Logit Model" .

Advanced Econometrics . Oxford: Basil Blackwell. pp.

295– 299.

ISBN 0-631-13345-3 .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐8487bf5649‐8gwv7
Cached time: 20250814065636
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.348 seconds
Real time usage: 0.744 seconds
Preprocessor visited node count: 1990/1000000
Revision size: 21258/2097152 bytes
Post‐expand include size: 90605/2097152 bytes
Template argument size: 1602/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 53804/5000000 bytes
Lua time usage: 0.188/10.000 seconds
Lua memory usage: 6196664/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  358.677      1 -total
 36.45%  130.735      1 Template:Reflist
 25.29%   90.714      6 Template:Cite_book
 23.09%   82.831      4 Template:Navbox
 22.65%   81.238      1 Template:Probability_distributions
 16.94%   60.749      1 Template:Short_description
 10.46%   37.530      2 Template:Pagetype
  6.06%   21.748      1 Template:About
  4.98%   17.852      3 Template:Cite_journal
  4.45%   15.978      1 Template:R Saved in parser cache with key enwiki:pcache:4107:|#|:idhash:canonical and timestamp 20250814065636 and revision id 1305812697. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Boltzmann_distribution&oldid=1305812697 " Categories : Statistical mechanics Ludwig Boltzmann Hidden categories: Articles with short description Short description matches Wikidata Use American English from March 2019 All Wikipedia articles written in American English This page was last edited on 14 August 2025, at 06:55 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Boltzmann distribution 28 languages Add topic

