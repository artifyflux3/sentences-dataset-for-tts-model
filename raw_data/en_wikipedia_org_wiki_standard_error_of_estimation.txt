Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Standard error of the sample mean Toggle Standard error of the sample mean subsection 1.1 Exact value 1.2 Estimate 1.2.1 Accuracy of the estimator 1.3 Derivation 1.4 Independent and identically distributed random variables with random sample size 2 Student approximation when σ value is unknown 3 Assumptions and usage Toggle Assumptions and usage subsection 3.1 Standard error of mean versus standard deviation 4 Extensions Toggle Extensions subsection 4.1 Finite population correction (FPC) 4.2 Correction for correlation in the sample 5 See also 6 References Toggle the table of contents Standard error 36 languages العربية Asturianu Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 हिन्दी Italiano עברית Magyar Македонски Bahasa Melayu Nederlands 日本語 Norsk bokmål Polski Português Русский Simple English Српски / srpski Sunda Suomi Svenska தமிழ் Türkçe Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Standard error of estimation ) Statistical property For the computer programming concept, see standard error stream .

For a value that is sampled with an unbiased normally distributed error, the above depicts the proportion of samples that would fall between 0, 1, 2, and 3 standard deviations above and below the actual value.

The standard error ( SE ) [ 1 ] of a statistic (usually an estimator of a parameter , like the average or mean) is the standard deviation of its sampling distribution .

[ 2 ] [ 1 ] The standard error is often used in calculations of confidence intervals .

[ 3 ] The sampling distribution of a mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different sample means, and this distribution has its own mean and variance . Mathematically, the variance of the sampling mean distribution obtained is equal to the variance of the population divided by the sample size. This is because as the sample size increases, sample means cluster more closely around the population mean.

Therefore, the relationship between the standard error of the mean and the standard deviation is such that, for a given sample size, the standard error of the mean equals the standard deviation divided by the square root of the sample size.

[ 1 ] In other words, the standard error of the mean is a measure of the dispersion of sample means around the population mean.

In regression analysis , the term "standard error" refers either to the square root of the reduced chi-squared statistic or the standard error for a particular regression coefficient (as used in, say, confidence intervals ).

Standard error of the sample mean [ edit ] Exact value [ edit ] Suppose a statistically independent sample of n {\displaystyle n} observations x 1 , x 2 , … … , x n {\displaystyle x_{1},x_{2},\ldots ,x_{n}} is taken from a statistical population with a standard deviation of σ σ {\displaystyle \sigma } (the standard deviation of the population). The mean value calculated from the sample, x ¯ ¯ {\displaystyle {\bar {x}}} , will have an associated standard error on the mean , σ σ x ¯ ¯ {\displaystyle {\sigma }_{\bar {x}}} , given by: [ 1 ] σ σ x ¯ ¯ = σ σ n .

{\displaystyle {\sigma }_{\bar {x}}={\frac {\sigma }{\sqrt {n}}}.} Practically this tells us that when trying to estimate the value of a population mean, due to the factor 1 / n {\displaystyle 1/{\sqrt {n}}} , reducing the error on the estimate by a factor of two requires acquiring four times as many observations in the sample; reducing it by a factor of ten requires a hundred times as many observations.

Estimate [ edit ] The standard deviation σ σ {\displaystyle \sigma } of the population being sampled is seldom known. Therefore, the standard error of the mean is usually estimated by replacing σ σ {\displaystyle \sigma } with the sample standard deviation σ σ x {\displaystyle \sigma _{x}} instead: σ σ x ¯ ¯ ≈ ≈ σ σ x n .

{\displaystyle {\sigma }_{\bar {x}}\ \approx {\frac {\sigma _{x}}{\sqrt {n}}}.} As this is only an estimator for the true "standard error", it is common to see other notations here such as: σ σ ^ ^ x ¯ ¯ := σ σ x n or s x ¯ ¯ := s n .

{\displaystyle {\widehat {\sigma }}_{\bar {x}}:={\frac {\sigma _{x}}{\sqrt {n}}}\qquad {\text{ or }}\qquad {s}_{\bar {x}}\ :={\frac {s}{\sqrt {n}}}.} A common source of confusion occurs when failing to distinguish clearly between: the standard deviation of the population ( σ σ {\displaystyle \sigma } ), the standard deviation of the sample ( σ σ x {\displaystyle \sigma _{x}} ), the standard deviation of the sample mean itself ( σ σ x ¯ ¯ {\displaystyle \sigma _{\bar {x}}} , which is the standard error), and the estimator of the standard deviation of the sample mean ( σ σ ^ ^ x ¯ ¯ {\displaystyle {\widehat {\sigma }}_{\bar {x}}} , which is the most often calculated quantity, and is also often colloquially called the standard error ).

Accuracy of the estimator [ edit ] When the sample size is small, using the standard deviation of the sample instead of the true standard deviation of the population will tend to systematically underestimate the population standard deviation, and therefore also the standard error. With n = 2, the underestimate is about 25%, but for n = 6, the underestimate is only 5%. Gurland and Tripathi (1971) provide a correction and equation for this effect.

[ 4 ] Sokal and Rohlf (1981) give an equation of the correction factor for small samples of n < 20.

[ 5 ] See unbiased estimation of standard deviation for further discussion.

Derivation [ edit ] The standard error on the mean may be derived from the variance of a sum of independent random variables, [ 6 ] given the definition of variance and some properties thereof. If x 1 , x 2 , … … , x n {\displaystyle x_{1},x_{2},\ldots ,x_{n}} is a sample of n {\displaystyle n} independent observations from a population with mean x {\displaystyle x} and standard deviation σ σ {\displaystyle \sigma } , then we can define the total T = ( x 1 + x 2 + ⋯ ⋯ + x n ) , {\displaystyle T=(x_{1}+x_{2}+\cdots +x_{n}),} which due to the Bienaymé formula , will have variance Var ⁡ ⁡ ( T ) = Var ⁡ ⁡ ( x 1 ) + Var ⁡ ⁡ ( x 2 ) + ⋯ ⋯ + Var ⁡ ⁡ ( x n ) = n σ σ 2 .

{\displaystyle \operatorname {Var} (T)=\operatorname {Var} (x_{1})+\operatorname {Var} (x_{2})+\cdots +\operatorname {Var} (x_{n})=n\sigma ^{2}.} The mean of these measurements x ¯ ¯ {\displaystyle {\bar {x}}} (sample mean) is given by x ¯ ¯ = T / n .

{\displaystyle {\bar {x}}=T/n.} The variance of the mean is then Var ⁡ ⁡ ( x ¯ ¯ ) = Var ⁡ ⁡ ( T n ) = 1 n 2 Var ⁡ ⁡ ( T ) = 1 n 2 n σ σ 2 = σ σ 2 n , {\displaystyle \operatorname {Var} ({\bar {x}})=\operatorname {Var} \left({\frac {T}{n}}\right)={\frac {1}{n^{2}}}\operatorname {Var} (T)={\frac {1}{n^{2}}}n\sigma ^{2}={\frac {\sigma ^{2}}{n}},} where a propagation in variance is used in the 2nd equality. The standard error is, by definition, the standard deviation of x ¯ ¯ {\displaystyle {\bar {x}}} which is the square root of the variance: σ σ x ¯ ¯ = σ σ 2 n = σ σ n .

{\displaystyle \sigma _{\bar {x}}={\sqrt {\frac {\sigma ^{2}}{n}}}={\frac {\sigma }{\sqrt {n}}}.} In other words, if there are a large number of observations per sampling ( n {\textstyle n} is high compared with the population variance σ σ {\textstyle \sigma } ), then the calculated mean per sample x ¯ ¯ {\textstyle {\bar {x}}} is expected to be close to the population mean x {\displaystyle x} .

For correlated random variables, the sample variance needs to be computed according to the Markov chain central limit theorem .

Independent and identically distributed random variables with random sample size [ edit ] There are cases when a sample is taken without knowing, in advance, how many observations will be acceptable according to some criterion. In such cases, the sample size N {\displaystyle N} is a random variable whose variation adds to the variation of X {\displaystyle X} such that, Var ⁡ ⁡ ( T ) = E ⁡ ⁡ ( N ) Var ⁡ ⁡ ( X ) + Var ⁡ ⁡ ( N ) ( E ⁡ ⁡ ( X ) ) 2 {\displaystyle \operatorname {Var} (T)=\operatorname {E} (N)\operatorname {Var} (X)+\operatorname {Var} (N){\big (}\operatorname {E} (X){\big )}^{2}} [ 7 ] which follows from the law of total variance .

If N {\displaystyle N} has a Poisson distribution , then E ⁡ ⁡ ( N ) = Var ⁡ ⁡ ( N ) {\displaystyle \operatorname {E} (N)=\operatorname {Var} (N)} with estimator n = N {\displaystyle n=N} . Hence the estimator of Var ⁡ ⁡ ( T ) {\displaystyle \operatorname {Var} (T)} becomes n S X 2 + n X ¯ ¯ 2 {\displaystyle nS_{X}^{2}+n{\bar {X}}^{2}} , leading the following formula for standard error: S t a n d a r d E r r o r ⁡ ⁡ ( X ¯ ¯ ) = S X 2 + X ¯ ¯ 2 n {\displaystyle \operatorname {Standard~Error} ({\bar {X}})={\sqrt {\frac {S_{X}^{2}+{\bar {X}}^{2}}{n}}}} (since the standard deviation is the square root of the variance).

Student approximation when σ value is unknown [ edit ] Further information: Student's t-distribution § Confidence intervals , and Normal distribution § Confidence intervals In many practical applications, the true value of σ is unknown. As a result, we need to use a distribution that takes into account that spread of possible σ' s.
When the true underlying distribution is known to be Gaussian, although with unknown σ, then the resulting estimated distribution follows the Student t-distribution. The standard error is the standard deviation of the Student t-distribution. T-distributions are slightly different from Gaussian, and vary depending on the size of the sample. Small samples are somewhat more likely to underestimate the population standard deviation and have a mean that differs from the true population mean, and the Student t-distribution accounts for the probability of these events with somewhat heavier tails compared to a Gaussian. To estimate the standard error of a Student t-distribution it is sufficient to use the sample standard deviation "s" instead of σ , and we could use this value to calculate confidence intervals.

Note: The Student's probability distribution is approximated well by the Gaussian distribution when the sample size is over 100. For such samples one can use the latter distribution, which is much simpler. Also, even though the 'true' distribution of the population is unknown, assuming normality of the sampling distribution makes sense for a reasonable sample size, and under certain sampling conditions, see CLT . If these conditions are not met, then using a Bootstrap distribution to estimate the Standard Error is often a good workaround, but it can be computationally intensive.

Assumptions and usage [ edit ] Further information: Confidence interval An example of how SE {\displaystyle \operatorname {SE} } (Standard Error) is used to make confidence intervals of the unknown population mean is shown. If the sampling distribution is normally distributed , the sample mean, the standard error, and the quantiles of the normal distribution can be used to calculate confidence intervals for the true population mean. The following expressions can be used to calculate the upper and lower 95% confidence limits, where x ¯ ¯ {\displaystyle {\bar {x}}} is for the sample mean, SE {\displaystyle \operatorname {SE} } is for the standard error for the sample mean (the standard deviation of sample mean values), and 1.96 is the approximate value of the 97.5 percentile point of the normal distribution : Upper 95% limit = x ¯ ¯ + ( SE × × 1.96 ) {\displaystyle {\bar {x}}+(\operatorname {SE} \times 1.96)} , and Lower 95% limit = x ¯ ¯ − − ( SE × × 1.96 ) {\displaystyle {\bar {x}}-(\operatorname {SE} \times 1.96)} .

In particular, the standard error of a sample statistic (such as sample mean ) is the actual or estimated standard deviation of the sample mean in the process by which it was generated. In other words, it is the actual or estimated standard deviation of the sampling distribution of the sample statistic. The notation for standard error can be any one of SE, SEM (for standard error of measurement or mean ), or S E .

Standard errors provide simple measures of uncertainty in a value and are often used because: in many cases, if the standard error of several individual quantities is known then the standard error of some function of the quantities can be easily calculated; when the probability distribution of the value is known, it can be used to calculate an exact confidence interval ; when the probability distribution is unknown, Chebyshev 's or the Vysochanskiï–Petunin inequalities can be used to calculate a conservative confidence interval; and as the sample size tends to infinity the central limit theorem guarantees that the sampling distribution of the mean is asymptotically normal .

Standard error of mean versus standard deviation [ edit ] In scientific and technical literature, experimental data are often summarized either using the mean and standard deviation of the sample data or the mean with the standard error. This often leads to confusion about their interchangeability. However, the mean and standard deviation are descriptive statistics , whereas the standard error of the mean is descriptive of the random sampling process. The standard deviation of the sample data is a description of the variation in measurements, while the standard error of the mean is a probabilistic statement about how the sample size will provide a better bound on estimates of the population mean, in light of the central limit theorem.

[ 8 ] Put simply, the standard error of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the standard deviation of the sample is the degree to which individuals within the sample differ from the sample mean.

[ 9 ] If the population standard deviation is finite, the standard error of the mean of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the standard deviation of the sample will tend to approximate the population standard deviation as the sample size increases.

Extensions [ edit ] Finite population correction (FPC) [ edit ] The formula given above for the standard error assumes that the population is infinite. Nonetheless, it is often used for finite populations when people are interested in measuring the process that created the existing finite population (this is called an analytic study ). Though the above formula is not exactly correct when the population is finite, the difference between the finite- and infinite-population versions will be small when sampling fraction is small (e.g. a small proportion of a finite population is studied). In this case people often do not correct for the finite population, essentially treating it as an "approximately infinite" population.

If one is interested in measuring an existing finite population that will not change over time, then it is necessary to adjust for the population size (called an enumerative study ). When the sampling fraction (often termed f ) is large (approximately at 5% or more) in an enumerative study , the estimate of the standard error must be corrected by multiplying by a ''finite population correction'' (a.k.a.: FPC ): [ 10 ] [ 11 ] FPC = N − − n N − − 1 {\displaystyle \operatorname {FPC} ={\sqrt {\frac {N-n}{N-1}}}} which, for large N : FPC ≈ ≈ 1 − − n N = 1 − − f {\displaystyle \operatorname {FPC} \approx {\sqrt {1-{\frac {n}{N}}}}={\sqrt {1-f}}} to account for the added precision gained by sampling close to a larger percentage of the population. The effect of the FPC is that the error becomes zero when the sample size n is equal to the population size N .

This happens in survey methodology when sampling without replacement . If sampling with replacement, then FPC does not come into play.

Correction for correlation in the sample [ edit ] Expected error in the mean of A for a sample of n data points with sample bias coefficient ρ . The unbiased standard error plots as the ρ = 0 diagonal line with log-log slope − 1 ⁄ 2 .

If values of the measured quantity A are not statistically independent but have been obtained from known locations in parameter space x , an unbiased estimate of the true standard error of the mean (actually a correction on the standard deviation part) may be obtained by multiplying the calculated standard error of the sample by the factor f : f = 1 + ρ ρ 1 − − ρ ρ , {\displaystyle f={\sqrt {\frac {1+\rho }{1-\rho }}},} where the sample bias coefficient ρ is the widely used Prais–Winsten estimate of the autocorrelation -coefficient (a quantity between −1 and +1) for all sample point pairs. This approximate formula is for moderate to large sample sizes; the reference gives the exact formulas for any sample size, and can be applied to heavily autocorrelated time series like Wall Street stock quotes. Moreover, this formula works for positive and negative ρ alike.

[ 12 ] See also unbiased estimation of standard deviation for more discussion.

See also [ edit ] Illustration of the central limit theorem Margin of error Probable error Standard error of the weighted mean Sample mean and sample covariance Standard error of the median Variance Variance of the mean and predicted responses References [ edit ] ^ a b c d Altman, Douglas G; Bland, J Martin (2005-10-15).

"Standard deviations and standard errors" .

BMJ: British Medical Journal .

331 (7521): 903.

doi : 10.1136/bmj.331.7521.903 .

ISSN 0959-8138 .

PMC 1255808 .

PMID 16223828 .

^ Everitt, B. S. (2003).

The Cambridge Dictionary of Statistics . Cambridge University Press.

ISBN 978-0-521-81099-9 .

^ Wooldridge, Jeffrey M. (2023).

"What is a standard error? (And how should we compute it?)" .

Journal of Econometrics .

237 (2, Part A).

doi : 10.1016/j.jeconom.2023.105517 .

ISSN 0304-4076 .

^ Gurland, J; Tripathi RC (1971). "A simple approximation for unbiased estimation of the standard deviation".

American Statistician .

25 (4): 30– 32.

doi : 10.2307/2682923 .

JSTOR 2682923 .

^ Sokal; Rohlf (1981).

Biometry: Principles and Practice of Statistics in Biological Research (2nd ed.). W. H. Freeman. p.

53 .

ISBN 978-0-7167-1254-1 .

^ Hutchinson, T. P. (1993).

Essentials of Statistical Methods, in 41 pages . Adelaide: Rumsby.

ISBN 978-0-646-12621-0 .

^ Cornell, J R; Benjamin, C A (1970).

Probability, Statistics, and Decisions for Civil Engineers . NY: McGraw-Hill. pp.

178– 179.

ISBN 0486796094 .

^ Barde, M. (2012).

"What to use to express the variability of data: Standard deviation or standard error of mean?" .

Perspect. Clin. Res.

3 (3): 113– 116.

doi : 10.4103/2229-3485.100662 .

PMC 3487226 .

PMID 23125963 .

^ Wassertheil-Smoller, Sylvia (1995).

Biostatistics and Epidemiology : A Primer for Health Professionals (Second ed.). New York: Springer. pp.

40– 43.

ISBN 0-387-94388-9 .

^ Isserlis, L. (1918).

"On the value of a mean as calculated from a sample" .

Journal of the Royal Statistical Society .

81 (1): 75– 81.

doi : 10.2307/2340569 .

JSTOR 2340569 .

(Equation 1) ^ Bondy, Warren; Zlot, William (1976). "The Standard Error of the Mean and the Difference Between Means for Finite Populations".

The American Statistician .

30 (2): 96– 97.

doi : 10.1080/00031305.1976.10479149 .

JSTOR 2683803 .

(Equation 2) ^ Bence, James R. (1995).

"Analysis of Short Time Series: Correcting for Autocorrelation" .

Ecology .

76 (2): 628– 639.

Bibcode : 1995Ecol...76..628B .

doi : 10.2307/1941218 .

JSTOR 1941218 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐api‐ext.codfw.main‐647cb4546b‐nqxnj
Cached time: 20250814020518
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.465 seconds
Real time usage: 0.603 seconds
Preprocessor visited node count: 2155/1000000
Revision size: 20511/2097152 bytes
Post‐expand include size: 170561/2097152 bytes
Template argument size: 1378/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 60209/5000000 bytes
Lua time usage: 0.261/10.000 seconds
Lua memory usage: 5979333/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  417.948      1 -total
 40.84%  170.692      1 Template:Reflist
 33.57%  140.301      1 Template:Statistics
 33.03%  138.065      1 Template:Navbox_with_collapsible_groups
 27.75%  115.974      7 Template:Cite_journal
 13.96%   58.364      1 Template:Short_description
 12.62%   52.748     11 Template:Navbox
 10.04%   41.973      2 Template:Pagetype
  7.79%   32.546      5 Template:Cite_book
  6.32%   26.431      1 Template:Hlist Saved in parser cache with key enwiki:pcache:552520:|#|:idhash:canonical and timestamp 20250814020518 and revision id 1305779920. Rendering was triggered because: unknown Retrieved from " https://en.wikipedia.org/w/index.php?title=Standard_error&oldid=1305779920 " Category : Statistical deviation and dispersion Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 14 August 2025, at 02:04 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Standard error 36 languages Add topic

