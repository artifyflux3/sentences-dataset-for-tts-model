Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Uses 2 Finding the matrix of a transformation Toggle Finding the matrix of a transformation subsection 2.1 Eigenbasis and diagonal matrix 3 Examples in 2 dimensions Toggle Examples in 2 dimensions subsection 3.1 Stretching 3.2 Squeezing 3.3 Rotation 3.4 Shearing 3.5 Reflection 3.6 Orthogonal projection 4 Examples in 3 dimensions Toggle Examples in 3 dimensions subsection 4.1 Rotation 4.2 Reflection 5 Composing and inverting transformations 6 Other kinds of transformations Toggle Other kinds of transformations subsection 6.1 Affine transformations 6.2 Perspective projection 7 See also 8 References 9 External links Toggle the table of contents Transformation matrix 21 languages العربية Català Čeština Deutsch Ελληνικά Español فارسی Français 한국어 Italiano 日本語 Norsk bokmål Polski Português Русский Slovenščina Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Central object in linear algebra; mapping vectors to vectors In linear algebra , linear transformations can be represented by matrices . If T {\displaystyle T} is a linear transformation mapping R n {\displaystyle \mathbb {R} ^{n}} to R m {\displaystyle \mathbb {R} ^{m}} and x {\displaystyle \mathbf {x} } is a column vector with n {\displaystyle n} entries, then there exists an m × × n {\displaystyle m\times n} matrix A {\displaystyle A} , called the transformation matrix of T {\displaystyle T} , [ 1 ] such that: T ( x ) = A x {\displaystyle T(\mathbf {x} )=A\mathbf {x} } Note that A {\displaystyle A} has m {\displaystyle m} rows and n {\displaystyle n} columns, whereas the transformation T {\displaystyle T} is from R n {\displaystyle \mathbb {R} ^{n}} to R m {\displaystyle \mathbb {R} ^{m}} .  There are alternative expressions of transformation matrices involving row vectors that are preferred by some authors.

[ 2 ] [ 3 ] Uses [ edit ] Matrices allow arbitrary linear transformations to be displayed in a consistent format, suitable for computation.

[ 1 ] This also allows transformations to be composed easily (by multiplying their matrices).

Linear transformations are not the only ones that can be represented by matrices.  Some transformations that are non-linear on an n-dimensional Euclidean space R n can be represented as linear transformations on the n +1-dimensional space R n +1 . These include both affine transformations (such as translation ) and projective transformations . For this reason, 4×4 transformation matrices are widely used in 3D computer graphics , as they allow to perform translation, scaling, and rotation of objects by repeated matrix multiplication. These n +1-dimensional transformation matrices are called, depending on their application, affine transformation matrices , projective transformation matrices , or more generally non-linear transformation matrices .  With respect to an n -dimensional matrix, an n +1-dimensional matrix can be described as an augmented matrix .

In the physical sciences , an active transformation is one which actually changes the physical position of a system , and makes sense even in the absence of a coordinate system whereas a passive transformation is a change in the coordinate description of the physical system ( change of basis ). The distinction between active and passive transformations is important. By default, by transformation , mathematicians usually mean active transformations, while physicists could mean either.

Put differently, a passive transformation refers to description of the same object as viewed from two different coordinate frames.

Finding the matrix of a transformation [ edit ] If one has a linear transformation T ( x ) {\displaystyle T(x)} in functional form, it is easy to determine the transformation matrix A by transforming each of the vectors of the standard basis by T , then inserting the result into the columns of a matrix. In other words, A = [ T ( e 1 ) T ( e 2 ) ⋯ ⋯ T ( e n ) ] {\displaystyle A={\begin{bmatrix}T(\mathbf {e} _{1})&T(\mathbf {e} _{2})&\cdots &T(\mathbf {e} _{n})\end{bmatrix}}} For example, the function T ( x ) = 5 x {\displaystyle T(x)=5x} is a linear transformation.  Applying the above process (suppose that n = 2 in this case) reveals that: T ( x ) = 5 x = 5 I x = [ 5 0 0 5 ] x {\displaystyle T(\mathbf {x} )=5\mathbf {x} =5I\mathbf {x} ={\begin{bmatrix}5&0\\0&5\end{bmatrix}}\mathbf {x} } The matrix representation of vectors and operators depends on the chosen basis; a similar matrix will result from an alternate basis. Nevertheless, the method to find the components remains the same.

To elaborate, vector v {\displaystyle \mathbf {v} } can be represented in basis vectors, E = [ e 1 e 2 ⋯ ⋯ e n ] {\displaystyle E={\begin{bmatrix}\mathbf {e} _{1}&\mathbf {e} _{2}&\cdots &\mathbf {e} _{n}\end{bmatrix}}} with coordinates [ v ] E = [ v 1 v 2 ⋯ ⋯ v n ] T {\displaystyle [\mathbf {v} ]_{E}={\begin{bmatrix}v_{1}&v_{2}&\cdots &v_{n}\end{bmatrix}}^{\mathrm {T} }} : v = v 1 e 1 + v 2 e 2 + ⋯ ⋯ + v n e n = ∑ ∑ i v i e i = E [ v ] E {\displaystyle \mathbf {v} =v_{1}\mathbf {e} _{1}+v_{2}\mathbf {e} _{2}+\cdots +v_{n}\mathbf {e} _{n}=\sum _{i}v_{i}\mathbf {e} _{i}=E[\mathbf {v} ]_{E}} Now, express the result of the transformation matrix A upon v {\displaystyle \mathbf {v} } , in the given basis: A ( v ) = A ( ∑ ∑ i v i e i ) = ∑ ∑ i v i A ( e i ) = [ A ( e 1 ) A ( e 2 ) ⋯ ⋯ A ( e n ) ] [ v ] E = A ⋅ ⋅ [ v ] E = [ e 1 e 2 ⋯ ⋯ e n ] [ a 1 , 1 a 1 , 2 ⋯ ⋯ a 1 , n a 2 , 1 a 2 , 2 ⋯ ⋯ a 2 , n ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ a n , 1 a n , 2 ⋯ ⋯ a n , n ] [ v 1 v 2 ⋮ ⋮ v n ] {\displaystyle {\begin{aligned}A(\mathbf {v} )&=A\left(\sum _{i}v_{i}\mathbf {e} _{i}\right)=\sum _{i}{v_{i}A(\mathbf {e} _{i})}\\&={\begin{bmatrix}A(\mathbf {e} _{1})&A(\mathbf {e} _{2})&\cdots &A(\mathbf {e} _{n})\end{bmatrix}}[\mathbf {v} ]_{E}=A\cdot [\mathbf {v} ]_{E}\\[3pt]&={\begin{bmatrix}\mathbf {e} _{1}&\mathbf {e} _{2}&\cdots &\mathbf {e} _{n}\end{bmatrix}}{\begin{bmatrix}a_{1,1}&a_{1,2}&\cdots &a_{1,n}\\a_{2,1}&a_{2,2}&\cdots &a_{2,n}\\\vdots &\vdots &\ddots &\vdots \\a_{n,1}&a_{n,2}&\cdots &a_{n,n}\\\end{bmatrix}}{\begin{bmatrix}v_{1}\\v_{2}\\\vdots \\v_{n}\end{bmatrix}}\end{aligned}}} The a i , j {\displaystyle a_{i,j}} elements of matrix A are determined for a given basis E by applying A to every e j = [ 0 0 ⋯ ⋯ ( v j = 1 ) ⋯ ⋯ 0 ] T {\displaystyle \mathbf {e} _{j}={\begin{bmatrix}0&0&\cdots &(v_{j}=1)&\cdots &0\end{bmatrix}}^{\mathrm {T} }} , and observing the response vector A e j = a 1 , j e 1 + a 2 , j e 2 + ⋯ ⋯ + a n , j e n = ∑ ∑ i a i , j e i .

{\displaystyle A\mathbf {e} _{j}=a_{1,j}\mathbf {e} _{1}+a_{2,j}\mathbf {e} _{2}+\cdots +a_{n,j}\mathbf {e} _{n}=\sum _{i}a_{i,j}\mathbf {e} _{i}.} This equation defines the wanted elements, a i , j {\displaystyle a_{i,j}} , of j -th column of the matrix A .

[ 4 ] Eigenbasis and diagonal matrix [ edit ] Main articles: Diagonal matrix and Eigenvalues and eigenvectors Yet, there is a special basis for an operator in which the components form a diagonal matrix and, thus, multiplication complexity reduces to n . Being diagonal means that all coefficients a i , j {\displaystyle a_{i,j}} except a i , i {\displaystyle a_{i,i}} are zeros leaving only one term in the sum ∑ ∑ a i , j e i {\textstyle \sum a_{i,j}\mathbf {e} _{i}} above. The surviving diagonal elements, a i , i {\displaystyle a_{i,i}} , are known as eigenvalues and designated with λ λ i {\displaystyle \lambda _{i}} in the defining equation, which reduces to A e i = λ λ i e i {\displaystyle A\mathbf {e} _{i}=\lambda _{i}\mathbf {e} _{i}} . The resulting equation is known as eigenvalue equation .

[ 5 ] The eigenvectors and eigenvalues are derived from it via the characteristic polynomial .

With diagonalization , it is often possible to translate to and from eigenbases.

Examples in 2 dimensions [ edit ] Most common geometric transformations that keep the origin fixed are linear, including rotation, scaling, shearing, reflection, and orthogonal projection; if an affine transformation is not a pure translation it keeps some point fixed, and that point can be chosen as origin to make the transformation linear.  In two dimensions, linear transformations can be represented using a 2×2 transformation matrix.

Stretching [ edit ] A stretch in the xy -plane is a linear transformation which enlarges all distances in a particular direction by a constant factor but does not affect distances in the perpendicular direction. We only consider stretches along the x-axis and y-axis. A stretch along the x-axis has the form x' = kx ; y' = y for some positive constant k . (Note that if k > 1 , then this really is a "stretch"; if k < 1 , it is technically a "compression", but we still call it a stretch. Also, if k = 1 , then the transformation is an identity, i.e. it has no effect.) The matrix associated with a stretch by a factor k along the x-axis is given by: [ k 0 0 1 ] {\displaystyle {\begin{bmatrix}k&0\\0&1\end{bmatrix}}} Similarly, a stretch by a factor k along the y-axis has the form x' = x ; y' = ky , so the matrix associated with this transformation is [ 1 0 0 k ] {\displaystyle {\begin{bmatrix}1&0\\0&k\end{bmatrix}}} Squeezing [ edit ] If the two stretches above are combined with reciprocal values, then the transformation matrix represents a squeeze mapping : [ k 0 0 1 / k ] .

{\displaystyle {\begin{bmatrix}k&0\\0&1/k\end{bmatrix}}.} A square with sides parallel to the axes is transformed to a rectangle that has the same area as the square. The reciprocal stretch and compression leave the area invariant.

Rotation [ edit ] For rotation by an angle θ counterclockwise (positive direction) about the origin the functional form is x ′ = x cos ⁡ ⁡ θ θ − − y sin ⁡ ⁡ θ θ {\displaystyle x'=x\cos \theta -y\sin \theta } and y ′ = x sin ⁡ ⁡ θ θ + y cos ⁡ ⁡ θ θ {\displaystyle y'=x\sin \theta +y\cos \theta } .  Written in matrix form, this becomes: [ 6 ] [ x ′ y ′ ] = [ cos ⁡ ⁡ θ θ − − sin ⁡ ⁡ θ θ sin ⁡ ⁡ θ θ cos ⁡ ⁡ θ θ ] [ x y ] {\displaystyle {\begin{bmatrix}x'\\y'\end{bmatrix}}={\begin{bmatrix}\cos \theta &-\sin \theta \\\sin \theta &\cos \theta \end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}} Similarly, for a rotation clockwise (negative direction) about the origin, the functional form is x ′ = x cos ⁡ ⁡ θ θ + y sin ⁡ ⁡ θ θ {\displaystyle x'=x\cos \theta +y\sin \theta } and y ′ = − − x sin ⁡ ⁡ θ θ + y cos ⁡ ⁡ θ θ {\displaystyle y'=-x\sin \theta +y\cos \theta } the matrix form is: [ x ′ y ′ ] = [ cos ⁡ ⁡ θ θ sin ⁡ ⁡ θ θ − − sin ⁡ ⁡ θ θ cos ⁡ ⁡ θ θ ] [ x y ] {\displaystyle {\begin{bmatrix}x'\\y'\end{bmatrix}}={\begin{bmatrix}\cos \theta &\sin \theta \\-\sin \theta &\cos \theta \end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}} These formulae assume that the x axis points right and the y axis points up.

Shearing [ edit ] For shear mapping (visually similar to slanting), there are two possibilities.

A shear parallel to the x axis has x ′ = x + k y {\displaystyle x'=x+ky} and y ′ = y {\displaystyle y'=y} . Written in matrix form, this becomes: [ x ′ y ′ ] = [ 1 k 0 1 ] [ x y ] {\displaystyle {\begin{bmatrix}x'\\y'\end{bmatrix}}={\begin{bmatrix}1&k\\0&1\end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}} A shear parallel to the y axis has x ′ = x {\displaystyle x'=x} and y ′ = y + k x {\displaystyle y'=y+kx} , which has matrix form: [ x ′ y ′ ] = [ 1 0 k 1 ] [ x y ] {\displaystyle {\begin{bmatrix}x'\\y'\end{bmatrix}}={\begin{bmatrix}1&0\\k&1\end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}} Reflection [ edit ] Main article: Householder transformation For reflection about a line that goes through the origin, let l = ( l x , l y ) {\displaystyle \mathbf {l} =(l_{x},l_{y})} be a vector in the direction of the line. Then the transformation matrix is: A = 1 ‖ ‖ l ‖ ‖ 2 [ l x 2 − − l y 2 2 l x l y 2 l x l y l y 2 − − l x 2 ] {\displaystyle \mathbf {A} ={\frac {1}{\lVert \mathbf {l} \rVert ^{2}}}{\begin{bmatrix}l_{x}^{2}-l_{y}^{2}&2l_{x}l_{y}\\2l_{x}l_{y}&l_{y}^{2}-l_{x}^{2}\end{bmatrix}}} Orthogonal projection [ edit ] Further information: Orthogonal projection To project a vector orthogonally onto a line that goes through the origin, let u = ( u x , u y ) {\displaystyle \mathbf {u} =(u_{x},u_{y})} be a vector in the direction of the line.  Then the transformation matrix is: A = 1 ‖ ‖ u ‖ ‖ 2 [ u x 2 u x u y u x u y u y 2 ] {\displaystyle \mathbf {A} ={\frac {1}{\lVert \mathbf {u} \rVert ^{2}}}{\begin{bmatrix}u_{x}^{2}&u_{x}u_{y}\\u_{x}u_{y}&u_{y}^{2}\end{bmatrix}}} As with reflections, the orthogonal projection onto a line that does not pass through the origin is an affine, not linear, transformation.

Parallel projections are also linear transformations and can be represented simply by a matrix.  However, perspective projections are not, and to represent these with a matrix, homogeneous coordinates can be used.

Examples in 3 dimensions [ edit ] Rotation [ edit ] The matrix to rotate an angle θ about any axis defined by unit vector ( x , y , z ) is [ 7 ] [ x x ( 1 − − cos ⁡ ⁡ θ θ ) + cos ⁡ ⁡ θ θ y x ( 1 − − cos ⁡ ⁡ θ θ ) − − z sin ⁡ ⁡ θ θ z x ( 1 − − cos ⁡ ⁡ θ θ ) + y sin ⁡ ⁡ θ θ x y ( 1 − − cos ⁡ ⁡ θ θ ) + z sin ⁡ ⁡ θ θ y y ( 1 − − cos ⁡ ⁡ θ θ ) + cos ⁡ ⁡ θ θ z y ( 1 − − cos ⁡ ⁡ θ θ ) − − x sin ⁡ ⁡ θ θ x z ( 1 − − cos ⁡ ⁡ θ θ ) − − y sin ⁡ ⁡ θ θ y z ( 1 − − cos ⁡ ⁡ θ θ ) + x sin ⁡ ⁡ θ θ z z ( 1 − − cos ⁡ ⁡ θ θ ) + cos ⁡ ⁡ θ θ ] .

{\displaystyle {\begin{bmatrix}xx(1-\cos \theta )+\cos \theta &yx(1-\cos \theta )-z\sin \theta &zx(1-\cos \theta )+y\sin \theta \\xy(1-\cos \theta )+z\sin \theta &yy(1-\cos \theta )+\cos \theta &zy(1-\cos \theta )-x\sin \theta \\xz(1-\cos \theta )-y\sin \theta &yz(1-\cos \theta )+x\sin \theta &zz(1-\cos \theta )+\cos \theta \end{bmatrix}}.} Reflection [ edit ] Main article: Householder transformation To reflect a point through a plane a x + b y + c z = 0 {\displaystyle ax+by+cz=0} (which goes through the origin), one can use A = I − − 2 N N T {\displaystyle \mathbf {A} =\mathbf {I} -2\mathbf {NN} ^{\mathrm {T} }} , where I {\displaystyle \mathbf {I} } is the 3×3 identity matrix and N {\displaystyle \mathbf {N} } is the three-dimensional unit vector for the vector normal of the plane.  If the L 2 norm of a {\displaystyle a} , b {\displaystyle b} , and c {\displaystyle c} is unity, the transformation matrix can be expressed as: A = [ 1 − − 2 a 2 − − 2 a b − − 2 a c − − 2 a b 1 − − 2 b 2 − − 2 b c − − 2 a c − − 2 b c 1 − − 2 c 2 ] {\displaystyle \mathbf {A} ={\begin{bmatrix}1-2a^{2}&-2ab&-2ac\\-2ab&1-2b^{2}&-2bc\\-2ac&-2bc&1-2c^{2}\end{bmatrix}}} Note that these are particular cases of a Householder reflection in two and three dimensions.  A reflection about a line or plane that does not go through the origin is not a linear transformation — it is an affine transformation — as a 4×4 affine transformation matrix, it can be expressed as follows (assuming the normal is a unit vector): [ x ′ y ′ z ′ 1 ] = [ 1 − − 2 a 2 − − 2 a b − − 2 a c − − 2 a d − − 2 a b 1 − − 2 b 2 − − 2 b c − − 2 b d − − 2 a c − − 2 b c 1 − − 2 c 2 − − 2 c d 0 0 0 1 ] [ x y z 1 ] {\displaystyle {\begin{bmatrix}x'\\y'\\z'\\1\end{bmatrix}}={\begin{bmatrix}1-2a^{2}&-2ab&-2ac&-2ad\\-2ab&1-2b^{2}&-2bc&-2bd\\-2ac&-2bc&1-2c^{2}&-2cd\\0&0&0&1\end{bmatrix}}{\begin{bmatrix}x\\y\\z\\1\end{bmatrix}}} where d = − − p ⋅ ⋅ N {\displaystyle d=-\mathbf {p} \cdot \mathbf {N} } for some point p {\displaystyle \mathbf {p} } on the plane, or equivalently, a x + b y + c z + d = 0 {\displaystyle ax+by+cz+d=0} .

If the 4th component of the vector is 0 instead of 1, then only the vector's direction is reflected and its magnitude remains unchanged, as if it were mirrored through a parallel plane that passes through the origin. This is a useful property as it allows the transformation of both positional vectors and normal vectors with the same matrix.  See homogeneous coordinates and affine transformations below for further explanation.

Composing and inverting transformations [ edit ] One of the main motivations for using matrices to represent linear transformations is that transformations can then be easily composed and inverted.

Composition is accomplished by matrix multiplication .

Row and column vectors are operated upon by matrices, rows on the left and columns on the right. Since text reads from left to right, column vectors are preferred when transformation matrices are composed: If A and B are the matrices of two linear transformations, then the effect of first applying A and then B to a column vector x {\displaystyle \mathbf {x} } is given by: B ( A x ) = ( B A ) x .

{\displaystyle \mathbf {B} (\mathbf {A} \mathbf {x} )=(\mathbf {BA} )\mathbf {x} .} In other words, the matrix of the combined transformation A followed by B is simply the product of the individual matrices.

When A is an invertible matrix there is a matrix A −1 that represents a transformation that "undoes" A since its composition with A is the identity matrix . In some practical applications, inversion can be computed using general inversion algorithms or by performing inverse operations (that have obvious geometric interpretation, like rotating in opposite direction) and then composing them in reverse order.  Reflection matrices are a special case because they are their own inverses and don't need to be separately calculated.

Other kinds of transformations [ edit ] Affine transformations [ edit ] Effect of applying various 2D affine transformation matrices on a unit square. Note that the reflection matrices are special cases of the scaling matrix.

Affine transformations on the 2D plane can be performed in three dimensions. Translation is done by shearing parallel to the xy plane, and rotation is performed around the z axis.

To represent affine transformations with matrices, we can use homogeneous coordinates .  This means representing a 2-vector ( x , y ) as a 3-vector ( x , y , 1), and similarly for higher dimensions.  Using this system, translation can be expressed with matrix multiplication.  The functional form x ′ = x + t x ; y ′ = y + t y {\displaystyle x'=x+t_{x};y'=y+t_{y}} becomes: [ x ′ y ′ 1 ] = [ 1 0 t x 0 1 t y 0 0 1 ] [ x y 1 ] .

{\displaystyle {\begin{bmatrix}x'\\y'\\1\end{bmatrix}}={\begin{bmatrix}1&0&t_{x}\\0&1&t_{y}\\0&0&1\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}.} All ordinary linear transformations are included in the set of affine transformations, and can be described as a simplified form of affine transformations. Therefore, any linear transformation can also be represented by a general transformation matrix. The latter is obtained by expanding the corresponding linear transformation matrix by one row and column, filling the extra space with zeros except for the lower-right corner, which must be set to 1. For example, the counter-clockwise rotation matrix from above becomes: [ cos ⁡ ⁡ θ θ − − sin ⁡ ⁡ θ θ 0 sin ⁡ ⁡ θ θ cos ⁡ ⁡ θ θ 0 0 0 1 ] {\displaystyle {\begin{bmatrix}\cos \theta &-\sin \theta &0\\\sin \theta &\cos \theta &0\\0&0&1\end{bmatrix}}} Using transformation matrices containing homogeneous coordinates, translations become linear, and thus can be seamlessly intermixed with all other types of transformations. The reason is that the real plane is mapped to the w = 1 plane in real projective space, and so translation in real Euclidean space can be represented as a shear in real projective space. Although a translation is a non- linear transformation in a 2-D or 3-D Euclidean space described by Cartesian coordinates (i.e. it can't be combined with other transformations while preserving commutativity and other properties), it becomes , in a 3-D or 4-D projective space described by homogeneous coordinates, a simple linear transformation (a shear ).

More affine transformations can be obtained by composition of two or more affine transformations. For example, given a translation T' with vector ( t x ′ , t y ′ ) , {\displaystyle (t'_{x},t'_{y}),} a rotation R by an angle θ counter-clockwise , a scaling S with factors ( s x , s y ) {\displaystyle (s_{x},s_{y})} and a translation T of vector ( t x , t y ) , {\displaystyle (t_{x},t_{y}),} the result M of T'RST is: [ 8 ] [ s x cos ⁡ ⁡ θ θ − − s y sin ⁡ ⁡ θ θ t x s x cos ⁡ ⁡ θ θ − − t y s y sin ⁡ ⁡ θ θ + t x ′ s x sin ⁡ ⁡ θ θ s y cos ⁡ ⁡ θ θ t x s x sin ⁡ ⁡ θ θ + t y s y cos ⁡ ⁡ θ θ + t y ′ 0 0 1 ] {\displaystyle {\begin{bmatrix}s_{x}\cos \theta &-s_{y}\sin \theta &t_{x}s_{x}\cos \theta -t_{y}s_{y}\sin \theta +t'_{x}\\s_{x}\sin \theta &s_{y}\cos \theta &t_{x}s_{x}\sin \theta +t_{y}s_{y}\cos \theta +t'_{y}\\0&0&1\end{bmatrix}}} When using affine transformations, the homogeneous component of a coordinate vector (normally called w ) will never be altered.  One can therefore safely assume that it is always 1 and ignore it.  However, this is not true when using perspective projections.

Perspective projection [ edit ] Main article: Perspective projection Further information: Pinhole camera model Comparison of the effects of applying 2D affine and perspective transformation matrices on a unit square.

Another type of transformation, of importance in 3D computer graphics , is the perspective projection .  Whereas parallel projections are used to project points onto the image plane along parallel lines, the perspective projection projects points onto the image plane along lines that emanate from a single point, called the center of projection.  This means that an object has a smaller projection when it is far away from the center of projection and a larger projection when it is closer (see also reciprocal function ).

The simplest perspective projection uses the origin as the center of projection, and the plane at z = 1 {\displaystyle z=1} as the image plane.  The functional form of this transformation is then x ′ = x / z {\displaystyle x'=x/z} ; y ′ = y / z {\displaystyle y'=y/z} .  We can express this in homogeneous coordinates as: [ x c y c z c w c ] = [ 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 ] [ x y z 1 ] = [ x y z z ] {\displaystyle {\begin{bmatrix}x_{c}\\y_{c}\\z_{c}\\w_{c}\end{bmatrix}}={\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&1&0\end{bmatrix}}{\begin{bmatrix}x\\y\\z\\1\end{bmatrix}}={\begin{bmatrix}x\\y\\z\\z\end{bmatrix}}} After carrying out the matrix multiplication , the homogeneous component w c {\displaystyle w_{c}} will be equal to the value of z {\displaystyle z} and the other three will not change. Therefore, to map back into the real plane we must perform the homogeneous divide or perspective divide by dividing each component by w c {\displaystyle w_{c}} : [ x ′ y ′ z ′ 1 ] = 1 w c [ x c y c z c w c ] = [ x / z y / z 1 1 ] {\displaystyle {\begin{bmatrix}x'\\y'\\z'\\1\end{bmatrix}}={\frac {1}{w_{c}}}{\begin{bmatrix}x_{c}\\y_{c}\\z_{c}\\w_{c}\end{bmatrix}}={\begin{bmatrix}x/z\\y/z\\1\\1\end{bmatrix}}} More complicated perspective projections can be composed by combining this one with rotations, scales, translations, and shears to move the image plane and center of projection wherever they are desired.

See also [ edit ] 3D projection Change of basis Image rectification Pose (computer vision) Rigid transformation Transformation (function) Transformation geometry References [ edit ] ^ a b Gentle, James E. (2007).

"Matrix Transformations and Factorizations" .

Matrix Algebra: Theory, Computations, and Applications in Statistics . Springer.

ISBN 9780387708737 .

^ Rafael Artzy (1965) Linear Geometry ^ J. W. P. Hirschfeld (1979) Projective Geometry of Finite Fields , Clarendon Press ^ Nearing, James (2010).

"Chapter 7.3 Examples of Operators" (PDF) .

Mathematical Tools for Physics .

ISBN 978-0486482125 . Retrieved January 1, 2012 .

^ Nearing, James (2010).

"Chapter 7.9: Eigenvalues and Eigenvectors" (PDF) .

Mathematical Tools for Physics .

ISBN 978-0486482125 . Retrieved January 1, 2012 .

^ "Lecture Notes" (PDF) .

ocw.mit.edu . Retrieved 2024-07-28 .

^ Szymanski, John E. (1989).

Basic Mathematics for Electronic Engineers:Models and Applications . Taylor & Francis. p. 154.

ISBN 0278000681 .

^ Cédric Jules (February 25, 2015).

"2D transformation matrices baking" .

External links [ edit ] The Matrix Page Practical examples in POV-Ray Reference page - Rotation of axes Linear Transformation Calculator Transformation Applet - Generate matrices from 2D transformations and vice versa.

Coordinate transformation under rotation in 2D Excel Fun - Build 3D graphics from a spreadsheet v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category v t e Matrix classes Explicitly constrained entries Alternant Anti-diagonal Anti-Hermitian Anti-symmetric Arrowhead Band Bidiagonal Bisymmetric Block-diagonal Block Block tridiagonal Boolean Cauchy Centrosymmetric Conference Complex Hadamard Copositive Diagonally dominant Diagonal Discrete Fourier Transform Elementary Equivalent Frobenius Generalized permutation Hadamard Hankel Hermitian Hessenberg Hollow Integer Logical Matrix unit Metzler Moore Nonnegative Pentadiagonal Permutation Persymmetric Polynomial Quaternionic Signature Skew-Hermitian Skew-symmetric Skyline Sparse Sylvester Symmetric Toeplitz Triangular Tridiagonal Vandermonde Walsh Z Constant Exchange Hilbert Identity Lehmer Of ones Pascal Pauli Redheffer Shift Zero Conditions on eigenvalues or eigenvectors Companion Convergent Defective Definite Diagonalizable Hurwitz-stable Positive-definite Stieltjes Satisfying conditions on products or inverses Congruent Idempotent or Projection Invertible Involutory Nilpotent Normal Orthogonal Unimodular Unipotent Unitary Totally unimodular Weighing With specific applications Adjugate Alternating sign Augmented Bézout Carleman Cartan Circulant Cofactor Commutation Confusion Coxeter Distance Duplication and elimination Euclidean distance Fundamental (linear differential equation) Generator Gram Hessian Householder Jacobian Moment Payoff Pick Random Rotation Routh-Hurwitz Seifert Shear Similarity Symplectic Totally positive Transformation Used in statistics Centering Correlation Covariance Design Doubly stochastic Fisher information Hat Precision Stochastic Transition Used in graph theory Adjacency Biadjacency Degree Edmonds Incidence Laplacian Seidel adjacency Tutte Used in science and engineering Cabibbo–Kobayashi–Maskawa Density Fundamental (computer vision) Fuzzy associative Gamma Gell-Mann Hamiltonian Irregular Overlap S State transition Substitution Z (chemistry) Related terms Jordan normal form Linear independence Matrix exponential Matrix representation of conic sections Perfect matrix Pseudoinverse Row echelon form Wronskian Mathematics portal List of matrices Category:Matrices (mathematics) NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐8g2tb
Cached time: 20250812001304
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.422 seconds
Real time usage: 0.742 seconds
Preprocessor visited node count: 1810/1000000
Revision size: 24425/2097152 bytes
Post‐expand include size: 56640/2097152 bytes
Template argument size: 1661/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 9/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 44631/5000000 bytes
Lua time usage: 0.212/10.000 seconds
Lua memory usage: 5522489/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  359.353      1 -total
 34.35%  123.430      1 Template:Reflist
 26.40%   94.887      2 Template:Navbox
 26.34%   94.658      4 Template:Cite_book
 23.32%   83.811      1 Template:Linear_algebra
 20.80%   74.730      1 Template:Short_description
 13.73%   49.340      2 Template:Pagetype
  6.80%   24.442      4 Template:Main
  4.56%   16.378      1 Template:Matrix_classes
  4.29%   15.425     11 Template:Main_other Saved in parser cache with key enwiki:pcache:692458:|#|:idhash:canonical and timestamp 20250812001304 and revision id 1300757933. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Transformation_matrix&oldid=1300757933 " Categories : Computer graphics Matrices (mathematics) Transformation (function) Hidden categories: Articles with short description Short description matches Wikidata Use American English from January 2019 All Wikipedia articles written in American English Articles containing video clips This page was last edited on 16 July 2025, at 06:51 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Transformation matrix 21 languages Add topic

