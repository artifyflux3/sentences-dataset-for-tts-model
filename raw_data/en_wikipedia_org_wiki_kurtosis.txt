Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Pearson moments Toggle Pearson moments subsection 1.1 Interpretation 1.2 Moors' interpretation 1.3 Maximal entropy 2 Excess kurtosis Toggle Excess kurtosis subsection 2.1 Mesokurtic 2.2 Leptokurtic 2.3 Platykurtic 3 Graphical examples Toggle Graphical examples subsection 3.1 The Pearson type VII family 3.2 Other well-known distributions 4 Sample kurtosis Toggle Sample kurtosis subsection 4.1 Definitions 4.1.1 A natural but biased estimator 4.1.2 Standard unbiased estimator 4.2 Upper bound 4.3 Variance under normality 5 Applications Toggle Applications subsection 5.1 Kurtosis convergence 5.2 Seismic signal analysis 5.3 Weather prediction 6 Other measures 7 See also 8 References 9 Further reading 10 External links Toggle the table of contents Kurtosis 27 languages العربية Català Čeština Deutsch Español Euskara فارسی Français Galego 한국어 Italiano עברית Bahasa Melayu Nederlands 日本語 Norsk bokmål Polski Português Shqip Српски / srpski Sunda Svenska Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Fourth standardized moment in statistics In probability theory and statistics , kurtosis (from Greek : κυρτός , kyrtos or kurtos , meaning "curved, arching") refers to the degree of “tailedness” in the probability distribution of a real-valued random variable . Similar to skewness , kurtosis provides insight into specific characteristics of a distribution. Various methods exist for quantifying kurtosis in theoretical distributions, and corresponding techniques allow estimation based on sample data from a population. It’s important to note that different measures of kurtosis can yield varying interpretations .

The standard measure of a distribution's kurtosis, originating with Karl Pearson , [ 1 ] is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution, not its peak; [ 2 ] hence, the sometimes-seen characterization of kurtosis as " peakedness " is incorrect. For this measure, higher kurtosis corresponds to greater extremity of deviations (or outliers ), and not the configuration of data near the mean .

Excess kurtosis, typically compared to a value of 0, characterizes the “tailedness” of a distribution. A univariate normal distribution has an excess kurtosis of 0. Negative excess kurtosis indicates a platykurtic distribution, which doesn’t necessarily have a flat top but produces fewer or less extreme outliers than the normal distribution. For instance, the uniform distribution (i.e. one that is uniformly finite over some bound and zero elsewhere) is platykurtic. On the other hand, positive excess kurtosis signifies a leptokurtic distribution. The Laplace distribution , for example, has tails that decay more slowly than a Gaussian, resulting in more outliers. To simplify comparison with the normal distribution, excess kurtosis is calculated as Pearson’s kurtosis minus 3. Some authors and software packages use “kurtosis” to refer specifically to excess kurtosis, but this article distinguishes between the two for clarity.

Alternative measures of kurtosis are: the L-kurtosis , which is a scaled version of the fourth L-moment ; measures based on four population or sample quantiles .

[ 3 ] These are analogous to the alternative measures of skewness that are not based on ordinary moments.

[ 3 ] Pearson moments [ edit ] The kurtosis is the fourth standardized moment , defined as Kurt ⁡ ⁡ [ X ] = E ⁡ ⁡ [ ( X − − μ μ σ σ ) 4 ] = E ⁡ ⁡ [ ( X − − μ μ ) 4 ] ( E ⁡ ⁡ [ ( X − − μ μ ) 2 ] ) 2 = μ μ 4 σ σ 4 , {\displaystyle \operatorname {Kurt} [X]=\operatorname {E} \left[{\left({\frac {X-\mu }{\sigma }}\right)}^{4}\right]={\frac {\operatorname {E} \left[(X-\mu )^{4}\right]}{\left(\operatorname {E} \left[(X-\mu )^{2}\right]\right)^{2}}}={\frac {\mu _{4}}{\sigma ^{4}}},} where μ 4 is the fourth central moment and σ is the standard deviation . Several letters are used in the literature to denote the kurtosis.  A very common choice is κ , which is fine as long as it is clear that it does not refer to a cumulant .  Other choices include γ 2 , to be similar to the notation for skewness, although sometimes this is instead reserved for the excess kurtosis.

The kurtosis is bounded below by the squared skewness plus 1: [ 4 ] : 432 μ μ 4 σ σ 4 ≥ ≥ ( μ μ 3 σ σ 3 ) 2 + 1 , {\displaystyle {\frac {\mu _{4}}{\sigma ^{4}}}\geq \left({\frac {\mu _{3}}{\sigma ^{3}}}\right)^{2}+1,} where μ 3 is the third central moment . The lower bound is realized by the Bernoulli distribution . There is no upper limit to the kurtosis of a general probability distribution, and it may be infinite.

A reason why some authors favor the excess kurtosis is that cumulants are extensive .  Formulas related to the extensive property are more naturally expressed in terms of the excess kurtosis.  For example, let X 1 , ..., X n be independent random variables for which the fourth moment exists, and let Y be the random variable defined by the sum of the X i .  The excess kurtosis of Y is Kurt ⁡ ⁡ [ Y ] − − 3 = 1 ( ∑ ∑ j = 1 n σ σ j 2 ) 2 ∑ ∑ i = 1 n σ σ i 4 ⋅ ⋅ ( Kurt ⁡ ⁡ [ X i ] − − 3 ) , {\displaystyle \operatorname {Kurt} [Y]-3={\frac {1}{\left(\sum _{j=1}^{n}\sigma _{j}^{\,2}\right)^{2}}}\sum _{i=1}^{n}\sigma _{i}^{\,4}\cdot \left(\operatorname {Kurt} \left[X_{i}\right]-3\right),} where σ σ i {\displaystyle \sigma _{i}} is the standard deviation of X i .  In particular if all of the X i have the same variance, then this simplifies to Kurt ⁡ ⁡ [ Y ] − − 3 = 1 n 2 ∑ ∑ i = 1 n ( Kurt ⁡ ⁡ [ X i ] − − 3 ) .

{\displaystyle \operatorname {Kurt} [Y]-3={\frac {1}{n^{2}}}\sum _{i=1}^{n}\left(\operatorname {Kurt} \left[X_{i}\right]-3\right).} The reason not to subtract 3 is that the bare moment better generalizes to multivariate distributions , especially when independence is not assumed.  The cokurtosis between pairs of variables is an order four tensor .  For a bivariate normal distribution, the cokurtosis tensor has off-diagonal terms that are neither 0 nor 3 in general, so attempting to "correct" for an excess becomes confusing.  It is true, however, that the joint cumulants of degree greater than two for any multivariate normal distribution are zero.

For two random variables, X and Y , not necessarily independent, the kurtosis of the sum, X + Y , is Kurt ⁡ ⁡ [ X + Y ] = 1 σ σ X + Y 4 ( σ σ X 4 Kurt ⁡ ⁡ [ X ] + 4 σ σ X 3 σ σ Y Cokurt ⁡ ⁡ [ X , X , X , Y ] + 6 σ σ X 2 σ σ Y 2 Cokurt ⁡ ⁡ [ X , X , Y , Y ] + 4 σ σ X σ σ Y 3 Cokurt ⁡ ⁡ [ X , Y , Y , Y ] + σ σ Y 4 Kurt ⁡ ⁡ [ Y ] ) .

{\displaystyle {\begin{aligned}\operatorname {Kurt} [X+Y]={\frac {1}{\sigma _{X+Y}^{4}}}{\big (}&\sigma _{X}^{4}\operatorname {Kurt} [X]\\&{}+4\sigma _{X}^{3}\sigma _{Y}\operatorname {Cokurt} [X,X,X,Y]\\[6pt]&{}+6\sigma _{X}^{2}\sigma _{Y}^{2}\operatorname {Cokurt} [X,X,Y,Y]\\[6pt]&{}+4\sigma _{X}\sigma _{Y}^{3}\operatorname {Cokurt} [X,Y,Y,Y]\\[6pt]&{}+\sigma _{Y}^{4}\operatorname {Kurt} [Y]{\big )}.\end{aligned}}} Note that the fourth-power binomial coefficients (1, 4, 6, 4, 1) appear in the above equation.

Interpretation [ edit ] The interpretation of the Pearson measure of kurtosis (or excess kurtosis) was once debated, but it is now well-established. As noted by Westfall in 2014 [ 2 ] , "...

its unambiguous interpretation relates to tail extremity.

Specifically, it reflects either the presence of existing outliers (for sample kurtosis) or the tendency to produce outliers (for the kurtosis of a probability distribution). The underlying logic is straightforward: Kurtosis represents the average (or expected value ) of standardized data raised to the fourth power. Standardized values less than 1—corresponding to data within one standard deviation of the mean (where the “peak” occurs)—contribute minimally to kurtosis. This is because raising a number less than 1 to the fourth power brings it closer to zero. The meaningful contributors to kurtosis are data values outside the peak region, i.e., the outliers. Therefore, kurtosis primarily measures outliers and provides no information about the central "peak".

Numerous misconceptions about kurtosis relate to notions of peakedness. One such misconception is that kurtosis measures both the “peakedness” of a distribution and the heaviness of its tail .

[ 5 ] Other incorrect interpretations include notions like “lack of shoulders” (where the “shoulder” refers vaguely to the area between the peak and the tail, or more specifically, the region about one standard deviation from the mean) or “bimodality.” [ 6 ] Balanda and MacGillivray argue that the standard definition of kurtosis “poorly captures the kurtosis, peakedness, or tail weight of a distribution.”Instead, they propose a vague definition of kurtosis as the location- and scale-free movement of probability mass from the distribution’s shoulders into its center and tails.

[ 5 ] Moors' interpretation [ edit ] In 1986, Moors gave an interpretation of kurtosis.

[ 7 ] Let Z = X − − μ μ σ σ , {\displaystyle Z={\frac {X-\mu }{\sigma }},} where X is a random variable, μ is the mean and σ is the standard deviation.

Now by definition of the kurtosis κ κ {\displaystyle \kappa } , and by the well-known identity E ⁡ ⁡ [ V 2 ] = var ⁡ ⁡ [ V ] + E ⁡ ⁡ [ V ] 2 , {\displaystyle \operatorname {E} \left[V^{2}\right]=\operatorname {var} [V]+\operatorname {E} [V]^{2},} κ κ = E ⁡ ⁡ [ Z 4 ] = var ⁡ ⁡ [ Z 2 ] + E ⁡ ⁡ [ Z 2 ] 2 = var ⁡ ⁡ [ Z 2 ] + var ⁡ ⁡ [ Z ] 2 = var ⁡ ⁡ [ Z 2 ] + 1.

{\displaystyle {\begin{aligned}\kappa &=\operatorname {E} \left[Z^{4}\right]\\&=\operatorname {var} \left[Z^{2}\right]+\operatorname {E} {\!\left[Z^{2}\right]}^{2}\\&=\operatorname {var} \left[Z^{2}\right]+\operatorname {var} [Z]^{2}=\operatorname {var} \left[Z^{2}\right]+1.\end{aligned}}} The kurtosis can now be seen as a measure of the dispersion of Z 2 around its expectation. Alternatively it can be seen to be a measure of the dispersion of Z around +1 and −1 .

κ attains its minimal value in a symmetric two-point distribution. In terms of the original variable X , the kurtosis is a measure of the dispersion of X around the two values μ ± σ .

High values of κ arise in two circumstances: where the probability mass is concentrated around the mean and the data-generating process produces occasional values far from the mean where the probability mass is concentrated in the tails of the distribution.

Maximal entropy [ edit ] The entropy of a distribution is − − ∫ ∫ p ( x ) ln ⁡ ⁡ p ( x ) d x .

{\textstyle -\!\int p(x)\ln p(x)\,dx.} For any μ μ ∈ ∈ R n , Σ Σ ∈ ∈ R n × × n {\displaystyle \mu \in \mathbb {R} ^{n},\Sigma \in \mathbb {R} ^{n\times n}} with Σ Σ {\displaystyle \Sigma } positive definite, among all probability distributions on R n {\displaystyle \mathbb {R} ^{n}} with mean μ μ {\displaystyle \mu } and covariance Σ Σ {\displaystyle \Sigma } , the normal distribution N ( μ μ , Σ Σ ) {\displaystyle {\mathcal {N}}(\mu ,\Sigma )} has the largest entropy.

Since mean μ μ {\displaystyle \mu } and covariance Σ Σ {\displaystyle \Sigma } are the first two moments, it is natural to consider extension to higher moments. In fact, by Lagrange multiplier method, for any prescribed first n moments, if there exists some probability distribution of form p ( x ) ∝ ∝ e ∑ ∑ i a i x i + ∑ ∑ i j b i j x i x j + ⋯ ⋯ + ∑ ∑ i 1 ⋯ ⋯ i n x i 1 ⋯ ⋯ x i n {\displaystyle p(x)\propto e^{\sum _{i}a_{i}x_{i}+\sum _{ij}b_{ij}x_{i}x_{j}+\cdots +\sum _{i_{1}\cdots i_{n}}x_{i_{1}}\cdots x_{i_{n}}}} that has the prescribed moments (if it is feasible), then it is the maximal entropy distribution under the given constraints.

[ 8 ] [ 9 ] By serial expansion, ∫ ∫ 1 2 π π e − − 1 2 x 2 − − 1 4 g x 4 x 2 n d x = 1 2 π π ∫ ∫ e − − 1 2 x 2 − − 1 4 g x 4 x 2 n d x = ∑ ∑ k 1 k !

( − − g 4 ) k ( 2 n + 4 k − − 1 ) !

!

= ( 2 n − − 1 ) !

!

− − 1 4 g ( 2 n + 3 ) !

!

+ O ( g 2 ) {\displaystyle {\begin{aligned}&\int {\frac {1}{\sqrt {2\pi }}}e^{-{\frac {1}{2}}x^{2}-{\frac {1}{4}}gx^{4}}x^{2n}\,dx\\[6pt]&={\frac {1}{\sqrt {2\pi }}}\int e^{-{\frac {1}{2}}x^{2}-{\frac {1}{4}}gx^{4}}x^{2n}\,dx\\[6pt]&=\sum _{k}{\frac {1}{k!}}\left(-{\frac {g}{4}}\right)^{k}(2n+4k-1)!!\\[6pt]&=(2n-1)!!-{\tfrac {1}{4}}g(2n+3)!!+O(g^{2})\end{aligned}}} so if a random variable has probability distribution p ( x ) = e − − 1 2 x 2 − − 1 4 g x 4 / Z {\displaystyle p(x)=e^{-{\frac {1}{2}}x^{2}-{\frac {1}{4}}gx^{4}}/Z} , where Z {\displaystyle Z} is a normalization constant, then its kurtosis is 3 − − 6 g + O ( g 2 ) {\displaystyle 3-6g+O(g^{2})} .

[ 10 ] Excess kurtosis [ edit ] The excess kurtosis is defined as kurtosis minus 3. There are 3 distinct regimes as described below.

Mesokurtic [ edit ] Distributions with zero excess kurtosis are called mesokurtic , or mesokurtotic . The most prominent example of a mesokurtic distribution is the normal distribution family, regardless of the values of its parameters . A few other well-known distributions can be mesokurtic, depending on parameter values: for example, the binomial distribution is mesokurtic for p = 1 / 2 ± ± 1 / 12 {\textstyle p=1/2\pm {\sqrt {1/12}}} .

Leptokurtic [ edit ] A distribution with positive excess kurtosis is called leptokurtic , or leptokurtotic .  A leptokurtic distribution has fatter tails . ("Lepto-" means "slender", originally referring to the peak.

[ 11 ] ) Examples of leptokurtic distributions include the Student's t-distribution , Rayleigh distribution , Laplace distribution , exponential distribution , Poisson distribution and the logistic distribution .  Such distributions are sometimes termed super-Gaussian .

[ 12 ] Three symmetric increasingly leptokurtic probability density functions; their intersections are indicated by vertical lines.

Platykurtic [ edit ] The coin toss is the most platykurtic distribution A distribution with negative excess kurtosis is called platykurtic , or platykurtotic .  A platykurtic distribution has thinner tails . ("Platy-" means "broad", originally referring to the peak.

[ 13 ] )  Examples of platykurtic distributions include the continuous and discrete uniform distributions , and the raised cosine distribution . The most platykurtic distribution of all is the Bernoulli distribution with p = 1/2 (for example the number of times one obtains "heads" when flipping a coin once, a coin toss ), for which the excess kurtosis is −2.

Graphical examples [ edit ] The Pearson type VII family [ edit ] pdf for the Pearson type VII distribution with excess kurtosis of infinity (red); 2 (blue); and 0 (black) log-pdf for the Pearson type VII distribution with excess kurtosis of infinity (red); 2 (blue); 1, 1/2, 1/4, 1/8, and 1/16 (gray); and 0 (black) The effects of kurtosis are illustrated using a parametric family of distributions whose kurtosis can be adjusted while their lower-order moments and cumulants remain constant. Consider the Pearson type VII family , which is a special case of the Pearson type IV family restricted to symmetric densities. The probability density function is given by f ( x ; a , m ) = Γ Γ ( m ) a π π Γ Γ ( m − − 1 / 2 ) [ 1 + ( x a ) 2 ] − − m , {\displaystyle f(x;a,m)={\frac {\Gamma (m)}{a\,{\sqrt {\pi }}\,\Gamma (m-1/2)}}\left[1+\left({\frac {x}{a}}\right)^{2}\right]^{-m},} where a is a scale parameter and m is a shape parameter .

All densities in this family are symmetric. The k -th moment exists provided m > ( k + 1)/2 . For the kurtosis to exist, we require m > 5/2 . Then the mean and skewness exist and are both identically zero. Setting a 2 = 2 m − 3 makes the variance equal to unity. Then the only free parameter is m , which controls the fourth moment (and cumulant) and hence the kurtosis.  One can reparameterize with m = 5 / 2 + 3 / γ γ 2 {\textstyle m=5/2+3/\gamma _{2}} , where γ γ 2 {\displaystyle \gamma _{2}} is the excess kurtosis as defined above. This yields a one-parameter leptokurtic family with zero mean, unit variance, zero skewness, and arbitrary non-negative excess kurtosis. The reparameterized density is g ( x ; γ γ 2 ) = f ( x ; a = 2 + 6 γ γ 2 − − 1 , m = 5 2 + 3 γ γ 2 − − 1 ) .

{\displaystyle g(x;\gamma _{2})=f{\left(x;\;a={\sqrt {2+6\gamma _{2}^{-1}}},\;m={\tfrac {5}{2}}+3\gamma _{2}^{-1}\right)}.} In the limit as γ γ 2 → → ∞ ∞ {\displaystyle \gamma _{2}\to \infty } one obtains the density g ( x ) = 3 ( 2 + x 2 ) − − 5 / 2 , {\displaystyle g(x)=3\left(2+x^{2}\right)^{-5/2},} which is shown as the red curve in the images on the right.

In the other direction as γ γ 2 → → 0 {\displaystyle \gamma _{2}\to 0} one obtains the standard normal density as the limiting distribution, shown as the black curve.

In the images on the right, the blue curve represents the density x ↦ ↦ g ( x ; 2 ) {\displaystyle x\mapsto g(x;2)} with excess kurtosis of 2.  The top image shows that leptokurtic densities in this family have a higher peak than the mesokurtic normal density, although this conclusion is only valid for this select family of distributions. The comparatively fatter tails of the leptokurtic densities are illustrated in the second image, which plots the natural logarithm of the Pearson type VII densities: the black curve is the logarithm of the standard normal density, which is a parabola . One can see that the normal density allocates little probability mass to the regions far from the mean ("has thin tails"), compared with the blue curve of the leptokurtic Pearson type VII density with excess kurtosis of 2.  Between the blue curve and the black are other Pearson type VII densities with γ 2 = 1, 1/2, 1/4, 1/8, and 1/16.  The red curve again shows the upper limit of the Pearson type VII family, with γ γ 2 = ∞ ∞ {\displaystyle \gamma _{2}=\infty } (which, strictly speaking, means that the fourth moment does not exist). The red curve decreases the slowest as one moves outward from the origin ("has fat tails").

Other well-known distributions [ edit ] Probability density functions for selected distributions with mean 0, variance 1 and different excess kurtosis Logarithms of probability density functions for selected distributions with mean 0, variance 1 and different excess kurtosis Several well-known, unimodal, and symmetric distributions from different parametric families are compared here.  Each has a mean and skewness of zero. The parameters have been chosen to result in a variance equal to 1 in each case. The images on the right show curves for the following seven densities, on a linear scale and logarithmic scale : D: Laplace distribution , also known as the double exponential distribution, red curve (two straight lines in the log-scale plot), excess kurtosis = 3 S: hyperbolic secant distribution , orange curve, excess kurtosis = 2 L: logistic distribution , green curve, excess kurtosis = 1.2 N: normal distribution , black curve (inverted parabola in the log-scale plot), excess kurtosis = 0 C: raised cosine distribution , cyan curve, excess kurtosis = −0.593762...

W: Wigner semicircle distribution , blue curve, excess kurtosis = −1 U: uniform distribution , magenta curve (shown for clarity as a rectangle in both images), excess kurtosis = −1.2.

Note that in these cases the platykurtic densities have bounded support , whereas the densities with positive or zero excess kurtosis are supported on the whole real line .

One cannot infer that high or low kurtosis distributions have the characteristics indicated by these examples. There exist platykurtic densities with infinite support, e.g., exponential power distributions with sufficiently large shape parameter b and there exist leptokurtic densities with finite support.

e.g., a distribution that is uniform between −3 and −0.3, between −0.3 and 0.3, and between 0.3 and 3, with the same density in the (−3, −0.3) and (0.3, 3) intervals, but with 20 times more density in the (−0.3, 0.3) interval Also, there exist platykurtic densities with infinite peakedness, e.g., an equal mixture of the beta distribution with parameters 0.5 and 1 with its reflection about 0.0 and there exist leptokurtic densities that appear flat-topped, e.g., a mixture of distribution that is uniform between −1 and 1 with a T(4.0000001) Student's t-distribution , with mixing probabilities 0.999 and 0.001.

Sample kurtosis [ edit ] Definitions [ edit ] A natural but biased estimator [ edit ] For a sample of n values, a method of moments estimator of the population excess kurtosis can be defined as g 2 = m 4 m 2 2 − − 3 = 1 n ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 4 [ 1 n ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 ] 2 − − 3 {\displaystyle g_{2}={\frac {m_{4}}{m_{2}^{2}}}-3={\frac {{\tfrac {1}{n}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{4}}{\left[{\tfrac {1}{n}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}\right]^{2}}}-3} where m 4 is the fourth sample moment about the mean , m 2 is the second sample moment about the mean (that is, the sample variance ), x i is the i -th value, and x ¯ ¯ {\displaystyle {\overline {x}}} is the sample mean .

This formula has the simpler representation, g 2 = 1 n ∑ ∑ i = 1 n z i 4 − − 3 {\displaystyle g_{2}={\frac {1}{n}}\sum _{i=1}^{n}z_{i}^{4}-3} where the z i {\displaystyle z_{i}} values are the standardized data values using the standard deviation defined using n rather than n − 1 in the denominator.

For example, suppose the data values are 0, 3, 4, 1, 2, 3, 0, 2, 1, 3, 2, 0, 2, 2, 3, 2, 5, 2, 3, 999.

Then the z i values are −0.239, −0.225, −0.221, −0.234, −0.230, −0.225, −0.239, −0.230, −0.234, −0.225, −0.230, −0.239, −0.230, −0.230, −0.225, −0.230, −0.216, −0.230, −0.225, 4.359 and the z i 4 values are  0.003, 0.003, 0.002, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.002, 0.003, 0.003, 360.976.

The average of these values is 18.05 and the excess kurtosis is thus 18.05 − 3 = 15.05 .  This example makes it clear that data near the "middle" or "peak" of the distribution do not contribute to the kurtosis statistic, hence kurtosis does not measure "peakedness". It is simply a measure of the outlier, 999 in this example.

Standard unbiased estimator [ edit ] Given a sub-set of samples from a population, the sample excess kurtosis g 2 {\displaystyle g_{2}} above is a biased estimator of the population excess kurtosis. An alternative estimator of the population excess kurtosis, which is unbiased in random samples of a normal distribution, is defined as follows: [ 3 ] G 2 = k 4 k 2 2 = n 2 [ ( n + 1 ) m 4 − − 3 ( n − − 1 ) m 2 2 ] ( n − − 1 ) ( n − − 2 ) ( n − − 3 ) ( n − − 1 ) 2 n 2 m 2 2 = n − − 1 ( n − − 2 ) ( n − − 3 ) [ ( n + 1 ) m 4 m 2 2 − − 3 ( n − − 1 ) ] = n − − 1 ( n − − 2 ) ( n − − 3 ) [ ( n + 1 ) g 2 + 6 ] = ( n + 1 ) n ( n − − 1 ) ( n − − 2 ) ( n − − 3 ) ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 4 ( ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 ) 2 − − 3 ( n − − 1 ) 2 ( n − − 2 ) ( n − − 3 ) = ( n + 1 ) n ( n − − 1 ) ( n − − 2 ) ( n − − 3 ) ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 4 k 2 2 − − 3 ( n − − 1 ) 2 ( n − − 2 ) ( n − − 3 ) {\displaystyle {\begin{aligned}G_{2}&={\frac {k_{4}}{k_{2}^{2}}}={\frac {n^{2}\,\left[(n+1)\,m_{4}-3\,(n-1)\,m_{2}^{2}\right]}{(n-1)\,(n-2)\,(n-3)}}\;{\frac {(n-1)^{2}}{n^{2}\,m_{2}^{2}}}\\[6pt]&={\frac {n-1}{(n-2)\,(n-3)}}\left[(n+1)\,{\frac {m_{4}}{m_{2}^{2}}}-3\,(n-1)\right]\\[6pt]&={\frac {n-1}{(n-2)\,(n-3)}}\left[(n+1)\,g_{2}+6\right]\\[6pt]&={\frac {(n+1)\,n\,(n-1)}{(n-2)\,(n-3)}}\;{\frac {\sum _{i=1}^{n}\left(x_{i}-{\bar {x}}\right)^{4}}{\left(\sum _{i=1}^{n}\left(x_{i}-{\bar {x}}\right)^{2}\right)^{2}}}-3\,{\frac {(n-1)^{2}}{(n-2)\,(n-3)}}\\[6pt]&={\frac {(n+1)\,n}{(n-1)\,(n-2)\,(n-3)}}\;{\frac {\sum _{i=1}^{n}\left(x_{i}-{\bar {x}}\right)^{4}}{k_{2}^{2}}}-3\,{\frac {(n-1)^{2}}{(n-2)(n-3)}}\end{aligned}}} where k 4 is the unique symmetric unbiased estimator of the fourth cumulant , k 2 is the unbiased estimate of the second cumulant (identical to the unbiased estimate of the sample variance), m 4 is the fourth sample moment about the mean, m 2 is the second sample moment about the mean, x i is the i -th value, and x ¯ ¯ {\displaystyle {\bar {x}}} is the sample mean. This adjusted Fisher–Pearson standardized moment coefficient G 2 {\displaystyle G_{2}} is the version found in Excel and several statistical packages including Minitab , SAS , and SPSS .

[ 14 ] Unfortunately, in nonnormal samples G 2 {\displaystyle G_{2}} is itself generally biased.

Upper bound [ edit ] An upper bound for the sample kurtosis of n ( n > 2 ) real numbers is [ 15 ] g 2 ≤ ≤ 1 2 n − − 3 n − − 2 g 1 2 + n 2 − − 3.

{\displaystyle g_{2}\leq {\frac {1}{2}}{\frac {n-3}{n-2}}g_{1}^{2}+{\frac {n}{2}}-3.} where g 1 = m 3 / m 2 3 / 2 {\displaystyle g_{1}=m_{3}/m_{2}^{3/2}} is the corresponding sample skewness.

Variance under normality [ edit ] The variance of the sample kurtosis of a sample of size n from the normal distribution is [ 16 ] var ⁡ ⁡ ( g 2 ) = 24 n ( n − − 1 ) 2 ( n − − 3 ) ( n − − 2 ) ( n + 3 ) ( n + 5 ) {\displaystyle \operatorname {var} (g_{2})={\frac {24n(n-1)^{2}}{(n-3)(n-2)(n+3)(n+5)}}} Stated differently, under the assumption that the underlying random variable X {\displaystyle X} is normally distributed, it can be shown that n g 2 → d N ( 0 , 24 ) {\displaystyle {\sqrt {n}}g_{2}\,\xrightarrow {d} \,{\mathcal {N}}(0,24)} .

[ 17 ] : Page number needed Applications [ edit ] The sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem, and may lead the researcher to choose alternative statistical methods.

D'Agostino's K-squared test is a goodness-of-fit normality test based on a combination of the sample skewness and sample kurtosis, as is the Jarque–Bera test for normality.

For non-normal samples, the variance of the sample variance depends on the kurtosis; for details, please see variance .

Pearson's definition of kurtosis is used as an indicator of intermittency in turbulence .

[ 18 ] It is also used in magnetic resonance imaging to quantify non-Gaussian diffusion.

[ 19 ] A concrete example is the following lemma by He, Zhang, and Zhang: [ 20 ] Assume a random variable X has expectation E ⁡ ⁡ [ X ] = μ μ {\displaystyle \operatorname {E} [X]=\mu } , variance E ⁡ ⁡ [ ( X − − μ μ ) 2 ] = σ σ 2 {\displaystyle \operatorname {E} \left[(X-\mu )^{2}\right]=\sigma ^{2}} and kurtosis κ κ = 1 σ σ 4 E ⁡ ⁡ [ ( X − − μ μ ) 4 ] .

{\textstyle \kappa ={\tfrac {1}{\sigma ^{4}}}\operatorname {E} \left[(X-\mu )^{4}\right].} Assume we sample n = 2 3 + 3 3 κ κ log ⁡ ⁡ 1 δ δ {\displaystyle n={\tfrac {2{\sqrt {3}}+3}{3}}\kappa \log {\tfrac {1}{\delta }}} many independent copies. Then Pr [ max i = 1 n X i ≤ ≤ μ μ ] ≤ ≤ δ δ and Pr [ min i = 1 n X i ≥ ≥ μ μ ] ≤ ≤ δ δ .

{\displaystyle \Pr \left[\max _{i=1}^{n}X_{i}\leq \mu \right]\leq \delta \quad {\text{and}}\quad \Pr \left[\min _{i=1}^{n}X_{i}\geq \mu \right]\leq \delta .} This shows that with Θ Θ ( κ κ log ⁡ ⁡ 1 δ δ ) {\displaystyle \Theta (\kappa \log {\tfrac {1}{\delta }})} many samples, we will see one that is above the expectation with probability at least 1 − − δ δ {\displaystyle 1-\delta } .
In other words: If the kurtosis is large, we might see a lot values either all below or above the mean.

Kurtosis convergence [ edit ] Applying band-pass filters to digital images , kurtosis values tend to be uniform, independent of the range of the filter. This behavior, termed kurtosis convergence , can be used to detect image splicing in forensic analysis .

[ 21 ] Seismic signal analysis [ edit ] Kurtosis can be used in geophysics to distinguish different types of seismic signals . It is particularly effective in differentiating seismic signals generated by human footsteps from other signals.

[ 22 ] This is useful in security and surveillance systems that rely on seismic detection.

Weather prediction [ edit ] In meteorology , kurtosis is used to analyze weather data distributions. It helps predict extreme weather events by assessing the probability of outlier values in historical data, [ 23 ] which is valuable for long-term climate studies and short-term weather forecasting.

Other measures [ edit ] A different measure of "kurtosis" is provided by using L-moments instead of the ordinary moments.

[ 24 ] [ 25 ] See also [ edit ] Wikimedia Commons has media related to Kurtosis .

Kurtosis risk Maximum entropy probability distribution References [ edit ] ^ Pearson, Karl (1905), "Das Fehlergesetz und seine Verallgemeinerungen durch Fechner und Pearson. A Rejoinder" [The Error Law and its Generalizations by Fechner and Pearson. A Rejoinder], Biometrika , 4 ( 1– 2): 169– 212, doi : 10.1093/biomet/4.1-2.169 , JSTOR 2331536 ^ a b Westfall, Peter H. (2014), "Kurtosis as Peakedness, 1905–2014.

R.I.P.

", The American Statistician , 68 (3): 191– 195, doi : 10.1080/00031305.2014.917055 , PMC 4321753 , PMID 25678714 ^ a b c Joanes, Derrick N.; Gill, Christine A. (1998), "Comparing measures of sample skewness and kurtosis", Journal of the Royal Statistical Society, Series D , 47 (1): 183– 189, doi : 10.1111/1467-9884.00122 , JSTOR 2988433 ^ Pearson, Karl (1916), "Mathematical Contributions to the Theory of Evolution. — XIX. Second Supplement to a Memoir on Skew Variation.", Philosophical Transactions of the Royal Society of London A , 216 (546): 429– 457, Bibcode : 1916RSPTA.216..429P , doi : 10.1098/rsta.1916.0009 , JSTOR 91092 ^ a b Balanda, Kevin P.; MacGillivray, Helen L.

(1988), "Kurtosis: A Critical Review", The American Statistician , 42 (2): 111– 119, doi : 10.2307/2684482 , JSTOR 2684482 ^ Darlington, Richard B. (1970), "Is Kurtosis Really 'Peakedness'?", The American Statistician , 24 (2): 19– 22, doi : 10.1080/00031305.1970.10478885 , JSTOR 2681925 ^ Moors, J. J. A. (1986), "The meaning of kurtosis: Darlington reexamined", The American Statistician , 40 (4): 283– 284, doi : 10.1080/00031305.1986.10475415 , JSTOR 2684603 ^ Tagliani, A. (1990-12-01).

"On the existence of maximum entropy distributions with four and more assigned moments" .

Probabilistic Engineering Mechanics .

5 (4): 167– 170.

Bibcode : 1990PEngM...5..167T .

doi : 10.1016/0266-8920(90)90017-E .

ISSN 0266-8920 .

^ Rockinger, Michael; Jondeau, Eric (2002-01-01).

"Entropy densities with an application to autoregressive conditional skewness and kurtosis" .

Journal of Econometrics .

106 (1): 119– 142.

doi : 10.1016/S0304-4076(01)00092-6 .

ISSN 0304-4076 .

^ Bradde, Serena; Bialek, William (2017-05-01).

"PCA Meets RG" .

Journal of Statistical Physics .

167 (3): 462– 475.

arXiv : 1610.09733 .

Bibcode : 2017JSP...167..462B .

doi : 10.1007/s10955-017-1770-6 .

ISSN 1572-9613 .

PMC 6054449 .

PMID 30034029 .

^ "Lepto-" .

^ Benveniste, Albert; Goursat, Maurice; Ruget, Gabriel (1980), "Robust identification of a nonminimum phase system: Blind adjustment of a linear equalizer in data communications", IEEE Transactions on Automatic Control , 25 (3): 385– 399, doi : 10.1109/tac.1980.1102343 ^ "platy-: definition, usage and pronunciation – YourDictionary.com" . Archived from the original on 2007-10-20.

^ Doane DP, Seward LE (2011) J Stat Educ 19 (2) ^ Sharma, Rajesh; Bhandari, Rajeev K. (2015), "Skewness, kurtosis and Newton's inequality" , Rocky Mountain Journal of Mathematics , 45 (5): 1639– 1643, arXiv : 1309.2896 , doi : 10.1216/RMJ-2015-45-5-1639 , S2CID 88513237 ^ Fisher, Ronald A.

(1930), "The Moments of the Distribution for Normal Samples of Measures of Departure from Normality", Proceedings of the Royal Society A , 130 (812): 16– 28, Bibcode : 1930RSPSA.130...16F , doi : 10.1098/rspa.1930.0185 , hdl : 2440/15205 , JSTOR 95586 , S2CID 121520301 ^ Kendall, Maurice G.; Stuart, Alan (1969), The Advanced Theory of Statistics, Volume 1: Distribution Theory (3rd ed.), London, UK: Charles Griffin & Company Limited, ISBN 0-85264-141-9 ^ Sandborn, Virgil A. (1959), "Measurements of Intermittency of Turbulent Motion in a Boundary Layer", Journal of Fluid Mechanics , 6 (2): 221– 240, Bibcode : 1959JFM.....6..221S , doi : 10.1017/S0022112059000581 , S2CID 121838685 ^ Jensen, J.; Helpern, J.; Ramani, A.; Lu, H.; Kaczynski, K. (19 May 2005).

"Diffusional kurtosis imaging: The quantification of non-Gaussian water diffusion by means of magnetic resonance imaging" .

Magn Reson Med .

53 (6): 1432– 1440.

doi : 10.1002/mrm.20508 .

PMID 15906300 .

S2CID 11865594 .

^ He, Simai; Zhang, Jiawei; Zhang, Shuzhong (2010). "Bounding probability of small deviation: A fourth moment approach".

Mathematics of Operations Research .

35 (1): 208– 232.

doi : 10.1287/moor.1090.0438 .

S2CID 11298475 .

^ Pan, Xunyu; Zhang, Xing; Lyu, Siwei (2012), "Exposing Image Splicing with Inconsistent Local Noise Variances", 2012 IEEE International Conference on Computational Photography (ICCP) , 28-29 April 2012; Seattle, WA, USA: IEEE, pp.

1– 10, doi : 10.1109/ICCPhot.2012.6215223 , ISBN 978-1-4673-1662-0 , S2CID 14386924 {{ citation }} :  CS1 maint: location ( link ) ^ Liang, Zhiqiang; Wei, Jianming; Zhao, Junyu; Liu, Haitao; Li, Baoqing; Shen, Jie; Zheng, Chunlei (2008-08-27).

"The Statistical Meaning of Kurtosis and Its New Application to Identification of Persons Based on Seismic Signals" .

Sensors .

8 (8): 5106– 5119.

Bibcode : 2008Senso...8.5106L .

doi : 10.3390/s8085106 .

ISSN 1424-8220 .

PMC 3705491 .

PMID 27873804 .

^ Supraja (2024-05-27).

"Kurtosis in Practice: Real-World Applications and Interpretations" .

Analytics Insight . Retrieved 2024-11-11 .

^ Hosking, Jonathan R. M. (1992), "Moments or L moments? An example comparing two measures of distributional shape", The American Statistician , 46 (3): 186– 189, doi : 10.1080/00031305.1992.10475880 , JSTOR 2685210 ^ Hosking, Jonathan R. M. (2006), "On the characterization of distributions by their L -moments", Journal of Statistical Planning and Inference , 136 (1): 193– 198, doi : 10.1016/j.jspi.2004.06.004 Further reading [ edit ] Kim, Tae-Hwan; White, Halbert (2003).

"On More Robust Estimation of Skewness and Kurtosis: Simulation and Application to the S&P500 Index" .

Finance Research Letters .

1 : 56– 70.

doi : 10.1016/S1544-6123(03)00003-5 .

S2CID 16913409 .

Alternative source (Comparison of kurtosis estimators) Seier, E.; Bonett, D.G. (2003). "Two families of kurtosis measures".

Metrika .

58 : 59– 70.

doi : 10.1007/s001840200223 .

S2CID 115990880 .

External links [ edit ] Wikiversity has learning resources about Kurtosis "Excess coefficient" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Kurtosis calculator Free Online Software (Calculator) computes various types of skewness and kurtosis statistics for any dataset (includes small and large sample tests)..

Kurtosis on the Earliest known uses of some of the words of mathematics Celebrating 100 years of Kurtosis a history of the topic, with different measures of kurtosis.

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Kurtosis&oldid=1306494530 " Categories : Moments (mathematics) Statistical deviation and dispersion Hidden categories: CS1 maint: location Articles with short description Short description matches Wikidata Use American English from January 2019 All Wikipedia articles written in American English Articles containing Greek-language text Commons category link is on Wikidata This page was last edited on 18 August 2025, at 01:29 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Kurtosis 27 languages Add topic

