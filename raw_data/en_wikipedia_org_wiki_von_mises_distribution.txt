Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Moments 3 Generation of von Mises Variates 4 Limiting behavior 5 Estimation of parameters 6 Distribution of the mean 7 Entropy 8 See also 9 References 10 Works cited Toggle the table of contents von Mises distribution 5 languages Català Deutsch Français 日本語 Türkçe Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution on the circle von Mises Probability density function The support is chosen to be [− π , π ] with μ = 0 Cumulative distribution function The support is chosen to be [− π , π ] with μ = 0 Parameters μ μ {\displaystyle \mu } real κ κ > 0 {\displaystyle \kappa >0} Support x ∈ ∈ {\displaystyle x\in } any interval of length 2π PDF e κ κ cos ⁡ ⁡ ( x − − μ μ ) 2 π π I 0 ( κ κ ) {\displaystyle {\frac {e^{\kappa \cos(x-\mu )}}{2\pi I_{0}(\kappa )}}} CDF (not analytic – see text) Mean μ μ {\displaystyle \mu } Median μ μ {\displaystyle \mu } Mode μ μ {\displaystyle \mu } Variance var ( x ) = 1 − − I 1 ( κ κ ) / I 0 ( κ κ ) {\displaystyle {\textrm {var}}(x)=1-I_{1}(\kappa )/I_{0}(\kappa )} (circular) Entropy − − κ κ I 1 ( κ κ ) I 0 ( κ κ ) + ln ⁡ ⁡ [ 2 π π I 0 ( κ κ ) ] {\displaystyle -\kappa {\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}+\ln[2\pi I_{0}(\kappa )]} (differential) CF I | t | ( κ κ ) I 0 ( κ κ ) e i t μ μ {\displaystyle {\frac {I_{|t|}(\kappa )}{I_{0}(\kappa )}}e^{it\mu }} In probability theory and directional statistics , the von Mises distribution (also known as the circular normal distribution or the Tikhonov distribution ) is a continuous probability distribution on the circle . It is a close approximation to the wrapped normal distribution , which is the circular analogue of the normal distribution . A freely diffusing angle θ θ {\displaystyle \theta } on a circle is a wrapped normally distributed random variable with an unwrapped variance that grows linearly in time. On the other hand, the von Mises distribution is the stationary distribution of a drift and diffusion process on the circle in a harmonic potential, i.e. with a preferred orientation.

[ 1 ] The von Mises distribution is the maximum entropy distribution for circular data when the real and imaginary parts of the first circular moment are specified. The von Mises distribution is a special case of the von Mises–Fisher distribution on the N -dimensional sphere.

Definition [ edit ] The von Mises probability density function for the angle x is given by: [ 2 ] f ( x ∣ ∣ μ μ , κ κ ) = exp ⁡ ⁡ ( κ κ cos ⁡ ⁡ ( x − − μ μ ) ) 2 π π I 0 ( κ κ ) {\displaystyle f(x\mid \mu ,\kappa )={\frac {\exp(\kappa \cos(x-\mu ))}{2\pi I_{0}(\kappa )}}} where I 0 ( κ κ {\displaystyle \kappa } ) is the modified Bessel function of the first kind of order 0, with this scaling constant chosen so that the distribution sums to unity: ∫ ∫ − − π π π π exp ⁡ ⁡ ( κ κ cos ⁡ ⁡ x ) d x = 2 π π I 0 ( κ κ ) .

{\textstyle \int _{-\pi }^{\pi }\exp(\kappa \cos x)dx={2\pi I_{0}(\kappa )}.} The parameters μ and 1/ κ κ {\displaystyle \kappa } are analogous to μ and σ 2 (the mean and variance) in the normal distribution: μ is a measure of location (the distribution is clustered around μ ), and κ κ {\displaystyle \kappa } is a measure of concentration (a reciprocal measure of dispersion , so 1/ κ κ {\displaystyle \kappa } is analogous to σ 2 ).

If κ κ {\displaystyle \kappa } is zero, the distribution is uniform, and for small κ κ {\displaystyle \kappa } , it is close to uniform.

If κ κ {\displaystyle \kappa } is large, the distribution becomes very concentrated about the angle μ with κ κ {\displaystyle \kappa } being a measure of the concentration. In fact, as κ κ {\displaystyle \kappa } increases, the distribution approaches a normal distribution in x with mean μ and variance 1/ κ κ {\displaystyle \kappa } .

The probability density can be expressed as a series of Bessel functions [ 3 ] f ( x ∣ ∣ μ μ , κ κ ) = 1 2 π π ( 1 + 2 I 0 ( κ κ ) ∑ ∑ j = 1 ∞ ∞ I j ( κ κ ) cos ⁡ ⁡ [ j ( x − − μ μ ) ] ) {\displaystyle f(x\mid \mu ,\kappa )={\frac {1}{2\pi }}\left(1+{\frac {2}{I_{0}(\kappa )}}\sum _{j=1}^{\infty }I_{j}(\kappa )\cos[j(x-\mu )]\right)} where I j ( x ) is the modified Bessel function of order j .

The cumulative distribution function is not analytic and is best found by integrating the above series. The indefinite integral of the probability density is: Φ Φ ( x ∣ ∣ μ μ , κ κ ) = ∫ ∫ f ( t ∣ ∣ μ μ , κ κ ) d t = 1 2 π π ( x + 2 I 0 ( κ κ ) ∑ ∑ j = 1 ∞ ∞ I j ( κ κ ) sin ⁡ ⁡ [ j ( x − − μ μ ) ] j ) .

{\displaystyle \Phi (x\mid \mu ,\kappa )=\int f(t\mid \mu ,\kappa )\,dt={\frac {1}{2\pi }}\left(x+{\frac {2}{I_{0}(\kappa )}}\sum _{j=1}^{\infty }I_{j}(\kappa ){\frac {\sin[j(x-\mu )]}{j}}\right).} The cumulative distribution function will be a function of the lower limit of
integration x 0 : F ( x ∣ ∣ μ μ , κ κ ) = Φ Φ ( x ∣ ∣ μ μ , κ κ ) − − Φ Φ ( x 0 ∣ ∣ μ μ , κ κ ) .

{\displaystyle F(x\mid \mu ,\kappa )=\Phi (x\mid \mu ,\kappa )-\Phi (x_{0}\mid \mu ,\kappa ).\,} Moments [ edit ] Further information: Circular mean The moments of the von Mises distribution are usually calculated as the moments of the complex exponential z = e ix rather than the angle x itself. These moments are referred to as circular moments . The variance calculated from these moments is referred to as the circular variance . The one exception to this is that the "mean" usually refers to the argument of the complex mean.

The n th raw moment of z is: m n = ⟨ ⟨ z n ⟩ ⟩ = ∫ ∫ Γ Γ z n f ( x | μ μ , κ κ ) d x {\displaystyle m_{n}=\langle z^{n}\rangle =\int _{\Gamma }z^{n}\,f(x|\mu ,\kappa )\,dx} = I | n | ( κ κ ) I 0 ( κ κ ) e i n μ μ {\displaystyle ={\frac {I_{|n|}(\kappa )}{I_{0}(\kappa )}}e^{in\mu }} where the integral is over any interval Γ Γ {\displaystyle \Gamma } of length 2π. In calculating the above integral, we use the fact that z n = cos( n x) + i sin( nx ) and the Bessel function identity: [ 4 ] I n ( κ κ ) = 1 π π ∫ ∫ 0 π π e κ κ cos ⁡ ⁡ ( x ) cos ⁡ ⁡ ( n x ) d x .

{\displaystyle I_{n}(\kappa )={\frac {1}{\pi }}\int _{0}^{\pi }e^{\kappa \cos(x)}\cos(nx)\,dx.} The mean of the complex exponential z is then just m 1 = I 1 ( κ κ ) I 0 ( κ κ ) e i μ μ {\displaystyle m_{1}={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}e^{i\mu }} and the circular mean value of the angle x is then taken to be the argument μ . This is the expected or preferred direction of the angular random variables. The circular variance of x is: V = 1 − − | E [ e i x ] | = 1 − − I 1 ( κ κ ) I 0 ( κ κ ) .

{\displaystyle V=1-|E[e^{ix}]|=1-{\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}.} Generation of von Mises Variates [ edit ] A notable advancement in generating Tikhonov (or von Mises) random variates was introduced by Abreu in 2008.

[ 5 ] This method, termed the "random mixture" (RM) technique, offers a simple and efficient alternative to traditional approaches like the accept-reject (AR) algorithm, which often suffer from inefficiency due to sample rejection and computational complexity. The RM method generates Tikhonov variates by randomly selecting samples from a predefined set of Cauchy and Gaussian generators, followed by a straightforward transformation. Specifically, it uses a bank of K {\displaystyle K} distinct generators (e.g., one Cauchy and two Gaussian processes), with mixture probabilities derived from the characteristic functions of the Cauchy, Gaussian, and Tikhonov distributions, all of which are available in closed form.

The technique leverages the circular moment-determinance property of the Tikhonov distribution, where the distribution is uniquely defined by its circular moments. By ensuring that the first N {\displaystyle N} dominant circular moments of the generated variates closely match the theoretical Tikhonov moments, the method achieves high accuracy. The mixture probabilities and parameters (e.g., variance for Gaussian and half-width for Cauchy) can be computed using either least squares (LS) optimization or a simpler Moore-Penrose pseudo-inverse approach, with the latter offering a practical trade-off between complexity and precision. Unlike AR methods, the RM technique consumes only one pair of uniform random numbers per Tikhonov sample, regardless of the concentration parameter α α {\displaystyle \alpha } , and avoids sample rejection or repetitive evaluation of complex functions.

[ 6 ] Limiting behavior [ edit ] When κ κ {\displaystyle \kappa } is large, the distribution resembles a normal distribution .

[ 7 ] More specifically, for large positive real numbers κ κ {\displaystyle \kappa } , f ( x ∣ ∣ μ μ , κ κ ) ≈ ≈ 1 σ σ 2 π π exp ⁡ ⁡ [ − − ( x − − μ μ ) 2 2 σ σ 2 ] {\displaystyle f(x\mid \mu ,\kappa )\approx {\frac {1}{\sigma {\sqrt {2\pi }}}}\exp \left[{\dfrac {-(x-\mu )^{2}}{2\sigma ^{2}}}\right]} where σ 2 = 1/ κ κ {\displaystyle \kappa } and the difference between the left hand side and the right hand side of the approximation converges uniformly to zero as κ κ {\displaystyle \kappa } goes to infinity.  Also, when κ κ {\displaystyle \kappa } is small, the probability density function resembles a uniform distribution : lim κ κ → → 0 f ( x ∣ ∣ μ μ , κ κ ) = U ( x ) {\displaystyle \lim _{\kappa \rightarrow 0}f(x\mid \mu ,\kappa )=\mathrm {U} (x)} where the interval for the uniform distribution U ( x ) {\displaystyle \mathrm {U} (x)} is the chosen interval of length 2 π π {\displaystyle 2\pi } (i.e.

U ( x ) = 1 / ( 2 π π ) {\displaystyle \mathrm {U} (x)=1/(2\pi )} when x {\displaystyle x} is in the interval and U ( x ) = 0 {\displaystyle \mathrm {U} (x)=0} when x {\displaystyle x} is not in the interval).

Estimation of parameters [ edit ] A series of N measurements z n = e i θ θ n {\displaystyle z_{n}=e^{i\theta _{n}}} drawn from a von Mises distribution may be used to estimate certain parameters of the distribution.

[ 8 ] The average of the series z ¯ ¯ {\displaystyle {\overline {z}}} is defined as z ¯ ¯ = 1 N ∑ ∑ n = 1 N z n {\displaystyle {\overline {z}}={\frac {1}{N}}\sum _{n=1}^{N}z_{n}} and its expectation value will be just the first moment: ⟨ ⟨ z ¯ ¯ ⟩ ⟩ = I 1 ( κ κ ) I 0 ( κ κ ) e i μ μ .

{\displaystyle \langle {\overline {z}}\rangle ={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}e^{i\mu }.} In other words, z ¯ ¯ {\displaystyle {\overline {z}}} is an unbiased estimator of the first moment. If we assume that the mean μ μ {\displaystyle \mu } lies in the interval [ − − π π , π π ] {\displaystyle [-\pi ,\pi ]} , then Arg ( z ¯ ¯ ) {\displaystyle ({\overline {z}})} will be a (biased) estimator of the mean μ μ {\displaystyle \mu } .

Viewing the z n {\displaystyle z_{n}} as a set of vectors in the complex plane, the R ¯ ¯ 2 {\displaystyle {\bar {R}}^{2}} statistic is the square of the length of the averaged vector: R ¯ ¯ 2 = z ¯ ¯ z ∗ ∗ ¯ ¯ = ( 1 N ∑ ∑ n = 1 N cos ⁡ ⁡ θ θ n ) 2 + ( 1 N ∑ ∑ n = 1 N sin ⁡ ⁡ θ θ n ) 2 {\displaystyle {\bar {R}}^{2}={\overline {z}}\,{\overline {z^{*}}}=\left({\frac {1}{N}}\sum _{n=1}^{N}\cos \theta _{n}\right)^{2}+\left({\frac {1}{N}}\sum _{n=1}^{N}\sin \theta _{n}\right)^{2}} and its expectation value is [ 9 ] ⟨ ⟨ R ¯ ¯ 2 ⟩ ⟩ = 1 N + N − − 1 N I 1 ( κ κ ) 2 I 0 ( κ κ ) 2 .

{\displaystyle \langle {\bar {R}}^{2}\rangle ={\frac {1}{N}}+{\frac {N-1}{N}}\,{\frac {I_{1}(\kappa )^{2}}{I_{0}(\kappa )^{2}}}.} In other words, the statistic R e 2 = N N − − 1 ( R ¯ ¯ 2 − − 1 N ) {\displaystyle R_{e}^{2}={\frac {N}{N-1}}\left({\bar {R}}^{2}-{\frac {1}{N}}\right)} will be an unbiased estimator of I 1 ( κ κ ) 2 I 0 ( κ κ ) 2 {\displaystyle {\frac {I_{1}(\kappa )^{2}}{I_{0}(\kappa )^{2}}}\,} and solving the equation R e = I 1 ( κ κ ) I 0 ( κ κ ) {\displaystyle R_{e}={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}\,} for κ κ {\displaystyle \kappa \,} will yield a (biased) estimator of κ κ {\displaystyle \kappa \,} . In analogy to the linear case, the solution to the equation R ¯ ¯ = I 1 ( κ κ ) I 0 ( κ κ ) {\displaystyle {\bar {R}}={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}\,} will yield the maximum likelihood estimate of κ κ {\displaystyle \kappa \,} and both will be equal in the limit of large N . For approximate solution to κ κ {\displaystyle \kappa \,} refer to von Mises–Fisher distribution .

Distribution of the mean [ edit ] The distribution of the sample mean z ¯ ¯ = R ¯ ¯ e i θ θ ¯ ¯ {\displaystyle {\overline {z}}={\bar {R}}e^{i{\overline {\theta }}}} for the von Mises distribution is given by: [ 10 ] P ( R ¯ ¯ , θ θ ¯ ¯ ) d R ¯ ¯ d θ θ ¯ ¯ = 1 ( 2 π π I 0 ( κ κ ) ) N ∫ ∫ Γ Γ ∏ ∏ n = 1 N ( e κ κ cos ⁡ ⁡ ( θ θ n − − μ μ ) d θ θ n ) = e κ κ N R ¯ ¯ cos ⁡ ⁡ ( θ θ ¯ ¯ − − μ μ ) I 0 ( κ κ ) N ( 1 ( 2 π π ) N ∫ ∫ Γ Γ ∏ ∏ n = 1 N d θ θ n ) {\displaystyle P({\bar {R}},{\bar {\theta }})\,d{\bar {R}}\,d{\bar {\theta }}={\frac {1}{(2\pi I_{0}(\kappa ))^{N}}}\int _{\Gamma }\prod _{n=1}^{N}\left(e^{\kappa \cos(\theta _{n}-\mu )}d\theta _{n}\right)={\frac {e^{\kappa N{\bar {R}}\cos({\bar {\theta }}-\mu )}}{I_{0}(\kappa )^{N}}}\left({\frac {1}{(2\pi )^{N}}}\int _{\Gamma }\prod _{n=1}^{N}d\theta _{n}\right)} where N is the number of measurements and Γ Γ {\displaystyle \Gamma \,} consists of intervals of 2 π π {\displaystyle 2\pi } in the variables, subject to the constraint that R ¯ ¯ {\displaystyle {\bar {R}}} and θ θ ¯ ¯ {\displaystyle {\bar {\theta }}} are constant, where R ¯ ¯ {\displaystyle {\bar {R}}} is the mean resultant: R ¯ ¯ 2 = | z ¯ ¯ | 2 = ( 1 N ∑ ∑ n = 1 N cos ⁡ ⁡ ( θ θ n ) ) 2 + ( 1 N ∑ ∑ n = 1 N sin ⁡ ⁡ ( θ θ n ) ) 2 {\displaystyle {\bar {R}}^{2}=|{\bar {z}}|^{2}=\left({\frac {1}{N}}\sum _{n=1}^{N}\cos(\theta _{n})\right)^{2}+\left({\frac {1}{N}}\sum _{n=1}^{N}\sin(\theta _{n})\right)^{2}} and θ θ ¯ ¯ {\displaystyle {\overline {\theta }}} is the mean angle: θ θ ¯ ¯ = A r g ( z ¯ ¯ ) .

{\displaystyle {\overline {\theta }}=\mathrm {Arg} ({\overline {z}}).\,} Note that the product term in parentheses is just the distribution of the mean for a circular uniform distribution .

[ 10 ] This means that the distribution of the mean direction μ μ {\displaystyle \mu } of a von Mises distribution V M ( μ μ , κ κ ) {\displaystyle VM(\mu ,\kappa )} is a von Mises distribution V M ( μ μ , R ¯ ¯ N κ κ ) {\displaystyle VM(\mu ,{\bar {R}}N\kappa )} , or, equivalently, V M ( μ μ , R κ κ ) {\displaystyle VM(\mu ,R\kappa )} .

Entropy [ edit ] By definition, the information entropy of the von Mises distribution is [ 2 ] H = − − ∫ ∫ Γ Γ f ( θ θ ; μ μ , κ κ ) ln ⁡ ⁡ ( f ( θ θ ; μ μ , κ κ ) ) d θ θ {\displaystyle H=-\int _{\Gamma }f(\theta ;\mu ,\kappa )\,\ln(f(\theta ;\mu ,\kappa ))\,d\theta \,} where Γ Γ {\displaystyle \Gamma } is any interval of length 2 π π {\displaystyle 2\pi } . The logarithm of the density of the Von Mises distribution is straightforward: ln ⁡ ⁡ ( f ( θ θ ; μ μ , κ κ ) ) = − − ln ⁡ ⁡ ( 2 π π I 0 ( κ κ ) ) + κ κ cos ⁡ ⁡ ( θ θ ) {\displaystyle \ln(f(\theta ;\mu ,\kappa ))=-\ln(2\pi I_{0}(\kappa ))+\kappa \cos(\theta )\,} The characteristic function representation for the Von Mises distribution is: f ( θ θ ; μ μ , κ κ ) = 1 2 π π ( 1 + 2 ∑ ∑ n = 1 ∞ ∞ ϕ ϕ n cos ⁡ ⁡ ( n θ θ ) ) {\displaystyle f(\theta ;\mu ,\kappa )={\frac {1}{2\pi }}\left(1+2\sum _{n=1}^{\infty }\phi _{n}\cos(n\theta )\right)} where ϕ ϕ n = I | n | ( κ κ ) / I 0 ( κ κ ) {\displaystyle \phi _{n}=I_{|n|}(\kappa )/I_{0}(\kappa )} . Substituting these expressions into the entropy integral, exchanging the order of integration and summation, and using the orthogonality of the cosines, the entropy may be written: H = ln ⁡ ⁡ ( 2 π π I 0 ( κ κ ) ) − − κ κ ϕ ϕ 1 = ln ⁡ ⁡ ( 2 π π I 0 ( κ κ ) ) − − κ κ I 1 ( κ κ ) I 0 ( κ κ ) {\displaystyle H=\ln(2\pi I_{0}(\kappa ))-\kappa \phi _{1}=\ln(2\pi I_{0}(\kappa ))-\kappa {\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}} For κ κ = 0 {\displaystyle \kappa =0} , the von Mises distribution becomes the circular uniform distribution and the entropy attains its maximum value of ln ⁡ ⁡ ( 2 π π ) {\displaystyle \ln(2\pi )} .

Notice that the Von Mises distribution maximizes the entropy when the real and imaginary parts of the first circular moment are specified [ 11 ] or, equivalently, the circular mean and circular variance are specified.

See also [ edit ] Bivariate von Mises distribution Directional statistics Von Mises–Fisher distribution Kent distribution References [ edit ] ^ Risken, H. (1989).

The Fokker–Planck Equation . Springer.

ISBN 978-3-540-61530-9 .

^ a b Mardia, Kantilal ; Jupp, Peter E. (1999).

Directional Statistics . Wiley.

ISBN 978-0-471-95333-3 .

^ see Abramowitz and Stegun §9.6.34 ^ See Abramowitz and Stegun §9.6.19 ^ de Abreu, Giuseppe Thadeu Freitas (2008). "On the Generation of Tikhonov Variates".

IEEE Transactions on Communications .

56 (7): 1157– 1168.

doi : 10.1109/TCOMM.2008.060510 .

^ [On the Generation of Tikhonov Random Variates] .

YouTube (Video). [The Wireless Channel]. 2025-03-19 . Retrieved 2025-03-21 .

^ Mardia, K. V.; Jupp, P. E. (2000). "Directional Statistics". Wiley Series in Probability and Statistics. Chichester: John Wiley & Sons. ISBN 978-0-471-95333-3. p. 36.

^ Borradaile, G. J. (2003).

Statistics of earth science data : their distribution in time, space, and orientation . Springer.

ISBN 978-3-662-05223-5 .

^ Kutil, Rade (August 2012).

"Biased and unbiased estimation of the circular mean resultant length and its variance" .

Statistics: A Journal of Theoretical and Applied Statistics .

46 (4): 549– 561.

CiteSeerX 10.1.1.302.8395 .

doi : 10.1080/02331888.2010.543463 .

S2CID 7045090 .

^ a b Jammalamadaka, S. Rao; Sengupta, A. (2001).

Topics in Circular Statistics . World Scientific Publishing Company.

ISBN 978-981-02-3778-3 .

^ Jammalamadaka, S. Rao; SenGupta, A. (2001).

Topics in circular statistics . New Jersey: World Scientific.

ISBN 981-02-3778-2 . Retrieved 2011-05-15 .

Works cited [ edit ] Abramowitz, M. and Stegun, I. A.  (ed.), Handbook of Mathematical Functions , National Bureau of Standards, 1964; reprinted Dover Publications , 1965.

ISBN 0-486-61272-4 v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Von_Mises_distribution&oldid=1281623580 " Categories : Continuous distributions Directional statistics Exponential family distributions Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 21 March 2025, at 13:57 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents von Mises distribution 5 languages Add topic

