Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Two-variable formulae 1.2 Additional constructions 2 History 3 Properties Toggle Properties subsection 3.1 Relationship to arithmetic mean and variance 3.2 Other relationships 4 Uses in statistics 5 See also 6 References 7 External links Toggle the table of contents Contraharmonic mean 1 language Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In mathematics, a contraharmonic mean (or antiharmonic mean [ 1 ] ) is a function complementary to the harmonic mean .  The contraharmonic mean is a special case of the Lehmer mean , L p {\displaystyle L_{p}} , where p = 2.

Definition [ edit ] The contraharmonic mean of a set of positive real numbers [ 2 ] is defined as the arithmetic mean of the squares of the numbers divided by the arithmetic mean of the numbers: C ⁡ ⁡ ( x 1 , x 2 , … … , x n ) = 1 n ( x 1 2 + x 2 2 + ⋯ ⋯ + x n 2 ) 1 n ( x 1 + x 2 + ⋯ ⋯ + x n ) , = x 1 2 + x 2 2 + ⋯ ⋯ + x n 2 x 1 + x 2 + ⋯ ⋯ + x n .

{\displaystyle {\begin{aligned}\operatorname {C} \left(x_{1},x_{2},\dots ,x_{n}\right)&={{1 \over n}\left(x_{1}^{2}+x_{2}^{2}+\cdots +x_{n}^{2}\right) \over {1 \over n}\left(x_{1}+x_{2}+\cdots +x_{n}\right)},\\[3pt]&={{x_{1}^{2}+x_{2}^{2}+\cdots +x_{n}^{2}} \over {x_{1}+x_{2}+\cdots +x_{n}}}.\end{aligned}}} Two-variable formulae [ edit ] From the formulas for the arithmetic mean and harmonic mean of two variables we have: A ⁡ ⁡ ( a , b ) = a + b 2 H ⁡ ⁡ ( a , b ) = 1 1 2 ⋅ ⋅ ( 1 a + 1 b ) = 2 a b a + b C ⁡ ⁡ ( a , b ) = 2 ⋅ ⋅ A ( a , b ) − − H ( a , b ) = a + b − − 2 a b a + b = ( a + b ) 2 − − 2 a b a + b = a 2 + b 2 a + b {\displaystyle {\begin{aligned}\operatorname {A} (a,b)&={{a+b} \over 2}\\\operatorname {H} (a,b)&={1 \over {{1 \over 2}\cdot {\left({1 \over a}+{1 \over b}\right)}}}={{2ab} \over {a+b}}\\\operatorname {C} (a,b)&=2\cdot A(a,b)-H(a,b)\\&=a+b-{{2ab} \over {a+b}}={{(a+b)^{2}-2ab} \over {a+b}}\\&={{a^{2}+b^{2}} \over {a+b}}\end{aligned}}} Notice that for two variables the average of the harmonic and contraharmonic means is exactly equal to the arithmetic mean: A ( H ( a , b ), C ( a , b )) = A ( a , b ) As a gets closer to 0 then H ( a , b ) also gets closer to 0.  The harmonic mean is very sensitive to low values.  On the other hand,  the contraharmonic mean is sensitive to larger values, so as a approaches 0 then C ( a , b ) approaches b (so their average remains A ( a , b )).

There are two other notable relationships between 2-variable means.  First, the geometric mean of the arithmetic and harmonic means is equal to the geometric mean of the two values: G ⁡ ⁡ ( A ⁡ ⁡ ( a , b ) , H ⁡ ⁡ ( a , b ) ) = G ⁡ ⁡ ( a + b 2 , 2 a b a + b ) = a + b 2 ⋅ ⋅ 2 a b a + b = a b = G ⁡ ⁡ ( a , b ) {\displaystyle \operatorname {G} (\operatorname {A} (a,b),\operatorname {H} (a,b))=\operatorname {G} \left({{a+b} \over 2},{{2ab} \over {a+b}}\right)={\sqrt {{{a+b} \over 2}\cdot {{2ab} \over {a+b}}}}={\sqrt {ab}}=\operatorname {G} (a,b)} The second relationship is that the geometric mean of the arithmetic and contraharmonic means is the root mean square: G ⁡ ⁡ ( A ⁡ ⁡ ( a , b ) , C ⁡ ⁡ ( a , b ) ) = G ⁡ ⁡ ( a + b 2 , a 2 + b 2 a + b ) = a + b 2 ⋅ ⋅ a 2 + b 2 a + b = a 2 + b 2 2 = R ⁡ ⁡ ( a , b ) {\displaystyle {\begin{aligned}&\operatorname {G} \left(\operatorname {A} (a,b),\operatorname {C} (a,b)\right)={}\operatorname {G} \left({{a+b} \over 2},{{a^{2}+b^{2}} \over {a+b}}\right)\\={}&{\sqrt {{{a+b} \over 2}\cdot {{a^{2}+b^{2}} \over {a+b}}}}={}{\sqrt {{a^{2}+b^{2}} \over 2}}\\[2pt]={}&\operatorname {R} (a,b)\end{aligned}}} The contraharmonic mean of two variables can be constructed geometrically using a trapezoid.

[ 3 ] Additional constructions [ edit ] The contraharmonic mean can be constructed on a circle similar to the way the Pythagorean means of two variables are constructed.

[ 4 ] The contraharmonic mean is the remainder of the diameter on which the harmonic mean lies.

[ 5 ] History [ edit ] The contraharmonic mean was discovered by the Greek mathematician Eudoxus in the 4th century BCE.

[ 6 ] Properties [ edit ] The contraharmonic mean satisfies characteristic properties of a mean of some list of positive values x {\textstyle \mathbf {x} } : min ( x ) ≤ ≤ C ⁡ ⁡ ( x ) ≤ ≤ max ( x ) {\displaystyle \min \left(\mathbf {x} \right)\leq \operatorname {C} \left(\mathbf {x} \right)\leq \max \left(\mathbf {x} \right)} C ⁡ ⁡ ( t ⋅ ⋅ x 1 , t ⋅ ⋅ x 2 , … … , t ⋅ ⋅ x n ) = t ⋅ ⋅ C ( x 1 , x 2 , … … , x n ) for t > 0 {\displaystyle \operatorname {C} \left(t\cdot \mathbf {x} _{1},t\cdot \mathbf {x} _{2},\,\dots ,\,t\cdot \mathbf {x} _{n}\right)=t\cdot C\left(\mathbf {x} _{1},\mathbf {x} _{2},\,\dots ,\,\mathbf {x} _{n}\right){\text{ for }}t>0} The first property implies the fixed point property , that for all k > 0, C ( k , k , ..., k ) = k It is not monotonic − increasing a value of x {\displaystyle \mathbf {x} } can decrease the value of the contraharmonic mean. For instance C(1, 4) > C(2, 4) .

The contraharmonic mean is higher in value than the arithmetic mean and also higher than the root mean square : min ( x ) ≤ ≤ H ⁡ ⁡ ( x ) ≤ ≤ G ⁡ ⁡ ( x ) ≤ ≤ L ⁡ ⁡ ( x ) ≤ ≤ A ⁡ ⁡ ( x ) ≤ ≤ R ⁡ ⁡ ( x ) ≤ ≤ C ⁡ ⁡ ( x ) ≤ ≤ max ( x ) {\displaystyle \min(\mathbf {x} )\leq \operatorname {H} (\mathbf {x} )\leq \operatorname {G} (\mathbf {x} )\leq \operatorname {L} (\mathbf {x} )\leq \operatorname {A} (\mathbf {x} )\leq \operatorname {R} (\mathbf {x} )\leq \operatorname {C} (\mathbf {x} )\leq \max(\mathbf {x} )} where x is a list of values, H is the harmonic mean, G is geometric mean , L is the logarithmic mean , A is the arithmetic mean , R is the root mean square and C is the contraharmonic mean.  Unless all values of x are the same, the ≤ signs above can be replaced by <.

The name contraharmonic may be due to the fact that when taking the mean of only two variables, the contraharmonic mean is as high above the arithmetic mean as the arithmetic mean is above the harmonic mean (i.e., the arithmetic mean of the two variables is equal to the arithmetic mean of their harmonic and contraharmonic means).

Relationship to arithmetic mean and variance [ edit ] The contraharmonic mean of a random variable is equal to the sum of the arithmetic mean and the variance divided by the arithmetic mean.

[ 7 ] C ⁡ ⁡ ( x ) = A ⁡ ⁡ ( x ) + Var ⁡ ⁡ ( x ) A ⁡ ⁡ ( x ) {\displaystyle \operatorname {C} (\mathbf {x} )=\operatorname {A} (\mathbf {x} )+{\frac {\operatorname {Var} (\mathbf {x} )}{\operatorname {A} (\mathbf {x} )}}} The ratio of the variance and the arithmetic mean was proposed as a test statistic by Clapham.

[ 8 ] Since the variance is always ≥0 the contraharmonic mean is always greater than or equal to the arithmetic mean.

Other relationships [ edit ] Any integer contraharmonic mean of two different positive integers is the hypotenuse of a Pythagorean triple , while any hypotenuse of a Pythagorean triple is a contraharmonic mean of two different positive integers.

[ 1 ] It is also related to Katz's statistic [ 9 ] J n = n 2 s 2 − − m m {\displaystyle J_{n}={\sqrt {\frac {n}{2}}}{\frac {s^{2}-m}{m}}} where m is the mean, s 2 the variance and n is the sample size.

J n is asymptotically normally distributed with a mean of zero and variance of 1.

Uses in statistics [ edit ] The problem of a size biased sample was discussed by Cox in 1969 on a problem of sampling fibres. The expectation of size biased sample is equal to its contraharmonic mean, [ 10 ] and the contraharmonic mean is also used to estimate bias fields in multiplicative models, rather than the arithmetic mean as used in additive models.

[ 11 ] The contraharmonic mean can be used to average the intensity value of neighbouring pixels in graphing, so as to reduce noise in images and make them clearer to the eye.

[ 12 ] The probability of a fibre being sampled is proportional to its length. Because of this the usual sample mean (arithmetic mean) is a biased estimator of the true mean. To see this consider g ( x ) = x f ( x ) m {\displaystyle g(x)={\frac {xf(x)}{m}}} where f ( x ) is the true population distribution, g ( x ) is the length weighted distribution and m is the sample mean. Taking the usual expectation of the mean here gives the contraharmonic mean rather than the usual (arithmetic) mean of the sample.

[ 13 ] This problem can be overcome by taking instead the expectation of the harmonic mean (1/ x ). The expectation and variance of 1/ x are E ⁡ ⁡ [ 1 x ] = 1 m {\displaystyle \operatorname {E} \left[{\frac {1}{x}}\right]={\frac {1}{m}}} and has variance Var ⁡ ⁡ ( 1 x ) = m E [ 1 x − − 1 ] n m 2 {\displaystyle \operatorname {Var} \left({\frac {1}{x}}\right)={\frac {mE\left[{\frac {1}{x}}-1\right]}{nm^{2}}}} where E is the expectation operator. Asymptotically E[1/ x ] is distributed normally.

The asymptotic efficiency of length biased sampling depends compared to random sampling on the underlying distribution. if f ( x ) is log normal the efficiency is 1 while if the population is gamma distributed with index b , the efficiency is b /( b − 1) . This distribution has been used in modelling consumer behaviour [ 14 ] as well as quality sampling.

It has been used longside the exponential distribution in transport planning in the form of its inverse.

[ 15 ] See also [ edit ] Harmonic mean Lehmer mean Pythagorean means References [ edit ] ^ a b Pahikkala, Jussi (2010).

"On contraharmonic mean and Pythagorean triples" .

Elemente der Mathematik .

65 (2): 62– 67.

doi : 10.4171/em/141 .

^ See "Means of Complex Numbers" (PDF) .

Texas College Mathematics Journal .

1 (1). January 1, 2005. Archived from the original (PDF) on September 9, 2006.

^ Umberger, Shannon.

"Construction of the Contraharmonic Mean in a Trapezoid" .

University of Georgia .

^ Nelsen, Roger B. (7 August 1997).

Proofs without Words/Exercises in Visual Thinking . Mathematical Association of America. p. 56.

ISBN 0-88385-700-6 .

^ Slaev, Valery A.; Chunovkina, Anna G.; Mironovsky, Leonid A. (2019).

Metrology and Theory of Measurement .

De Gruyter . p. 217.

ISBN 9783110652505 .

^ Antoine, C. (1998).

Les Moyennes . Paris: Presses Universitaires de France.

^ Kingley, Michael C.S. (1989). "The distribution of hauled out ringed seals an interpretation of Taylor's law".

Oecologia .

79 (79): 106– 110.

Bibcode : 1989Oecol..79..106K .

doi : 10.1007/BF00378246 .

PMID 28312819 .

^ Clapham, Arthur Roy (1936). "Overdispersion in grassland communities and the use of statistical methods in plant ecology".

The Journal of Ecology .

24 (14): 232– 251.

Bibcode : 1936JEcol..24..232C .

doi : 10.2307/2256277 .

JSTOR 2256277 .

^ Katz, L. (1965).

United treatment of a broad class of discrete probability distributions . Proceedings of the International Symposium on Discrete Distributions.

Montreal .

^ Zelen, Marvin (1972).

Length-biased sampling and biomedical problems . Biometric Society Meeting.

Dallas , Texas .

^ Banerjee, Abhirup; Maji, Pradipta (2013).

Rough Sets for Bias Field Correction in MR Images Using Contraharmonic Mean and Quantitative Index . IEEE Transactions on Medical Imaging.

^ Mitra, Sabry (October 2021). "Contraharmonic Mean Filter".

Kajian Ilmiah Informatika Dan Komputer .

2 (2): 75– 79.

^ Sudman, Seymour (1980).

Quota sampling techniques and weighting procedures to correct for frequency bias .

^ Keillor, Bruce D.; D'Amico, Michael; Horton, Veronica (2001). "Global Consumer Tendencies".

Psychology and Marketing .

18 (1): 1– 19.

doi : 10.1002/1520-6793(200101)18:1<1::AID-MAR1>3.0.CO;2-U .

^ Amreen, Mohammed; Venkateswarlu, Bandi (2024). "A New Way for Solving Transportation Issues Based on the Exponential Distribution and the Contraharmonic Mean".

Journal of Applied Mathematics and Informatics .

42 (3): 647– 661.

External links [ edit ] Contraharmonic Proportion at PlanetMath .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.eqiad.main‐5f99d748c‐snjtp
Cached time: 20250728155515
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.613 seconds
Real time usage: 0.750 seconds
Preprocessor visited node count: 2088/1000000
Revision size: 12495/2097152 bytes
Post‐expand include size: 167939/2097152 bytes
Template argument size: 1825/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 63517/5000000 bytes
Lua time usage: 0.342/10.000 seconds
Lua memory usage: 4475794/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  527.178      1 -total
 47.44%  250.086      1 Template:Reflist
 42.19%  222.403      1 Template:Statistics
 41.56%  219.119      1 Template:Navbox_with_collapsible_groups
 30.94%  163.127      7 Template:Cite_journal
 16.14%   85.097     11 Template:Navbox
  9.08%   47.868      1 Template:Hlist
  6.86%   36.177      2 Template:Block_indent
  6.00%   31.614      6 Template:Math
  5.04%   26.562      4 Template:Cite_book Saved in parser cache with key enwiki:pcache:8302382:|#|:idhash:canonical and timestamp 20250728155515 and revision id 1278228415. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Contraharmonic_mean&oldid=1278228415 " Category : Means This page was last edited on 1 March 2025, at 06:06 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Contraharmonic mean 1 language Add topic

