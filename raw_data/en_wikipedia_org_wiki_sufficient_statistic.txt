Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Background 2 Mathematical definition Toggle Mathematical definition subsection 2.1 Example 3 Fisher–Neyman factorization theorem Toggle Fisher–Neyman factorization theorem subsection 3.1 Likelihood principle interpretation 3.2 Proof 3.3 Another proof 4 Minimal sufficiency 5 Examples Toggle Examples subsection 5.1 Bernoulli distribution 5.2 Uniform distribution 5.3 Uniform distribution (with two parameters) 5.4 Poisson distribution 5.5 Normal distribution 5.6 Exponential distribution 5.7 Gamma distribution 6 Rao–Blackwell theorem 7 Exponential family 8 Other types of sufficiency Toggle Other types of sufficiency subsection 8.1 Bayesian sufficiency 8.2 Linear sufficiency 9 See also 10 Notes 11 References Toggle the table of contents Sufficient statistic 14 languages Deutsch Español فارسی Français 한국어 Italiano עברית Nederlands 日本語 Русский Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical principle In statistics , sufficiency is a property of a statistic computed on a sample dataset in relation to a parametric model of the dataset. A sufficient statistic contains all of the information that the dataset provides about the model parameters. It is closely related to the concepts of an ancillary statistic which contains no information about the model parameters, and of a complete statistic which only contains information about the parameters and no ancillary information.

A related concept is that of linear sufficiency , which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators.

[ 1 ] The Kolmogorov structure function deals with individual finite data; the related notion there is the algorithmic sufficient statistic.

The concept is due to Sir Ronald Fisher in 1920.

[ 2 ] Stephen Stigler noted in 1973 that the concept of sufficiency had fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see Pitman–Koopman–Darmois theorem below), but remained very important in theoretical work.

[ 3 ] Background [ edit ] Roughly, given a set X {\displaystyle \mathbf {X} } of independent identically distributed data conditioned on an unknown parameter θ θ {\displaystyle \theta } , a sufficient statistic is a function T ( X ) {\displaystyle T(\mathbf {X} )} whose value contains all the information needed to compute any estimate of the parameter (e.g. a maximum likelihood estimate).  Due to the factorization theorem ( see below ), for a sufficient statistic T ( X ) {\displaystyle T(\mathbf {X} )} , the probability density can be written as f X ( x ; θ θ ) = h ( x ) g ( θ θ , T ( x ) ) {\displaystyle f_{\mathbf {X} }(x;\theta )=h(x)\,g(\theta ,T(x))} .  From this factorization, it can easily be seen that the maximum likelihood estimate of θ θ {\displaystyle \theta } will interact with X {\displaystyle \mathbf {X} } only through T ( X ) {\displaystyle T(\mathbf {X} )} .  Typically, the sufficient statistic is a simple function of the data, e.g. the sum of all the data points.

More generally, the "unknown parameter" may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified.  In such a case, the sufficient statistic may be a set of functions, called a jointly sufficient statistic .  Typically, there are as many functions as there are parameters.  For example, for a Gaussian distribution with unknown mean and variance , the jointly sufficient statistic, from which maximum likelihood estimates of both parameters can be estimated, consists of two functions, the sum of all data points and the sum of all squared data points (or equivalently, the sample mean and sample variance ).

In other words, the joint probability distribution of the data is conditionally independent of the parameter given the value of the sufficient statistic for the parameter . Both the statistic and the underlying parameter can be vectors.

Mathematical definition [ edit ] A statistic t = T ( X ) is sufficient for underlying parameter θ precisely if the conditional probability distribution of the data X , given the statistic t = T ( X ), does not depend on the  parameter θ .

[ 4 ] Alternatively, one can say the statistic T ( X ) is sufficient for θ if, for all prior distributions on θ , the mutual information between θ and T(X) equals the mutual information between θ and X .

[ 5 ] In other words, the data processing inequality becomes an equality: I ( θ θ ; T ( X ) ) = I ( θ θ ; X ) {\displaystyle I{\bigl (}\theta ;T(X){\bigr )}=I(\theta ;X)} Example [ edit ] As an example, the sample mean is sufficient for the (unknown) mean μ of a normal distribution with known variance. Once the sample mean is known, no further information about μ can be obtained from the sample itself.  On the other hand, for an arbitrary distribution the median is not sufficient for the mean: even if the median of the sample is known, knowing the sample itself would provide further information about the population mean.  For example, if the observations that are less than the median are only slightly less, but observations exceeding the median exceed it by a large amount, then this would have a bearing on one's inference about the population mean.

Fisher–Neyman factorization theorem [ edit ] Fisher's factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic.  If the probability density function is ƒ θ ( x ), then T is sufficient for θ if and only if nonnegative functions g and h can be found such that f ( x ; θ θ ) = h ( x ) g ( θ θ , T ( x ) ) , {\displaystyle f(x;\theta )=h(x)\,g(\theta ,T(x)),} i.e., the density ƒ can be factored into a product such that one factor, h , does not depend on θ and the other factor, which does depend on θ , depends on x only through T ( x ). A general proof of this was given by Halmos and Savage [ 6 ] and the theorem is sometimes referred to as the Halmos–Savage factorization theorem.

[ 7 ] The proofs below handle special cases, but an alternative general proof along the same lines can be given.

[ 8 ] In many simple cases the probability density function is fully specified by θ θ {\displaystyle \theta } and T ( x ) {\displaystyle T(x)} , and h ( x ) = 1 {\displaystyle h(x)=1} (see Examples ).

It is easy to see that if F ( t ) is a one-to-one function and T is a sufficient
statistic, then F ( T ) is a sufficient statistic. In particular we can multiply a
sufficient statistic by a nonzero constant and get another sufficient statistic.

Likelihood principle interpretation [ edit ] An implication of the theorem is that when using likelihood-based inference, two sets of data yielding the same value for the sufficient statistic T ( X ) will always yield the same inferences about θ .  By the factorization criterion, the likelihood's dependence on θ is only in conjunction with T ( X ).  As this is the same in both cases, the dependence on θ will be the same as well, leading to identical inferences.

Proof [ edit ] Due to Hogg and Craig.

[ 9 ] Let X 1 , X 2 , … … , X n {\displaystyle X_{1},X_{2},\ldots ,X_{n}} ,  denote a random sample from a distribution having the pdf f ( x , θ ) for ι < θ < δ . Let Y 1 = u 1 ( X 1 , X 2 , ..., X n ) be a statistic whose pdf is g 1 ( y 1 ; θ ). What we want to prove is that Y 1 = u 1 ( X 1 , X 2 , ..., X n ) is a sufficient statistic for θ if and only if, for some function H , ∏ ∏ i = 1 n f ( x i ; θ θ ) = g 1 [ u 1 ( x 1 , x 2 , … … , x n ) ; θ θ ] H ( x 1 , x 2 , … … , x n ) .

{\displaystyle \prod _{i=1}^{n}f(x_{i};\theta )=g_{1}\left[u_{1}(x_{1},x_{2},\dots ,x_{n});\theta \right]H(x_{1},x_{2},\dots ,x_{n}).} First, suppose that ∏ ∏ i = 1 n f ( x i ; θ θ ) = g 1 [ u 1 ( x 1 , x 2 , … … , x n ) ; θ θ ] H ( x 1 , x 2 , … … , x n ) .

{\displaystyle \prod _{i=1}^{n}f(x_{i};\theta )=g_{1}\left[u_{1}(x_{1},x_{2},\dots ,x_{n});\theta \right]H(x_{1},x_{2},\dots ,x_{n}).} We shall make the transformation y i = u i ( x 1 , x 2 , ..., x n ), for i = 1, ..., n , having inverse functions x i = w i ( y 1 , y 2 , ..., y n ), for i = 1, ..., n , and Jacobian J = [ w i / y j ] {\displaystyle J=\left[w_{i}/y_{j}\right]} . Thus, ∏ ∏ i = 1 n f [ w i ( y 1 , y 2 , … … , y n ) ; θ θ ] = | J | g 1 ( y 1 ; θ θ ) H [ w 1 ( y 1 , y 2 , … … , y n ) , … … , w n ( y 1 , y 2 , … … , y n ) ] .

{\displaystyle \prod _{i=1}^{n}f\left[w_{i}(y_{1},y_{2},\dots ,y_{n});\theta \right]=|J|g_{1}(y_{1};\theta )H\left[w_{1}(y_{1},y_{2},\dots ,y_{n}),\dots ,w_{n}(y_{1},y_{2},\dots ,y_{n})\right].} The left-hand member is the joint pdf g ( y 1 , y 2 , ..., y n ; θ) of Y 1 = u 1 ( X 1 , ..., X n ), ..., Y n = u n ( X 1 , ..., X n ).  In the right-hand member, g 1 ( y 1 ; θ θ ) {\displaystyle g_{1}(y_{1};\theta )} is the pdf of Y 1 {\displaystyle Y_{1}} , so that H [ w 1 , … … , w n ] | J | {\displaystyle H[w_{1},\dots ,w_{n}]|J|} is the quotient of g ( y 1 , … … , y n ; θ θ ) {\displaystyle g(y_{1},\dots ,y_{n};\theta )} and g 1 ( y 1 ; θ θ ) {\displaystyle g_{1}(y_{1};\theta )} ; that is, it is the conditional pdf h ( y 2 , … … , y n ∣ ∣ y 1 ; θ θ ) {\displaystyle h(y_{2},\dots ,y_{n}\mid y_{1};\theta )} of Y 2 , … … , Y n {\displaystyle Y_{2},\dots ,Y_{n}} given Y 1 = y 1 {\displaystyle Y_{1}=y_{1}} .

But H ( x 1 , x 2 , … … , x n ) {\displaystyle H(x_{1},x_{2},\dots ,x_{n})} , and thus H [ w 1 ( y 1 , … … , y n ) , … … , w n ( y 1 , … … , y n ) ) ] {\displaystyle H\left[w_{1}(y_{1},\dots ,y_{n}),\dots ,w_{n}(y_{1},\dots ,y_{n}))\right]} , was given not to depend upon θ θ {\displaystyle \theta } . Since θ θ {\displaystyle \theta } was not introduced in the transformation and accordingly not in the Jacobian J {\displaystyle J} , it follows that h ( y 2 , … … , y n ∣ ∣ y 1 ; θ θ ) {\displaystyle h(y_{2},\dots ,y_{n}\mid y_{1};\theta )} does not depend upon θ θ {\displaystyle \theta } and that Y 1 {\displaystyle Y_{1}} is a sufficient statistics for θ θ {\displaystyle \theta } .

The converse is proven by taking: g ( y 1 , … … , y n ; θ θ ) = g 1 ( y 1 ; θ θ ) h ( y 2 , … … , y n ∣ ∣ y 1 ) , {\displaystyle g(y_{1},\dots ,y_{n};\theta )=g_{1}(y_{1};\theta )h(y_{2},\dots ,y_{n}\mid y_{1}),} where h ( y 2 , … … , y n ∣ ∣ y 1 ) {\displaystyle h(y_{2},\dots ,y_{n}\mid y_{1})} does not depend upon θ θ {\displaystyle \theta } because Y 2 .

.

.

Y n {\displaystyle Y_{2}...Y_{n}} depend only upon X 1 .

.

.

X n {\displaystyle X_{1}...X_{n}} , which are independent on Θ Θ {\displaystyle \Theta } when conditioned by Y 1 {\displaystyle Y_{1}} , a sufficient statistics by hypothesis. Now divide both members by the absolute value of the non-vanishing Jacobian J {\displaystyle J} , and replace y 1 , … … , y n {\displaystyle y_{1},\dots ,y_{n}} by the functions u 1 ( x 1 , … … , x n ) , … … , u n ( x 1 , … … , x n ) {\displaystyle u_{1}(x_{1},\dots ,x_{n}),\dots ,u_{n}(x_{1},\dots ,x_{n})} in x 1 , … … , x n {\displaystyle x_{1},\dots ,x_{n}} . This yields g [ u 1 ( x 1 , … … , x n ) , … … , u n ( x 1 , … … , x n ) ; θ θ ] | J ∗ ∗ | = g 1 [ u 1 ( x 1 , … … , x n ) ; θ θ ] h ( u 2 , … … , u n ∣ ∣ u 1 ) | J ∗ ∗ | {\displaystyle {\frac {g\left[u_{1}(x_{1},\dots ,x_{n}),\dots ,u_{n}(x_{1},\dots ,x_{n});\theta \right]}{|J^{*}|}}=g_{1}\left[u_{1}(x_{1},\dots ,x_{n});\theta \right]{\frac {h(u_{2},\dots ,u_{n}\mid u_{1})}{|J^{*}|}}} where J ∗ ∗ {\displaystyle J^{*}} is the Jacobian with y 1 , … … , y n {\displaystyle y_{1},\dots ,y_{n}} replaced by their value in terms x 1 , … … , x n {\displaystyle x_{1},\dots ,x_{n}} . The left-hand member is necessarily the joint pdf f ( x 1 ; θ θ ) ⋯ ⋯ f ( x n ; θ θ ) {\displaystyle f(x_{1};\theta )\cdots f(x_{n};\theta )} of X 1 , … … , X n {\displaystyle X_{1},\dots ,X_{n}} . Since h ( y 2 , … … , y n ∣ ∣ y 1 ) {\displaystyle h(y_{2},\dots ,y_{n}\mid y_{1})} , and thus h ( u 2 , … … , u n ∣ ∣ u 1 ) {\displaystyle h(u_{2},\dots ,u_{n}\mid u_{1})} , does not depend upon θ θ {\displaystyle \theta } , then H ( x 1 , … … , x n ) = h ( u 2 , … … , u n ∣ ∣ u 1 ) | J ∗ ∗ | {\displaystyle H(x_{1},\dots ,x_{n})={\frac {h(u_{2},\dots ,u_{n}\mid u_{1})}{|J^{*}|}}} is a function that does not depend upon θ θ {\displaystyle \theta } .

Another proof [ edit ] A simpler more illustrative  proof is as follows, although it applies only in the discrete case.

We use the shorthand notation to denote the joint probability density of ( X , T ( X ) ) {\displaystyle (X,T(X))} by f θ θ ( x , t ) {\displaystyle f_{\theta }(x,t)} . Since T {\displaystyle T} is a deterministic function of X {\displaystyle X} , we have f θ θ ( x , t ) = f θ θ ( x ) {\displaystyle f_{\theta }(x,t)=f_{\theta }(x)} , as long as t = T ( x ) {\displaystyle t=T(x)} and zero otherwise. Therefore: f θ θ ( x ) = f θ θ ( x , t ) = f θ θ ( x ∣ ∣ t ) f θ θ ( t ) = f ( x ∣ ∣ t ) f θ θ ( t ) {\displaystyle {\begin{aligned}f_{\theta }(x)&=f_{\theta }(x,t)\\[5pt]&=f_{\theta }(x\mid t)f_{\theta }(t)\\[5pt]&=f(x\mid t)f_{\theta }(t)\end{aligned}}} with the last equality being true by the definition of sufficient statistics. Thus f θ θ ( x ) = a ( x ) b θ θ ( t ) {\displaystyle f_{\theta }(x)=a(x)b_{\theta }(t)} with a ( x ) = f X ∣ ∣ t ( x ) {\displaystyle a(x)=f_{X\mid t}(x)} and b θ θ ( t ) = f θ θ ( t ) {\displaystyle b_{\theta }(t)=f_{\theta }(t)} .

Conversely, if f θ θ ( x ) = a ( x ) b θ θ ( t ) {\displaystyle f_{\theta }(x)=a(x)b_{\theta }(t)} , we have f θ θ ( t ) = ∑ ∑ x : T ( x ) = t f θ θ ( x , t ) = ∑ ∑ x : T ( x ) = t f θ θ ( x ) = ∑ ∑ x : T ( x ) = t a ( x ) b θ θ ( t ) = ( ∑ ∑ x : T ( x ) = t a ( x ) ) b θ θ ( t ) .

{\displaystyle {\begin{aligned}f_{\theta }(t)&=\sum _{x:T(x)=t}f_{\theta }(x,t)\\[5pt]&=\sum _{x:T(x)=t}f_{\theta }(x)\\[5pt]&=\sum _{x:T(x)=t}a(x)b_{\theta }(t)\\[5pt]&=\left(\sum _{x:T(x)=t}a(x)\right)b_{\theta }(t).\end{aligned}}} With the first equality by the definition of pdf for multiple variables , the second by the remark above, the third by hypothesis, and the fourth because the summation is not over t {\displaystyle t} .

Let f X ∣ ∣ t ( x ) {\displaystyle f_{X\mid t}(x)} denote the conditional probability density of X {\displaystyle X} given T ( X ) {\displaystyle T(X)} . Then we can derive an explicit expression for this: f X ∣ ∣ t ( x ) = f θ θ ( x , t ) f θ θ ( t ) = f θ θ ( x ) f θ θ ( t ) = a ( x ) b θ θ ( t ) ( ∑ ∑ x : T ( x ) = t a ( x ) ) b θ θ ( t ) = a ( x ) ∑ ∑ x : T ( x ) = t a ( x ) .

{\displaystyle {\begin{aligned}f_{X\mid t}(x)&={\frac {f_{\theta }(x,t)}{f_{\theta }(t)}}\\[5pt]&={\frac {f_{\theta }(x)}{f_{\theta }(t)}}\\[5pt]&={\frac {a(x)b_{\theta }(t)}{\left(\sum _{x:T(x)=t}a(x)\right)b_{\theta }(t)}}\\[5pt]&={\frac {a(x)}{\sum _{x:T(x)=t}a(x)}}.\end{aligned}}} With the first equality by definition of conditional probability density, the second by the remark above, the third by the equality proven above, and the fourth by simplification. This expression does not depend on θ θ {\displaystyle \theta } and thus T {\displaystyle T} is a sufficient statistic.

[ 10 ] Minimal sufficiency [ edit ] A sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic. In other words, S ( X ) is minimal sufficient if and only if [ 11 ] S ( X ) is sufficient, and if T ( X ) is sufficient, then there exists a function f such that S ( X ) = f ( T ( X )).

Intuitively, a minimal sufficient statistic most efficiently captures all possible information about the parameter θ .

A useful characterization of minimal sufficiency is that when the density f θ exists, S ( X ) is minimal sufficient if f θ θ ( x ) f θ θ ( y ) {\displaystyle {\frac {f_{\theta }(x)}{f_{\theta }(y)}}} is independent of θ : ⟺ ⟺ {\displaystyle \Longleftrightarrow } S ( x ) = S ( y ) This follows as a consequence from Fisher's factorization theorem stated above.

A case in which there is no minimal sufficient statistic was shown by Bahadur, 1954.

[ 12 ] However, under mild conditions, a minimal sufficient statistic does always exist. In particular, in Euclidean space, these conditions always hold if the random variables (associated with P θ θ {\displaystyle P_{\theta }} ) are all discrete or are all continuous.

If there exists a minimal sufficient statistic, and this is usually the case, then every complete sufficient statistic is necessarily minimal sufficient [ 13 ] (note that this statement does not exclude a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic). While it is hard to find cases in which a minimal sufficient statistic does not exist, it is not so hard to find cases in which there is no complete statistic.

The collection of likelihood ratios { L ( X ∣ ∣ θ θ i ) L ( X ∣ ∣ θ θ 0 ) } {\displaystyle \left\{{\frac {L(X\mid \theta _{i})}{L(X\mid \theta _{0})}}\right\}} for i = 1 , .

.

.

, k {\displaystyle i=1,...,k} , is a minimal sufficient statistic if the parameter space is discrete { θ θ 0 , .

.

.

, θ θ k } {\displaystyle \left\{\theta _{0},...,\theta _{k}\right\}} .

Examples [ edit ] Bernoulli distribution [ edit ] If X 1 , ...., X n are independent Bernoulli-distributed random variables with expected value p , then the sum T ( X ) = X 1 + ... + X n is a sufficient statistic for p (here 'success' corresponds to X i = 1 and 'failure' to X i = 0; so T is the total number of successes) This is seen by considering the joint probability distribution: Pr { X = x } = Pr { X 1 = x 1 , X 2 = x 2 , … … , X n = x n } .

{\displaystyle \Pr\{X=x\}=\Pr\{X_{1}=x_{1},X_{2}=x_{2},\ldots ,X_{n}=x_{n}\}.} Because the observations are independent, this can be written as p x 1 ( 1 − − p ) 1 − − x 1 p x 2 ( 1 − − p ) 1 − − x 2 ⋯ ⋯ p x n ( 1 − − p ) 1 − − x n {\displaystyle p^{x_{1}}(1-p)^{1-x_{1}}p^{x_{2}}(1-p)^{1-x_{2}}\cdots p^{x_{n}}(1-p)^{1-x_{n}}} and, collecting powers of p and 1 − p ,  gives p ∑ ∑ x i ( 1 − − p ) n − − ∑ ∑ x i = p T ( x ) ( 1 − − p ) n − − T ( x ) {\displaystyle p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}=p^{T(x)}(1-p)^{n-T(x)}} which satisfies the factorization criterion, with h ( x ) = 1 being just a constant.

Note the crucial feature: the unknown parameter p interacts with the data x only via the statistic T ( x ) = Σ x i .

As a concrete application, this gives a procedure for distinguishing a fair coin from a biased coin .

Uniform distribution [ edit ] See also: German tank problem If X 1 , ...., X n are independent and uniformly distributed on the interval [0, θ ], then T ( X ) = max( X 1 , ..., X n ) is sufficient for θ — the sample maximum is a sufficient statistic for the population maximum.

To see this, consider the joint probability density function of X ( X 1 ,..., X n ). Because the observations are independent, the pdf can be written as a product of individual densities f θ θ ( x 1 , … … , x n ) = 1 θ θ 1 { 0 ≤ ≤ x 1 ≤ ≤ θ θ } ⋯ ⋯ 1 θ θ 1 { 0 ≤ ≤ x n ≤ ≤ θ θ } = 1 θ θ n 1 { 0 ≤ ≤ min { x i } } 1 { max { x i } ≤ ≤ θ θ } {\displaystyle {\begin{aligned}f_{\theta }(x_{1},\ldots ,x_{n})&={\frac {1}{\theta }}\mathbf {1} _{\{0\leq x_{1}\leq \theta \}}\cdots {\frac {1}{\theta }}\mathbf {1} _{\{0\leq x_{n}\leq \theta \}}\\[5pt]&={\frac {1}{\theta ^{n}}}\mathbf {1} _{\{0\leq \min\{x_{i}\}\}}\mathbf {1} _{\{\max\{x_{i}\}\leq \theta \}}\end{aligned}}} where 1 { ...

} is the indicator function .  Thus the density takes form required by the Fisher–Neyman factorization theorem, where h ( x ) = 1 {min{ x i }≥0} , and the rest of the expression is a function of only θ and T ( x ) = max{ x i }.

In fact, the minimum-variance unbiased estimator (MVUE) for θ is n + 1 n T ( X ) .

{\displaystyle {\frac {n+1}{n}}T(X).} This is the sample maximum, scaled to correct for the bias , and is MVUE by the Lehmann–Scheffé theorem . Unscaled sample maximum T ( X ) is the maximum likelihood estimator for θ .

Uniform distribution (with two parameters) [ edit ] If X 1 , .

.

.

, X n {\displaystyle X_{1},...,X_{n}} are independent and uniformly distributed on the interval [ α α , β β ] {\displaystyle [\alpha ,\beta ]} (where α α {\displaystyle \alpha } and β β {\displaystyle \beta } are unknown parameters), then T ( X 1 n ) = ( min 1 ≤ ≤ i ≤ ≤ n X i , max 1 ≤ ≤ i ≤ ≤ n X i ) {\displaystyle T(X_{1}^{n})=\left(\min _{1\leq i\leq n}X_{i},\max _{1\leq i\leq n}X_{i}\right)} is a two-dimensional sufficient statistic for ( α α , β β ) {\displaystyle (\alpha \,,\,\beta )} .

To see this, consider the joint probability density function of X 1 n = ( X 1 , … … , X n ) {\displaystyle X_{1}^{n}=(X_{1},\ldots ,X_{n})} . Because the observations are independent, the pdf can be written as a product of individual densities, i.e.

f X 1 n ( x 1 n ) = ∏ ∏ i = 1 n ( 1 β β − − α α ) 1 { α α ≤ ≤ x i ≤ ≤ β β } = ( 1 β β − − α α ) n 1 { α α ≤ ≤ x i ≤ ≤ β β , ∀ ∀ i = 1 , … … , n } = ( 1 β β − − α α ) n 1 { α α ≤ ≤ min 1 ≤ ≤ i ≤ ≤ n X i } 1 { max 1 ≤ ≤ i ≤ ≤ n X i ≤ ≤ β β } .

{\displaystyle {\begin{aligned}f_{X_{1}^{n}}(x_{1}^{n})&=\prod _{i=1}^{n}\left({1 \over \beta -\alpha }\right)\mathbf {1} _{\{\alpha \leq x_{i}\leq \beta \}}=\left({1 \over \beta -\alpha }\right)^{n}\mathbf {1} _{\{\alpha \leq x_{i}\leq \beta ,\,\forall \,i=1,\ldots ,n\}}\\&=\left({1 \over \beta -\alpha }\right)^{n}\mathbf {1} _{\{\alpha \,\leq \,\min _{1\leq i\leq n}X_{i}\}}\mathbf {1} _{\{\max _{1\leq i\leq n}X_{i}\,\leq \,\beta \}}.\end{aligned}}} The joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting h ( x 1 n ) = 1 , g ( α α , β β ) ( x 1 n ) = ( 1 β β − − α α ) n 1 { α α ≤ ≤ min 1 ≤ ≤ i ≤ ≤ n X i } 1 { max 1 ≤ ≤ i ≤ ≤ n X i ≤ ≤ β β } .

{\displaystyle {\begin{aligned}h(x_{1}^{n})=1,\quad g_{(\alpha ,\beta )}(x_{1}^{n})=\left({1 \over \beta -\alpha }\right)^{n}\mathbf {1} _{\{\alpha \,\leq \,\min _{1\leq i\leq n}X_{i}\}}\mathbf {1} _{\{\max _{1\leq i\leq n}X_{i}\,\leq \,\beta \}}.\end{aligned}}} Since h ( x 1 n ) {\displaystyle h(x_{1}^{n})} does not depend on the parameter ( α α , β β ) {\displaystyle (\alpha ,\beta )} and g ( α α , β β ) ( x 1 n ) {\displaystyle g_{(\alpha \,,\,\beta )}(x_{1}^{n})} depends only on x 1 n {\displaystyle x_{1}^{n}} through the function T ( X 1 n ) = ( min 1 ≤ ≤ i ≤ ≤ n X i , max 1 ≤ ≤ i ≤ ≤ n X i ) , {\displaystyle T(X_{1}^{n})=\left(\min _{1\leq i\leq n}X_{i},\max _{1\leq i\leq n}X_{i}\right),} the Fisher–Neyman factorization theorem implies T ( X 1 n ) = ( min 1 ≤ ≤ i ≤ ≤ n X i , max 1 ≤ ≤ i ≤ ≤ n X i ) {\displaystyle T(X_{1}^{n})=\left(\min _{1\leq i\leq n}X_{i},\max _{1\leq i\leq n}X_{i}\right)} is a sufficient statistic for ( α α , β β ) {\displaystyle (\alpha \,,\,\beta )} .

Poisson distribution [ edit ] If X 1 , ...., X n are independent and have a Poisson distribution with parameter λ , then the sum T ( X ) = X 1 + ... + X n is a sufficient statistic for λ .

To see this, consider the joint probability distribution: Pr ( X = x ) = P ( X 1 = x 1 , X 2 = x 2 , … … , X n = x n ) .

{\displaystyle \Pr(X=x)=P(X_{1}=x_{1},X_{2}=x_{2},\ldots ,X_{n}=x_{n}).} Because the observations are independent, this can be written as e − − λ λ λ λ x 1 x 1 !

⋅ ⋅ e − − λ λ λ λ x 2 x 2 !

⋯ ⋯ e − − λ λ λ λ x n x n !

{\displaystyle {e^{-\lambda }\lambda ^{x_{1}} \over x_{1}!}\cdot {e^{-\lambda }\lambda ^{x_{2}} \over x_{2}!}\cdots {e^{-\lambda }\lambda ^{x_{n}} \over x_{n}!}} which may be written as e − − n λ λ λ λ ( x 1 + x 2 + ⋯ ⋯ + x n ) ⋅ ⋅ 1 x 1 !

x 2 !

⋯ ⋯ x n !

{\displaystyle e^{-n\lambda }\lambda ^{(x_{1}+x_{2}+\cdots +x_{n})}\cdot {1 \over x_{1}!x_{2}!\cdots x_{n}!}} which shows that the factorization criterion is satisfied, where h ( x ) is the reciprocal of the product of the factorials.  Note the parameter λ interacts with the data only through its sum T ( X ).

Normal distribution [ edit ] If X 1 , … … , X n {\displaystyle X_{1},\ldots ,X_{n}} are independent and normally distributed with expected value θ θ {\displaystyle \theta } (a parameter) and known finite variance σ σ 2 , {\displaystyle \sigma ^{2},} then T ( X 1 n ) = x ¯ ¯ = 1 n ∑ ∑ i = 1 n X i {\displaystyle T(X_{1}^{n})={\overline {x}}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}} is a sufficient statistic for θ θ .

{\displaystyle \theta .} To see this, consider the joint probability density function of X 1 n = ( X 1 , … … , X n ) {\displaystyle X_{1}^{n}=(X_{1},\dots ,X_{n})} . Because the observations are independent, the pdf can be written as a product of individual densities, i.e.

f X 1 n ( x 1 n ) = ∏ ∏ i = 1 n 1 2 π π σ σ 2 exp ⁡ ⁡ ( − − ( x i − − θ θ ) 2 2 σ σ 2 ) = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − ∑ ∑ i = 1 n ( x i − − θ θ ) 2 2 σ σ 2 ) = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − ∑ ∑ i = 1 n ( ( x i − − x ¯ ¯ ) − − ( θ θ − − x ¯ ¯ ) ) 2 2 σ σ 2 ) = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − 1 2 σ σ 2 ( ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 + ∑ ∑ i = 1 n ( θ θ − − x ¯ ¯ ) 2 − − 2 ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) ( θ θ − − x ¯ ¯ ) ) ) = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − 1 2 σ σ 2 ( ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 + n ( θ θ − − x ¯ ¯ ) 2 ) ) ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) ( θ θ − − x ¯ ¯ ) = 0 = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − 1 2 σ σ 2 ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 ) exp ⁡ ⁡ ( − − n 2 σ σ 2 ( θ θ − − x ¯ ¯ ) 2 ) {\displaystyle {\begin{aligned}f_{X_{1}^{n}}(x_{1}^{n})&=\prod _{i=1}^{n}{\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left(-{\frac {(x_{i}-\theta )^{2}}{2\sigma ^{2}}}\right)\\[6pt]&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-\sum _{i=1}^{n}{\frac {(x_{i}-\theta )^{2}}{2\sigma ^{2}}}\right)\\[6pt]&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-\sum _{i=1}^{n}{\frac {\left(\left(x_{i}-{\overline {x}}\right)-\left(\theta -{\overline {x}}\right)\right)^{2}}{2\sigma ^{2}}}\right)\\[6pt]&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-{1 \over 2\sigma ^{2}}\left(\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}+\sum _{i=1}^{n}(\theta -{\overline {x}})^{2}-2\sum _{i=1}^{n}(x_{i}-{\overline {x}})(\theta -{\overline {x}})\right)\right)\\[6pt]&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-{1 \over 2\sigma ^{2}}\left(\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}+n(\theta -{\overline {x}})^{2}\right)\right)&&\sum _{i=1}^{n}(x_{i}-{\overline {x}})(\theta -{\overline {x}})=0\\[6pt]&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-{1 \over 2\sigma ^{2}}\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}\right)\exp \left(-{\frac {n}{2\sigma ^{2}}}(\theta -{\overline {x}})^{2}\right)\end{aligned}}} The joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting h ( x 1 n ) = ( 2 π π σ σ 2 ) − − n 2 exp ⁡ ⁡ ( − − 1 2 σ σ 2 ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 ) g θ θ ( x 1 n ) = exp ⁡ ⁡ ( − − n 2 σ σ 2 ( θ θ − − x ¯ ¯ ) 2 ) {\displaystyle {\begin{aligned}h(x_{1}^{n})&=(2\pi \sigma ^{2})^{-{\frac {n}{2}}}\exp \left(-{1 \over 2\sigma ^{2}}\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}\right)\\[6pt]g_{\theta }(x_{1}^{n})&=\exp \left(-{\frac {n}{2\sigma ^{2}}}(\theta -{\overline {x}})^{2}\right)\end{aligned}}} Since h ( x 1 n ) {\displaystyle h(x_{1}^{n})} does not depend on the parameter θ θ {\displaystyle \theta } and g θ θ ( x 1 n ) {\displaystyle g_{\theta }(x_{1}^{n})} depends only on x 1 n {\displaystyle x_{1}^{n}} through the function T ( X 1 n ) = x ¯ ¯ = 1 n ∑ ∑ i = 1 n X i , {\displaystyle T(X_{1}^{n})={\overline {x}}={\frac {1}{n}}\sum _{i=1}^{n}X_{i},} the Fisher–Neyman factorization theorem implies T ( X 1 n ) {\displaystyle T(X_{1}^{n})} is a sufficient statistic for θ θ {\displaystyle \theta } .

If σ σ 2 {\displaystyle \sigma ^{2}} is unknown and since s 2 = 1 n − − 1 ∑ ∑ i = 1 n ( x i − − x ¯ ¯ ) 2 {\displaystyle s^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}\left(x_{i}-{\overline {x}}\right)^{2}} ,  the above likelihood can be rewritten as f X 1 n ( x 1 n ) = ( 2 π π σ σ 2 ) − − n / 2 exp ⁡ ⁡ ( − − n − − 1 2 σ σ 2 s 2 ) exp ⁡ ⁡ ( − − n 2 σ σ 2 ( θ θ − − x ¯ ¯ ) 2 ) .

{\displaystyle {\begin{aligned}f_{X_{1}^{n}}(x_{1}^{n})=(2\pi \sigma ^{2})^{-n/2}\exp \left(-{\frac {n-1}{2\sigma ^{2}}}s^{2}\right)\exp \left(-{\frac {n}{2\sigma ^{2}}}(\theta -{\overline {x}})^{2}\right).\end{aligned}}} The Fisher–Neyman factorization theorem still holds and implies that ( x ¯ ¯ , s 2 ) {\displaystyle ({\overline {x}},s^{2})} is a joint sufficient statistic for ( θ θ , σ σ 2 ) {\displaystyle (\theta ,\sigma ^{2})} .

Exponential distribution [ edit ] If X 1 , … … , X n {\displaystyle X_{1},\dots ,X_{n}} are independent and exponentially distributed with expected value θ (an unknown real-valued positive parameter), then T ( X 1 n ) = ∑ ∑ i = 1 n X i {\displaystyle T(X_{1}^{n})=\sum _{i=1}^{n}X_{i}} is a sufficient statistic for θ.

To see this, consider the joint probability density function of X 1 n = ( X 1 , … … , X n ) {\displaystyle X_{1}^{n}=(X_{1},\dots ,X_{n})} . Because the observations are independent, the pdf can be written as a product of individual densities, i.e.

f X 1 n ( x 1 n ) = ∏ ∏ i = 1 n 1 θ θ e − − 1 θ θ x i = 1 θ θ n e − − 1 θ θ ∑ ∑ i = 1 n x i .

{\displaystyle {\begin{aligned}f_{X_{1}^{n}}(x_{1}^{n})&=\prod _{i=1}^{n}{1 \over \theta }\,e^{{-1 \over \theta }x_{i}}={1 \over \theta ^{n}}\,e^{{-1 \over \theta }\sum _{i=1}^{n}x_{i}}.\end{aligned}}} The joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting h ( x 1 n ) = 1 , g θ θ ( x 1 n ) = 1 θ θ n e − − 1 θ θ ∑ ∑ i = 1 n x i .

{\displaystyle {\begin{aligned}h(x_{1}^{n})=1,\,\,\,g_{\theta }(x_{1}^{n})={1 \over \theta ^{n}}\,e^{{-1 \over \theta }\sum _{i=1}^{n}x_{i}}.\end{aligned}}} Since h ( x 1 n ) {\displaystyle h(x_{1}^{n})} does not depend on the parameter θ θ {\displaystyle \theta } and g θ θ ( x 1 n ) {\displaystyle g_{\theta }(x_{1}^{n})} depends only on x 1 n {\displaystyle x_{1}^{n}} through the function T ( X 1 n ) = ∑ ∑ i = 1 n X i {\displaystyle T(X_{1}^{n})=\sum _{i=1}^{n}X_{i}} the Fisher–Neyman factorization theorem implies T ( X 1 n ) = ∑ ∑ i = 1 n X i {\displaystyle T(X_{1}^{n})=\sum _{i=1}^{n}X_{i}} is a sufficient statistic for θ θ {\displaystyle \theta } .

Gamma distribution [ edit ] If X 1 , … … , X n {\displaystyle X_{1},\dots ,X_{n}} are independent and distributed as a Γ Γ ( α α , β β ) {\displaystyle \Gamma (\alpha \,,\,\beta )} , where α α {\displaystyle \alpha } and β β {\displaystyle \beta } are unknown parameters of a Gamma distribution , then T ( X 1 n ) = ( ∏ ∏ i = 1 n X i , ∑ ∑ i = 1 n X i ) {\displaystyle T(X_{1}^{n})=\left(\prod _{i=1}^{n}{X_{i}},\sum _{i=1}^{n}X_{i}\right)} is a two-dimensional sufficient statistic for ( α α , β β ) {\displaystyle (\alpha ,\beta )} .

To see this, consider the joint probability density function of X 1 n = ( X 1 , … … , X n ) {\displaystyle X_{1}^{n}=(X_{1},\dots ,X_{n})} . Because the observations are independent, the pdf can be written as a product of individual densities, i.e.

f X 1 n ( x 1 n ) = ∏ ∏ i = 1 n ( 1 Γ Γ ( α α ) β β α α ) x i α α − − 1 e ( − − 1 / β β ) x i = ( 1 Γ Γ ( α α ) β β α α ) n ( ∏ ∏ i = 1 n x i ) α α − − 1 e − − 1 β β ∑ ∑ i = 1 n x i .

{\displaystyle {\begin{aligned}f_{X_{1}^{n}}(x_{1}^{n})&=\prod _{i=1}^{n}\left({1 \over \Gamma (\alpha )\beta ^{\alpha }}\right)x_{i}^{\alpha -1}e^{(-1/\beta )x_{i}}\\[5pt]&=\left({1 \over \Gamma (\alpha )\beta ^{\alpha }}\right)^{n}\left(\prod _{i=1}^{n}x_{i}\right)^{\alpha -1}e^{{-1 \over \beta }\sum _{i=1}^{n}x_{i}}.\end{aligned}}} The joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting h ( x 1 n ) = 1 , g ( α α , β β ) ( x 1 n ) = ( 1 Γ Γ ( α α ) β β α α ) n ( ∏ ∏ i = 1 n x i ) α α − − 1 e − − 1 β β ∑ ∑ i = 1 n x i .

{\displaystyle {\begin{aligned}h(x_{1}^{n})=1,\,\,\,g_{(\alpha \,,\,\beta )}(x_{1}^{n})=\left({1 \over \Gamma (\alpha )\beta ^{\alpha }}\right)^{n}\left(\prod _{i=1}^{n}x_{i}\right)^{\alpha -1}e^{{-1 \over \beta }\sum _{i=1}^{n}x_{i}}.\end{aligned}}} Since h ( x 1 n ) {\displaystyle h(x_{1}^{n})} does not depend on the parameter ( α α , β β ) {\displaystyle (\alpha \,,\,\beta )} and g ( α α , β β ) ( x 1 n ) {\displaystyle g_{(\alpha \,,\,\beta )}(x_{1}^{n})} depends only on x 1 n {\displaystyle x_{1}^{n}} through the function T ( x 1 n ) = ( ∏ ∏ i = 1 n x i , ∑ ∑ i = 1 n x i ) , {\displaystyle T(x_{1}^{n})=\left(\prod _{i=1}^{n}x_{i},\sum _{i=1}^{n}x_{i}\right),} the Fisher–Neyman factorization theorem implies T ( X 1 n ) = ( ∏ ∏ i = 1 n X i , ∑ ∑ i = 1 n X i ) {\displaystyle T(X_{1}^{n})=\left(\prod _{i=1}^{n}X_{i},\sum _{i=1}^{n}X_{i}\right)} is a sufficient statistic for ( α α , β β ) .

{\displaystyle (\alpha \,,\,\beta ).} Rao–Blackwell theorem [ edit ] Sufficiency finds a useful application in the Rao–Blackwell theorem , which  states that if g ( X ) is any kind of estimator of θ , then typically the conditional expectation of g ( X ) given sufficient statistic T ( X ) is a better (in the sense of having lower variance ) estimator of θ , and is never worse.  Sometimes one can very easily construct a very crude estimator g ( X ), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.

Exponential family [ edit ] Main article: Exponential family According to the Pitman–Koopman–Darmois theorem, among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases. Intuitively, this states that nonexponential families of distributions on the real line require nonparametric statistics to fully capture the information in the data.

Less tersely, suppose X n , n = 1 , 2 , 3 , … … {\displaystyle X_{n},n=1,2,3,\dots } are independent identically distributed real random variables whose distribution is known to be in some family of probability distributions, parametrized by θ θ {\displaystyle \theta } , satisfying certain technical regularity conditions, then that family is an exponential family if and only if there is a R m {\displaystyle \mathbb {R} ^{m}} -valued sufficient statistic T ( X 1 , … … , X n ) {\displaystyle T(X_{1},\dots ,X_{n})} whose number of scalar components m {\displaystyle m} does not increase as the sample size n increases.

[ 14 ] This theorem shows that the existence of a finite-dimensional, real-vector-valued sufficient statistics sharply restricts the possible forms of a family of distributions on the real line .

When the parameters or the random variables are no longer real-valued, the situation is more complex.

[ 15 ] Other types of sufficiency [ edit ] Bayesian sufficiency [ edit ] An alternative formulation of the condition that a statistic be sufficient, set in a Bayesian context, involves the posterior distributions obtained by using the full data-set and by using only a statistic. Thus the requirement is that, for almost every x , Pr ( θ θ ∣ ∣ X = x ) = Pr ( θ θ ∣ ∣ T ( X ) = t ( x ) ) .

{\displaystyle \Pr(\theta \mid X=x)=\Pr(\theta \mid T(X)=t(x)).} More generally, without assuming a parametric model, we can say that the statistics T is predictive sufficient if Pr ( X ′ = x ′ ∣ ∣ X = x ) = Pr ( X ′ = x ′ ∣ ∣ T ( X ) = t ( x ) ) .

{\displaystyle \Pr(X'=x'\mid X=x)=\Pr(X'=x'\mid T(X)=t(x)).} It turns out that this "Bayesian sufficiency" is a consequence of the formulation above, [ 16 ] however they are not directly equivalent in the infinite-dimensional case.

[ 17 ] A range of theoretical results for sufficiency in a Bayesian context is available.

[ 18 ] Linear sufficiency [ edit ] A concept called "linear sufficiency" can be formulated in a Bayesian context, [ 19 ] and more generally.

[ 20 ] First define the best linear predictor of a vector Y based on X as E ^ ^ [ Y ∣ ∣ X ] {\displaystyle {\hat {E}}[Y\mid X]} . Then a linear statistic T ( x ) is linear sufficient [ 21 ] if E ^ ^ [ θ θ ∣ ∣ X ] = E ^ ^ [ θ θ ∣ ∣ T ( X ) ] .

{\displaystyle {\hat {E}}[\theta \mid X]={\hat {E}}[\theta \mid T(X)].} See also [ edit ] Completeness of a statistic Basu's theorem on independence of complete sufficient and ancillary statistics Lehmann–Scheffé theorem : a complete sufficient estimator is the best estimator of its expectation Rao–Blackwell theorem Chentsov's theorem Sufficient dimension reduction Ancillary statistic Notes [ edit ] ^ Dodge, Y. (2003) — entry for linear sufficiency ^ Fisher, R.A.

(1922).

"On the mathematical foundations of theoretical statistics" .

Philosophical Transactions of the Royal Society A .

222 ( 594– 604): 309– 368.

Bibcode : 1922RSPTA.222..309F .

doi : 10.1098/rsta.1922.0009 .

hdl : 2440/15172 .

JFM 48.1280.02 .

JSTOR 91208 .

^ Stigler, Stephen (December 1973). "Studies in the History of Probability and Statistics. XXXII: Laplace, Fisher and the Discovery of the Concept of Sufficiency".

Biometrika .

60 (3): 439– 445.

doi : 10.1093/biomet/60.3.439 .

JSTOR 2334992 .

MR 0326872 .

^ Casella, George; Berger, Roger L. (2002).

Statistical Inference, 2nd ed . Duxbury Press.

^ Cover, Thomas M. (2006).

Elements of Information Theory . Joy A. Thomas (2nd ed.). Hoboken, N.J.: Wiley-Interscience. p. 36.

ISBN 0-471-24195-4 .

OCLC 59879802 .

^ Halmos, P. R.; Savage, L. J. (1949).

"Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics" .

The Annals of Mathematical Statistics .

20 (2): 225– 241.

doi : 10.1214/aoms/1177730032 .

ISSN 0003-4851 .

^ "Factorization theorem - Encyclopedia of Mathematics" .

encyclopediaofmath.org . Retrieved 2022-09-07 .

^ Taraldsen, G. (2022). "The Factorization Theorem for Sufficiency".

Preprint .

doi : 10.13140/RG.2.2.15068.87687 .

^ Hogg, Robert V.; Craig, Allen T. (1995).

Introduction to Mathematical Statistics . Prentice Hall.

ISBN 978-0-02-355722-4 .

^ "The Fisher–Neyman Factorization Theorem" .

. Webpage at Connexions (cnx.org) ^ Dodge (2003) — entry for minimal sufficient statistics ^ Lehmann and Casella (1998), Theory of Point Estimation , 2nd Edition, Springer, p 37 ^ Lehmann and Casella (1998), Theory of Point Estimation , 2nd Edition, Springer, page 42 ^ Tikochinsky, Y.; Tishby, N. Z.; Levine, R. D. (1984-11-01).

"Alternative approach to maximum-entropy inference" .

Physical Review A .

30 (5): 2638– 2644.

Bibcode : 1984PhRvA..30.2638T .

doi : 10.1103/physreva.30.2638 .

ISSN 0556-2791 .

^ Andersen, Erling Bernhard (September 1970).

"Sufficiency and Exponential Families for Discrete Sample Spaces" .

Journal of the American Statistical Association .

65 (331): 1248– 1255.

doi : 10.1080/01621459.1970.10481160 .

ISSN 0162-1459 .

^ Bernardo, J.M.

; Smith, A.F.M.

(1994). "Section 5.1.4".

Bayesian Theory . Wiley.

ISBN 0-471-92416-4 .

^ Blackwell, D.

; Ramamoorthi, R. V. (1982).

"A Bayes but not classically sufficient statistic" .

Annals of Statistics .

10 (3): 1025– 1026.

doi : 10.1214/aos/1176345895 .

MR 0663456 .

Zbl 0485.62004 .

^ Nogales, A.G.; Oyola, J.A.; Perez, P. (2000).

"On conditional independence and the relationship between sufficiency and invariance under the Bayesian point of view" .

Statistics & Probability Letters .

46 (1): 75– 84.

doi : 10.1016/S0167-7152(99)00089-9 .

MR 1731351 .

Zbl 0964.62003 .

^ Goldstein, M.; O'Hagan, A. (1996). "Bayes Linear Sufficiency and Systems of Expert Posterior Assessments".

Journal of the Royal Statistical Society . Series B.

58 (2): 301– 316.

doi : 10.1111/j.2517-6161.1996.tb02083.x .

JSTOR 2345978 .

^ Godambe, V. P. (1966). "A New Approach to Sampling from Finite Populations. II Distribution-Free Sufficiency".

Journal of the Royal Statistical Society . Series B.

28 (2): 320– 328.

doi : 10.1111/j.2517-6161.1966.tb00645.x .

JSTOR 2984375 .

^ Witting, T. (1987).

"The linear Markov property in credibility theory" .

ASTIN Bulletin .

17 (1): 71– 84.

doi : 10.2143/ast.17.1.2014984 .

hdl : 20.500.11850/422507 .

References [ edit ] Kholevo, A.S. (2001) [1994], "Sufficient statistic" , Encyclopedia of Mathematics , EMS Press Lehmann, E. L.; Casella, G. (1998).

Theory of Point Estimation (2nd ed.). Springer. Chapter 4.

ISBN 0-387-98502-6 .

Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP.

ISBN 0-19-920613-9 v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐dndrc
Cached time: 20250812002707
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.669 seconds
Real time usage: 0.886 seconds
Preprocessor visited node count: 3168/1000000
Revision size: 36345/2097152 bytes
Post‐expand include size: 184417/2097152 bytes
Template argument size: 1494/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 89330/5000000 bytes
Lua time usage: 0.324/10.000 seconds
Lua memory usage: 6068882/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  512.143      1 -total
 39.82%  203.916      1 Template:Reflist
 30.00%  153.649      1 Template:Statistics
 29.58%  151.512      1 Template:Navbox_with_collapsible_groups
 28.65%  146.730     11 Template:Cite_journal
 18.65%   95.499      1 Template:Short_description
 12.23%   62.615      2 Template:Pagetype
 10.11%   51.800     11 Template:Navbox
  6.17%   31.604      1 Template:Hlist
  5.66%   29.004      5 Template:Cite_book Saved in parser cache with key enwiki:pcache:140841:|#|:idhash:canonical and timestamp 20250812002707 and revision id 1297010506. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Sufficient_statistic&oldid=1297010506 " Categories : Statistical theory Statistical principles Factorization Hidden categories: Articles with short description Short description is different from Wikidata Articles containing proofs This page was last edited on 23 June 2025, at 17:16 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Sufficient statistic 14 languages Add topic

