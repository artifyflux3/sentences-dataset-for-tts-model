Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Types of means Toggle Types of means subsection 1.1 Pythagorean means 1.1.1 Arithmetic mean (AM) 1.1.2 Geometric mean (GM) 1.1.3 Harmonic mean (HM) 1.1.4 Relationship between AM, GM, and HM 1.2 Statistical location 1.2.1 Mean of a probability distribution 1.3 Generalized means 1.3.1 Power mean 1.3.2 Quasi-arithmetic mean 1.4 Weighted arithmetic mean 1.5 Truncated mean 1.6 Interquartile mean 1.7 Mean of a function 1.8 Mean of angles and cyclical quantities 1.9 Fréchet mean 1.10 Triangular sets 1.11 Swanson's rule 1.12 Other means 2 See also 3 Notes 4 References Toggle the table of contents Mean 48 languages Afrikaans العربية تۆرکجه Беларуская Bosanski Català Чӑвашла Cymraeg Dansk Deutsch Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Íslenska Italiano ລາວ Latina Lietuvių Magyar Bahasa Melayu Nederlands Norsk bokmål Norsk nynorsk پښتو Română Русский සිංහල Simple English Slovenčina کوردی Српски / srpski Sunda Svenska தமிழ் ไทย Українська Tiếng Việt 文言 粵語 中文 Kadazandusun Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Numeric quantity representing the center of a collection of numbers This article is about quantifying the concept of "typical value". For other uses, see Mean (disambiguation) .

For broader coverage of this topic, see Average .

For the state of being mean or cruel, see Meanness .

A mean is a quantity representing the "center" of a collection of numbers and is intermediate to the extreme values of the set of numbers.

[ 1 ] There are several kinds of means (or "measures of central tendency ") in mathematics , especially in statistics . Each attempts to summarize or typify a given group of data , illustrating the magnitude and sign of the data set . Which of these measures is most illuminating depends on what is being measured, and on context and purpose.

[ 2 ] The arithmetic mean , also known as "arithmetic average", is the sum of the values divided by the number of values. The arithmetic mean of a set of numbers x 1 , x 2 , ..., x n is typically denoted using an overhead bar , x ¯ ¯ {\displaystyle {\bar {x}}} .

[ note 1 ] If the numbers are from observing a sample of a larger group , the arithmetic mean is termed the sample mean ( x ¯ ¯ {\displaystyle {\bar {x}}} ) to distinguish it from the group mean (or expected value ) of the underlying distribution, denoted μ μ {\displaystyle \mu } or μ μ x {\displaystyle \mu _{x}} .

[ note 2 ] [ 3 ] Outside probability and statistics, a wide range of other notions of mean are often used in geometry and mathematical analysis ; examples are given below.

Types of means [ edit ] Pythagorean means [ edit ] Main article: Pythagorean means In mathematics, the three classical Pythagorean means are the arithmetic mean (AM), the geometric mean (GM), and the harmonic mean (HM). These means were studied with proportions by Pythagoreans and later generations of Greek mathematicians [ 4 ] because of their importance in geometry and music.

Arithmetic mean (AM) [ edit ] Main article: Arithmetic mean The arithmetic mean (or simply mean or average ) of a list of numbers, is the sum of all of the numbers divided by their count. Similarly, the mean of a sample x 1 , x 2 , … … , x n {\displaystyle x_{1},x_{2},\ldots ,x_{n}} , usually denoted by x ¯ ¯ {\displaystyle {\bar {x}}} , is the sum of the sampled values divided by the number of items in the sample.

x ¯ ¯ = 1 n ∑ ∑ i = 1 n x i = x 1 + x 2 + ⋯ ⋯ + x n n {\displaystyle {\bar {x}}={\frac {1}{n}}\sum _{i=1}^{n}{x_{i}}={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}} For example, the arithmetic mean of five values: 4, 36, 45, 50, 75 is: 4 + 36 + 45 + 50 + 75 5 = 210 5 = 42.

{\displaystyle {\frac {4+36+45+50+75}{5}}={\frac {210}{5}}=42.} Geometric mean (GM) [ edit ] The geometric mean is an average that is useful for sets of positive numbers, that are interpreted according to their product (as is the case with rates of growth) and not their sum (as is the case with the arithmetic mean): [ 1 ] x ¯ ¯ = ( ∏ ∏ i = 1 n x i ) 1 n = ( x 1 x 2 ⋯ ⋯ x n ) 1 n {\displaystyle {\bar {x}}=\left(\prod _{i=1}^{n}{x_{i}}\right)^{\frac {1}{n}}=\left(x_{1}x_{2}\cdots x_{n}\right)^{\frac {1}{n}}} For example, the geometric mean of five values: 4, 36, 45, 50, 75 is: ( 4 × × 36 × × 45 × × 50 × × 75 ) 1 5 = 24 300 000 5 = 30.

{\displaystyle (4\times 36\times 45\times 50\times 75)^{\frac {1}{5}}={\sqrt[{5}]{24\;300\;000}}=30.} Harmonic mean (HM) [ edit ] The harmonic mean is an average which is useful for sets of numbers which are defined in relation to some unit , as in the case of speed (i.e., distance per unit of time): x ¯ ¯ = n ( ∑ ∑ i = 1 n 1 x i ) − − 1 {\displaystyle {\bar {x}}=n\left(\sum _{i=1}^{n}{\frac {1}{x_{i}}}\right)^{-1}} For example, the harmonic mean of the five values: 4, 36, 45, 50, 75 is 5 1 4 + 1 36 + 1 45 + 1 50 + 1 75 = 5 1 3 = 15.

{\displaystyle {\frac {5}{{\tfrac {1}{4}}+{\tfrac {1}{36}}+{\tfrac {1}{45}}+{\tfrac {1}{50}}+{\tfrac {1}{75}}}}={\frac {5}{\;{\tfrac {1}{3}}\;}}=15.} If we have five pumps that can empty a tank of a certain size in respectively 4, 36, 45, 50, and 75 minutes, then the harmonic mean of 15 {\displaystyle 15} tells us that these five different pumps working together will pump at the same rate as five pumps that can each empty the tank in 15 {\displaystyle 15} minutes.

Relationship between AM, GM, and HM [ edit ] Proof without words of the AM–GM inequality : PR is the diameter of a circle centered on O; its radius AO is the arithmetic mean of a and b . Triangle PGR is a right triangle from Thales's theorem , enabling use of the geometric mean theorem to show that its altitude GQ is the geometric mean . For any ratio a : b , AO ≥ GQ.

Main article: QM-AM-GM-HM inequalities AM, GM, and HM of nonnegative real numbers satisfy these inequalities: [ 5 ] A M ≥ ≥ G M ≥ ≥ H M {\displaystyle \mathrm {AM} \geq \mathrm {GM} \geq \mathrm {HM} \,} Equality holds if all the elements of the given sample are equal.

Statistical location [ edit ] See also: Average § Statistical location Comparison of the arithmetic mean , median , and mode of two skewed ( log-normal ) distributions Geometric visualization of the mode, median and mean of an arbitrary probability density function [ 6 ] In descriptive statistics , the mean may be confused with the median , mode or mid-range , as any of these may incorrectly be called an "average" (more formally, a measure of central tendency ). The mean of a set of observations is the arithmetic average of the values; however, for skewed distributions , the mean is not necessarily the same as the middle value (median), or the most likely value (mode). For example, mean income is typically skewed upwards by a small number of people with very large incomes, so that the majority have an income lower than the mean. By contrast, the median income is the level at which half the population is below and half is above. The mode income is the most likely income and favors the larger number of people with lower incomes. While the median and mode are often more intuitive measures for such skewed data, many skewed distributions are in fact best described by their mean, including the exponential and Poisson distributions.

Mean of a probability distribution [ edit ] Main article: Expected value See also: Population mean The mean of a probability distribution is the long-run arithmetic average value of a random variable having that distribution. If the random variable is denoted by X {\displaystyle X} , then the mean is also known as the expected value of X {\displaystyle X} (denoted E ( X ) {\displaystyle E(X)} ). For a discrete probability distribution , the mean is given by ∑ ∑ x P ( x ) {\displaystyle \textstyle \sum xP(x)} , where the sum is taken over all possible values of the random variable and P ( x ) {\displaystyle P(x)} is the probability mass function . For a continuous distribution , the mean is ∫ ∫ − − ∞ ∞ ∞ ∞ x f ( x ) d x {\displaystyle \textstyle \int _{-\infty }^{\infty }xf(x)\,dx} , where f ( x ) {\displaystyle f(x)} is the probability density function .

[ 7 ] In all cases, including those in which the distribution is neither discrete nor continuous, the mean is the Lebesgue integral of the random variable with respect to its probability measure . The mean need not exist or be finite; for some probability distributions the mean is infinite ( +∞ or −∞ ), while for others the mean is undefined .

Generalized means [ edit ] Power mean [ edit ] Main article: Generalized mean The generalized mean, also known as the power mean or Hölder mean, abstracts several other means. It is defined for positive numbers x 1 , … … , x n {\displaystyle x_{1},\dots ,x_{n}} by [ 1 ] M p ( x 1 , … … , x n ) = ( 1 n ∑ ∑ i = 1 n x i p ) 1 / p .

{\displaystyle M_{p}(x_{1},\dots ,x_{n})=\left({\frac {1}{n}}\sum _{i=1}^{n}x_{i}^{p}\right)^{1/p}.} This, as a function of p {\displaystyle p} , is well defined on R ∖ ∖ { 0 } {\displaystyle \mathbb {R} \setminus \{0\}} , but can be extended continuously to R ∪ ∪ { − − ∞ ∞ , + ∞ ∞ } {\displaystyle \mathbb {R} \cup \{-\infty ,+\infty \}} .

[ 8 ] By choosing different values for m {\displaystyle m} , other well know means are retrieved.

Name Exponent Value Minimum p = − − ∞ ∞ {\displaystyle p=-\infty } min { x 1 , … … , x n } {\displaystyle \min\{x_{1},\dots ,x_{n}\}} Harmonic mean p = − − 1 {\displaystyle p=-1} n 1 x 1 + ⋯ ⋯ + 1 x n {\displaystyle {\frac {n}{{\frac {1}{x_{1}}}+\dots +{\frac {1}{x_{n}}}}}} Geometric mean p = 0 {\displaystyle p=0} x 1 … … x n n {\displaystyle {\sqrt[{n}]{x_{1}\dots x_{n}}}} Arithmetic mean p = 1 {\displaystyle p=1} x 1 + ⋯ ⋯ + x n n {\displaystyle {\frac {x_{1}+\dots +x_{n}}{n}}} Root mean square p = 2 {\displaystyle p=2} x 1 2 + ⋯ ⋯ + x n 2 n {\displaystyle {\sqrt {\frac {x_{1}^{2}+\dots +x_{n}^{2}}{n}}}} Cubic mean p = 3 {\displaystyle p=3} x 1 3 + ⋯ ⋯ + x n 3 n 3 {\displaystyle {\sqrt[{3}]{\frac {x_{1}^{3}+\dots +x_{n}^{3}}{n}}}} Maximum p = + ∞ ∞ {\displaystyle p=+\infty } max { x 1 , … … , x n } {\displaystyle \max\{x_{1},\dots ,x_{n}\}} Quasi-arithmetic mean [ edit ] Main article: Quasi-arithmetic mean A similar approach to the power mean is the f {\displaystyle f} -mean, also known as the quasi-arithmetic mean.
For an injective function f : : I → → R {\displaystyle f\colon I\rightarrow \mathbb {R} } on an interval I ⊂ ⊂ R {\displaystyle I\subset \mathbb {R} } and real numbers x 1 , … … , x n ∈ ∈ I {\displaystyle x_{1},\dots ,x_{n}\in I} we define their f {\displaystyle f} -mean as M f ( x 1 , … … , x n ) = f − − 1 ( 1 n ∑ ∑ i = 1 n f ( x i ) ) .

{\displaystyle M_{f}(x_{1},\dots ,x_{n})=f^{-1}\left({{\frac {1}{n}}\sum _{i=1}^{n}{f\left(x_{i}\right)}}\right).} By choosing different functions f {\displaystyle f} , other well know means are retrieved.

Mean I {\displaystyle I} Function [ note 3 ] Arithmetic mean R {\displaystyle \mathbb {R} } x ↦ ↦ x {\displaystyle x\mapsto x} Geometric mean ] 0 , + ∞ ∞ [ {\displaystyle ]0,+\infty [} [ note 4 ] x ↦ ↦ ln ⁡ ⁡ ( x ) {\displaystyle x\mapsto \ln(x)} Harmonic mean R ∖ ∖ { 0 } {\displaystyle \mathbb {R} \setminus \{0\}} x ↦ ↦ x − − 1 {\displaystyle x\mapsto x^{-1}} Power mean R ∖ ∖ { 0 } {\displaystyle \mathbb {R} \setminus \{0\}} [ note 5 ] x ↦ ↦ x m {\displaystyle x\mapsto x^{m}} Weighted arithmetic mean [ edit ] The weighted arithmetic mean (or weighted average) is used if one wants to combine average values from different sized samples of the same population, and is define by [ 1 ] x ¯ ¯ = ∑ ∑ i = 1 n w i x i ∑ ∑ i = 1 n w i , {\displaystyle {\bar {x}}={\frac {\sum _{i=1}^{n}{w_{i}x_{i}}}{\sum _{i=1}^{n}w_{i}}},} where x i {\displaystyle x_{i}} and w i {\displaystyle w_{i}} are the mean and size of sample i {\displaystyle i} respectively. In other applications, they represent a measure for the reliability of the influence upon the mean by the respective values.

Truncated mean [ edit ] Sometimes, a set of numbers might contain outliers (i.e., data values which are much lower or much higher than the others). Often, outliers are erroneous data caused by artifacts . In this case, one can use a truncated mean . It involves discarding given parts of the data at the top or the bottom end, typically an equal amount at each end and then taking the arithmetic mean of the remaining data. The number of values removed is indicated as a percentage of the total number of values.

Interquartile mean [ edit ] The interquartile mean is a specific example of a truncated mean. It is simply the arithmetic mean after removing the lowest and the highest quarter of values.

x ¯ ¯ = 2 n ∑ ∑ i = n 4 + 1 3 4 n x i {\displaystyle {\bar {x}}={\frac {2}{n}}\;\sum _{i={\frac {n}{4}}+1}^{{\frac {3}{4}}n}\!\!x_{i}} assuming the values have been ordered, so is simply a specific example of a weighted mean for a specific set of weights.

Mean of a function [ edit ] Main article: Mean of a function In some circumstances, mathematicians may calculate a mean of an infinite (or even an uncountable ) set of values. This can happen when calculating the mean value y avg {\displaystyle y_{\text{avg}}} of a function f ( x ) {\displaystyle f(x)} . Intuitively, a mean of a function can be thought of as calculating the area under a section of a curve, and then dividing by the length of that section. This can be done crudely by counting squares on graph paper, or more precisely by integration . The integration formula is written as: y avg ( a , b ) = 1 b − − a ∫ ∫ a b f ( x ) d x .

{\displaystyle y_{\text{avg}}(a,b)={\frac {1}{b-a}}\int _{a}^{b}f(x)\,dx.} In this case, care must be taken to make sure that the integral converges. But the mean may be finite even if the function itself tends to infinity at some points.

Mean of angles and cyclical quantities [ edit ] Angles , times of day, and other cyclical quantities require modular arithmetic to add and otherwise combine numbers. These quantities can be averaged using the circular mean . In all these situations, it is possible that no mean exists, for example if all points being averaged are equidistant. Consider a color wheel —there is no mean to the set of all colors. Additionally, there may not be a unique mean for a set of values: for example, when averaging points on a clock, the mean of the locations of 11:00 and 13:00 is 12:00, but this location is equivalent to that of 00:00.

Fréchet mean [ edit ] The Fréchet mean gives a manner for determining the "center" of a mass distribution on a surface or, more generally, Riemannian manifold . Unlike many other means, the Fréchet mean is defined on a space whose elements cannot necessarily be added together or multiplied by scalars.
It is sometimes also known as the Karcher mean (named after Hermann Karcher).

Triangular sets [ edit ] In geometry, there are thousands of different
definitions for the center of a triangle that can all be interpreted as the mean of a triangular set of points in the plane.

[ 9 ] Swanson's rule [ edit ] This is an approximation to the mean for a moderately skewed distribution.

[ 10 ] It is used in hydrocarbon exploration and is defined as: m = 0.3 P 10 + 0.4 P 50 + 0.3 P 90 {\displaystyle m=0.3P_{10}+0.4P_{50}+0.3P_{90}} where P 10 {\textstyle P_{10}} , P 50 {\textstyle P_{50}} and P 90 {\textstyle P_{90}} are the 10th, 50th and 90th percentiles of the distribution, respectively.

Other means [ edit ] Main category: Means Arithmetic-geometric mean Arithmetic-harmonic mean Cesàro mean Chisini mean Contraharmonic mean Elementary symmetric mean Geometric-harmonic mean Grand mean Heinz mean Heronian mean Identric mean Lehmer mean Logarithmic mean Moving average Neuman–Sándor mean Quasi-arithmetic mean Root mean square (quadratic mean) Rényi's entropy (a generalized f-mean ) Spherical mean Stolarsky mean Weighted geometric mean Weighted harmonic mean See also [ edit ] Mathematics portal Statistical dispersion Central tendency Median Mode Descriptive statistics Kurtosis Law of averages Mean value theorem Moment (mathematics) Summary statistics Taylor's law Notes [ edit ] ^ Pronounced " x bar".

^ Greek letter μ , pronounced /'mjuː/.

^ For this column we will use the "mapping arrow" to denote a function. Under this notation, the function f {\displaystyle f} is denoted by x ↦ ↦ f ( x ) {\displaystyle x\mapsto f(x)} .

^ The geometric mean is well defined on [ 0 , + ∞ ∞ [ {\displaystyle [0,+\infty [} , but this is not captured by this approach.

^ For m ≠ ≠ 0 {\displaystyle m\neq 0} the domain can be R {\displaystyle \mathbb {R} } .

References [ edit ] ^ a b c d "Mean | mathematics" .

Encyclopedia Britannica . Retrieved 2020-08-21 .

^ Why Few Math Students Actually Understand the Meaning of Means (YouTube video). Math The World. 2024-08-27 . Retrieved 2024-09-10 .

^ Underhill, L.G.; Bradfield d. (1998) Introstat , Juta and Company Ltd.

ISBN 0-7021-3838-X p. 181 ^ Heath, Thomas.

History of Ancient Greek Mathematics .

^ Djukić, Dušan; Janković, Vladimir; Matić, Ivan; Petrović, Nikola (2011-05-05).

The IMO Compendium: A Collection of Problems Suggested for The International Mathematical Olympiads: 1959-2009 Second Edition . Springer Science & Business Media.

ISBN 978-1-4419-9854-5 .

^ "AP Statistics Review - Density Curves and the Normal Distributions" . Archived from the original on 2 April 2015 . Retrieved 16 March 2015 .

^ Weisstein, Eric W.

"Population Mean" .

mathworld.wolfram.com . Retrieved 2020-08-21 .

^ P. S. Bullen: Handbook of Means and Their Inequalities . Dordrecht, Netherlands: Kluwer, 2003, pp. 176.

^ Narboux, Julien; Braun, David (2016).

"Towards a certified version of the encyclopedia of triangle centers" (PDF) .

Mathematics in Computer Science .

10 (1): 57– 73.

doi : 10.1007/s11786-016-0254-4 .

MR 3483261 .

under the guidance of Clark Kimberling, an electronic encyclopedia of triangle centers (ETC) has been developed, it contains more than 7000 centers and many properties of these points ^ Hurst A, Brown GC, Swanson  RI (2000) Swanson's 30-40-30 Rule.  American Association of Petroleum Geologists Bulletin 84(12) 1883-1891 v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Authority control databases National Germany United States Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐vwvbc
Cached time: 20250811235038
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.483 seconds
Real time usage: 0.777 seconds
Preprocessor visited node count: 2994/1000000
Revision size: 17633/2097152 bytes
Post‐expand include size: 168690/2097152 bytes
Template argument size: 2752/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 13/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 56348/5000000 bytes
Lua time usage: 0.241/10.000 seconds
Lua memory usage: 7324911/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  429.009      1 -total
 28.15%  120.783      1 Template:Statistics
 27.69%  118.786      1 Template:Navbox_with_collapsible_groups
 26.14%  112.131      2 Template:Reflist
 16.19%   69.448      3 Template:Cite_web
 13.55%   58.112      1 Template:Short_description
 10.86%   46.594     11 Template:Navbox
  8.55%   36.679      2 Template:Pagetype
  7.51%   32.221      1 Template:Hatnote_group
  6.95%   29.809      1 Template:Authority_control Saved in parser cache with key enwiki:pcache:19192:|#|:idhash:canonical and timestamp 20250811235038 and revision id 1304339925. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Mean&oldid=1304339925 " Categories : Means Moments (mathematics) Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 5 August 2025, at 12:31 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Mean 48 languages Add topic

