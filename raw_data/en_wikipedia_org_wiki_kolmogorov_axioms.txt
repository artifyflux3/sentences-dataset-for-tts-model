Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Kolmogorov axioms Toggle Kolmogorov axioms subsection 1.1 First axiom 1.2 Second axiom 1.3 Third axiom 2 Consequences Toggle Consequences subsection 2.1 Monotonicity 2.1.1 Proof of monotonicity 2.2 The probability of the empty set 2.2.1 Proof of the probability of the empty set 2.3 The complement rule 2.3.1 Proof of the complement rule 2.4 The numeric bound 2.4.1 Proof of the numeric bound 3 Further consequences 4 Simple example: coin toss 5 See also 6 References 7 Further reading Toggle the table of contents Probability axioms 35 languages Afrikaans العربية Беларуская Català Чӑвашла Čeština Cymraeg Español Euskara فارسی Français Galego 한국어 Hrvatski Bahasa Indonesia Íslenska Italiano עברית ქართული Nederlands 日本語 Polski Português Română Русский Slovenščina Српски / srpski Sunda Suomi Svenska ไทย Türkçe Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Kolmogorov axioms ) Foundations of probability theory Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e The standard probability axioms are the foundations of probability theory introduced by Russian mathematician Andrey Kolmogorov in 1933.

[ 1 ] These axioms remain central and have direct contributions to mathematics, the physical sciences, and real-world probability cases.

[ 2 ] There are several other (equivalent) approaches to formalising probability.

Bayesians will often motivate the Kolmogorov axioms by invoking Cox's theorem or the Dutch book arguments instead.

[ 3 ] [ 4 ] Kolmogorov axioms [ edit ] The assumptions as to setting up the axioms can be summarised as follows: Let ( Ω Ω , F , P ) {\displaystyle (\Omega ,F,P)} be a measure space such that P ( E ) {\displaystyle P(E)} is the probability of some event E {\displaystyle E} , and P ( Ω Ω ) = 1 {\displaystyle P(\Omega )=1} . Then ( Ω Ω , F , P ) {\displaystyle (\Omega ,F,P)} is a probability space , with sample space Ω Ω {\displaystyle \Omega } , event space F {\displaystyle F} and probability measure P {\displaystyle P} .

[ 1 ] First axiom [ edit ] The probability of an event is a non-negative real number: P ( E ) ∈ ∈ R , P ( E ) ≥ ≥ 0 ∀ ∀ E ∈ ∈ F {\displaystyle P(E)\in \mathbb {R} ,P(E)\geq 0\qquad \forall E\in F} where F {\displaystyle F} is the event space. It follows (when combined with the second axiom) that P ( E ) {\displaystyle P(E)} is always finite, in contrast with more general measure theory . Theories which assign negative probability relax the first axiom.

Second axiom [ edit ] This is the assumption of unit measure : that the probability that at least one of the elementary events in the entire sample space will occur is 1.

P ( Ω Ω ) = 1 {\displaystyle P(\Omega )=1} Third axiom [ edit ] This is the assumption of σ-additivity : Any countable sequence of disjoint sets (synonymous with mutually exclusive events) E 1 , E 2 , … … {\displaystyle E_{1},E_{2},\ldots } satisfies P ( ⋃ ⋃ i = 1 ∞ ∞ E i ) = ∑ ∑ i = 1 ∞ ∞ P ( E i ) .

{\displaystyle P\left(\bigcup _{i=1}^{\infty }E_{i}\right)=\sum _{i=1}^{\infty }P(E_{i}).} Some authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets , rather than a σ-algebra .

[ 5 ] Quasiprobability distributions in general relax the third axiom.

Consequences [ edit ] From the Kolmogorov axioms, one can deduce other useful rules for studying probabilities. The proofs [ 6 ] [ 7 ] [ 8 ] of these rules are a very insightful procedure that illustrates the power of the third axiom, and its interaction with the prior two axioms. Four of the immediate corollaries and their proofs are shown below: Monotonicity [ edit ] if A ⊆ ⊆ B then P ( A ) ≤ ≤ P ( B ) .

{\displaystyle \quad {\text{if}}\quad A\subseteq B\quad {\text{then}}\quad P(A)\leq P(B).} If A is a subset of, or equal to B, then the probability of A is less than, or equal to the probability of B.

Proof of monotonicity [ edit ] Source: [ 6 ] In order to verify the monotonicity property, we set E 1 = A {\displaystyle E_{1}=A} and E 2 = B ∖ ∖ A {\displaystyle E_{2}=B\setminus A} , where A ⊆ ⊆ B {\displaystyle A\subseteq B} and E i = ∅ ∅ {\displaystyle E_{i}=\varnothing } for i ≥ ≥ 3 {\displaystyle i\geq 3} . From the properties of the empty set ( ∅ ∅ {\displaystyle \varnothing } ), it is easy to see that the sets E i {\displaystyle E_{i}} are pairwise disjoint and E 1 ∪ ∪ E 2 ∪ ∪ ⋯ ⋯ = B {\displaystyle E_{1}\cup E_{2}\cup \cdots =B} . Hence, we obtain from the third axiom that P ( A ) + P ( B ∖ ∖ A ) + ∑ ∑ i = 3 ∞ ∞ P ( E i ) = P ( B ) .

{\displaystyle P(A)+P(B\setminus A)+\sum _{i=3}^{\infty }P(E_{i})=P(B).} Since, by the first axiom, the left-hand side of this equation is a series of non-negative numbers, and since it converges to P ( B ) {\displaystyle P(B)} which is finite, we obtain both P ( A ) ≤ ≤ P ( B ) {\displaystyle P(A)\leq P(B)} and P ( ∅ ∅ ) = 0 {\displaystyle P(\varnothing )=0} .

The probability of the empty set [ edit ] P ( ∅ ∅ ) = 0.

{\displaystyle P(\varnothing )=0.} In many cases, ∅ ∅ {\displaystyle \varnothing } is not the only event with probability 0.

Proof of the probability of the empty set [ edit ] P ( ∅ ∅ ∪ ∪ ∅ ∅ ) = P ( ∅ ∅ ) {\displaystyle P(\varnothing \cup \varnothing )=P(\varnothing )} since ∅ ∅ ∪ ∪ ∅ ∅ = ∅ ∅ {\displaystyle \varnothing \cup \varnothing =\varnothing } , P ( ∅ ∅ ) + P ( ∅ ∅ ) = P ( ∅ ∅ ) {\displaystyle P(\varnothing )+P(\varnothing )=P(\varnothing )} by applying the third axiom to the left-hand side 
(note ∅ ∅ {\displaystyle \varnothing } is disjoint with itself), and so P ( ∅ ∅ ) = 0 {\displaystyle P(\varnothing )=0} by subtracting P ( ∅ ∅ ) {\displaystyle P(\varnothing )} from each side of the equation.

The complement rule [ edit ] P ( A ∁ ∁ ) = P ( Ω Ω − − A ) = 1 − − P ( A ) {\displaystyle P\left(A^{\complement }\right)=P(\Omega -A)=1-P(A)} Proof of the complement rule [ edit ] Given A {\displaystyle A} and A ∁ ∁ {\displaystyle A^{\complement }} are mutually exclusive and that A ∪ ∪ A ∁ ∁ = Ω Ω {\displaystyle A\cup A^{\complement }=\Omega } : P ( A ∪ ∪ A ∁ ∁ ) = P ( A ) + P ( A ∁ ∁ ) {\displaystyle P(A\cup A^{\complement })=P(A)+P(A^{\complement })} ... (by axiom 3) and, P ( A ∪ ∪ A ∁ ∁ ) = P ( Ω Ω ) = 1 {\displaystyle P(A\cup A^{\complement })=P(\Omega )=1} ...

(by axiom 2) ⇒ ⇒ P ( A ) + P ( A ∁ ∁ ) = 1 {\displaystyle \Rightarrow P(A)+P(A^{\complement })=1} ∴ ∴ P ( A ∁ ∁ ) = 1 − − P ( A ) {\displaystyle \therefore P(A^{\complement })=1-P(A)} The numeric bound [ edit ] It immediately follows from the monotonicity property that 0 ≤ ≤ P ( E ) ≤ ≤ 1 ∀ ∀ E ∈ ∈ F .

{\displaystyle 0\leq P(E)\leq 1\qquad \forall E\in F.} Proof of the numeric bound [ edit ] Given the complement rule P ( E c ) = 1 − − P ( E ) {\displaystyle P(E^{c})=1-P(E)} and axiom 1 P ( E c ) ≥ ≥ 0 {\displaystyle P(E^{c})\geq 0} : 1 − − P ( E ) ≥ ≥ 0 {\displaystyle 1-P(E)\geq 0} ⇒ ⇒ 1 ≥ ≥ P ( E ) {\displaystyle \Rightarrow 1\geq P(E)} ∴ ∴ 0 ≤ ≤ P ( E ) ≤ ≤ 1 {\displaystyle \therefore 0\leq P(E)\leq 1} Further consequences [ edit ] Another important property is: P ( A ∪ ∪ B ) = P ( A ) + P ( B ) − − P ( A ∩ ∩ B ) .

{\displaystyle P(A\cup B)=P(A)+P(B)-P(A\cap B).} This is called the addition law of probability, or the sum rule.
That is, the probability that an event in A or B will happen is the sum of the probability of an event in A and the probability of an event in B , minus the probability of an event that is in both A and B . The proof of this is as follows: Firstly, P ( A ∪ ∪ B ) = P ( A ) + P ( B ∖ ∖ A ) {\displaystyle P(A\cup B)=P(A)+P(B\setminus A)} .

(by Axiom 3) So, P ( A ∪ ∪ B ) = P ( A ) + P ( B ∖ ∖ ( A ∩ ∩ B ) ) {\displaystyle P(A\cup B)=P(A)+P(B\setminus (A\cap B))} (by B ∖ ∖ A = B ∖ ∖ ( A ∩ ∩ B ) {\displaystyle B\setminus A=B\setminus (A\cap B)} ).

Also, P ( B ) = P ( B ∖ ∖ ( A ∩ ∩ B ) ) + P ( A ∩ ∩ B ) {\displaystyle P(B)=P(B\setminus (A\cap B))+P(A\cap B)} and eliminating P ( B ∖ ∖ ( A ∩ ∩ B ) ) {\displaystyle P(B\setminus (A\cap B))} from both equations gives us the desired result.

An extension of the addition law to any number of sets is the inclusion–exclusion principle .

Setting B to the complement A c of A in the addition law gives P ( A c ) = P ( Ω Ω ∖ ∖ A ) = 1 − − P ( A ) {\displaystyle P\left(A^{c}\right)=P(\Omega \setminus A)=1-P(A)} That is, the probability that any event will not happen (or the event's complement ) is 1 minus the probability that it will.

Simple example: coin toss [ edit ] Consider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both).  No assumption is made as to whether the coin is fair or as to whether or not any bias depends on how the coin is tossed.

[ 9 ] We may define: Ω Ω = { H , T } {\displaystyle \Omega =\{H,T\}} F = { ∅ ∅ , { H } , { T } , { H , T } } {\displaystyle F=\{\varnothing ,\{H\},\{T\},\{H,T\}\}} Kolmogorov's axioms imply that: P ( ∅ ∅ ) = 0 {\displaystyle P(\varnothing )=0} The probability of neither heads nor tails, is 0.

P ( { H , T } c ) = 0 {\displaystyle P(\{H,T\}^{c})=0} The probability of either heads or tails, is 1.

P ( { H } ) + P ( { T } ) = 1 {\displaystyle P(\{H\})+P(\{T\})=1} The sum of the probability of heads and the probability of tails, is 1.

See also [ edit ] Borel algebra – Class of mathematical sets Pages displaying short descriptions of redirect targets Conditional probability – Probability of an event occurring, given that another event has already occurred Fully probabilistic design Intuitive statistics Quasiprobability – Concept in statistics Pages displaying short descriptions of redirect targets Set theory – Branch of mathematics that studies sets σ-algebra – Algebraic structure of set algebra Pages displaying short descriptions of redirect targets References [ edit ] ^ a b Kolmogorov, Andrey (1950) [1933].

Foundations of the theory of probability . New York, US: Chelsea Publishing Company.

^ Aldous, David.

"What is the significance of the Kolmogorov axioms?" .

David Aldous . Retrieved November 19, 2019 .

^ Cox, R. T.

(1946). "Probability, Frequency and Reasonable Expectation".

American Journal of Physics .

14 (1): 1– 10.

Bibcode : 1946AmJPh..14....1C .

doi : 10.1119/1.1990764 .

^ Cox, R. T.

(1961).

The Algebra of Probable Inference . Baltimore, MD: Johns Hopkins University Press.

^ Hájek, Alan (August 28, 2019).

"Interpretations of Probability" .

Stanford Encyclopedia of Philosophy . Retrieved November 17, 2019 .

^ a b Ross, Sheldon M. (2014).

A first course in probability (Ninth ed.). Upper Saddle River, New Jersey. pp. 27, 28.

ISBN 978-0-321-79477-2 .

OCLC 827003384 .

{{ cite book }} :  CS1 maint: location missing publisher ( link ) ^ Gerard, David (December 9, 2017).

"Proofs from axioms" (PDF) . Retrieved November 20, 2019 .

^ Jackson, Bill (2010).

"Probability (Lecture Notes - Week 3)" (PDF) .

School of Mathematics, Queen Mary University of London . Retrieved November 20, 2019 .

^ Diaconis, Persi; Holmes, Susan; Montgomery, Richard (2007).

"Dynamical Bias in the Coin Toss" (PDF) .

SIAM Review .

49 ( 211– 235): 211– 235.

Bibcode : 2007SIAMR..49..211D .

doi : 10.1137/S0036144504446436 . Retrieved 5 January 2024 .

Further reading [ edit ] DeGroot, Morris H.

(1975).

Probability and Statistics . Reading: Addison-Wesley. pp.

12–16 .

ISBN 0-201-01503-X .

McCord, James R.; Moroney, Richard M. (1964).

"Axiomatic Probability" .

Introduction to Probability Theory . New York: Macmillan. pp.

13–28 .

Formal definition of probability in the Mizar system , and the list of theorems formally proved about it.

Retrieved from " https://en.wikipedia.org/w/index.php?title=Probability_axioms&oldid=1286322081 " Categories : Probability theory Mathematical axioms Hidden categories: CS1 maint: location missing publisher Articles with short description Short description is different from Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link This page was last edited on 19 April 2025, at 04:30 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Probability axioms 35 languages Add topic

