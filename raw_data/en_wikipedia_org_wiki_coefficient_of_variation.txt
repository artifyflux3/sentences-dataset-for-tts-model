Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Examples 3 Estimation Toggle Estimation subsection 3.1 Log-normal data 4 Comparison to standard deviation Toggle Comparison to standard deviation subsection 4.1 Advantages 4.2 Disadvantages 5 Applications Toggle Applications subsection 5.1 Laboratory measures of intra-assay and inter-assay CVs 5.2 As a measure of economic inequality 5.3 As a measure of standardisation of archaeological artefacts 6 Examples of misuse 7 Distribution Toggle Distribution subsection 7.1 Alternative 8 Similar ratios 9 See also 10 References 11 External links Toggle the table of contents Coefficient of variation 30 languages العربية Aragonés Català Čeština Deutsch Eesti Español Euskara فارسی Français Galego 한국어 Hrvatski Italiano Lietuvių Nederlands 日本語 Norsk bokmål Polski Português Русский Slovenščina Српски / srpski Suomi Svenska Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical parameter Not to be confused with Coefficient of determination .

In probability theory and statistics , the coefficient of variation ( CV ), also known as normalized root-mean-square deviation (NRMSD), percent RMS , and relative standard deviation ( RSD ), is a standardized measure of dispersion of a probability distribution or frequency distribution . It is defined as the ratio of the standard deviation σ σ {\displaystyle \sigma } to the mean μ μ {\displaystyle \mu } (or its absolute value , | μ μ | {\displaystyle |\mu |} ) , and often expressed as a percentage ("%RSD"). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay . It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R , [ citation needed ] by economists and investors in economic models , in epidemiology , and in psychology / neuroscience .

Definition [ edit ] The coefficient of variation (CV) is defined as the ratio of the standard deviation σ σ {\displaystyle \sigma } to the mean μ μ {\displaystyle \mu } , C V = σ σ μ μ .

{\displaystyle CV={\frac {\sigma }{\mu }}.} [ 1 ] It shows the extent of variability in relation to the mean of the population.
The coefficient of variation should be computed only for data measured on scales that have a meaningful zero ( ratio scale ) and hence allow relative comparison of two measurements (i.e., division of one measurement by the other). The coefficient of variation may not have any meaning for data on an interval scale .

[ 2 ] For example, most temperature scales (e.g., Celsius, Fahrenheit etc.) are interval scales with arbitrary zeros, so the computed coefficient of variation would be different depending on the scale used. On the other hand, Kelvin temperature has a meaningful zero, the complete absence of thermal energy, and thus is a ratio scale. In plain language, it is meaningful to say that 20 Kelvin is twice as hot as 10 Kelvin, but only in this scale with a true absolute zero. While a standard deviation (SD) can be measured in Kelvin, Celsius, or Fahrenheit, the value computed is only applicable to that scale. Only the Kelvin scale can be used to compute a valid coefficient of variability.

Measurements that are log-normally distributed exhibit stationary CV; in contrast, SD varies depending upon the expected value of measurements.

A more robust possibility is the quartile coefficient of dispersion , half the interquartile range ( Q 3 − − Q 1 ) / 2 {\displaystyle {(Q_{3}-Q_{1})/2}} divided by the average of the quartiles (the midhinge ), ( Q 1 + Q 3 ) / 2 {\displaystyle {(Q_{1}+Q_{3})/2}} .

In most cases, a CV is computed for a single independent variable (e.g., a single factory product) with numerous, repeated measures of a dependent variable (e.g., error in the production process). However, data that are linear or even logarithmically non-linear and include a continuous range for the independent variable with sparse measurements across each value (e.g., scatter-plot) may be amenable to single CV calculation using a maximum-likelihood estimation approach.

[ 3 ] Examples [ edit ] In the examples below, we will take the values given as randomly chosen from a larger population of values .

The data set [100, 100, 100] has constant values.  Its standard deviation is 0 and average is 100, giving the coefficient of variation as 0 / 100 = 0 The data set [90, 100, 110] has more variability.  Its standard deviation is 10 and its average is 100, giving the coefficient of variation as 10 / 100 = 0.1 The data set [1, 5, 6, 8, 10, 40, 65, 88] has still more variability. Its standard deviation is 32.9 and its average is 27.9, giving a coefficient of variation of 32.9 / 27.9 = 1.18 In these examples, we will take the values given as the entire population of values .

The data set [100, 100, 100] has a population standard deviation of 0 and a coefficient of variation of 0 / 100 = 0 The data set [90, 100, 110] has a population standard deviation of 8.16 and a coefficient of variation of 8.16 / 100 = 0.0816 The data set [1, 5, 6, 8, 10, 40, 65, 88] has a population standard deviation of 30.8 and a coefficient of variation of 30.8 / 27.9 = 1.10 Estimation [ edit ] When only a sample of data from a population is available, the population CV can be estimated using the ratio of the sample standard deviation s {\displaystyle s\,} to the sample mean x ¯ ¯ {\displaystyle {\bar {x}}} : c v ^ ^ = s x ¯ ¯ {\displaystyle {\widehat {c_{\rm {v}}}}={\frac {s}{\bar {x}}}} But this estimator, when applied to a small or moderately sized sample, tends to be too low: it is a biased estimator .  For normally distributed data, an unbiased estimator [ 4 ] for a sample of size n is: c v ^ ^ ∗ ∗ = ( 1 + 1 4 n ) c v ^ ^ {\displaystyle {\widehat {c_{\rm {v}}}}^{*}={\bigg (}1+{\frac {1}{4n}}{\bigg )}{\widehat {c_{\rm {v}}}}} Log-normal data [ edit ] Many datasets follow an approximately log-normal distribution.

[ 5 ] In such cases, a more accurate estimate, derived from the properties of the log-normal distribution , [ 6 ] [ 7 ] [ 8 ] is defined as: c v ^ ^ r a w = e s ln 2 − − 1 {\displaystyle {\widehat {cv}}_{\rm {raw}}={\sqrt {\mathrm {e} ^{s_{\ln }^{2}}-1}}} where s ln {\displaystyle {s_{\ln }}\,} is the sample standard deviation of the data after a natural log transformation.  (In the event that measurements are recorded using any other logarithmic base, b, their standard deviation s b {\displaystyle s_{b}\,} is converted to base e using s ln = s b ln ⁡ ⁡ ( b ) {\displaystyle s_{\ln }=s_{b}\ln(b)\,} , and the formula for c v ^ ^ r a w {\displaystyle {\widehat {cv}}_{\rm {raw}}\,} remains the same.

[ 9 ] )  This estimate is sometimes referred to as the "geometric CV" (GCV) [ 10 ] [ 11 ] in order to distinguish it from the simple estimate above.  However, "geometric coefficient of variation" has also been defined by Kirkwood [ 12 ] as: G C V K = e s ln − − 1 {\displaystyle \mathrm {GCV_{K}} ={\mathrm {e} ^{s_{\ln }}\!\!-1}} This term was intended to be analogous to the coefficient of variation, for describing multiplicative variation in log-normal data, but this definition of GCV has no theoretical basis as an estimate of c v {\displaystyle c_{\rm {v}}\,} itself.

For many practical purposes (such as sample size determination and calculation of confidence intervals ) it is s l n {\displaystyle s_{ln}\,} which is of most use in the context of log-normally distributed data.  If necessary, this can be derived from an estimate of c v {\displaystyle c_{\rm {v}}\,} or GCV by inverting the corresponding formula.

Comparison to standard deviation [ edit ] Advantages [ edit ] The coefficient of variation is useful because the standard deviation of data must always be understood in the context of the mean of the data. 
In contrast, the actual value of the CV is independent of the unit in which the measurement has been taken, so it is a dimensionless number . 
For comparison between data sets with different units or widely different means, one should use the coefficient of variation instead of the standard deviation.

Disadvantages [ edit ] When the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.

Unlike the standard deviation, it cannot be used directly to construct confidence intervals for the mean.

Applications [ edit ] The coefficient of variation is also common in applied probability fields such as renewal theory , queueing theory , and reliability theory .  In these fields, the exponential distribution is often more important than the normal distribution .
The standard deviation of an exponential distribution is equal to its mean, so its coefficient of variation is equal to 1.  Distributions with CV < 1 (such as an Erlang distribution ) are considered low-variance, while those with CV > 1 (such as a hyper-exponential distribution ) are considered high-variance [ citation needed ] .  Some formulas in these fields are expressed using the squared coefficient of variation , often abbreviated SCV. In modeling, a variation of the CV is the CV(RMSD).  Essentially the CV(RMSD) replaces the standard deviation term with the Root Mean Square Deviation (RMSD) . While many natural processes indeed show a correlation between the average value and the amount of variation around it, accurate sensor devices need to be designed in such a way that the coefficient of variation is close to zero, i.e., yielding a constant absolute error over their working range.

In actuarial science , the CV is known as unitized risk .

[ 13 ] In industrial solids processing, CV is particularly important to measure the degree of homogeneity of a powder mixture. Comparing the calculated CV to a specification will allow to define if a sufficient degree of mixing has been reached.

[ 14 ] In fluid dynamics , the CV , also referred to as Percent RMS , %RMS , %RMS Uniformity , or Velocity RMS , is a useful determination of flow uniformity for industrial processes.  The term is used widely in the design of pollution control equipment, such as electrostatic precipitators (ESPs), [ 15 ] selective catalytic reduction (SCR), scrubbers, and similar devices.  The Institute of Clean Air Companies (ICAC) references RMS deviation of velocity in the design of fabric filters (ICAC document F-7).

[ 16 ] The guiding principle is that many of these pollution control devices require "uniform flow" entering and through the control zone.  This can be related to uniformity of velocity profile, temperature distribution, gas species (such as ammonia for an SCR, or activated carbon injection for mercury absorption), and other flow-related parameters.  The Percent RMS also is used to assess flow uniformity in combustion systems, HVAC systems, ductwork, inlets to fans and filters, air handling units, etc. where performance of the equipment is influenced by the incoming flow distribution.

Laboratory measures of intra-assay and inter-assay CVs [ edit ] CV measures are often used as quality controls for quantitative laboratory assays . While intra-assay and inter-assay CVs might be assumed to be calculated by simply averaging CV values across CV values for multiple samples within one assay or by averaging multiple inter-assay CV estimates, it has been suggested that these practices are incorrect and that a more complex computational process is required.

[ 17 ] It has also been noted that CV values are not an ideal index of the certainty of a measurement when the number of replicates varies across samples − in this case standard error in percent is suggested to be superior.

[ 18 ] If measurements do not have a natural zero point then the CV is not a valid measurement and alternative measures such as the intraclass correlation coefficient are recommended.

[ 19 ] As a measure of economic inequality [ edit ] The coefficient of variation fulfills the requirements for a measure of economic inequality .

[ 20 ] [ 21 ] [ 22 ] If x (with entries x i ) is a list of the values of an economic indicator (e.g. wealth), with x i being the wealth of agent i , then the following requirements are met: Anonymity – c v is independent of the ordering of the list x . This follows from the fact that the variance and mean are independent of the ordering of x .

Scale invariance: c v ( x ) = c v (α x ) where α is a real number.

[ 22 ] Population independence – If { x , x } is the list x appended to itself, then c v ({ x , x }) = c v ( x ). This follows from the fact that the variance and mean both obey this principle.

Pigou–Dalton transfer principle: when wealth is transferred from a wealthier agent i to a poorer agent j (i.e.

x i > x j ) without altering their rank, then c v decreases and vice versa.

[ 22 ] c v assumes its minimum value of zero for complete equality (all x i are equal).

[ 22 ] Its most notable drawback is that it is not bounded from above, so it cannot be normalized to be within a fixed range (e.g. like the Gini coefficient which is constrained to be between 0 and 1).

[ 22 ] It is, however, more mathematically tractable than the Gini coefficient.

As a measure of standardisation of archaeological artefacts [ edit ] Archaeologists often use CV values to compare the degree of standardisation of ancient artefacts.

[ 23 ] [ 24 ] Variation in CVs has been interpreted to indicate different cultural transmission contexts for the adoption of new technologies.

[ 25 ] Coefficients of variation have also been used to investigate pottery standardisation relating to changes in social organisation.

[ 26 ] Archaeologists also use several methods for comparing CV values, for example the modified signed-likelihood ratio (MSLR) test for equality of CVs.

[ 27 ] [ 28 ] Examples of misuse [ edit ] Comparing coefficients of variation between parameters using relative units can result in differences that may not be real. If we compare the same set of temperatures in Celsius and Fahrenheit (both relative units, where kelvin and Rankine scale are their associated absolute values): Celsius: [0, 10, 20, 30, 40] Fahrenheit: [32, 50, 68, 86, 104] The sample standard deviations are 15.81 and 28.46, respectively. The CV of the first set is 15.81/20 = 79%. For the second set (which are the same temperatures) it is 28.46/68 = 42%.

If, for example, the data sets are temperature readings from two different sensors (a Celsius sensor and a Fahrenheit sensor) and you want to know which sensor is better by picking the one with the least variance, then you will be misled if you use CV. The problem here is that you have divided by a relative value rather than an absolute.

Comparing the same data set, now in absolute units: Kelvin: [273.15, 283.15, 293.15, 303.15, 313.15] Rankine: [491.67, 509.67, 527.67, 545.67, 563.67] The sample standard deviations are still 15.81 and 28.46, respectively, because the standard deviation is not affected by a constant offset. The coefficients of variation, however, are now both equal to 5.39%.

Mathematically speaking, the coefficient of variation is not entirely linear.  That is, for a random variable X {\displaystyle X} , the coefficient of variation of a X + b {\displaystyle aX+b} is equal to the coefficient of variation of X {\displaystyle X} only when b = 0 {\displaystyle b=0} .  In the above example, Celsius can only be converted to Fahrenheit through a linear transformation of the form a x + b {\displaystyle ax+b} with b ≠ ≠ 0 {\displaystyle b\neq 0} , whereas Kelvins can be converted to Rankines through a transformation of the form a x {\displaystyle ax} .

Distribution [ edit ] Provided that negative and small positive values of the sample mean occur with negligible frequency, the probability distribution of the coefficient of variation for a sample of size n {\displaystyle n} of i.i.d. normal random variables has been shown by Hendricks and Robey to be [ 29 ] d F c v = 2 π π 1 / 2 Γ Γ ( n − − 1 2 ) exp ⁡ ⁡ ( − − n 2 ( σ σ μ μ ) 2 ⋅ ⋅ c v 2 1 + c v 2 ) c v n − − 2 ( 1 + c v 2 ) n / 2 ∑ ∑ ∑ ∑ ′ ′ i = 0 n − − 1 ⁡ ⁡ ( n − − 1 ) !

Γ Γ ( n − − i 2 ) ( n − − 1 − − i ) !

i !

⋅ ⋅ n i / 2 2 i / 2 ⋅ ⋅ ( σ σ μ μ ) i ⋅ ⋅ 1 ( 1 + c v 2 ) i / 2 d c v , {\displaystyle \mathrm {d} F_{c_{\rm {v}}}={\frac {2}{\pi ^{1/2}\Gamma {\left({\frac {n-1}{2}}\right)}}}\exp \left(-{\frac {n}{2\left({\frac {\sigma }{\mu }}\right)^{2}}}\cdot {\frac {{c_{\rm {v}}}^{2}}{1+{c_{\rm {v}}}^{2}}}\right){\frac {{c_{\rm {v}}}^{n-2}}{(1+{c_{\rm {v}}}^{2})^{n/2}}}\sideset {}{^{\prime }}\sum _{i=0}^{n-1}{\frac {(n-1)!\,\Gamma \left({\frac {n-i}{2}}\right)}{(n-1-i)!\,i!\,}}\cdot {\frac {n^{i/2}}{2^{i/2}\cdot \left({\frac {\sigma }{\mu }}\right)^{i}}}\cdot {\frac {1}{(1+{c_{\rm {v}}}^{2})^{i/2}}}\,\mathrm {d} c_{\rm {v}},} where the symbol ∑ ∑ ∑ ∑ ′ ′ {\textstyle \sideset {}{^{\prime }}\sum } indicates that the summation is over only even values of n − − 1 − − i {\displaystyle n-1-i} , i.e., if n {\displaystyle n} is odd, sum over even values of i {\displaystyle i} and if n {\displaystyle n} is even, sum only over odd values of i {\displaystyle i} .

This is useful, for instance, in the construction of hypothesis tests or confidence intervals . 
Statistical inference for the coefficient of variation in normally distributed data is often based on McKay's chi-square approximation for the coefficient of variation.

[ 30 ] [ 31 ] [ 32 ] [ 33 ] [ 34 ] [ 35 ] Alternative [ edit ] Liu (2012) reviews methods for the construction of a confidence interval for the coefficient of variation.

[ 36 ] Notably, Lehmann (1986) derived the sampling distribution for the coefficient of variation using a non-central t-distribution to give an exact method for the construction of the CI.

[ 37 ] Similar ratios [ edit ] Standardized moments are similar ratios, μ μ k / σ σ k {\displaystyle {\mu _{k}}/{\sigma ^{k}}} where μ μ k {\displaystyle \mu _{k}} is the k th moment about the mean, which are also dimensionless and scale invariant. The variance-to-mean ratio , σ σ 2 / μ μ {\displaystyle \sigma ^{2}/\mu } , is another similar ratio, but is not dimensionless, and hence not scale invariant. See Normalization (statistics) for further ratios.

In signal processing , particularly image processing , the reciprocal ratio μ μ / σ σ {\displaystyle \mu /\sigma } (or its square) is referred to as the signal-to-noise ratio in general and signal-to-noise ratio (imaging) in particular.

Other related ratios include: Efficiency , σ σ 2 / μ μ 2 {\displaystyle \sigma ^{2}/\mu ^{2}} Standardized moment , μ μ k / σ σ k {\displaystyle \mu _{k}/\sigma ^{k}} Variance-to-mean ratio (or relative variance), σ σ 2 / μ μ {\displaystyle \sigma ^{2}/\mu } Fano factor , σ σ W 2 / μ μ W {\displaystyle \sigma _{W}^{2}/\mu _{W}} (windowed VMR) See also [ edit ] Standard score Information ratio Omega ratio Sampling (statistics) Variance function References [ edit ] ^ Everitt, Brian (1998).

The Cambridge Dictionary of Statistics . Cambridge, UK New York: Cambridge University Press.

ISBN 978-0521593465 .

^ "What is the difference between ordinal, interval and ratio variables? Why should I care?" . GraphPad Software Inc.

Archived from the original on 15 December 2008 . Retrieved 22 February 2008 .

^ Odic, Darko; Im, Hee Yeon; Eisinger, Robert; Ly, Ryan; Halberda, Justin (June 2016).

"PsiMLE: A maximum-likelihood estimation approach to estimating psychophysical scaling and variability more reliably, efficiently, and flexibly" .

Behavior Research Methods .

48 (2): 445– 462.

doi : 10.3758/s13428-015-0600-5 .

ISSN 1554-3528 .

PMID 25987306 .

^ Sokal RR & Rohlf FJ.

Biometry (3rd Ed). New York: Freeman, 1995. p. 58.

ISBN 0-7167-2411-1 ^ Limpert, Eckhard; Stahel, Werner A.; Abbt, Markus (2001).

"Log-normal Distributions across the Sciences: Keys and Clues" .

BioScience .

51 (5): 341– 352.

doi : 10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2 .

^ Koopmans, L. H.; Owen, D. B.; Rosenblatt, J. I. (1964). "Confidence intervals for the coefficient of variation for the normal and log normal distributions".

Biometrika .

51 ( 1– 2): 25– 32.

doi : 10.1093/biomet/51.1-2.25 .

^ Diletti, E; Hauschke, D; Steinijans, VW (1992). "Sample size determination for bioequivalence assessment by means of confidence intervals".

International Journal of Clinical Pharmacology, Therapy, and Toxicology .

30 (Suppl 1): S51–8.

PMID 1601532 .

^ Julious, Steven A.; Debarnot, Camille A. M. (2000). "Why Are Pharmacokinetic Data Summarized by Arithmetic Means?".

Journal of Biopharmaceutical Statistics .

10 (1): 55– 71.

doi : 10.1081/BIP-100101013 .

PMID 10709801 .

S2CID 2805094 .

^ Reed, JF; Lynn, F; Meade, BD (2002).

"Use of Coefficient of Variation in Assessing Variability of Quantitative Assays" .

Clin Diagn Lab Immunol .

9 (6): 1235– 1239.

doi : 10.1128/CDLI.9.6.1235-1239.2002 .

PMC 130103 .

PMID 12414755 .

^ Sawant, S.; Mohan, N. (2011) "FAQ: Issues with Efficacy Analysis of Clinical Trial Data Using SAS" Archived 24 August 2011 at the Wayback Machine , PharmaSUG2011 , Paper PO08 ^ Schiff, MH; et al. (2014).

"Head-to-head, randomised, crossover study of oral versus subcutaneous methotrexate in patients with rheumatoid arthritis: drug-exposure limitations of oral methotrexate at doses >=15 mg may be overcome with subcutaneous administration" .

Ann Rheum Dis .

73 (8): 1– 3.

doi : 10.1136/annrheumdis-2014-205228 .

PMC 4112421 .

PMID 24728329 .

^ Kirkwood, TBL (1979). "Geometric means and measures of dispersion".

Biometrics .

35 (4): 908– 9.

JSTOR 2530139 .

^ Broverman, Samuel A. (2001).

Actex study manual, Course 1, Examination of the Society of Actuaries, Exam 1 of the Casualty Actuarial Society (2001 ed.). Winsted, CT: Actex Publications. p. 104.

ISBN 9781566983969 . Retrieved 7 June 2014 .

^ "Measuring Degree of Mixing – Homogeneity of powder mix - Mixture quality - PowderProcess.net" .

www.powderprocess.net .

Archived from the original on 14 November 2017 . Retrieved 2 May 2018 .

^ Banka, A; Dumont, B; Franklin, J; Klemm, G; Mudry, R (2018).

"Improved Methodology for Accurate CFD and Physical Modeling of ESPs" (PDF) . International Society of Electrostatic Precipitation (ISESP) Conference 2018.

^ "F7 - Fabric Filter Gas Flow Model Studies" (PDF) . Institute of Clean Air Companies (ICAC). 1996.

^ Rodbard, D (October 1974).

"Statistical quality control and routine data processing for radioimmunoassays and immunoradiometric assays" .

Clinical Chemistry .

20 (10): 1255– 70.

doi : 10.1093/clinchem/20.10.1255 .

PMID 4370388 .

^ Eisenberg, Dan (2015).

"Improving qPCR telomere length assays: Controlling for well position effects increases statistical power" .

American Journal of Human Biology .

27 (4): 570– 5.

doi : 10.1002/ajhb.22690 .

PMC 4478151 .

PMID 25757675 .

^ Eisenberg, Dan T. A. (30 August 2016).

"Telomere length measurement validity: the coefficient of variation is invalid and cannot be used to compare quantitative polymerase chain reaction and Southern blot telomere length measurement technique" .

International Journal of Epidemiology .

45 (4): 1295– 1298.

doi : 10.1093/ije/dyw191 .

ISSN 0300-5771 .

PMID 27581804 .

^ Champernowne, D. G.; Cowell, F. A. (1999).

Economic Inequality and Income Distribution . Cambridge University Press.

^ Campano, F.; Salvatore, D. (2006).

Income distribution . Oxford University Press.

^ a b c d e Bellu, Lorenzo Giovanni; Liberati, Paolo (2006).

"Policy Impacts on Inequality – Simple Inequality Measures" (PDF) .

EASYPol, Analytical tools . Policy Support Service, Policy Assistance Division, FAO.

Archived (PDF) from the original on 5 August 2016 . Retrieved 13 June 2016 .

^ Eerkens, Jelmer W.; Bettinger, Robert L. (July 2001). "Techniques for Assessing Standardization in Artifact Assemblages: Can We Scale Material Variability?".

American Antiquity .

66 (3): 493– 504.

doi : 10.2307/2694247 .

JSTOR 2694247 .

S2CID 163507589 .

^ Roux, Valentine (2003).

"Ceramic Standardization and Intensity of Production: Quantifying Degrees of Specialization" .

American Antiquity .

68 (4): 768– 782.

doi : 10.2307/3557072 .

ISSN 0002-7316 .

JSTOR 3557072 .

S2CID 147444325 .

^ Bettinger, Robert L.; Eerkens, Jelmer (April 1999). "Point Typologies, Cultural Transmission, and the Spread of Bow-and-Arrow Technology in the Prehistoric Great Basin".

American Antiquity .

64 (2): 231– 242.

doi : 10.2307/2694276 .

JSTOR 2694276 .

S2CID 163198451 .

^ Wang, Li-Ying; Marwick, Ben (October 2020).

"Standardization of ceramic shape: A case study of Iron Age pottery from northeastern Taiwan" .

Journal of Archaeological Science: Reports .

33 : 102554.

Bibcode : 2020JArSR..33j2554W .

doi : 10.1016/j.jasrep.2020.102554 .

S2CID 224904703 .

^ Krishnamoorthy, K.; Lee, Meesook (February 2014). "Improved tests for the equality of normal coefficients of variation".

Computational Statistics .

29 ( 1– 2): 215– 232.

doi : 10.1007/s00180-013-0445-2 .

S2CID 120898013 .

^ Marwick, Ben; Krishnamoorthy, K (2019).

cvequality: Tests for the equality of coefficients of variation from multiple groups . R package version 0.2.0.

^ Hendricks, Walter A.; Robey, Kate W. (1936).

"The Sampling Distribution of the Coefficient of Variation" .

The Annals of Mathematical Statistics .

7 (3): 129– 32.

doi : 10.1214/aoms/1177732503 .

JSTOR 2957564 .

^ Iglevicz, Boris; Myers, Raymond (1970). "Comparisons of approximations to the percentage points of the sample coefficient of variation".

Technometrics .

12 (1): 166– 169.

doi : 10.2307/1267363 .

JSTOR 1267363 .

^ Bennett, B. M. (1976). "On an Approximate Test for Homogeneity of Coefficients of Variation".

Contribution to Applied Statistics . Experientia Supplementum. Vol. 22. pp.

169– 171.

doi : 10.1007/978-3-0348-5513-6_16 .

ISBN 978-3-0348-5515-0 .

^ Vangel, Mark G. (1996). "Confidence intervals for a normal coefficient of variation".

The American Statistician .

50 (1): 21– 26.

doi : 10.1080/00031305.1996.10473537 .

JSTOR 2685039 .

.

^ Feltz, Carol J; Miller, G. Edward (1996). "An asymptotic test for the equality of coefficients of variation from k populations".

Statistics in Medicine .

15 (6): 647.

doi : 10.1002/(SICI)1097-0258(19960330)15:6<647::AID-SIM184>3.0.CO;2-P .

PMID 8731006 .

^ Forkman, Johannes (2009).

"Estimator and tests for common coefficients of variation in normal distributions" (PDF) .

Communications in Statistics – Theory and Methods .

38 (2): 21– 26.

doi : 10.1080/03610920802187448 .

S2CID 29168286 .

Archived (PDF) from the original on 6 December 2013 . Retrieved 23 September 2013 .

^ Krishnamoorthy, K; Lee, Meesook (2013). "Improved tests for the equality of normal coefficients of variation".

Computational Statistics .

29 ( 1– 2): 215– 232.

doi : 10.1007/s00180-013-0445-2 .

S2CID 120898013 .

^ Liu, Shuang (2012).

Confidence Interval Estimation for Coefficient of Variation (Thesis). Georgia State University. p.3.

Archived from the original on 1 March 2014 . Retrieved 25 February 2014 .

^ Lehmann, E. L. (1986).

Testing Statistical Hypothesis.

2nd ed. New York: Wiley.

External links [ edit ] cvequality : R package to test for significant differences between multiple coefficients of variation v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Coefficient_of_variation&oldid=1286059102 " Categories : Statistical deviation and dispersion Statistical ratios Income inequality metrics Hidden categories: Webarchive template wayback links Articles with short description Short description is different from Wikidata Use American English from January 2019 All Wikipedia articles written in American English Use dmy dates from October 2017 All articles with unsourced statements Articles with unsourced statements from September 2016 Articles with unsourced statements from June 2019 This page was last edited on 17 April 2025, at 13:36 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Coefficient of variation 30 languages Add topic

