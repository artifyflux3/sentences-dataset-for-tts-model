Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 The problem 2 Methods Toggle Methods subsection 2.1 Euler method 2.2 Backward Euler method 2.3 First-order exponential integrator method 2.4 Generalizations 2.5 Advanced features 2.6 Alternative methods 2.7 Parallel-in-time methods 3 Analysis Toggle Analysis subsection 3.1 Convergence 3.2 Consistency and order 3.3 Stability and stiffness 4 History 5 Numerical solutions to second-order one-dimensional boundary value problems 6 See also 7 Notes 8 References 9 External links Toggle the table of contents Numerical methods for ordinary differential equations 12 languages العربية Català Čeština Español فارسی हिन्दी Italiano 日本語 Português Scots Simple English 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Numerical ordinary differential equations ) Methods used to find numerical solutions of ordinary differential equations Illustration of numerical integration for the differential equation y ′ = y , y ( 0 ) = 1.

{\displaystyle y'=y,y(0)=1.} Blue: Euler method Green: Midpoint method Red: Exact solution: y = e t {\textstyle y=e^{t}} .

The step size is h = 1.0 {\displaystyle h=1.0} .

The same illustration for h = 0.25.

{\displaystyle h=0.25.} The midpoint method converges faster than the Euler method, as h → → 0 {\displaystyle h\to 0} .

Numerical methods for ordinary differential equations are methods used to find numerical approximations to the solutions of ordinary differential equations (ODEs). Their use is also known as " numerical integration ", although this term can also refer to the computation of integrals .

Many differential equations cannot be solved exactly. For practical purposes, however – such as in engineering – a numeric approximation to the solution is often sufficient. The algorithms studied here can be used to compute such an approximation. An alternative method is to use techniques from calculus to obtain a series expansion of the solution.

Ordinary differential equations occur in many scientific disciplines, including physics , chemistry , biology , and economics .

[ 1 ] In addition, some methods in numerical partial differential equations convert the partial differential equation into an ordinary differential equation, which must then be solved.

The problem [ edit ] A first-order differential equation is an Initial value problem (IVP) of the form, [ 2 ] y ′ ( t ) = f ( t , y ( t ) ) , y ( t 0 ) = y 0 , {\displaystyle y'(t)=f(t,y(t)),\qquad y(t_{0})=y_{0},} 1 where f {\displaystyle f} is a function f : [ t 0 , ∞ ∞ ) × × R d → → R d {\displaystyle f:[t_{0},\infty )\times \mathbb {R} ^{d}\to \mathbb {R} ^{d}} , and the initial condition y 0 ∈ ∈ R d {\displaystyle y_{0}\in \mathbb {R} ^{d}} is a given vector.

First-order means that only the first derivative of y appears in the equation, and higher derivatives are absent.

Without loss of generality to higher-order systems, we restrict ourselves to first-order differential equations, because a higher-order ODE can be converted into a larger system of first-order equations by introducing extra variables.  For example, the second-order equation y ′′ = − y can be rewritten as two first-order equations: y ′ = z and z ′ = − y .

In this section, we describe numerical methods for IVPs, and remark that boundary value problems (BVPs) require a different set of tools.  In a BVP, one defines values, or components of the solution y at more than one point.  Because of this, different methods need to be used to solve BVPs.  For example, the shooting method (and its variants) or global methods like finite differences , [ 3 ] Galerkin methods , [ 4 ] or collocation methods are appropriate for that class of problems.

The Picard–Lindelöf theorem states that there is a unique solution, provided f is Lipschitz-continuous .

Methods [ edit ] Numerical methods for solving first-order IVPs often fall into one of two large categories: [ 5 ] linear multistep methods , or Runge–Kutta methods . A further division can be realized by dividing methods into those that are explicit and those that are implicit. For example,  implicit linear multistep methods include Adams-Moulton methods , and backward differentiation methods (BDF), whereas implicit Runge–Kutta methods [ 6 ] include diagonally implicit Runge–Kutta (DIRK), [ 7 ] [ 8 ] singly diagonally implicit Runge–Kutta (SDIRK), [ 9 ] and Gauss–Radau [ 10 ] (based on Gaussian quadrature [ 11 ] ) numerical methods. Explicit examples from the linear multistep family include the Adams–Bashforth methods , and any Runge–Kutta method with a lower diagonal Butcher tableau is explicit . A loose rule of thumb dictates that stiff differential equations require the use of implicit schemes, whereas non-stiff problems can be solved more efficiently with explicit schemes.

The so-called general linear methods (GLMs) are a generalization of the above two large classes of methods.

[ 12 ] Euler method [ edit ] Further information: Euler method From any point on a curve, you can find an approximation of a nearby point on the curve by moving a short distance along a line tangent to the curve.

Starting with the differential equation ( 1 ), we replace the derivative y ′ by the finite difference approximation y ′ ( t ) ≈ ≈ y ( t + h ) − − y ( t ) h , {\displaystyle y'(t)\approx {\frac {y(t+h)-y(t)}{h}},} 2 which when re-arranged yields the following formula y ( t + h ) ≈ ≈ y ( t ) + h y ′ ( t ) {\displaystyle y(t+h)\approx y(t)+hy'(t)} and using ( 1 ) gives: y ( t + h ) ≈ ≈ y ( t ) + h f ( t , y ( t ) ) .

{\displaystyle y(t+h)\approx y(t)+hf(t,y(t)).} 3 This formula is usually applied in the following way. We choose a step size h , and we construct the sequence t 0 , t 1 = t 0 + h , t 2 = t 0 + 2 h , .

.

.

{\displaystyle t_{0},t_{1}=t_{0}+h,t_{2}=t_{0}+2h,...} We denote by y n {\displaystyle y_{n}} a numerical estimate of the exact solution y ( t n ) {\displaystyle y(t_{n})} . Motivated by ( 3 ), we compute these estimates by the following recursive scheme y n + 1 = y n + h f ( t n , y n ) .

{\displaystyle y_{n+1}=y_{n}+hf(t_{n},y_{n}).} 4 This is the Euler method (or forward Euler method , in contrast with the backward Euler method , to be described below). The method is named after Leonhard Euler who described it in 1768.

The Euler method is an example of an explicit method. This means that the new value y n +1 is defined in terms of things that are already known, like y n .

Backward Euler method [ edit ] Further information: Backward Euler method If, instead of ( 2 ), we use the approximation y ′ ( t ) ≈ ≈ y ( t ) − − y ( t − − h ) h , {\displaystyle y'(t)\approx {\frac {y(t)-y(t-h)}{h}},} 5 we get the backward Euler method : y n + 1 = y n + h f ( t n + 1 , y n + 1 ) .

{\displaystyle y_{n+1}=y_{n}+hf(t_{n+1},y_{n+1}).} 6 The backward Euler method is an implicit method, meaning that we have to solve an equation to find y n +1 . One often uses fixed-point iteration or (some modification of) the Newton–Raphson method to achieve this.

It costs more time to solve this equation than explicit methods; this cost must be taken into consideration when one selects the method to use. The advantage of implicit methods such as ( 6 ) is that they are usually more stable for solving a stiff equation , meaning that a larger step size h can be used.

First-order exponential integrator method [ edit ] Further information: Exponential integrator Exponential integrators describe a large class of integrators that have recently seen a lot of development.

[ 13 ] They date back to at least the 1960s.

In place of ( 1 ), we assume the differential equation is either of the form y ′ ( t ) = − − A y + N ( y ) , {\displaystyle y'(t)=-A\,y+{\mathcal {N}}(y),} 7 or it has been locally linearized about a background state to produce a linear term − − A y {\displaystyle -Ay} and a nonlinear term N ( y ) {\displaystyle {\mathcal {N}}(y)} .

Exponential integrators are constructed by multiplying ( 7 ) by e A t {\textstyle e^{At}} , and exactly integrating the result over
a time interval [ t n , t n + 1 = t n + h ] {\displaystyle [t_{n},t_{n+1}=t_{n}+h]} : y n + 1 = e − − A h y n + ∫ ∫ 0 h e − − ( h − − τ τ ) A N ( y ( t n + τ τ ) ) d τ τ .

{\displaystyle y_{n+1}=e^{-Ah}y_{n}+\int _{0}^{h}e^{-(h-\tau )A}{\mathcal {N}}\left(y\left(t_{n}+\tau \right)\right)\,d\tau .} This integral equation is exact, but it doesn't define the integral.

The first-order exponential integrator can be realized by holding N ( y ( t n + τ τ ) ) {\displaystyle {\mathcal {N}}(y(t_{n}+\tau ))} constant over the full interval: y n + 1 = e − − A h y n + A − − 1 ( 1 − − e − − A h ) N ( y ( t n ) ) .

{\displaystyle y_{n+1}=e^{-Ah}y_{n}+A^{-1}(1-e^{-Ah}){\mathcal {N}}(y(t_{n}))\ .} 8 Generalizations [ edit ] The Euler method is often not accurate enough. In more precise terms, it only has order one (the concept of order is explained below). This caused mathematicians to look for higher-order methods.

One possibility is to use not only the previously computed value y n to determine y n +1 , but to make the solution depend on more past values. This yields a so-called multistep method . Perhaps the simplest is the leapfrog method which is second order and (roughly speaking) relies on two time values.

Almost all practical multistep methods fall within the family of linear multistep methods , which have the form α α k y n + k + α α k − − 1 y n + k − − 1 + ⋯ ⋯ + α α 0 y n = h [ β β k f ( t n + k , y n + k ) + β β k − − 1 f ( t n + k − − 1 , y n + k − − 1 ) + ⋯ ⋯ + β β 0 f ( t n , y n ) ] .

{\displaystyle {\begin{aligned}&{}\alpha _{k}y_{n+k}+\alpha _{k-1}y_{n+k-1}+\cdots +\alpha _{0}y_{n}\\&{}\quad =h\left[\beta _{k}f(t_{n+k},y_{n+k})+\beta _{k-1}f(t_{n+k-1},y_{n+k-1})+\cdots +\beta _{0}f(t_{n},y_{n})\right].\end{aligned}}} Another possibility is to use more points in the interval [ t n , t n + 1 ] {\displaystyle [t_{n},t_{n+1}]} . This leads to the family of Runge–Kutta methods , named after Carl Runge and Martin Kutta . One of their fourth-order methods is especially popular.

Advanced features [ edit ] A good implementation of one of these methods for solving an ODE entails more than the time-stepping formula.

It is often inefficient to use the same step size all the time, so variable step-size methods have been developed. Usually, the step size is chosen such that the (local) error per step is below some tolerance level. This means that the methods must also compute an error indicator , an estimate of the local error.

An extension of this idea is to choose dynamically between different methods of different orders (this is called a variable order method ). Methods based on Richardson extrapolation , [ 14 ] such as the Bulirsch–Stoer algorithm , [ 15 ] [ 16 ] are often used to construct various methods of different orders.

Other desirable features include: dense output : cheap numerical approximations for the whole integration interval, and not only at the points t 0 , t 1 , t 2 , ...

event location : finding the times where, say, a particular function vanishes. This typically requires the use of a root-finding algorithm .

support for parallel computing .

when used for integrating with respect to time, time reversibility Alternative methods [ edit ] Many methods do not fall within the framework discussed here. Some classes of alternative methods are: multiderivative methods , which use not only the function f but also its derivatives. This class includes Hermite–Obreschkoff methods and Fehlberg methods , as well as methods like the Parker–Sochacki method [ 17 ] or Bychkov–Scherbakov method, which compute the coefficients of the Taylor series of the solution y recursively.

methods for second order ODEs . We said that all higher-order ODEs can be transformed to first-order ODEs of the form (1). While this is certainly true, it may not be the best way to proceed. In particular, Nyström methods work directly with second-order equations.

geometric integration methods [ 18 ] [ 19 ] are especially designed for special classes of ODEs (for example, symplectic integrators for the solution of Hamiltonian equations ). They take care that the numerical solution respects the underlying structure or geometry of these classes.

Quantized state systems methods are a family of ODE integration methods based on the idea of state quantization. They are efficient when simulating sparse systems with frequent discontinuities.

Parallel-in-time methods [ edit ] Some IVPs require integration at such high temporal resolution and/or over such long time intervals that classical serial time-stepping methods become computationally infeasible to run in real-time (e.g. IVPs in numerical weather prediction, plasma modelling, and molecular dynamics).

Parallel-in-time (PinT) methods have been developed in response to these issues in order to reduce simulation runtimes through the use of parallel computing .

Early PinT methods (the earliest being proposed in the 1960s) [ 20 ] were initially overlooked by researchers due to the fact that the parallel computing architectures that they required were not yet widely available. With more computing power available, interest was renewed in the early 2000s with the development of Parareal , a flexible, easy-to-use PinT algorithm that is suitable for solving a wide variety of IVPs. The advent of exascale computing has meant that PinT algorithms are attracting increasing research attention and are being developed in such a way that they can harness the world's most powerful supercomputers . The most popular methods as of 2023 include Parareal, PFASST, ParaDiag, and MGRIT.

[ 21 ] Analysis [ edit ] Numerical analysis is not only the design of numerical methods, but also their analysis. Three central concepts in this analysis are: convergence : whether the method approximates the solution, order : how well it approximates the solution, and stability : whether errors are damped out.

[ 22 ] Convergence [ edit ] Main articles: Sequence , Limit (mathematics) , and Limit of a sequence A numerical method is said to be convergent if the numerical solution approaches the exact solution as the step size h goes to 0. More precisely, we require that for every ODE (1) with a Lipschitz function f and every t * > 0, lim h → → 0 + max n = 0 , 1 , … … , ⌊ ⌊ t ∗ ∗ / h ⌋ ⌋ ‖ y n , h − − y ( t n ) ‖ = 0.

{\displaystyle \lim _{h\to 0^{+}}\max _{n=0,1,\dots ,\lfloor t^{*}/h\rfloor }\left\|y_{n,h}-y(t_{n})\right\|=0.} All the methods mentioned above are convergent.

Consistency and order [ edit ] Further information: Truncation error (numerical integration) Suppose the numerical method is y n + k = Ψ Ψ ( t n + k ; y n , y n + 1 , … … , y n + k − − 1 ; h ) .

{\displaystyle y_{n+k}=\Psi (t_{n+k};y_{n},y_{n+1},\dots ,y_{n+k-1};h).\,} The local (truncation) error of the method is the error committed by one step of the method. That is, it is the difference between the result given by the method, assuming that no error was made in earlier steps, and the exact solution: δ δ n + k h = Ψ Ψ ( t n + k ; y ( t n ) , y ( t n + 1 ) , … … , y ( t n + k − − 1 ) ; h ) − − y ( t n + k ) .

{\displaystyle \delta _{n+k}^{h}=\Psi \left(t_{n+k};y(t_{n}),y(t_{n+1}),\dots ,y(t_{n+k-1});h\right)-y(t_{n+k}).} The method is said to be consistent if lim h → → 0 δ δ n + k h h = 0.

{\displaystyle \lim _{h\to 0}{\frac {\delta _{n+k}^{h}}{h}}=0.} The method has order p {\displaystyle p} if δ δ n + k h = O ( h p + 1 ) as h → → 0.

{\displaystyle \delta _{n+k}^{h}=O(h^{p+1})\quad {\mbox{as }}h\to 0.} Hence a method is consistent if it has an order greater than 0. The (forward) Euler method (4) and the backward Euler method (6) introduced above both have order 1, so they are consistent. Most methods being used in practice attain higher order. Consistency is a necessary condition for convergence [ citation needed ] , but not sufficient; for a method to be convergent, it must be both consistent and zero-stable .

A related concept is the global (truncation) error , the error sustained in all the steps one needs to reach a fixed time t {\displaystyle t} . Explicitly, the global error at time t {\displaystyle t} is y N − − y ( t ) {\displaystyle y_{N}-y(t)} where N = ( t − − t 0 ) / h {\displaystyle N=(t-t_{0})/h} . The global error of a p {\displaystyle p} th order one-step method is O ( h p ) {\displaystyle O(h^{p})} ; in particular, such a method is convergent.  This statement is not necessarily true for multi-step methods.

Stability and stiffness [ edit ] Further information: Stiff equation For some differential equations, application of standard methods—such as the Euler method, explicit Runge–Kutta methods , or multistep methods (for example, Adams–Bashforth methods)—exhibit instability in the solutions, though other methods may produce stable solutions. This "difficult behaviour" in the equation (which may not necessarily be complex itself) is described as stiffness , and is often caused by the presence of different time scales in the underlying problem.

[ 23 ] For example, a collision in a mechanical system like in an impact oscillator typically occurs at much smaller time scale than the time for the motion of objects; this discrepancy makes for very "sharp turns" in the curves of the state parameters.

Stiff problems are ubiquitous in chemical kinetics , control theory , solid mechanics , weather forecasting , biology , plasma physics , and electronics . One way to overcome stiffness is to extend the notion of differential equation to that of differential inclusion , which allows for and models non-smoothness.

[ 24 ] [ 25 ] History [ edit ] Below is a timeline of some important developments in this field.

[ 26 ] [ 27 ] 1768 - Leonhard Euler publishes his method.

1824 - Augustin Louis Cauchy proves convergence of the Euler method. In this proof, Cauchy uses the implicit Euler method.

1855 - First mention of the multistep methods of John Couch Adams in a letter written by Francis Bashforth .

1895 - Carl Runge publishes the first Runge–Kutta method .

1901 - Martin Kutta describes the popular fourth-order Runge–Kutta method .

1910 - Lewis Fry Richardson announces his extrapolation method , Richardson extrapolation .

1952 - Charles F. Curtiss and Joseph Oakland Hirschfelder coin the term stiff equations .

1963 - Germund Dahlquist introduces A-stability of integration methods.

Numerical solutions to second-order one-dimensional boundary value problems [ edit ] Boundary value problems (BVPs) are usually solved numerically by solving an approximately equivalent matrix problem obtained by discretizing the original BVP.

[ 28 ] The most commonly used method for numerically solving BVPs in one dimension is called the Finite Difference Method .

[ 3 ] This method takes advantage of linear combinations of point values to construct finite difference coefficients that describe derivatives of the function. For example, the second-order central difference approximation to the first derivative is given by: u i + 1 − − u i − − 1 2 h = u ′ ( x i ) + O ( h 2 ) , {\displaystyle {\frac {u_{i+1}-u_{i-1}}{2h}}=u'(x_{i})+{\mathcal {O}}(h^{2}),} and the second-order central difference for the second derivative is given by: u i + 1 − − 2 u i + u i − − 1 h 2 = u ″ ( x i ) + O ( h 2 ) .

{\displaystyle {\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}=u''(x_{i})+{\mathcal {O}}(h^{2}).} In both of these formulae, h = x i − − x i − − 1 {\displaystyle h=x_{i}-x_{i-1}} is the distance between neighbouring x values on the discretized domain. One then constructs a linear system that can then be solved by standard matrix methods . For example, suppose the equation to be solved is: d 2 u d x 2 − − u = 0 , u ( 0 ) = 0 , u ( 1 ) = 1.

{\displaystyle {\begin{aligned}&{}{\frac {d^{2}u}{dx^{2}}}-u=0,\\&{}u(0)=0,\\&{}u(1)=1.\end{aligned}}} The next step would be to discretize the problem and use linear derivative approximations such as u i ″ = u i + 1 − − 2 u i + u i − − 1 h 2 {\displaystyle u''_{i}={\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}} and solve the resulting system of linear equations. This would lead to equations such as: u i + 1 − − 2 u i + u i − − 1 h 2 − − u i = 0 , ∀ ∀ i = 1 , 2 , 3 , .

.

.

, n − − 1 .

{\displaystyle {\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}-u_{i}=0,\quad \forall i={1,2,3,...,n-1}.} On first viewing, this system of equations appears to have difficulty associated with the fact that the equation involves no terms that are not multiplied by variables, but in fact this is false. At i = 1 and n − 1 there is a term involving the boundary values u ( 0 ) = u 0 {\displaystyle u(0)=u_{0}} and u ( 1 ) = u n {\displaystyle u(1)=u_{n}} and since these two values are known, one can simply substitute them into this equation and as a result have a non-homogeneous system of linear equations that has non-trivial solutions.

See also [ edit ] Courant–Friedrichs–Lewy condition Energy drift General linear methods List of numerical analysis topics#Numerical methods for ordinary differential equations Reversible reference system propagation algorithm Modelica Language and OpenModelica software Notes [ edit ] ^ Chicone, C. (2006). Ordinary differential equations with applications (Vol. 34). Springer Science & Business Media.

^ Bradie (2006 , pp. 533–655) ^ a b LeVeque, R. J. (2007). Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems (Vol. 98). SIAM.

^ Slimane Adjerid and Mahboub Baccouch (2010) Galerkin methods. Scholarpedia, 5(10):10056.

^ Griffiths, D. F., & Higham, D. J. (2010). Numerical methods for ordinary differential equations: initial value problems. Springer Science & Business Media.

^ Hairer, Nørsett & Wanner (1993 , pp. 204–215) ^ Alexander, R. (1977). Diagonally implicit Runge–Kutta methods for stiff ODE’s. SIAM Journal on Numerical Analysis, 14(6), 1006-1021.

^ Cash, J. R. (1979). Diagonally implicit Runge-Kutta formulae with error estimates. IMA Journal of Applied Mathematics, 24(3), 293-301.

^ Ferracina, L., & Spijker, M. N. (2008). Strong stability of singly-diagonally-implicit Runge–Kutta methods. Applied Numerical Mathematics, 58(11), 1675-1686.

^ Everhart, E. (1985). An efficient integrator that uses Gauss-Radau spacings. In International Astronomical Union Colloquium (Vol. 83, pp. 185–202). Cambridge University Press.

^ Weisstein, Eric W. "Gaussian Quadrature." From MathWorld--A Wolfram Web Resource.

https://mathworld.wolfram.com/GaussianQuadrature.html ^ Butcher, J. C. (1987). The numerical analysis of ordinary differential equations: Runge-Kutta and general linear methods. Wiley-Interscience.

^ Hochbruck & Ostermann (2010 , pp. 209–286) This is a modern and extensive review paper for exponential integrators ^ Brezinski, C., & Zaglia, M. R. (2013). Extrapolation methods: theory and practice. Elsevier.

^ Monroe, J. L. (2002). Extrapolation and the Bulirsch-Stoer algorithm. Physical Review E, 65(6), 066116.

^ Kirpekar, S. (2003). Implementation of the Bulirsch Stoer extrapolation method. Department of Mechanical Engineering, UC Berkeley/California.

^ Nurminskii, E. A., & Buryi, A. A. (2011). Parker-Sochacki method for solving systems of ordinary differential equations using graphics processors. Numerical Analysis and Applications, 4(3), 223.

^ Hairer, E., Lubich, C., & Wanner, G. (2006). Geometric numerical integration: structure-preserving algorithms for ordinary differential equations (Vol. 31). Springer Science & Business Media.

^ Hairer, E., Lubich, C., & Wanner, G. (2003). Geometric numerical integration illustrated by the Störmer–Verlet method. Acta Numerica, 12, 399-450.

^ Nievergelt, Jürg (1964).

"Parallel methods for integrating ordinary differential equations" .

Communications of the ACM .

7 (12): 731– 733.

doi : 10.1145/355588.365137 .

S2CID 6361754 .

^ "Parallel-in-Time.org" .

Parallel-in-Time.org . Retrieved 15 November 2023 .

^ Higham, N. J. (2002). Accuracy and stability of numerical algorithms (Vol. 80). SIAM.

^ Miranker, A. (2001). Numerical Methods for Stiff Equations and Singular Perturbation Problems: and singular perturbation problems (Vol. 5). Springer Science & Business Media.

^ Markus Kunze; Tassilo Kupper (2001). "Non-smooth Dynamical Systems: An Overview". In Bernold Fiedler (ed.).

Ergodic Theory, Analysis, and Efficient Simulation of Dynamical Systems . Springer Science & Business Media. p. 431.

ISBN 978-3-540-41290-8 .

^ Thao Dang (2011). "Model-Based Testing of Hybrid Systems". In Justyna Zander, Ina Schieferdecker and Pieter J. Mosterman (ed.).

Model-Based Testing for Embedded Systems . CRC Press. p. 411.

ISBN 978-1-4398-1845-9 .

^ Brezinski, C., & Wuytack, L. (2012). Numerical analysis: Historical developments in the 20th century. Elsevier.

^ Butcher, J. C. (1996). A history of Runge-Kutta methods. Applied numerical mathematics, 20(3), 247-260.

^ Ascher, U. M., Mattheij, R. M., & Russell, R. D. (1995). Numerical solution of boundary value problems for ordinary differential equations. Society for Industrial and Applied Mathematics.

References [ edit ] Bradie, Brian (2006).

A Friendly Introduction to Numerical Analysis . Upper Saddle River, New Jersey: Pearson Prentice Hall.

ISBN 978-0-13-013054-9 .

J. C. Butcher , Numerical methods for ordinary differential equations , ISBN 0-471-96758-0 Hairer, E.; Nørsett, S. P.; Wanner, G. (1993).

Solving Ordinary Differential Equations. I. Nonstiff Problems . Springer Series in Computational Mathematics. Vol. 8 (2nd ed.). Springer-Verlag, Berlin.

ISBN 3-540-56670-8 .

MR 1227985 .

Ernst Hairer and Gerhard Wanner, Solving ordinary differential equations II: Stiff and differential-algebraic problems, second edition, Springer Verlag, Berlin, 1996.

ISBN 3-540-60452-9 .

(This two-volume monograph systematically covers all aspects of the field.) Hochbruck, Marlis ; Ostermann, Alexander (May 2010). "Exponential integrators".

Acta Numerica .

19 : 209– 286.

Bibcode : 2010AcNum..19..209H .

CiteSeerX 10.1.1.187.6794 .

doi : 10.1017/S0962492910000048 .

S2CID 4841957 .

Arieh Iserles, A First Course in the Numerical Analysis of Differential Equations, Cambridge University Press, 1996.

ISBN 0-521-55376-8 (hardback), ISBN 0-521-55655-4 (paperback).

(Textbook, targeting advanced undergraduate and postgraduate students in mathematics, which also discusses numerical partial differential equations .) John Denholm Lambert, Numerical Methods for Ordinary Differential Systems, John Wiley & Sons, Chichester, 1991.

ISBN 0-471-92990-5 .

(Textbook, slightly more demanding than the book by Iserles.) External links [ edit ] Joseph W. Rudmin, Application of the Parker–Sochacki Method to Celestial Mechanics Archived 2016-05-16 at the Portuguese Web Archive , 1998.

Dominique Tournès, L'intégration approchée des équations différentielles ordinaires (1671–1914) , thèse de doctorat de l'université Paris 7 - Denis Diderot, juin 1996. Réimp. Villeneuve d'Ascq : Presses universitaires du Septentrion, 1997, 468 p. (Extensive online material on ODE numerical analysis history, for English-language material on the history of ODE numerical analysis, see, for example, the paper books by Chabert and Goldstine quoted by him.) Pchelintsev, A.N. (2020). "An accurate numerical method and algorithm for constructing solutions of chaotic systems".

Journal of Applied Nonlinear Dynamics .

9 (2): 207– 221.

arXiv : 2011.10664 .

doi : 10.5890/JAND.2020.06.004 .

S2CID 225853788 .

kv on GitHub ( C++ library with rigorous ODE solvers) INTLAB (A library made by MATLAB / GNU Octave which includes rigorous ODE solvers) v t e Numerical methods for ordinary differential equations First-order methods Euler method Backward Euler Semi-implicit Euler Exponential Euler Second-order methods Verlet integration Velocity Verlet Trapezoidal rule Beeman's algorithm Midpoint method Heun's method Newmark-beta method Leapfrog integration Higher-order methods Exponential integrator Runge–Kutta methods List of Runge–Kutta methods Linear multistep method General linear methods Backward differentiation formula Yoshida Gauss–Legendre method Theory Symplectic integrator Related Numerical methods for partial differential equations Numerical integration v t e Industrial and applied mathematics Computational Algorithms design analysis Automata theory Automated theorem proving Coding theory Computational geometry Constraint satisfaction Constraint programming Computational logic Cryptography Information theory Statistics Mathematical software Arbitrary-precision arithmetic Finite element analysis Tensor software Interactive geometry software Optimization software Statistical software Numerical-analysis software Numerical libraries Solvers Discrete Computer algebra Computational number theory Combinatorics Graph theory Discrete geometry Analysis Approximation theory Clifford analysis Clifford algebra Differential equations Ordinary differential equations Partial differential equations Stochastic differential equations Differential geometry Differential forms Gauge theory Geometric analysis Dynamical systems Chaos theory Control theory Functional analysis Operator algebra Operator theory Harmonic analysis Fourier analysis Multilinear algebra Exterior Geometric Tensor Vector Multivariable calculus Exterior Geometric Tensor Vector Numerical analysis Numerical linear algebra Numerical methods for ordinary differential equations Numerical methods for partial differential equations Validated numerics Variational calculus Probability theory Distributions ( random variables ) Stochastic processes / analysis Path integral Stochastic variational calculus Mathematical physics Analytical mechanics Lagrangian Hamiltonian Field theory Classical Conformal Effective Gauge Quantum Statistical Topological Perturbation theory in quantum mechanics Potential theory String theory Bosonic Topological Supersymmetry Supersymmetric quantum mechanics Supersymmetric theory of stochastic dynamics Algebraic structures Algebra of physical space Feynman integral Poisson algebra Quantum group Renormalization group Representation theory Spacetime algebra Superalgebra Supersymmetry algebra Decision sciences Game theory Operations research Optimization Social choice theory Statistics Mathematical economics Mathematical finance Other applications Biology Chemistry Psychology Sociology " The Unreasonable Effectiveness of Mathematics in the Natural Sciences " Related Mathematics Organizations Society for Industrial and Applied Mathematics Japan Society for Industrial and Applied Mathematics Société de Mathématiques Appliquées et Industrielles International Council for Industrial and Applied Mathematics European Community on Computational Methods in Applied Sciences Category Mathematics portal / outline / topics list NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐lddt5
Cached time: 20250811235651
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.559 seconds
Real time usage: 0.804 seconds
Preprocessor visited node count: 2563/1000000
Revision size: 28382/2097152 bytes
Post‐expand include size: 71355/2097152 bytes
Template argument size: 3631/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 87027/5000000 bytes
Lua time usage: 0.276/10.000 seconds
Lua memory usage: 7792622/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  561.979      1 -total
 28.40%  159.611      1 Template:Reflist
 17.59%   98.861      3 Template:Cite_journal
 16.81%   94.459      4 Template:Navbox
 15.51%   87.145      1 Template:Short_description
 14.11%   79.285      1 Template:Numerical_integrators
  8.41%   47.237      2 Template:Pagetype
  8.24%   46.288      8 Template:NumBlk
  7.10%   39.889      3 Template:Harvtxt
  6.28%   35.290      1 Template:Citation_needed Saved in parser cache with key enwiki:pcache:272829:|#|:idhash:canonical and timestamp 20250811235651 and revision id 1272123190. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Numerical_methods_for_ordinary_differential_equations&oldid=1272123190 " Categories : Numerical differential equations Ordinary differential equations Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from September 2019 Webarchive template other archives This page was last edited on 27 January 2025, at 07:09 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Numerical methods for ordinary differential equations 12 languages Add topic

