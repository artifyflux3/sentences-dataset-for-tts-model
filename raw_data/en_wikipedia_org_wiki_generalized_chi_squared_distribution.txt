Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Parameter conversions 3 Support and tails 4 Computing the PDF/CDF/inverse CDF/random numbers 5 Applications Toggle Applications subsection 5.1 In model fitting and selection 5.2 Classifying normal vectors using Gaussian discriminant analysis 5.3 In signal processing 6 See also 7 References 8 External links Toggle the table of contents Generalized chi-squared distribution Add languages Add links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Kind of probability distribution Generalized chi-squared distribution Probability density function Cumulative distribution function Notation χ χ ~ ~ ( w , k , λ λ , s , m ) {\displaystyle {\tilde {\chi }}({\boldsymbol {w}},{\boldsymbol {k}},{\boldsymbol {\lambda }},s,m)} Parameters w {\displaystyle {\boldsymbol {w}}} , vector of weights of noncentral chi-square components k {\displaystyle {\boldsymbol {k}}} , vector of degrees of freedom of noncentral chi-square components λ λ {\displaystyle {\boldsymbol {\lambda }}} , vector of non-centrality parameters of chi-square components s {\displaystyle s} , scale of normal term m {\displaystyle m} , offset Support x ∈ ∈ { [ m , + ∞ ∞ ) if w i ≥ ≥ 0 , s = 0 , ( − − ∞ ∞ , m ] if w i ≤ ≤ 0 , s = 0 , R otherwise.

{\displaystyle x\in {\begin{cases}[m,+\infty ){\text{ if }}w_{i}\geq 0,s=0,\\(-\infty ,m]{\text{ if }}w_{i}\leq 0,s=0,\\\mathbb {R} {\text{ otherwise.}}\end{cases}}} PDF no closed-form expression CDF no closed-form expression Mean ∑ ∑ j w j ( k j + λ λ j ) + m {\displaystyle \sum _{j}w_{j}(k_{j}+\lambda _{j})+m} Variance 2 ∑ ∑ j w j 2 ( k j + 2 λ λ j ) + s 2 {\displaystyle 2\sum _{j}w_{j}^{2}(k_{j}+2\lambda _{j})+s^{2}} MGF exp ⁡ ⁡ [ t ( m + ∑ ∑ j w j λ λ j 1 − − 2 w j t ) + s 2 t 2 2 ] ∏ ∏ j ( 1 − − 2 w j t ) k j / 2 {\displaystyle {\frac {\exp \left[t\left(m+\sum _{j}{\frac {w_{j}\lambda _{j}}{1-2w_{j}t}}\right)+{\frac {s^{2}t^{2}}{2}}\right]}{\prod _{j}\left(1-2w_{j}t\right)^{k_{j}/2}}}} CF exp ⁡ ⁡ [ i t ( m + ∑ ∑ j w j λ λ j 1 − − 2 i w j t ) − − s 2 t 2 2 ] ∏ ∏ j ( 1 − − 2 i w j t ) k j / 2 {\displaystyle {\frac {\exp \left[it\left(m+\sum _{j}{\frac {w_{j}\lambda _{j}}{1-2iw_{j}t}}\right)-{\frac {s^{2}t^{2}}{2}}\right]}{\prod _{j}\left(1-2iw_{j}t\right)^{k_{j}/2}}}} In probability theory and statistics , the generalized chi-squared distribution (or generalized chi-square distribution ) is the distribution of a quadratic function of a multinormal variable (normal vector) , or a linear combination of different normal variables and squares of normal variables. Equivalently, it is also a linear sum of independent noncentral chi-square variables and a normal variable . There are several other such generalizations for which the same term is sometimes used; some of them are special cases of the family discussed here, for example the gamma distribution .

Definition [ edit ] The generalized chi-squared variable may be described in multiple ways. One is to write it as a weighted sum of independent noncentral chi-square variables χ χ ′ 2 {\displaystyle {{\chi }'}^{2}} and a standard normal variable z {\displaystyle z} : [ 1 ] [ 2 ] [ 3 ] [ 4 ] χ χ ~ ~ ( w , k , λ λ , s , m ) = ∑ ∑ i w i χ χ ′ 2 ( k i , λ λ i ) + s z + m .

{\displaystyle {\tilde {\chi }}({\boldsymbol {w}},{\boldsymbol {k}},{\boldsymbol {\lambda }},s,m)=\sum _{i}w_{i}{{\chi }'}^{2}(k_{i},\lambda _{i})+sz+m.} Here the parameters are the weights w i {\displaystyle w_{i}} , the degrees of freedom k i {\displaystyle k_{i}} and non-centralities λ λ i {\displaystyle \lambda _{i}} of the constituent non-central chi-squares, and the coefficients s {\displaystyle s} and m {\displaystyle m} of the normal. Some important special cases of this have all weights w i {\displaystyle w_{i}} of the same sign, or have central chi-squared components, or omit the normal term.

Since a non-central chi-squared variable is a sum of squares of normal variables with different means, the generalized chi-square variable is also defined as a sum of squares of independent normal variables, plus an independent normal variable: that is, a quadratic in normal variables.

Another equivalent way is to formulate it as a quadratic form of a normal vector x {\displaystyle {\boldsymbol {x}}} : [ 5 ] [ 6 ] [ 4 ] [ 3 ] χ χ ~ ~ = q ( x ) = x ′ Q 2 x + q 1 ′ x + q 0 {\displaystyle {\tilde {\chi }}=q({\boldsymbol {x}})={\boldsymbol {x}}'\mathbf {Q_{2}} {\boldsymbol {x}}+{\boldsymbol {q_{1}}}'{\boldsymbol {x}}+q_{0}} .

Here Q 2 {\displaystyle \mathbf {Q_{2}} } is a matrix, q 1 {\displaystyle {\boldsymbol {q_{1}}}} is a vector, and q 0 {\displaystyle q_{0}} is a scalar. These, together with the mean μ μ {\displaystyle {\boldsymbol {\mu }}} and covariance matrix Σ Σ {\displaystyle \mathbf {\Sigma } } of the normal vector x {\displaystyle {\boldsymbol {x}}} , parameterize the distribution.

For the most general case, a reduction towards a common standard form can be made by using a representation of the following form: [ 7 ] X = ( z + a ) T A ( z + a ) + c T z = ( x + b ) T D ( x + b ) + d T x + e , {\displaystyle X=(z+a)^{\mathrm {T} }A(z+a)+c^{\mathrm {T} }z=(x+b)^{\mathrm {T} }D(x+b)+d^{\mathrm {T} }x+e,} where D is a diagonal matrix and where x represents a vector of uncorrelated standard normal random variables.

Parameter conversions [ edit ] A generalized chi-square variable or distribution can be parameterized in two ways. The first is in terms of the weights w i {\displaystyle w_{i}} , the degrees of freedom k i {\displaystyle k_{i}} and non-centralities λ λ i {\displaystyle \lambda _{i}} of the constituent non-central chi-squares, and the coefficients s {\displaystyle s} and m {\displaystyle m} of the added normal term. The second parameterization is using the quadratic form of a normal vector, where the paremeters are the matrix Q 2 {\displaystyle \mathbf {Q_{2}} } , the vector q 1 {\displaystyle {\boldsymbol {q_{1}}}} , and the scalar q 0 {\displaystyle q_{0}} , and the mean μ μ {\displaystyle {\boldsymbol {\mu }}} and covariance matrix Σ Σ {\displaystyle \mathbf {\Sigma } } of the normal vector.

The parameters of the first expression (in terms of non-central chi-squares, a normal and a constant) can be calculated in terms of the parameters of the second expression (quadratic form of a normal vector).

[ 6 ] The parameters of the second expression (quadratic form of a normal vector) can also be calculated in terms of the parameters of the first expression (in terms of non-central chi-squares, a normal and a constant).

[ 4 ] There exists Matlab code to convert from one set of parameters to another.

Support and tails [ edit ] When s = 0 {\displaystyle s=0} and w i {\displaystyle w_{i}} are all positive or negative, the quadratic is an ellipse. Then the distribution starts from the point m {\displaystyle m} at one end, which is called a finite tail. The other end tails off at + or − − ∞ ∞ {\displaystyle -\infty } respectively, which is called an infinite tail. When w i {\displaystyle w_{i}} have mixed signs, and/or there is a normal s {\displaystyle s} term, both tails are infinite, and the support is the entire real line.
The methods to compute the CDF and PDF of the distribution behave differently in finite vs. infinite tails (see table below for best method to use in each case).

[ 4 ] [ 3 ] Computing the PDF/CDF/inverse CDF/random numbers [ edit ] The probability density, cumulative distribution, and inverse cumulative distribution functions of a generalized chi-squared variable do not have simple closed-form expressions. But there exist several methods to compute them numerically: Ruben's method, [ 8 ] Imhof's method, [ 9 ] IFFT method, [ 4 ] [ 3 ] ray method, [ 4 ] [ 3 ] ellipse approximation, [ 4 ] [ 3 ] infinite-tail approximation, [ 4 ] [ 3 ] Pearson and extended Pearson's approximations [ 4 ] [ 3 ] and Liu-Tang-Zhang approximation.

Numerical algorithms [ 7 ] [ 2 ] [ 9 ] [ 6 ] [ 4 ] [ 3 ] and computer code ( Fortran and C , Matlab , R , Python , Julia ) have been published that implement some of these methods to compute the PDF, CDF, and inverse CDF, and to generate random numbers.

The following table shows the best methods to use to compute the CDF and PDF for the different parts of the generalized chi-square distribution in different cases.

[ 4 ] [ 3 ] 'Tail' refers to the infinite-tail approximation.

χ χ ~ ~ {\displaystyle {\tilde {\chi }}} type part best cdf/pdf method(s) ellipse: w i {\displaystyle w_{i}} same sign, s = 0 {\displaystyle s=0} body Ruben, Imhof, IFFT, ray finite tail Ruben, ray (if λ λ i = 0 {\displaystyle \lambda _{i}=0} ), ellipse infinite tail Ruben, ray, tail not ellipse: w i {\displaystyle w_{i}} mixed signs, and/or s ≠ ≠ 0 {\displaystyle s\neq 0} body Imhof, IFFT, ray infinite tails ray, tail sphere: non-central χ χ 2 {\displaystyle \chi ^{2}} (only one term) body Matlab's ncx2cdf / ncx2pdf finite tail ncx2cdf / ncx2pdf , ellipse infinite tail ncx2pdf , ray, tail Applications [ edit ] The generalized chi-squared is the distribution of statistical estimates in cases where the usual statistical theory does not hold, as in the examples below.

In model fitting and selection [ edit ] If a predictive model is fitted by least squares , but the residuals have either autocorrelation or heteroscedasticity , then alternative models can be compared (in model selection ) by relating changes in the sum of squares to an asymptotically valid generalized chi-squared distribution.

[ 5 ] Classifying normal vectors using Gaussian discriminant analysis [ edit ] If x {\displaystyle {\boldsymbol {x}}} is a normal vector, its log likelihood is a quadratic form of x {\displaystyle {\boldsymbol {x}}} , and is hence distributed as a generalized chi-squared. The log likelihood ratio that x {\displaystyle {\boldsymbol {x}}} arises from one normal distribution versus another is also a quadratic form , so distributed as a generalized chi-squared.

[ 6 ] In Gaussian discriminant analysis, samples from multinormal distributions are optimally separated by using a quadratic classifier , a boundary that is a quadratic function (e.g. the curve defined by setting the likelihood ratio between two Gaussians to 1). The classification error rates of different types (false positives and false negatives) are integrals of the normal distributions within the quadratic regions defined by this classifier. Since this is mathematically equivalent to integrating a quadratic form of a normal vector, the result is an integral of a generalized-chi-squared variable.

[ 6 ] In signal processing [ edit ] The following application arises in the context of Fourier analysis in signal processing , renewal theory in probability theory , and multi-antenna systems in wireless communication . The common factor of these areas is that the sum of exponentially distributed variables is of importance (or identically, the sum of squared magnitudes of circularly-symmetric centered complex Gaussian variables).

If Z i {\displaystyle Z_{i}} are k independent , circularly-symmetric centered complex Gaussian random variables with mean 0 and variance σ σ i 2 {\displaystyle \sigma _{i}^{2}} , then the random variable Q ~ ~ = ∑ ∑ i = 1 k | Z i | 2 {\displaystyle {\tilde {Q}}=\sum _{i=1}^{k}|Z_{i}|^{2}} has a generalized chi-squared distribution of a particular form. The difference from the standard chi-squared distribution is that Z i {\displaystyle Z_{i}} are complex and can have different variances, and the difference from the more general generalized chi-squared distribution is that the relevant scaling matrix A is diagonal. If μ μ = σ σ i 2 {\displaystyle \mu =\sigma _{i}^{2}} for all i , then Q ~ ~ {\displaystyle {\tilde {Q}}} , scaled down by μ μ / 2 {\displaystyle \mu /2} (i.e. multiplied by 2 / μ μ {\displaystyle 2/\mu } ), has a chi-squared distribution , χ χ 2 ( 2 k ) {\displaystyle \chi ^{2}(2k)} , also known as an Erlang distribution . If σ σ i 2 {\displaystyle \sigma _{i}^{2}} have distinct values for all i , then Q ~ ~ {\displaystyle {\tilde {Q}}} has the pdf [ 10 ] f ( x ; k , σ σ 1 2 , … … , σ σ k 2 ) = ∑ ∑ i = 1 k e − − x σ σ i 2 σ σ i 2 ∏ ∏ j = 1 , j ≠ ≠ i k ( 1 − − σ σ j 2 σ σ i 2 ) for x ≥ ≥ 0.

{\displaystyle f(x;k,\sigma _{1}^{2},\ldots ,\sigma _{k}^{2})=\sum _{i=1}^{k}{\frac {e^{-{\frac {x}{\sigma _{i}^{2}}}}}{\sigma _{i}^{2}\prod _{j=1,j\neq i}^{k}\left(1-{\frac {\sigma _{j}^{2}}{\sigma _{i}^{2}}}\right)}}\quad {\text{for }}x\geq 0.} If there are sets of repeated variances among σ σ i 2 {\displaystyle \sigma _{i}^{2}} , assume that they are divided into M sets, each representing a certain variance value.  Denote r = ( r 1 , r 2 , … … , r M ) {\displaystyle \mathbf {r} =(r_{1},r_{2},\dots ,r_{M})} to be the number of repetitions in each group. That is, the m th set contains r m {\displaystyle r_{m}} variables that have variance σ σ m 2 .

{\displaystyle \sigma _{m}^{2}.} It represents an arbitrary linear combination of independent χ χ 2 {\displaystyle \chi ^{2}} -distributed random variables with different degrees of freedom: Q ~ ~ = ∑ ∑ m = 1 M σ σ m 2 / 2 ∗ ∗ Q m , Q m ∼ ∼ χ χ 2 ( 2 r m ) .

{\displaystyle {\tilde {Q}}=\sum _{m=1}^{M}\sigma _{m}^{2}/2*Q_{m},\quad Q_{m}\sim \chi ^{2}(2r_{m})\,.} The pdf of Q ~ ~ {\displaystyle {\tilde {Q}}} is [ 11 ] f ( x ; r , σ σ 1 2 , … … σ σ M 2 ) = ∏ ∏ m = 1 M 1 σ σ m 2 r m ∑ ∑ k = 1 M ∑ ∑ l = 1 r k Ψ Ψ k , l , r ( r k − − l ) !

( − − x ) r k − − l e − − x σ σ k 2 , for x ≥ ≥ 0 , {\displaystyle f(x;\mathbf {r} ,\sigma _{1}^{2},\dots \sigma _{M}^{2})=\prod _{m=1}^{M}{\frac {1}{\sigma _{m}^{2r_{m}}}}\sum _{k=1}^{M}\sum _{l=1}^{r_{k}}{\frac {\Psi _{k,l,\mathbf {r} }}{(r_{k}-l)!}}(-x)^{r_{k}-l}e^{-{\frac {x}{\sigma _{k}^{2}}}},\quad {\text{ for }}x\geq 0,} where Ψ Ψ k , l , r = ( − − 1 ) r k − − 1 ∑ ∑ i ∈ ∈ Ω Ω k , l ∏ ∏ j ≠ ≠ k ( i j + r j − − 1 i j ) ( 1 σ σ j 2 − − 1 σ σ k 2 ) − − ( r j + i j ) , {\displaystyle \Psi _{k,l,\mathbf {r} }=(-1)^{r_{k}-1}\sum _{\mathbf {i} \in \Omega _{k,l}}\prod _{j\neq k}{\binom {i_{j}+r_{j}-1}{i_{j}}}\left({\frac {1}{\sigma _{j}^{2}}}\!-\!{\frac {1}{\sigma _{k}^{2}}}\right)^{-(r_{j}+i_{j})},} with i = [ i 1 , … … , i M ] T {\displaystyle \mathbf {i} =[i_{1},\ldots ,i_{M}]^{T}} from the set Ω Ω k , l {\displaystyle \Omega _{k,l}} of
all partitions of l − − 1 {\displaystyle l-1} (with i k = 0 {\displaystyle i_{k}=0} ) defined as Ω Ω k , l = { [ i 1 , … … , i m ] ∈ ∈ Z m ; ∑ ∑ j = 1 M i j = l − − 1 , i k = 0 , i j ≥ ≥ 0 for all j } .

{\displaystyle \Omega _{k,l}=\left\{[i_{1},\ldots ,i_{m}]\in \mathbb {Z} ^{m};\sum _{j=1}^{M}i_{j}\!=l-1,i_{k}=0,i_{j}\geq 0{\text{ for all }}j\right\}.} See also [ edit ] Noncentral chi-squared distribution Chi-squared distribution References [ edit ] ^ Davies, R. B. (1973). "Numerical inversion of a characteristic function".

Biometrika .

60 (2): 415– 417.

doi : 10.1093/biomet/60.2.415 .

^ a b Davies, R. B. (1980). "Algorithm AS155: The distribution of a linear combination of χ 2 random variables".

Journal of the Royal Statistical Society . Series C (Applied Statistics).

29 (3): 323– 333.

doi : 10.2307/2346911 .

JSTOR 2346911 .

^ a b c d e f g h i j Das, Abhranil (2025). "New methods to compute the generalized chi-square distribution".

Journal of Statistical Computation and Simulation . Taylor & Francis: 1– 36.

^ a b c d e f g h i j k Das, Abhranil (2024). "New methods to compute the generalized chi-square distribution".

arXiv : 2404.05062 [ stat.CO ].

^ a b Jones, D. A. (1983). "Statistical analysis of empirical models fitted by optimisation".

Biometrika .

70 (1): 67– 88.

doi : 10.1093/biomet/70.1.67 .

^ a b c d e Das, Abhranil; Wilson S Geisler (2020). "Methods to integrate multinormals and compute classification measures".

arXiv : 2012.14331 [ stat.ML ].

^ a b Sheil, J.; O'Muircheartaigh, I. (1977). "Algorithm AS106: The distribution of non-negative quadratic forms in normal variables".

Journal of the Royal Statistical Society . Series C (Applied Statistics).

26 (1): 92– 98.

doi : 10.2307/2346884 .

JSTOR 2346884 .

^ Ruben, Harold (1962). "Probability content of regions under spherical normal distributions, IV: The distribution of homogeneous and non-homogeneous quadratic functions of normal variables".

The Annals of Mathematical Statistics .

33 (2): 542-570.

doi : 10.1214/aoms/1177704580 .

^ a b Imhof, J. P. (1961).

"Computing the Distribution of Quadratic Forms in Normal Variables" (PDF) .

Biometrika .

48 (3/4): 419– 426.

doi : 10.2307/2332763 .

JSTOR 2332763 .

^ D. Hammarwall, M. Bengtsson, B. Ottersten (2008) "Acquiring Partial CSI for Spatially Selective Transmission by Instantaneous Channel Norm Feedback", IEEE Transactions on Signal Processing , 56, 1188–1204 ^ E. Björnson, D. Hammarwall, B. Ottersten (2009) "Exploiting Quantized Channel Norm Feedback through Conditional Statistics in Arbitrarily Correlated MIMO Systems" , IEEE Transactions on Signal Processing , 57, 4027–4041 External links [ edit ] Davies, R.B.: Fortran and C source code for "Linear combination of chi-squared random variables" Das, A: MATLAB code to compute the statistics, pdf, cdf, inverse cdf and random numbers of the generalized chi-square distribution.

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Generalized_chi-squared_distribution&oldid=1298592202 " Category : Continuous distributions Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 3 July 2025, at 12:26 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Generalized chi-squared distribution Add languages Add topic

