Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Definition Toggle Definition subsection 2.1 Classification of multiple hypothesis tests 3 Controlling procedures Toggle Controlling procedures subsection 3.1 Multiple testing correction 4 Large-scale multiple testing Toggle Large-scale multiple testing subsection 4.1 Assessing whether any alternative hypotheses are true 5 See also 6 References 7 Further reading Toggle the table of contents Multiple comparisons problem 12 languages Deutsch فارسی 한국어 עברית Nederlands 日本語 Polski Русский Svenska Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Multiple comparisons ) Statistical interpretation with many tests An example of coincidence produced by data dredging (uncorrected multiple comparisons) showing a correlation between the number of letters in a spelling bee's winning word and the number of people in the United States killed by venomous spiders. Given a large enough pool of variables for the same time period, it is possible to find a pair of graphs that show a spurious correlation .

Multiple comparisons , multiplicity or multiple testing problem occurs in statistics when one considers a set of statistical inferences simultaneously [ 1 ] or estimates a subset of parameters selected based on the observed values.

[ 2 ] The larger the number of inferences made, the more likely erroneous inferences become.  Several statistical techniques have been developed to address this problem, for example, by requiring a stricter significance threshold for individual comparisons, so as to compensate for the number of inferences being made. Methods for family-wise error rate give the probability of false positives resulting from the multiple comparisons problem.

History [ edit ] The problem of multiple comparisons received increased attention in the 1950s with the work of statisticians such as Tukey and Scheffé . Over the ensuing decades, many procedures were developed to address the problem. In 1996, the first international conference on multiple comparison procedures took place in Tel Aviv .

[ 3 ] This is an active research area with work being done by, for example Emmanuel Candès and Vladimir Vovk .

Definition [ edit ] Production of a small p-value by multiple testing.

30 samples of 10 dots of random color (blue or red) are observed. On each sample, a two-tailed binomial test of the null hypothesis that blue and red are equally probable is performed. The first row shows the possible p-values as a function of the number of blue and red dots in the sample.

Although the 30 samples were all simulated under the null, one of the resulting p-values is small enough to produce a false rejection at the typical level 0.05 in the absence of correction.

Multiple comparisons arise when a statistical analysis involves multiple simultaneous statistical tests, each of which has a potential to produce a "discovery". A stated confidence level generally applies only to each test considered individually, but often it is desirable to have a confidence level for the whole family of simultaneous tests.

[ 4 ] Failure to compensate for multiple comparisons can have important real-world consequences, as illustrated by the following examples: Suppose the treatment is a new way of teaching writing to students, and the control is the standard way of teaching writing.  Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on.  As more attributes are compared, it becomes increasingly likely that the treatment and control groups will appear to differ on at least one attribute due to random sampling error alone.

Suppose we consider the efficacy of a drug in terms of the reduction of any one of a number of disease symptoms.  As more symptoms are considered, it becomes increasingly likely that the drug will appear to be an improvement over existing drugs in terms of at least one symptom.

In both examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.

For example, if one test is performed at the 5% level and the corresponding null hypothesis is true, there is only a 5% risk of incorrectly rejecting the null hypothesis.  However, if 100 tests are each conducted at the 5% level and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors ) is 5.  If the tests are statistically independent from each other (i.e. are performed on independent samples), the probability of at least one incorrect rejection is approximately 99.4%.

The multiple comparisons problem also applies to confidence intervals . A single confidence interval with a 95% coverage probability level will contain the true value of the parameter in 95% of samples.  However, if one considers 100 confidence intervals simultaneously, each with 95% coverage probability, the expected number of non-covering intervals is 5. If the intervals are statistically independent from each other, the probability that at least one interval does not contain the population parameter is 99.4%.

Techniques have been developed to prevent the inflation of false positive rates and non-coverage rates that occur with multiple statistical tests.

Classification of multiple hypothesis tests [ edit ] The following table defines the possible outcomes when testing multiple null hypotheses. 
Suppose we have a number m of null hypotheses, denoted by: H 1 , H 2 , ..., H m .

Using a statistical test , we reject the null hypothesis if the test is declared significant. We do not reject the null hypothesis if the test is non-significant.
Summing each type of outcome over all H i yields the following random variables: Null hypothesis is true (H 0 ) Alternative hypothesis is true (H A ) Total Test is declared significant V S R Test is declared non-significant U T m − − R {\displaystyle m-R} Total m 0 {\displaystyle m_{0}} m − − m 0 {\displaystyle m-m_{0}} m m is the total number hypotheses tested m 0 {\displaystyle m_{0}} is the number of true null hypotheses , an unknown parameter m − − m 0 {\displaystyle m-m_{0}} is the number of true alternative hypotheses V is the number of false positives (Type I error) (also called "false discoveries") S is the number of true positives (also called "true discoveries") T is the number of false negatives (Type II error) U is the number of true negatives R = V + S {\displaystyle R=V+S} is the number of rejected null hypotheses (also called "discoveries", either true or false) In m hypothesis tests of which m 0 {\displaystyle m_{0}} are true null hypotheses, R is an observable random variable, and S , T , U , and V are unobservable random variables .

Controlling procedures [ edit ] Further information: Family-wise error rate § Controlling procedures See also: False coverage rate § Controlling procedures , and False discovery rate § Controlling procedures This graph was using the legacy Graph extension , which is no longer supported. It needs to be converted to the new Chart extension .

Probability that at least one null hypothesis is wrongly rejected, for α α per comparison = 0.05 {\displaystyle \alpha _{\text{per comparison}}=0.05} , as a function of the number of independent tests m {\displaystyle m} .

Multiple testing correction [ edit ] This section may need to be cleaned up.

It has been merged from Multiple testing correction .

Multiple testing correction refers to making statistical tests more stringent in order to counteract the problem of multiple testing.  The best known such adjustment is the Bonferroni correction , but other methods have been developed. Such methods are typically designed to control the family-wise error rate or the false discovery rate .

If m independent comparisons are performed, the family-wise error rate (FWER), is given by α α ¯ ¯ = 1 − − ( 1 − − α α { per comparison } ) m .

{\displaystyle {\bar {\alpha }}=1-\left(1-\alpha _{\{{\text{per comparison}}\}}\right)^{m}.} Hence, unless the tests are perfectly positively dependent (i.e., identical), α α ¯ ¯ {\displaystyle {\bar {\alpha }}} increases as the number of comparisons increases.
If we do not assume that the comparisons are independent, then we can still say: α α ¯ ¯ ≤ ≤ m ⋅ ⋅ α α { per comparison } , {\displaystyle {\bar {\alpha }}\leq m\cdot \alpha _{\{{\text{per comparison}}\}},} which follows from Boole's inequality . Example: 0.2649 = 1 − − ( 1 − − .05 ) 6 ≤ ≤ .05 × × 6 = 0.3 {\displaystyle 0.2649=1-(1-.05)^{6}\leq .05\times 6=0.3} There are different ways to assure that the family-wise error rate is at most α α {\displaystyle \alpha } . The most conservative method, which is free of dependence and distributional assumptions, is the Bonferroni correction α α { p e r c o m p a r i s o n } = α α / m {\displaystyle \alpha _{\mathrm {\{per\ comparison\}} }={\alpha }/m} .  A marginally less conservative correction can be obtained by solving the equation for the family-wise error rate of m {\displaystyle m} independent comparisons for α α { p e r c o m p a r i s o n } {\displaystyle \alpha _{\mathrm {\{per\ comparison\}} }} . This yields α α { per comparison } = 1 − − ( 1 − − α α ) 1 / m {\displaystyle \alpha _{\{{\text{per comparison}}\}}=1-{(1-{\alpha })}^{1/m}} , which is known as the Šidák correction . Another procedure is the Holm–Bonferroni method , which uniformly delivers more power than the simple Bonferroni correction, by testing only the lowest p-value ( i = 1 {\displaystyle i=1} ) against the strictest criterion, and the higher p-values ( i > 1 {\displaystyle i>1} ) against progressively less strict criteria.

[ 5 ] α α { p e r c o m p a r i s o n } = α α / ( m − − i + 1 ) {\displaystyle \alpha _{\mathrm {\{per\ comparison\}} }={\alpha }/(m-i+1)} .

For continuous problems, one can employ Bayesian logic to compute m {\displaystyle m} from the prior-to-posterior volume ratio. Continuous generalizations of the Bonferroni and Šidák correction are presented in.

[ 6 ] Large-scale multiple testing [ edit ] Traditional methods for multiple comparisons adjustments focus on correcting for modest numbers of comparisons, often in an analysis of variance .  A different set of techniques have been developed for "large-scale multiple testing", in which thousands or even greater numbers of tests are performed. For example, in genomics , when using technologies such as microarrays , expression levels of tens of thousands of genes can be measured, and genotypes for millions of genetic markers can be measured.  Particularly in the field of genetic association studies, there has been a serious problem with non-replication — a result being strongly statistically significant in one study but failing to be replicated in a follow-up study.  Such non-replication can have many causes, but it is widely considered that failure to fully account for the consequences of making multiple comparisons is one of the causes.

[ 7 ] It has been argued that advances in measurement and information technology have made it far easier to generate large datasets for exploratory analysis , often leading to the testing of large numbers of hypotheses with no prior basis for expecting many of the hypotheses to be true.  In this situation, very high false positive rates are expected unless multiple comparisons adjustments are made.

For large-scale testing problems where the goal is to provide definitive results, the family-wise error rate remains the most accepted parameter for ascribing significance levels to statistical tests.  Alternatively, if a study is viewed as exploratory, or if significant results can be easily re-tested in an independent study, control of the false discovery rate (FDR) [ 8 ] [ 9 ] [ 10 ] is often preferred.  The FDR, loosely defined as the expected proportion of false positives among all significant tests, allows researchers to identify a set of "candidate positives" that can be more rigorously evaluated in a follow-up study.

[ 11 ] The practice of trying many unadjusted comparisons in the hope of finding a significant one is a known problem, whether applied unintentionally or deliberately, is sometimes called " p-hacking ".

[ 12 ] [ 13 ] Assessing whether any alternative hypotheses are true [ edit ] A normal quantile plot for a simulated set of test statistics that have been standardized to be Z-scores under the null hypothesis. The departure of the upper tail of the distribution from the expected trend along the diagonal is due to the presence of substantially more large test statistic values than would be expected if all null hypotheses were true.  The red point corresponds to the fourth largest observed test statistic, which is 3.13, versus an expected value of 2.06.  The blue point corresponds to the fifth smallest test statistic, which is -1.75, versus an expected value of -1.96.  The graph suggests that it is unlikely that all the null hypotheses are true, and that most or all instances of a true alternative hypothesis result from deviations in the positive direction.

A basic question faced at the outset of analyzing a large set of testing results is whether there is evidence that any of the alternative hypotheses are true.  One simple meta-test that can be applied when it is assumed that the tests are independent of each other is to use the Poisson distribution as a model for the number of significant results at a given level α that would be found when all null hypotheses are true.

[ citation needed ] If the observed number of positives is substantially greater than what should be expected, this suggests that there are likely to be some true positives among the significant results.

For example, if 1000 independent tests are performed, each at level α = 0.05, we expect 0.05 × 1000 = 50 significant tests to occur when all null hypotheses are true.  Based on the Poisson distribution with mean 50, the probability of observing more than 61 significant tests is less than 0.05, so if more than 61 significant results are observed, it is very likely that some of them correspond to situations where the alternative hypothesis holds.  A drawback of this approach is that it overstates the evidence that some of the alternative hypotheses are true when the test statistics are positively correlated, which commonly occurs in practice.

[ citation needed ] . On the other hand, the approach remains valid even in the presence of correlation among the test statistics, as long as the Poisson distribution can be shown to provide a good approximation for the number of significant results. This scenario arises, for instance, when mining significant frequent itemsets from transactional datasets. Furthermore, a careful two stage analysis can bound the FDR at a pre-specified level.

[ 14 ] Another common approach that can be used in situations where the test statistics can be standardized to Z-scores is to make a normal quantile plot of the test statistics.  If the observed quantiles are markedly more dispersed than the normal quantiles, this suggests that some of the significant results may be true positives.

[ citation needed ] See also [ edit ] q -value Key concepts Family-wise error rate False positive rate False discovery rate (FDR) False coverage rate (FCR) Interval estimation Post-hoc analysis Experimentwise error rate Statistical hypothesis testing General methods of alpha adjustment for multiple comparisons Closed testing procedure Bonferroni correction Boole– Bonferroni bound Duncan's new multiple range test Holm–Bonferroni method Harmonic mean p-value procedure Benjamini–Hochberg procedure E-values Related concepts Testing hypotheses suggested by the data Texas sharpshooter fallacy Model selection Look-elsewhere effect Data dredging References [ edit ] ^ Miller, R.G. (1981).

Simultaneous Statistical Inference 2nd Ed . Springer Verlag New York.

ISBN 978-0-387-90548-8 .

^ Benjamini, Y. (2010). "Simultaneous and selective inference: Current successes and future challenges".

Biometrical Journal .

52 (6): 708– 721.

doi : 10.1002/bimj.200900299 .

PMID 21154895 .

S2CID 8806192 .

^ "Home" .

mcp-conference.org .

^ Kutner, Michael; Nachtsheim, Christopher; Neter, John ; Li, William (2005).

Applied Linear Statistical Models . McGraw-Hill Irwin. pp.

744 –745.

ISBN 9780072386882 .

^ Aickin, M; Gensler, H (May 1996).

"Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods" .

Am J Public Health .

86 (5): 726– 728.

doi : 10.2105/ajph.86.5.726 .

PMC 1380484 .

PMID 8629727 .

^ Bayer, Adrian E.; Seljak, Uroš (2020).

"The look-elsewhere effect from a unified Bayesian and frequentist perspective" .

Journal of Cosmology and Astroparticle Physics .

2020 (10): 009.

arXiv : 2007.13821 .

Bibcode : 2020JCAP...10..009B .

doi : 10.1088/1475-7516/2020/10/009 .

S2CID 220830693 .

^ Qu, Hui-Qi; Tien, Matthew; Polychronakos, Constantin (2010-10-01).

"Statistical significance in genetic association studies" .

Clinical and Investigative Medicine .

33 (5): E266 – E270 .

ISSN 0147-958X .

PMC 3270946 .

PMID 20926032 .

^ Benjamini, Yoav; Hochberg, Yosef (1995). "Controlling the false discovery rate: a practical and powerful approach to multiple testing".

Journal of the Royal Statistical Society, Series B .

57 (1): 125– 133.

JSTOR 2346101 .

^ Storey, JD; Tibshirani, Robert (2003).

"Statistical significance for genome-wide studies" .

PNAS .

100 (16): 9440– 9445.

Bibcode : 2003PNAS..100.9440S .

doi : 10.1073/pnas.1530509100 .

JSTOR 3144228 .

PMC 170937 .

PMID 12883005 .

^ Efron, Bradley; Tibshirani, Robert; Storey, John D.; Tusher, Virginia (2001). "Empirical Bayes analysis of a microarray experiment".

Journal of the American Statistical Association .

96 (456): 1151– 1160.

doi : 10.1198/016214501753382129 .

JSTOR 3085878 .

S2CID 9076863 .

^ Noble, William S. (2009-12-01).

"How does multiple testing correction work?" .

Nature Biotechnology .

27 (12): 1135– 1137.

doi : 10.1038/nbt1209-1135 .

ISSN 1087-0156 .

PMC 2907892 .

PMID 20010596 .

^ Young, S. S., Karr, A. (2011).

"Deming, data and observational studies" (PDF) .

Significance .

8 (3): 116– 120.

doi : 10.1111/j.1740-9713.2011.00506.x .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) ^ Smith, G. D., Shah, E. (2002).

"Data dredging, bias, or confounding" .

BMJ .

325 (7378): 1437– 1438.

doi : 10.1136/bmj.325.7378.1437 .

PMC 1124898 .

PMID 12493654 .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) ^ Kirsch, A; Mitzenmacher, M ; Pietracaprina, A; Pucci, G; Upfal, E ; Vandin, F (June 2012). "An Efficient Rigorous Approach for Identifying Statistically Significant Frequent Itemsets".

Journal of the ACM .

59 (3): 12:1–12:22.

arXiv : 1002.1104 .

doi : 10.1145/2220357.2220359 .

Further reading [ edit ] F. Bretz, T. Hothorn, P. Westfall (2010), Multiple Comparisons Using R , CRC Press S. Dudoit and M. J. van der Laan (2008), Multiple Testing Procedures with Application to Genomics , Springer Farcomeni, A. (2008). "A Review of Modern Multiple Hypothesis Testing, with particular attention to the false discovery proportion".

Statistical Methods in Medical Research .

17 (4): 347– 388.

doi : 10.1177/0962280206079046 .

hdl : 11573/142139 .

PMID 17698936 .

S2CID 12777404 .

Phipson, B.; Smyth, G. K. (2010). "Permutation P-values Should Never Be Zero: Calculating Exact P-values when Permutations are Randomly Drawn".

Statistical Applications in Genetics and Molecular Biology .

9 : Article39.

arXiv : 1603.05766 .

doi : 10.2202/1544-6115.1585 .

PMID 21044043 .

S2CID 10735784 .

P. H. Westfall and S. S. Young (1993), Resampling-based Multiple Testing: Examples and Methods for p-Value Adjustment , Wiley P. Westfall, R. Tobias, R. Wolfinger (2011) Multiple comparisons and multiple testing using SAS , 2nd edn, SAS Institute A gallery of examples of implausible correlations sourced by data dredging [1] An xkcd comic about the multiple comparisons problem, using jelly beans and acne as an example v t e Design of experiments Scientific method Scientific experiment Statistical design Control Internal and external validity Experimental unit Blinding Optimal design : Bayesian Random assignment Randomization Restricted randomization Replication versus subsampling Sample size Treatment and blocking Treatment Effect size Contrast Interaction Confounding Orthogonality Blocking Covariate Nuisance variable Models and inference Linear regression Ordinary least squares Bayesian Random effect Mixed model Hierarchical model: Bayesian Analysis of variance (Anova) Cochran's theorem Manova ( multivariate ) Ancova ( covariance ) Compare means Multiple comparison Designs Completely randomized Factorial Fractional factorial Plackett–Burman Taguchi Response surface methodology Polynomial and rational modeling Box–Behnken Central composite Block Generalized randomized block design (GRBD) Latin square Graeco-Latin square Orthogonal array Latin hypercube Repeated measures design Crossover study Randomized controlled trial Sequential analysis Sequential probability ratio test Glossary Category Mathematics portal Statistical outline Statistical topics v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐pnfd8
Cached time: 20250812021316
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.739 seconds
Real time usage: 1.034 seconds
Preprocessor visited node count: 3168/1000000
Revision size: 21811/2097152 bytes
Post‐expand include size: 218029/2097152 bytes
Template argument size: 3989/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 84858/5000000 bytes
Lua time usage: 0.391/10.000 seconds
Lua memory usage: 6819961/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  797.109      1 -total
 27.38%  218.216      1 Template:Reflist
 21.23%  169.207      1 Template:Image_frame
 20.75%  165.384      1 Template:Graph:Chart
 18.72%  149.187     12 Template:Navbox
 13.27%  105.787     13 Template:Cite_journal
 12.27%   97.842      2 Template:Cite_book
 11.72%   93.400      1 Template:Experimental_design
  9.08%   72.345      1 Template:Statistics
  8.96%   71.458      1 Template:Short_description Saved in parser cache with key enwiki:pcache:9444220:|#|:idhash:canonical and timestamp 20250812021316 and revision id 1294435405. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Multiple_comparisons_problem&oldid=1294435405 " Category : Multiple comparisons Hidden categories: Pages using the Graph extension Pages with disabled graphs CS1 maint: multiple names: authors list Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from June 2016 Articles with unsourced statements from August 2012 Articles with unsourced statements from January 2012 This page was last edited on 7 June 2025, at 18:14 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Multiple comparisons problem 12 languages Add topic

