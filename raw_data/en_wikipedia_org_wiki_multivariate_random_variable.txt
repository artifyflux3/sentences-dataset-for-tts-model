Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Probability distribution 2 Operations on random vectors Toggle Operations on random vectors subsection 2.1 Affine transformations 2.2 Invertible mappings 3 Expected value 4 Covariance and cross-covariance Toggle Covariance and cross-covariance subsection 4.1 Definitions 4.2 Properties 4.3 Uncorrelatedness 5 Correlation and cross-correlation Toggle Correlation and cross-correlation subsection 5.1 Definitions 5.2 Properties 6 Orthogonality 7 Independence 8 Characteristic function 9 Further properties Toggle Further properties subsection 9.1 Expectation of a quadratic form 9.2 Expectation of the product of two different quadratic forms 10 Applications Toggle Applications subsection 10.1 Portfolio theory 10.2 Regression theory 10.3 Vector time series 11 References 12 Further reading Toggle the table of contents Multivariate random variable 12 languages Беларуская Català Čeština Deutsch Español Français Italiano עברית Magyar Polski Русский Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Random variable with multiple component dimensions Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e For broader coverage of this topic, see Multivariate statistics .

In probability , and statistics , a multivariate random variable or random vector is a list or vector of mathematical variables each of whose value is unknown, either because the value has not yet occurred or because there is imperfect knowledge of its value.  The individual variables in a random vector are grouped together because they are all part of a single mathematical system — often they represent different properties of an individual statistical unit . For example, while a given person has a specific age, height and weight, the representation of these features of an unspecified person from within a group would be a random vector. Normally each element of a random vector is a real number .

Random vectors are often used as the underlying implementation of various types of aggregate random variables , e.g. a random matrix , random tree , random sequence , stochastic process , etc.

Formally, a multivariate random variable is a column vector X = ( X 1 , … … , X n ) T {\displaystyle \mathbf {X} =(X_{1},\dots ,X_{n})^{\mathsf {T}}} (or its transpose , which is a row vector ) whose components are random variables on the probability space ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},P)} , where Ω Ω {\displaystyle \Omega } is the sample space , F {\displaystyle {\mathcal {F}}} is the sigma-algebra (the collection of all events), and P {\displaystyle P} is the probability measure (a function returning each event's probability ).

Probability distribution [ edit ] Main article: Multivariate probability distribution Every random vector gives rise to a probability measure on R n {\displaystyle \mathbb {R} ^{n}} with the Borel algebra as the underlying sigma-algebra. This measure is also known as the joint probability distribution , the joint distribution, or the multivariate distribution of the random vector.

The distributions of each of the component random variables X i {\displaystyle X_{i}} are called marginal distributions . The conditional probability distribution of X i {\displaystyle X_{i}} given X j {\displaystyle X_{j}} is the probability distribution of X i {\displaystyle X_{i}} when X j {\displaystyle X_{j}} is known to be a particular value.

The cumulative distribution function F X : R n ↦ ↦ [ 0 , 1 ] {\displaystyle F_{\mathbf {X} }:\mathbb {R} ^{n}\mapsto [0,1]} of a random vector X = ( X 1 , … … , X n ) T {\displaystyle \mathbf {X} =(X_{1},\dots ,X_{n})^{\mathsf {T}}} is defined as [ 1 ] : p.15 F X ( x ) = P ⁡ ⁡ ( X 1 ≤ ≤ x 1 , … … , X n ≤ ≤ x n ) {\displaystyle F_{\mathbf {X} }(\mathbf {x} )=\operatorname {P} (X_{1}\leq x_{1},\ldots ,X_{n}\leq x_{n})} Eq.1 where x = ( x 1 , … … , x n ) T {\displaystyle \mathbf {x} =(x_{1},\dots ,x_{n})^{\mathsf {T}}} .

Operations on random vectors [ edit ] Random vectors can be subjected to the same kinds of algebraic operations as can non-random vectors: addition, subtraction, multiplication by a scalar , and the taking of inner products .

Affine transformations [ edit ] Similarly, a new random vector Y {\displaystyle \mathbf {Y} } can be defined by applying an affine transformation g : : R n → → R n {\displaystyle g\colon \mathbb {R} ^{n}\to \mathbb {R} ^{n}} to a random vector X {\displaystyle \mathbf {X} } : Y = A X + b {\displaystyle \mathbf {Y} =\mathbf {A} \mathbf {X} +b} , where A {\displaystyle \mathbf {A} } is an n × × n {\displaystyle n\times n} matrix and b {\displaystyle b} is an n × × 1 {\displaystyle n\times 1} column vector.

If A {\displaystyle \mathbf {A} } is an invertible matrix and X {\displaystyle \textstyle \mathbf {X} } has a probability density function f X {\displaystyle f_{\mathbf {X} }} , then the probability density of Y {\displaystyle \mathbf {Y} } is f Y ( y ) = f X ( A − − 1 ( y − − b ) ) | det A | {\displaystyle f_{\mathbf {Y} }(y)={\frac {f_{\mathbf {X} }(\mathbf {A} ^{-1}(y-b))}{|\det \mathbf {A} |}}} .

Invertible mappings [ edit ] More generally we can study invertible mappings of random vectors.

[ 2 ] : p.284–285 Let g {\displaystyle g} be a one-to-one mapping from an open subset D {\displaystyle {\mathcal {D}}} of R n {\displaystyle \mathbb {R} ^{n}} onto a subset R {\displaystyle {\mathcal {R}}} of R n {\displaystyle \mathbb {R} ^{n}} , let g {\displaystyle g} have continuous partial derivatives in D {\displaystyle {\mathcal {D}}} and let the Jacobian determinant det ( ∂ ∂ y ∂ ∂ x ) {\displaystyle \det \left({\frac {\partial \mathbf {y} }{\partial \mathbf {x} }}\right)} of g {\displaystyle g} be zero at no point of D {\displaystyle {\mathcal {D}}} . Assume that the real random vector X {\displaystyle \mathbf {X} } has a probability density function f X ( x ) {\displaystyle f_{\mathbf {X} }(\mathbf {x} )} and satisfies P ( X ∈ ∈ D ) = 1 {\displaystyle P(\mathbf {X} \in {\mathcal {D}})=1} . Then the random vector Y = g ( X ) {\displaystyle \mathbf {Y} =g(\mathbf {X} )} is of probability density f Y ( y ) = f X ( x ) | det ( ∂ ∂ y ∂ ∂ x ) | | x = g − − 1 ( y ) 1 ( y ∈ ∈ R Y ) {\displaystyle \left.f_{\mathbf {Y} }(\mathbf {y} )={\frac {f_{\mathbf {X} }(\mathbf {x} )}{\left|\det \left({\frac {\partial \mathbf {y} }{\partial \mathbf {x} }}\right)\right|}}\right|_{\mathbf {x} =g^{-1}(\mathbf {y} )}\mathbf {1} (\mathbf {y} \in R_{\mathbf {Y} })} where 1 {\displaystyle \mathbf {1} } denotes the indicator function and set R Y = { y = g ( x ) : f X ( x ) > 0 } ⊆ ⊆ R {\displaystyle R_{\mathbf {Y} }=\{\mathbf {y} =g(\mathbf {x} ):f_{\mathbf {X} }(\mathbf {x} )>0\}\subseteq {\mathcal {R}}} denotes support of Y {\displaystyle \mathbf {Y} } .

Expected value [ edit ] The expected value or mean of a random vector X {\displaystyle \mathbf {X} } is a fixed vector E ⁡ ⁡ [ X ] {\displaystyle \operatorname {E} [\mathbf {X} ]} whose elements are the expected values of the respective random variables.

[ 3 ] : p.333 E ⁡ ⁡ [ X ] = ( E ⁡ ⁡ [ X 1 ] , .

.

.

, E ⁡ ⁡ [ X n ] ) T {\displaystyle \operatorname {E} [\mathbf {X} ]=(\operatorname {E} [X_{1}],...,\operatorname {E} [X_{n}])^{\mathrm {T} }} Eq.2 Covariance and cross-covariance [ edit ] Definitions [ edit ] The covariance matrix (also called second central moment or variance-covariance matrix) of an n × × 1 {\displaystyle n\times 1} random vector is an n × × n {\displaystyle n\times n} matrix whose ( i,j ) th element is the covariance between the i th and the j th random variables. The covariance matrix is the expected value, element by element, of the n × × n {\displaystyle n\times n} matrix computed as [ X − − E ⁡ ⁡ [ X ] ] [ X − − E ⁡ ⁡ [ X ] ] T {\displaystyle [\mathbf {X} -\operatorname {E} [\mathbf {X} ]][\mathbf {X} -\operatorname {E} [\mathbf {X} ]]^{T}} , where the superscript T refers to the transpose of the indicated vector: [ 2 ] : p. 464 [ 3 ] : p.335 K X X = Var ⁡ ⁡ [ X ] = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T ] = E ⁡ ⁡ [ X X T ] − − E ⁡ ⁡ [ X ] E ⁡ ⁡ [ X ] T {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }=\operatorname {Var} [\mathbf {X} ]=\operatorname {E} [(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {X} -\operatorname {E} [\mathbf {X} ])^{T}]=\operatorname {E} [\mathbf {X} \mathbf {X} ^{T}]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {X} ]^{T}} Eq.3 By extension, the cross-covariance matrix between two random vectors X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } ( X {\displaystyle \mathbf {X} } having n {\displaystyle n} elements and Y {\displaystyle \mathbf {Y} } having p {\displaystyle p} elements) is the n × × p {\displaystyle n\times p} matrix [ 3 ] : p.336 K X Y = Cov ⁡ ⁡ [ X , Y ] = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( Y − − E ⁡ ⁡ [ Y ] ) T ] = E ⁡ ⁡ [ X Y T ] − − E ⁡ ⁡ [ X ] E ⁡ ⁡ [ Y ] T {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {Y} }=\operatorname {Cov} [\mathbf {X} ,\mathbf {Y} ]=\operatorname {E} [(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{T}]=\operatorname {E} [\mathbf {X} \mathbf {Y} ^{T}]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{T}} Eq.4 where again the matrix expectation is taken element-by-element in the matrix. Here the ( i,j ) th element is the covariance between the i th element of X {\displaystyle \mathbf {X} } and the j th element of Y {\displaystyle \mathbf {Y} } .

Properties [ edit ] The covariance matrix is a symmetric matrix , i.e.

[ 2 ] : p. 466 K X X T = K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }^{T}=\operatorname {K} _{\mathbf {X} \mathbf {X} }} .

The covariance matrix is a positive semidefinite matrix , i.e.

[ 2 ] : p. 465 a T K X X ⁡ ⁡ a ≥ ≥ 0 for all a ∈ ∈ R n {\displaystyle \mathbf {a} ^{T}\operatorname {K} _{\mathbf {X} \mathbf {X} }\mathbf {a} \geq 0\quad {\text{for all }}\mathbf {a} \in \mathbb {R} ^{n}} .

The cross-covariance matrix Cov ⁡ ⁡ [ Y , X ] {\displaystyle \operatorname {Cov} [\mathbf {Y} ,\mathbf {X} ]} is simply the transpose of the matrix Cov ⁡ ⁡ [ X , Y ] {\displaystyle \operatorname {Cov} [\mathbf {X} ,\mathbf {Y} ]} , i.e.

K Y X = K X Y T {\displaystyle \operatorname {K} _{\mathbf {Y} \mathbf {X} }=\operatorname {K} _{\mathbf {X} \mathbf {Y} }^{T}} .

Uncorrelatedness [ edit ] Two random vectors X = ( X 1 , .

.

.

, X m ) T {\displaystyle \mathbf {X} =(X_{1},...,X_{m})^{T}} and Y = ( Y 1 , .

.

.

, Y n ) T {\displaystyle \mathbf {Y} =(Y_{1},...,Y_{n})^{T}} are called uncorrelated if E ⁡ ⁡ [ X Y T ] = E ⁡ ⁡ [ X ] E ⁡ ⁡ [ Y ] T {\displaystyle \operatorname {E} [\mathbf {X} \mathbf {Y} ^{T}]=\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{T}} .

They are uncorrelated if and only if their cross-covariance matrix K X Y {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {Y} }} is zero.

[ 3 ] : p.337 Correlation and cross-correlation [ edit ] Definitions [ edit ] The correlation matrix (also called second moment ) of an n × × 1 {\displaystyle n\times 1} random vector is an n × × n {\displaystyle n\times n} matrix whose ( i,j ) th element is the correlation between the i th and the j th random variables. The correlation matrix is the expected value, element by element, of the n × × n {\displaystyle n\times n} matrix computed as X X T {\displaystyle \mathbf {X} \mathbf {X} ^{T}} , where the superscript T refers to the transpose of the indicated vector: [ 4 ] : p.190 [ 3 ] : p.334 R X X = E ⁡ ⁡ [ X X T ] {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {X} }=\operatorname {E} [\mathbf {X} \mathbf {X} ^{\mathrm {T} }]} Eq.5 By extension, the cross-correlation matrix between two random vectors X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } ( X {\displaystyle \mathbf {X} } having n {\displaystyle n} elements and Y {\displaystyle \mathbf {Y} } having p {\displaystyle p} elements) is the n × × p {\displaystyle n\times p} matrix R X Y = E ⁡ ⁡ [ X Y T ] {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {Y} }=\operatorname {E} [\mathbf {X} \mathbf {Y} ^{T}]} Eq.6 Properties [ edit ] The correlation matrix is related to the covariance matrix by R X X = K X X + E ⁡ ⁡ [ X ] E ⁡ ⁡ [ X ] T {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {X} }=\operatorname {K} _{\mathbf {X} \mathbf {X} }+\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {X} ]^{T}} .

Similarly for the cross-correlation matrix and the cross-covariance matrix: R X Y = K X Y + E ⁡ ⁡ [ X ] E ⁡ ⁡ [ Y ] T {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {Y} }=\operatorname {K} _{\mathbf {X} \mathbf {Y} }+\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{T}} Orthogonality [ edit ] Two random vectors of the same size X = ( X 1 , .

.

.

, X n ) T {\displaystyle \mathbf {X} =(X_{1},...,X_{n})^{T}} and Y = ( Y 1 , .

.

.

, Y n ) T {\displaystyle \mathbf {Y} =(Y_{1},...,Y_{n})^{T}} are called orthogonal if E ⁡ ⁡ [ X T Y ] = 0 {\displaystyle \operatorname {E} [\mathbf {X} ^{T}\mathbf {Y} ]=0} .

Independence [ edit ] Main article: Independence (probability theory) Two random vectors X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are called independent if for all x {\displaystyle \mathbf {x} } and y {\displaystyle \mathbf {y} } F X , Y ( x , y ) = F X ( x ) ⋅ ⋅ F Y ( y ) {\displaystyle F_{\mathbf {X,Y} }(\mathbf {x,y} )=F_{\mathbf {X} }(\mathbf {x} )\cdot F_{\mathbf {Y} }(\mathbf {y} )} where F X ( x ) {\displaystyle F_{\mathbf {X} }(\mathbf {x} )} and F Y ( y ) {\displaystyle F_{\mathbf {Y} }(\mathbf {y} )} denote the cumulative distribution functions of X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } and F X , Y ( x , y ) {\displaystyle F_{\mathbf {X,Y} }(\mathbf {x,y} )} denotes their joint cumulative distribution function. Independence of X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } is often denoted by X ⊥ ⊥ ⊥ ⊥ Y {\displaystyle \mathbf {X} \perp \!\!\!\perp \mathbf {Y} } .
Written component-wise, X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are called independent if for all x 1 , … … , x m , y 1 , … … , y n {\displaystyle x_{1},\ldots ,x_{m},y_{1},\ldots ,y_{n}} F X 1 , … … , X m , Y 1 , … … , Y n ( x 1 , … … , x m , y 1 , … … , y n ) = F X 1 , … … , X m ( x 1 , … … , x m ) ⋅ ⋅ F Y 1 , … … , Y n ( y 1 , … … , y n ) {\displaystyle F_{X_{1},\ldots ,X_{m},Y_{1},\ldots ,Y_{n}}(x_{1},\ldots ,x_{m},y_{1},\ldots ,y_{n})=F_{X_{1},\ldots ,X_{m}}(x_{1},\ldots ,x_{m})\cdot F_{Y_{1},\ldots ,Y_{n}}(y_{1},\ldots ,y_{n})} .

Characteristic function [ edit ] The characteristic function of a random vector X {\displaystyle \mathbf {X} } with n {\displaystyle n} components is a function R n → → C {\displaystyle \mathbb {R} ^{n}\to \mathbb {C} } that maps every vector ω ω = ( ω ω 1 , … … , ω ω n ) T {\displaystyle \mathbf {\omega } =(\omega _{1},\ldots ,\omega _{n})^{T}} to a complex number. It is defined by [ 2 ] : p. 468 φ φ X ( ω ω ) = E ⁡ ⁡ [ e i ( ω ω T X ) ] = E ⁡ ⁡ [ e i ( ω ω 1 X 1 + … … + ω ω n X n ) ] {\displaystyle \varphi _{\mathbf {X} }(\mathbf {\omega } )=\operatorname {E} \left[e^{i(\mathbf {\omega } ^{T}\mathbf {X} )}\right]=\operatorname {E} \left[e^{i(\omega _{1}X_{1}+\ldots +\omega _{n}X_{n})}\right]} .

Further properties [ edit ] Expectation of a quadratic form [ edit ] One can take the expectation of a quadratic form in the random vector X {\displaystyle \mathbf {X} } as follows: [ 5 ] : p.170–171 E ⁡ ⁡ [ X T A X ] = E ⁡ ⁡ [ X ] T A E ⁡ ⁡ [ X ] + tr ⁡ ⁡ ( A K X X ) , {\displaystyle \operatorname {E} [\mathbf {X} ^{T}A\mathbf {X} ]=\operatorname {E} [\mathbf {X} ]^{T}A\operatorname {E} [\mathbf {X} ]+\operatorname {tr} (AK_{\mathbf {X} \mathbf {X} }),} where K X X {\displaystyle K_{\mathbf {X} \mathbf {X} }} is the covariance matrix of X {\displaystyle \mathbf {X} } and tr {\displaystyle \operatorname {tr} } refers to the trace of a matrix — that is, to the sum of the elements on its main diagonal (from upper left to lower right).  Since the quadratic form is a scalar, so is its expectation.

Proof : Let z {\displaystyle \mathbf {z} } be an m × × 1 {\displaystyle m\times 1} random vector with E ⁡ ⁡ [ z ] = μ μ {\displaystyle \operatorname {E} [\mathbf {z} ]=\mu } and Cov ⁡ ⁡ [ z ] = V {\displaystyle \operatorname {Cov} [\mathbf {z} ]=V} and let A {\displaystyle A} be an m × × m {\displaystyle m\times m} non-stochastic matrix.

Then based on the formula for the covariance, if we denote z T = X {\displaystyle \mathbf {z} ^{T}=\mathbf {X} } and z T A T = Y {\displaystyle \mathbf {z} ^{T}A^{T}=\mathbf {Y} } , we see that: Cov ⁡ ⁡ [ X , Y ] = E ⁡ ⁡ [ X Y T ] − − E ⁡ ⁡ [ X ] E ⁡ ⁡ [ Y ] T {\displaystyle \operatorname {Cov} [\mathbf {X} ,\mathbf {Y} ]=\operatorname {E} [\mathbf {X} \mathbf {Y} ^{T}]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{T}} Hence E ⁡ ⁡ [ X Y T ] = Cov ⁡ ⁡ [ X , Y ] + E ⁡ ⁡ [ X ] E ⁡ ⁡ [ Y ] T E ⁡ ⁡ [ z T A z ] = Cov ⁡ ⁡ [ z T , z T A T ] + E ⁡ ⁡ [ z T ] E ⁡ ⁡ [ z T A T ] T = Cov ⁡ ⁡ [ z T , z T A T ] + μ μ T ( μ μ T A T ) T = Cov ⁡ ⁡ [ z T , z T A T ] + μ μ T A μ μ , {\displaystyle {\begin{aligned}\operatorname {E} [XY^{T}]&=\operatorname {Cov} [X,Y]+\operatorname {E} [X]\operatorname {E} [Y]^{T}\\\operatorname {E} [z^{T}Az]&=\operatorname {Cov} [z^{T},z^{T}A^{T}]+\operatorname {E} [z^{T}]\operatorname {E} [z^{T}A^{T}]^{T}\\&=\operatorname {Cov} [z^{T},z^{T}A^{T}]+\mu ^{T}(\mu ^{T}A^{T})^{T}\\&=\operatorname {Cov} [z^{T},z^{T}A^{T}]+\mu ^{T}A\mu ,\end{aligned}}} which leaves us to show that Cov ⁡ ⁡ [ z T , z T A T ] = tr ⁡ ⁡ ( A V ) .

{\displaystyle \operatorname {Cov} [z^{T},z^{T}A^{T}]=\operatorname {tr} (AV).} This is true based on the fact that one can cyclically permute matrices when taking a trace without changing the end result (e.g.: tr ⁡ ⁡ ( A B ) = tr ⁡ ⁡ ( B A ) {\displaystyle \operatorname {tr} (AB)=\operatorname {tr} (BA)} ).

We see that Cov ⁡ ⁡ [ z T , z T A T ] = E ⁡ ⁡ [ ( z T − − E ( z T ) ) ( z T A T − − E ( z T A T ) ) T ] = E ⁡ ⁡ [ ( z T − − μ μ T ) ( z T A T − − μ μ T A T ) T ] = E ⁡ ⁡ [ ( z − − μ μ ) T ( A z − − A μ μ ) ] .

{\displaystyle {\begin{aligned}\operatorname {Cov} [z^{T},z^{T}A^{T}]&=\operatorname {E} \left[\left(z^{T}-E(z^{T})\right)\left(z^{T}A^{T}-E\left(z^{T}A^{T}\right)\right)^{T}\right]\\&=\operatorname {E} \left[(z^{T}-\mu ^{T})(z^{T}A^{T}-\mu ^{T}A^{T})^{T}\right]\\&=\operatorname {E} \left[(z-\mu )^{T}(Az-A\mu )\right].\end{aligned}}} And since ( z − − μ μ ) T ( A z − − A μ μ ) {\displaystyle \left({z-\mu }\right)^{T}\left({Az-A\mu }\right)} is a scalar , then ( z − − μ μ ) T ( A z − − A μ μ ) = tr ⁡ ⁡ ( ( z − − μ μ ) T ( A z − − A μ μ ) ) = tr ⁡ ⁡ ( ( z − − μ μ ) T A ( z − − μ μ ) ) {\displaystyle (z-\mu )^{T}(Az-A\mu )=\operatorname {tr} \left({(z-\mu )^{T}(Az-A\mu )}\right)=\operatorname {tr} \left((z-\mu )^{T}A(z-\mu )\right)} trivially. Using the permutation we get: tr ⁡ ⁡ ( ( z − − μ μ ) T A ( z − − μ μ ) ) = tr ⁡ ⁡ ( A ( z − − μ μ ) ( z − − μ μ ) T ) , {\displaystyle \operatorname {tr} \left({(z-\mu )^{T}A(z-\mu )}\right)=\operatorname {tr} \left({A(z-\mu )(z-\mu )^{T}}\right),} and by plugging this into the original formula we get: Cov ⁡ ⁡ [ z T , z T A T ] = E [ ( z − − μ μ ) T ( A z − − A μ μ ) ] = E [ tr ⁡ ⁡ ( A ( z − − μ μ ) ( z − − μ μ ) T ) ] = tr ⁡ ⁡ ( A ⋅ ⋅ E ⁡ ⁡ ( ( z − − μ μ ) ( z − − μ μ ) T ) ) = tr ⁡ ⁡ ( A V ) .

{\displaystyle {\begin{aligned}\operatorname {Cov} \left[{z^{T},z^{T}A^{T}}\right]&=E\left[{\left({z-\mu }\right)^{T}(Az-A\mu )}\right]\\&=E\left[\operatorname {tr} \left(A(z-\mu )(z-\mu )^{T}\right)\right]\\&=\operatorname {tr} \left({A\cdot \operatorname {E} \left((z-\mu )(z-\mu )^{T}\right)}\right)\\&=\operatorname {tr} (AV).\end{aligned}}} Expectation of the product of two different quadratic forms [ edit ] One can take the expectation of the product of two different quadratic forms in a zero-mean Gaussian random vector X {\displaystyle \mathbf {X} } as follows: [ 5 ] : pp. 162–176 E ⁡ ⁡ [ ( X T A X ) ( X T B X ) ] = 2 tr ⁡ ⁡ ( A K X X B K X X ) + tr ⁡ ⁡ ( A K X X ) tr ⁡ ⁡ ( B K X X ) {\displaystyle \operatorname {E} \left[(\mathbf {X} ^{T}A\mathbf {X} )(\mathbf {X} ^{T}B\mathbf {X} )\right]=2\operatorname {tr} (AK_{\mathbf {X} \mathbf {X} }BK_{\mathbf {X} \mathbf {X} })+\operatorname {tr} (AK_{\mathbf {X} \mathbf {X} })\operatorname {tr} (BK_{\mathbf {X} \mathbf {X} })} where again K X X {\displaystyle K_{\mathbf {X} \mathbf {X} }} is the covariance matrix of X {\displaystyle \mathbf {X} } . Again, since both quadratic forms are scalars and hence their product is a scalar, the expectation of their product is also a scalar.

Applications [ edit ] Portfolio theory [ edit ] In portfolio theory in finance , an objective often is to choose a portfolio of risky assets such that the distribution of the random portfolio return has desirable properties. For example, one might want to choose the portfolio return having the lowest variance for a given expected value.  Here the random vector is the vector r {\displaystyle \mathbf {r} } of random returns on the individual assets, and the portfolio return p (a random scalar) is the inner product of the vector of random returns with a vector w of portfolio weights — the fractions of the portfolio placed in the respective assets. Since p = w T r {\displaystyle \mathbf {r} } , the expected value of the portfolio return is w T E( r {\displaystyle \mathbf {r} } ) and the variance of the portfolio return can be shown to be w T C w , where C is the covariance matrix of r {\displaystyle \mathbf {r} } .

Regression theory [ edit ] In linear regression theory, we have data on n observations on a dependent variable y and n observations on each of k independent variables x j . The observations on the dependent variable are stacked into a column vector y ; the observations on each independent variable are also stacked into column vectors, and these latter column vectors are combined into a design matrix X (not denoting a random vector in this context) of observations on the independent variables.  Then the following regression equation is postulated as a description of the process that generated the data: y = X β β + e , {\displaystyle y=X\beta +e,} where β is a postulated fixed but unknown vector of k response coefficients, and e is an unknown random vector reflecting random influences on the dependent variable.  By some chosen technique such as ordinary least squares , a vector β β ^ ^ {\displaystyle {\hat {\beta }}} is chosen as an estimate of β, and the estimate of the vector e , denoted e ^ ^ {\displaystyle {\hat {e}}} , is computed as e ^ ^ = y − − X β β ^ ^ .

{\displaystyle {\hat {e}}=y-X{\hat {\beta }}.} Then the statistician must analyze the properties of β β ^ ^ {\displaystyle {\hat {\beta }}} and e ^ ^ {\displaystyle {\hat {e}}} , which are viewed as random vectors since a randomly different selection of n cases to observe would have resulted in different values for them.

Vector time series [ edit ] The evolution of a k ×1 random vector X {\displaystyle \mathbf {X} } through time can be modelled as a vector autoregression (VAR) as follows: X t = c + A 1 X t − − 1 + A 2 X t − − 2 + ⋯ ⋯ + A p X t − − p + e t , {\displaystyle \mathbf {X} _{t}=c+A_{1}\mathbf {X} _{t-1}+A_{2}\mathbf {X} _{t-2}+\cdots +A_{p}\mathbf {X} _{t-p}+\mathbf {e} _{t},\,} where the i -periods-back vector observation X t − − i {\displaystyle \mathbf {X} _{t-i}} is called the i -th lag of X {\displaystyle \mathbf {X} } , c is a k × 1 vector of constants ( intercepts ), A i is a time-invariant k × k matrix and e t {\displaystyle \mathbf {e} _{t}} is a k × 1 random vector of error terms.

References [ edit ] ^ Gallager, Robert G. (2013).

Stochastic Processes Theory for Applications . Cambridge University Press.

ISBN 978-1-107-03975-9 .

^ a b c d e Taboga, Marco (2017).

Lectures on Probability Theory and Mathematical Statistics . CreateSpace Independent Publishing Platform.

ISBN 978-1981369195 .

^ a b c d e Gubner, John A. (2006).

Probability and Random Processes for Electrical and Computer Engineers . Cambridge University Press.

ISBN 978-0-521-86470-1 .

^ Papoulis, Athanasius (1991).

Probability, Random Variables and Stochastic Processes (Third ed.). McGraw-Hill.

ISBN 0-07-048477-5 .

^ a b Kendrick, David (1981).

Stochastic Control for Economic Models . McGraw-Hill.

ISBN 0-07-033962-7 .

Further reading [ edit ] Stark, Henry; Woods, John W. (2012). "Random Vectors".

Probability, Statistics, and Random Processes for Engineers (Fourth ed.). Pearson. pp.

295– 339.

ISBN 978-0-13-231123-6 .

NewPP limit report
Parsed by mw‐api‐ext.codfw.main‐5cf8f5fb49‐6wk69
Cached time: 20250812001805
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.656 seconds
Real time usage: 0.956 seconds
Preprocessor visited node count: 5401/1000000
Revision size: 21631/2097152 bytes
Post‐expand include size: 35626/2097152 bytes
Template argument size: 5305/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 52621/5000000 bytes
Lua time usage: 0.289/10.000 seconds
Lua memory usage: 5309204/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  542.353      1 -total
 23.65%  128.264      1 Template:Reflist
 22.60%  122.568      6 Template:Cite_book
 19.49%  105.713      1 Template:Short_description
 19.33%  104.821     14 Template:Rp
 18.34%   99.470      1 Template:Probability_fundamentals
 17.87%   96.938      1 Template:Sidebar
 17.18%   93.181     14 Template:R/superscript
 12.26%   66.498      2 Template:Pagetype
  9.50%   51.542      6 Template:Equation_box_1 Saved in parser cache with key enwiki:pcache:49821:|#|:idhash:canonical and timestamp 20250812001805 and revision id 1303428131. Rendering was triggered because: unknown Retrieved from " https://en.wikipedia.org/w/index.php?title=Multivariate_random_variable&oldid=1303428131 " Categories : Multivariate statistics Algebra of random variables Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 30 July 2025, at 23:51 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Multivariate random variable 12 languages Add topic

