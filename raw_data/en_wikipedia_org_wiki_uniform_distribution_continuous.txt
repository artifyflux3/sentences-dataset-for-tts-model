Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Probability density function 1.2 Cumulative distribution function 1.2.1 Example 1. Using the continuous uniform distribution function 1.2.2 Example 2. Using the continuous uniform distribution function (conditional) 1.3 Generating functions 1.3.1 Moment-generating function 1.3.2 Cumulant-generating function 1.4 Standard uniform distribution 1.5 Relationship to other functions 2 Properties Toggle Properties subsection 2.1 Moments 2.2 Order statistics 2.3 Uniformity 2.4 Uniform distribution on more general sets 3 Related distributions 4 Statistical inference Toggle Statistical inference subsection 4.1 Estimation of parameters 4.1.1 Estimation of maximum 4.1.1.1 Minimum-variance unbiased estimator 4.1.1.2 Method of moments estimator 4.1.1.3 Maximum likelihood estimator 4.1.2 Estimation of minimum 4.1.3 Estimation of midpoint 4.2 Confidence interval 4.2.1 For the maximum 5 Occurrence and applications Toggle Occurrence and applications subsection 5.1 Economics example for uniform distribution 5.2 Sampling from an arbitrary distribution 5.3 Quantization error 6 Random variate generation 7 History 8 See also 9 References 10 Further reading 11 External links Toggle the table of contents Continuous uniform distribution 44 languages العربية Asturianu বাংলা Беларуская Català Чӑвашла Čeština Dansk Deutsch Ελληνικά Español Esperanto Euskara فارسی Français Gaeilge Galego 한국어 Italiano עברית ქართული Latviešu Magyar Македонски Nederlands 日本語 Norsk bokmål Polski Português Русский Shqip Slovenčina Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Sunda Suomi Svenska ไทย Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Uniform distribution (continuous) ) Uniform distribution on an interval Continuous uniform Probability density function Using maximum convention Cumulative distribution function Notation U [ a , b ] {\displaystyle {\mathcal {U}}_{[a,b]}} Parameters − − ∞ ∞ < a < b < ∞ ∞ {\displaystyle -\infty <a<b<\infty } Support [ a , b ] {\displaystyle [a,b]} PDF { 1 b − − a for x ∈ ∈ [ a , b ] 0 otherwise {\displaystyle {\begin{cases}{\frac {1}{b-a}}&{\text{for }}x\in [a,b]\\0&{\text{otherwise}}\end{cases}}} CDF { 0 for x < a x − − a b − − a for x ∈ ∈ [ a , b ] 1 for x > b {\displaystyle {\begin{cases}0&{\text{for }}x<a\\{\frac {x-a}{b-a}}&{\text{for }}x\in [a,b]\\1&{\text{for }}x>b\end{cases}}} Mean 1 2 ( a + b ) {\displaystyle {\tfrac {1}{2}}(a+b)} Median 1 2 ( a + b ) {\displaystyle {\tfrac {1}{2}}(a+b)} Mode any value in ( a , b ) {\displaystyle {\text{any value in }}(a,b)} Variance 1 12 ( b − − a ) 2 {\displaystyle {\tfrac {1}{12}}(b-a)^{2}} MAD 1 4 ( b − − a ) {\displaystyle {\tfrac {1}{4}}(b-a)} Skewness 0 {\displaystyle 0} Excess kurtosis − − 6 5 {\displaystyle -{\tfrac {6}{5}}} Entropy log ⁡ ⁡ ( b − − a ) {\displaystyle \log(b-a)} MGF { e t b − − e t a t ( b − − a ) for t ≠ ≠ 0 1 for t = 0 {\displaystyle {\begin{cases}{\frac {\mathrm {e} ^{tb}-\mathrm {e} ^{ta}}{t(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}}} CF { e i t b − − e i t a i t ( b − − a ) for t ≠ ≠ 0 1 for t = 0 {\displaystyle {\begin{cases}{\frac {\mathrm {e} ^{\mathrm {i} tb}-\mathrm {e} ^{\mathrm {i} ta}}{\mathrm {i} t(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}}} In probability theory and statistics , the continuous uniform distributions or rectangular distributions are a family of symmetric probability distributions . Such a distribution describes an experiment where there is an arbitrary outcome that lies between certain bounds.

[ 1 ] The bounds are defined by the parameters, a {\displaystyle a} and b , {\displaystyle b,} which are the minimum and maximum values. The interval can either be closed (i.e.

[ a , b ] {\displaystyle [a,b]} ) or open (i.e.

( a , b ) {\displaystyle (a,b)} ).

[ 2 ] Therefore, the distribution is often abbreviated U ( a , b ) , {\displaystyle U(a,b),} where U {\displaystyle U} stands for uniform distribution.

[ 1 ] The difference between the bounds defines the interval length; all intervals of the same length on the distribution's support are equally probable. It is the maximum entropy probability distribution for a random variable X {\displaystyle X} under no constraint other than that it is contained in the distribution's support.

[ 3 ] Definitions [ edit ] Probability density function [ edit ] The probability density function of the continuous uniform distribution is f ( x ) = { 1 b − − a for a ≤ ≤ x ≤ ≤ b , 0 for x < a or x > b .

{\displaystyle f(x)={\begin{cases}{\dfrac {1}{b-a}}&{\text{for }}a\leq x\leq b,\\[8pt]0&{\text{for }}x<a\ {\text{ or }}\ x>b.\end{cases}}} The values of f ( x ) {\displaystyle f(x)} at the two boundaries a {\displaystyle a} and b {\displaystyle b} are usually unimportant, because they do not alter the value of ∫ ∫ c d f ( x ) d x {\textstyle \int _{c}^{d}f(x)dx} over any interval [ c , d ] , {\displaystyle [c,d],} nor of ∫ ∫ a b x f ( x ) d x , {\textstyle \int _{a}^{b}xf(x)\,dx,} nor of any higher moment. Sometimes they are chosen to be zero, and sometimes chosen to be 1 b − − a .

{\displaystyle {\tfrac {1}{b-a}}.} The latter is appropriate in the context of estimation by the method of maximum likelihood . In the context of Fourier analysis , one may take the value of f ( a ) {\displaystyle f(a)} or f ( b ) {\displaystyle f(b)} to be 1 2 ( b − − a ) , {\displaystyle {\tfrac {1}{2(b-a)}},} because then the inverse transform of many integral transforms of this uniform function will yield back the function itself, rather than a function which is equal " almost everywhere ", i.e. except on a set of points with zero measure . Also, it is consistent with the sign function , which has no such ambiguity.

Any probability density function integrates to 1 , {\displaystyle 1,} so the probability density function of the continuous uniform distribution is graphically portrayed as a rectangle where ⁠ b − − a {\displaystyle b-a} ⁠ is the base length and ⁠ 1 b − − a {\displaystyle {\tfrac {1}{b-a}}} ⁠ is the height. As the base length increases, the height (the density at any particular value within the distribution boundaries) decreases.

[ 4 ] In terms of mean μ μ {\displaystyle \mu } and variance σ σ 2 , {\displaystyle \sigma ^{2},} the probability density function of the continuous uniform distribution is f ( x ) = { 1 2 σ σ 3 for − − σ σ 3 ≤ ≤ x − − μ μ ≤ ≤ σ σ 3 , 0 otherwise .

{\displaystyle f(x)={\begin{cases}{\dfrac {1}{2\sigma {\sqrt {3}}}}&{\text{for }}-\sigma {\sqrt {3}}\leq x-\mu \leq \sigma {\sqrt {3}},\\[2pt]0&{\text{otherwise}}.\end{cases}}} Cumulative distribution function [ edit ] The cumulative distribution function of the continuous uniform distribution is: F ( x ) = { 0 for x < a , x − − a b − − a for a ≤ ≤ x ≤ ≤ b , 1 for x > b .

{\displaystyle F(x)={\begin{cases}0&{\text{for }}x<a,\\[8pt]{\frac {x-a}{b-a}}&{\text{for }}a\leq x\leq b,\\[8pt]1&{\text{for }}x>b.\end{cases}}} Its inverse is: F − − 1 ( p ) = a + p ( b − − a ) for 0 < p < 1.

{\displaystyle F^{-1}(p)=a+p(b-a)\quad {\text{ for }}0<p<1.} In terms of mean μ μ {\displaystyle \mu } and variance σ σ 2 , {\displaystyle \sigma ^{2},} the cumulative distribution function of the continuous uniform distribution is: F ( x ) = { 0 for x − − μ μ < − − σ σ 3 , 1 2 ( x − − μ μ σ σ 3 + 1 ) for − − σ σ 3 ≤ ≤ x − − μ μ < σ σ 3 , 1 for x − − μ μ ≥ ≥ σ σ 3 ; {\displaystyle F(x)={\begin{cases}0&{\text{for }}x-\mu <-\sigma {\sqrt {3}},\\{\frac {1}{2}}\left({\frac {x-\mu }{\sigma {\sqrt {3}}}}+1\right)&{\text{for }}-\sigma {\sqrt {3}}\leq x-\mu <\sigma {\sqrt {3}},\\1&{\text{for }}x-\mu \geq \sigma {\sqrt {3}};\end{cases}}} its inverse is: F − − 1 ( p ) = σ σ 3 ( 2 p − − 1 ) + μ μ for 0 ≤ ≤ p ≤ ≤ 1.

{\displaystyle F^{-1}(p)=\sigma {\sqrt {3}}(2p-1)+\mu \quad {\text{ for }}0\leq p\leq 1.} Example 1.

Using the continuous uniform distribution function [ edit ] For a random variable X ∼ ∼ U ( 0 , 23 ) , {\displaystyle X\sim U(0,23),} find Pr ( 2 < X < 18 ) : {\displaystyle \Pr(2<X<18):} Pr ( 2 < X < 18 ) = ( 18 − − 2 ) ⋅ ⋅ 1 23 − − 0 = 16 23 .

{\displaystyle \Pr(2<X<18)=(18-2)\cdot {\frac {1}{23-0}}={\frac {16}{23}}.} In a graphical representation of the continuous uniform distribution function [ f ( x ) vs x ] , {\displaystyle [f(x){\text{ vs }}x],} the area under the curve within the specified bounds, displaying the probability, is a rectangle. For the specific example above, the base would be ⁠ 16 , {\displaystyle 16,} ⁠ and the height would be ⁠ 1 23 .

{\displaystyle {\tfrac {1}{23}}.} ⁠ [ 5 ] Example 2.

Using the continuous uniform distribution function (conditional) [ edit ] For a random variable X ∼ ∼ U ( 0 , 23 ) , {\displaystyle X\sim U(0,23),} find Pr ( X > 12 ∣ ∣ X > 8 ) : {\displaystyle \Pr(X>12\mid X>8):} Pr ( X > 12 ∣ ∣ X > 8 ) = ( 23 − − 12 ) ⋅ ⋅ 1 23 − − 8 = 11 15 .

{\displaystyle \Pr(X>12\mid X>8)=(23-12)\cdot {\frac {1}{23-8}}={\frac {11}{15}}.} The example above is a conditional probability case for the continuous uniform distribution: given that ⁠ X > 8 {\displaystyle X>8} ⁠ is true, what is the probability that ⁠ X > 12 ?

{\displaystyle X>12?} ⁠ Conditional probability changes the sample space, so a new interval length ⁠ b − − a ′ {\displaystyle b-a'} ⁠ has to be calculated, where b = 23 {\displaystyle b=23} and a ′ = 8.

{\displaystyle a'=8.} [ 5 ] The graphical representation would still follow Example 1, where the area under the curve within the specified bounds displays the probability; the base of the rectangle would be ⁠ 11 , {\displaystyle 11,} ⁠ and the height would be ⁠ 1 15 .

{\displaystyle {\tfrac {1}{15}}.} ⁠ [ 5 ] Generating functions [ edit ] Moment-generating function [ edit ] The moment-generating function of the continuous uniform distribution is: [ 6 ] M X = E ⁡ ⁡ [ e t X ] = ∫ ∫ a b e t x d x b − − a = e t b − − e t a t ( b − − a ) = B t − − A t t ( b − − a ) , {\displaystyle M_{X}=\operatorname {E} \left[e^{tX}\right]=\int _{a}^{b}e^{tx}{\frac {dx}{b-a}}={\frac {e^{tb}-e^{ta}}{t(b-a)}}={\frac {B^{t}-A^{t}}{t(b-a)}},} from which we may calculate the raw moments m k : {\displaystyle m_{k}:} m 1 = a + b 2 , {\displaystyle m_{1}={\frac {a+b}{2}},} m 2 = a 2 + a b + b 2 3 , {\displaystyle m_{2}={\frac {a^{2}+ab+b^{2}}{3}},} m k = ∑ ∑ i = 0 k a i b k − − i k + 1 .

{\displaystyle m_{k}={\frac {\sum _{i=0}^{k}a^{i}b^{k-i}}{k+1}}.} For a random variable following the continuous uniform distribution, the expected value is m 1 = a + b 2 , {\displaystyle m_{1}={\tfrac {a+b}{2}},} and the variance is m 2 − − m 1 2 = ( b − − a ) 2 12 .

{\displaystyle m_{2}-m_{1}^{2}={\tfrac {(b-a)^{2}}{12}}.} For the special case a = − − b , {\displaystyle a=-b,} the probability density function of the continuous uniform distribution is: f ( x ) = { 1 2 b for − − b ≤ ≤ x ≤ ≤ b , 0 otherwise ; {\displaystyle f(x)={\begin{cases}{\frac {1}{2b}}&{\text{for }}-b\leq x\leq b,\\[8pt]0&{\text{otherwise}};\end{cases}}} the moment-generating function reduces to the simple form: M X = sinh ⁡ ⁡ b t b t .

{\displaystyle M_{X}={\frac {\sinh bt}{bt}}.} Cumulant-generating function [ edit ] For ⁠ n ≥ ≥ 2 , {\displaystyle n\geq 2,} ⁠ the n {\displaystyle n} -th cumulant of the continuous uniform distribution on the interval ⁠ [ − − 1 2 , 1 2 ] {\displaystyle [-{\tfrac {1}{2}},{\tfrac {1}{2}}]} ⁠ is B n n , {\displaystyle {\tfrac {B_{n}}{n}},} where B n {\displaystyle B_{n}} is the n {\displaystyle n} -th Bernoulli number .

[ 7 ] Standard uniform distribution [ edit ] The continuous uniform distribution with parameters a = 0 {\displaystyle a=0} and b = 1 , {\displaystyle b=1,} i.e.

U ( 0 , 1 ) , {\displaystyle U(0,1),} is called the standard uniform distribution .

One interesting property of the standard uniform distribution is that if u 1 {\displaystyle u_{1}} has a standard uniform distribution, then so does 1 − − u 1 .

{\displaystyle 1-u_{1}.} This property can be used for generating antithetic variates , among other things. In other words, this property is known as the inversion method where the continuous standard uniform distribution can be used to generate random numbers for any other continuous distribution.

[ 4 ] If u 1 {\displaystyle u_{1}} is a uniform random number with standard uniform distribution, i.e. with U ( 0 , 1 ) , {\displaystyle U(0,1),} then x = F − − 1 ( u 1 ) {\displaystyle x=F^{-1}(u_{1})} generates a random number x {\displaystyle x} from any continuous distribution with the specified cumulative distribution function F .

{\displaystyle F.} [ 4 ] Relationship to other functions [ edit ] As long as the same conventions are followed at the transition points, the probability density function of the continuous uniform distribution may also be expressed in terms of the Heaviside step function as: f ( x ) = H ⁡ ⁡ ( x − − a ) − − H ⁡ ⁡ ( x − − b ) b − − a , {\displaystyle f(x)={\frac {\operatorname {H} (x-a)-\operatorname {H} (x-b)}{b-a}},} or in terms of the rectangle function as: f ( x ) = 1 b − − a rect ⁡ ⁡ ( x − − a + b 2 b − − a ) .

{\displaystyle f(x)={\frac {1}{b-a}}\ \operatorname {rect} \left({\frac {x-{\frac {a+b}{2}}}{b-a}}\right).} There is no ambiguity at the transition point of the sign function . Using the half-maximum convention at the transition points, the continuous uniform distribution may be expressed in terms of the sign function as: f ( x ) = sgn ⁡ ⁡ ( x − − a ) − − sgn ⁡ ⁡ ( x − − b ) 2 ( b − − a ) .

{\displaystyle f(x)={\frac {\operatorname {sgn} {(x-a)}-\operatorname {sgn} {(x-b)}}{2(b-a)}}.} Properties [ edit ] Moments [ edit ] The mean (first raw moment ) of the continuous uniform distribution is: E ⁡ ⁡ [ X ] = ∫ ∫ a b x d x b − − a = b 2 − − a 2 2 ( b − − a ) = b + a 2 .

{\displaystyle \operatorname {E} [X]=\int _{a}^{b}x{\frac {dx}{b-a}}={\frac {b^{2}-a^{2}}{2(b-a)}}={\frac {b+a}{2}}.} The second raw moment of this distribution is: E ⁡ ⁡ [ X 2 ] = ∫ ∫ a b x 2 d x b − − a = b 3 − − a 3 3 ( b − − a ) .

{\displaystyle \operatorname {E} \left[X^{2}\right]=\int _{a}^{b}x^{2}{\frac {dx}{b-a}}={\frac {b^{3}-a^{3}}{3(b-a)}}.} In general, the n {\displaystyle n} -th raw moment of this distribution is: E ⁡ ⁡ [ X n ] = ∫ ∫ a b x n d x b − − a = b n + 1 − − a n + 1 ( n + 1 ) ( b − − a ) .

{\displaystyle \operatorname {E} \left[X^{n}\right]=\int _{a}^{b}x^{n}{\frac {dx}{b-a}}={\frac {b^{n+1}-a^{n+1}}{(n+1)(b-a)}}.} The variance (second central moment ) of this distribution is: Var ⁡ ⁡ [ X ] = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) 2 ] = ∫ ∫ a b ( x − − a + b 2 ) 2 d x b − − a = ( b − − a ) 2 12 .

{\displaystyle \operatorname {Var} [X]=\operatorname {E} \left[{\left(X-\operatorname {E} [X]\right)}^{2}\right]=\int _{a}^{b}\left(x-{\frac {a+b}{2}}\right)^{2}{\frac {dx}{b-a}}={\frac {(b-a)^{2}}{12}}.} Order statistics [ edit ] Let X 1 , .

.

.

, X n {\displaystyle X_{1},...,X_{n}} be an i.i.d.

sample from U ( 0 , 1 ) , {\displaystyle U(0,1),} and let X ( k ) {\displaystyle X_{(k)}} be the k {\displaystyle k} -th order statistic from this sample.

X ( k ) {\displaystyle X_{(k)}} has a beta distribution , with parameters k {\displaystyle k} and ⁠ n − − k + 1.

{\displaystyle n-k+1.} ⁠ The expected value is: E ⁡ ⁡ [ X ( k ) ] = k n + 1 .

{\displaystyle \operatorname {E} \left[X_{(k)}\right]={\frac {k}{n+1}}.} This fact is useful when making Q–Q plots .

The variance is: Var ⁡ ⁡ [ X ( k ) ] = k ( n − − k + 1 ) ( n + 1 ) 2 ( n + 2 ) .

{\displaystyle \operatorname {Var} \left[X_{(k)}\right]={\frac {k(n-k+1)}{(n+1)^{2}(n+2)}}.} See also: Order statistic § Probability distributions of order statistics Uniformity [ edit ] The probability that a continuously uniformly distributed random variable falls within any interval of fixed length is independent of the location of the interval itself (but it is dependent on the interval size ( ℓ ℓ ) {\displaystyle (\ell )} ), so long as the interval is contained in the distribution's support.

Indeed, if X ∼ ∼ U ( a , b ) {\displaystyle X\sim U(a,b)} and if [ x , x + ℓ ℓ ] {\displaystyle [x,x+\ell ]} is a subinterval of [ a , b ] {\displaystyle [a,b]} with fixed ℓ ℓ > 0 , {\displaystyle \ell >0,} then: Pr ( X ∈ ∈ [ x , x + ℓ ℓ ] ) = ∫ ∫ x x + ℓ ℓ d y b − − a = ℓ ℓ b − − a , {\displaystyle \Pr {\big (}X\in [x,x+\ell ]{\big )}=\int _{x}^{x+\ell }{\frac {dy}{b-a}}={\frac {\ell }{b-a}},} which is independent of x .

{\displaystyle x.} This fact motivates the distribution's name.

Uniform distribution on more general sets [ edit ] The uniform distribution can be generalized to sets more general than intervals.

Formally, let S {\displaystyle S} be a Borel set of positive, finite Lebesgue measure λ λ ( S ) , {\displaystyle \lambda (S),} i.e.

0 < λ λ ( S ) < + ∞ ∞ .

{\displaystyle 0<\lambda (S)<+\infty .} The uniform distribution on S {\displaystyle S} can be specified by defining the probability density function to be zero outside S {\displaystyle S} and constantly equal to 1 λ λ ( S ) {\displaystyle {\tfrac {1}{\lambda (S)}}} on S .

{\displaystyle S.} An interesting special case is when the set S is a simplex . It is possible to obtain a uniform distribution on the standard n -vertex simplex in the following way.

[ 8 ] : Thm.4.1 take n independent random variables with the same exponential distribution ; denote them by X 1 ,...,X n ; and let Y i := X i / (sum i X i ). Then, the vector Y 1 ,...,Y n is uniformly distributed on the simplex.

Related distributions [ edit ] If X has a standard uniform distribution, then by the inverse transform sampling method, Y = − λ −1 ln( X ) has an exponential distribution with (rate) parameter λ .

If X has a standard uniform distribution, then Y = X n has a beta distribution with parameters (1/ n ,1). As such, The Irwin–Hall distribution is the sum of n i.i.d.

U (0,1) distributions.

The Bates distribution is the average of n i.i.d.

U (0,1) distributions.

The standard uniform distribution is a special case of the beta distribution , with parameters (1,1).

The sum of two independent uniform distributions U 1 (a,b)+ U 2 (c,d) yields a trapezoidal distribution , symmetric about its mean, on the support [a+c,b+d]. The plateau has width equals to the absolute different of the width of U 1 and U 2 . The width of the sloped parts corresponds to the width of the narrowest uniform distribution.

If the uniform distributions have the same width w, the result is a triangular distribution , symmetric about its mean, on the support [a+c,a+c+2w].

The sum of two independent, equally distributed, uniform distributions U 1 (a,b)+ U 2 (a,b) yields a symmetric triangular distribution on the support [2a,2b].

The distance between two i.i.d.

uniform random variables | U 1 (a,b)- U 2 (a,b)| also has a triangular distribution , although not symmetric, on the support [0,b-a].

Statistical inference [ edit ] Estimation of parameters [ edit ] Estimation of maximum [ edit ] Minimum-variance unbiased estimator [ edit ] Main article: German tank problem Given a uniform distribution on [ 0 , b ] {\displaystyle [0,b]} with unknown b , {\displaystyle b,} the minimum-variance unbiased estimator (UMVUE) for the maximum is: b ^ ^ UMVU = k + 1 k m = m + m k , {\displaystyle {\hat {b}}_{\text{UMVU}}={\frac {k+1}{k}}m=m+{\frac {m}{k}},} where m {\displaystyle m} is the sample maximum and k {\displaystyle k} is the sample size , sampling without replacement (though this distinction almost surely makes no difference for a continuous distribution). This follows for the same reasons as estimation for the discrete distribution , and can be seen as a very simple case of maximum spacing estimation . This problem is commonly known as the German tank problem , due to application of maximum estimation to estimates of German tank production during World War II .

Method of moments estimator [ edit ] The method of moments estimator is: b ^ ^ M M = 2 X ¯ ¯ , {\displaystyle {\hat {b}}_{MM}=2{\bar {X}},} where X ¯ ¯ {\displaystyle {\bar {X}}} is the sample mean.

Maximum likelihood estimator [ edit ] The maximum likelihood estimator is: b ^ ^ M L = m , {\displaystyle {\hat {b}}_{ML}=m,} where m {\displaystyle m} is the sample maximum , also denoted as m = X ( n ) , {\displaystyle m=X_{(n)},} the maximum order statistic of the sample.

Estimation of minimum [ edit ] Given a uniform distribution on [ a , b ] {\displaystyle [a,b]} with unknown a , the maximum likelihood estimator for a is: a ^ ^ M L = min { X 1 , … … , X n } {\displaystyle {\hat {a}}_{ML}=\min\{X_{1},\dots ,X_{n}\}} ,
the sample minimum .

[ 9 ] Estimation of midpoint [ edit ] The midpoint of the distribution, a + b 2 , {\displaystyle {\tfrac {a+b}{2}},} is both the mean and the median of the uniform distribution. Although both the sample mean and the sample median are unbiased estimators of the midpoint, neither is as efficient as the sample mid-range , i.e. the arithmetic mean of the sample maximum and the sample minimum, which is the UMVU estimator of the midpoint (and also the maximum likelihood estimate ).

Confidence interval [ edit ] For the maximum [ edit ] Let X 1 , X 2 , X 3 , .

.

.

, X n {\displaystyle X_{1},X_{2},X_{3},...,X_{n}} be a sample from U [ 0 , L ] , {\displaystyle U_{[0,L]},} where L {\displaystyle L} is the maximum value in the population. Then X ( n ) = max ( X 1 , X 2 , X 3 , .

.

.

, X n ) {\displaystyle X_{(n)}=\max(X_{1},X_{2},X_{3},...,X_{n})} has the Lebesgue–Borel density f = d Pr X ( n ) d λ λ : {\displaystyle f={\frac {d\Pr _{X_{(n)}}}{d\lambda }}:} [ 10 ] f ( t ) = n 1 L ( t L ) n − − 1 = n t n − − 1 L n 1 1 [ 0 , L ] ( t ) , {\displaystyle f(t)=n{\frac {1}{L}}\left({\frac {t}{L}}\right)^{n-1}\!=n{\frac {t^{n-1}}{L^{n}}}1\!\!1_{[0,L]}(t),} where 1 1 [ 0 , L ] {\displaystyle 1\!\!1_{[0,L]}} is the indicator function of [ 0 , L ] .

{\displaystyle [0,L].} The confidence interval given before is mathematically incorrect, as Pr ( [ θ θ ^ ^ , θ θ ^ ^ + ε ε ] ∋ ∋ θ θ ) ≥ ≥ 1 − − α α {\displaystyle \Pr {\big (}[{\hat {\theta }},{\hat {\theta }}+\varepsilon ]\ni \theta {\big )}\geq 1-\alpha } cannot be solved for ε ε {\displaystyle \varepsilon } without knowledge of θ θ {\displaystyle \theta } . However, one can solve Pr ( [ θ θ ^ ^ , θ θ ^ ^ ( 1 + ε ε ) ] ∋ ∋ θ θ ) ≥ ≥ 1 − − α α {\displaystyle \Pr {\big (}[{\hat {\theta }},{\hat {\theta }}(1+\varepsilon )]\ni \theta {\big )}\geq 1-\alpha } for ε ε ≥ ≥ ( 1 − − α α ) − − 1 / n − − 1 {\displaystyle \varepsilon \geq (1-\alpha )^{-1/n}-1} for any unknown but valid θ θ ; {\displaystyle \theta ;} one then chooses the smallest ε ε {\displaystyle \varepsilon } possible satisfying the condition above. Note that the interval length depends upon the random variable θ θ ^ ^ .

{\displaystyle {\hat {\theta }}.} Occurrence and applications [ edit ] The probabilities for uniform distribution function are simple to calculate due to the simplicity of the function form.

[ 2 ] Therefore, there are various applications that this distribution can be used for as shown below: hypothesis testing situations, random sampling cases, finance, etc. Furthermore, generally, experiments of physical origin follow a uniform distribution (e.g. emission of radioactive particles ).

[ 1 ] However, it is important to note that in any application, there is the unchanging assumption that the probability of falling in an interval of fixed length is constant.

[ 2 ] Economics example for uniform distribution [ edit ] In the field of economics, usually demand and replenishment may not follow the expected normal distribution. As a result, other distribution models are used to better predict probabilities and trends such as Bernoulli process .

[ 11 ] But according to Wanke (2008), in the particular case of investigating lead-time for inventory management at the beginning of the life cycle when a completely new product is being analyzed, the uniform distribution proves to be more useful.

[ 11 ] In this situation, other distribution may not be viable since there is no existing data on the new product or that the demand history is unavailable so there isn't really an appropriate or known distribution.

[ 11 ] The uniform distribution would be ideal in this situation since the random variable of lead-time (related to demand) is unknown for the new product but the results are likely to range between a plausible range of two values.

[ 11 ] The lead-time would thus represent the random variable. From the uniform distribution model, other factors related to lead-time were able to be calculated such as cycle service level and shortage per cycle . It was also noted that the uniform distribution was also used due to the simplicity of the calculations.

[ 11 ] Sampling from an arbitrary distribution [ edit ] Main article: Inverse transform sampling The uniform distribution is useful for sampling from arbitrary distributions. A general method is the inverse transform sampling method, which uses the cumulative distribution function (CDF) of the target random variable. This method is very useful in theoretical work. Since simulations using this method require inverting the CDF of the target variable, alternative methods have been devised for the cases where the CDF is not known in closed form. One such method is rejection sampling .

The normal distribution is an important example where the inverse transform method is not efficient. However, there is an exact method, the Box–Muller transformation , which uses the inverse transform to convert two independent uniform random variables into two independent normally distributed random variables.

Quantization error [ edit ] Main article: Quantization error In analog-to-digital conversion, a quantization error occurs. This error is either due to rounding or truncation. When the original signal is much larger than one least significant bit (LSB) , the quantization error is not significantly correlated with the signal, and has an approximately uniform distribution. The RMS error therefore follows from the variance of this distribution.

Random variate generation [ edit ] There are many applications in which it is useful to run simulation experiments. Many programming languages come with implementations to generate pseudo-random numbers which are effectively distributed according to the standard uniform distribution.

On the other hand, the uniformly distributed numbers are often used as the basis for non-uniform random variate generation .

If u {\displaystyle u} is a value sampled from the standard uniform distribution, then the value a + ( b − − a ) u {\displaystyle a+(b-a)u} follows the uniform distribution parameterized by a {\displaystyle a} and b , {\displaystyle b,} as described above.

History [ edit ] While the historical origins in the conception of uniform distribution are inconclusive, it is speculated that the term "uniform" arose from the concept of equiprobability in dice games (note that the dice games would have discrete and not continuous uniform sample space).

Equiprobability was mentioned in Gerolamo Cardano 's Liber de Ludo Aleae , a manual written in 16th century and detailed on advanced probability calculus in relation to dice.

[ 12 ] See also [ edit ] Discrete uniform distribution Beta distribution Box–Muller transform Probability plot Q–Q plot Rectangular function Irwin–Hall distribution — In the degenerate case where n=1, the Irwin-Hall distribution generates a uniform distribution between 0 and 1.

Bates distribution — Similar to the Irwin-Hall distribution, but rescaled for n. Like the Irwin-Hall distribution, in the degenerate case where n=1, the Bates distribution generates a uniform distribution between 0 and 1.

References [ edit ] ^ a b c Dekking, Michel (2005).

A modern introduction to probability and statistics : understanding why and how . London, UK: Springer. pp.

60 –61.

ISBN 978-1-85233-896-1 .

^ a b c Walpole, Ronald; et al. (2012).

Probability & Statistics for Engineers and Scientists . Boston, USA: Prentice Hall. pp.

171– 172.

ISBN 978-0-321-62911-1 .

^ Park, Sung Y.; Bera, Anil K. (2009). "Maximum entropy autoregressive conditional heteroskedasticity model".

Journal of Econometrics .

150 (2): 219– 230.

CiteSeerX 10.1.1.511.9750 .

doi : 10.1016/j.jeconom.2008.12.014 .

^ a b c "Uniform Distribution (Continuous)" .

MathWorks . 2019 . Retrieved November 22, 2019 .

^ a b c Illowsky, Barbara; et al. (2013).

Introductory Statistics . Rice University, Houston, Texas, USA: OpenStax College. pp.

296 –304.

ISBN 978-1-938168-20-8 .

^ Casella & Berger 2001 , p. 626 ^ Wichura, Michael J. (January 11, 2001).

"Cumulants" (PDF) .

Stat 304 Handouts . University of Chicago.

^ Non-Uniform Random Variate Generation .

doi : 10.1007/978-1-4613-8643-8 .

^ L n ( a , b ) = ∏ ∏ i = 1 n f ( X i ) = 1 ( b − − a ) n 1 [ a , b ] ( X 1 , … … , X n ) {\displaystyle L_{n}(a,b)=\prod _{i=1}^{n}f(X_{i})={\frac {1}{(b-a)^{n}}}\mathbf {1} _{[a,b]}(X_{1},\dots ,X_{n})} = 1 ( b − − a ) n 1 { a ≤ ≤ min { X 1 , … … , X n } } 1 { max { X 1 , … … , X n } ≤ ≤ b } {\displaystyle ={\frac {1}{(b-a)^{n}}}\mathbf {1} _{\{a\leq \min\{X_{1},\dots ,X_{n}\}\}}\mathbf {1} _{\{\max\{X_{1},\dots ,X_{n}\}\leq b\}}} .

Since we have n ≥ ≥ 1 {\displaystyle n\geq 1} the factor 1 ( b − − a ) n {\displaystyle {\frac {1}{(b-a)^{n}}}} is maximized by biggest possible a , which is limited in L n ( a , b ) {\displaystyle L_{n}(a,b)} by min { X 1 , … … , X n } {\displaystyle \min\{X_{1},\dots ,X_{n}\}} . Therefore a = min { X 1 , … … , X n } {\displaystyle a=\min\{X_{1},\dots ,X_{n}\}} is the maximum of L n ( a , b ) {\displaystyle L_{n}(a,b)} .

^ Nechval KN, Nechval NA, Vasermanis EK, Makeev VY (2002) Constructing shortest-length confidence intervals . Transport and Telecommunication 3 (1) 95-103 ^ a b c d e Wanke, Peter (2008).

"The uniform distribution as a first practical approach to new product inventory management" .

International Journal of Production Economics .

114 (2): 811– 819.

doi : 10.1016/j.ijpe.2008.04.004 – via Research Gate.

^ Bellhouse, David (May 2005).

"Decoding Cardano's Liber de Ludo" .

Historia Mathematica .

32 : 180– 202.

doi : 10.1016/j.hm.2004.04.001 .

Further reading [ edit ] Casella, George; Roger L. Berger (2001), Statistical Inference (2nd ed.), Thomson Learning, ISBN 978-0-534-24312-8 , LCCN 2001025794 External links [ edit ] Online calculator of Uniform distribution (continuous) v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Authority control databases National United States Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐rmngm
Cached time: 20250817043924
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.746 seconds
Real time usage: 1.066 seconds
Preprocessor visited node count: 2858/1000000
Revision size: 28710/2097152 bytes
Post‐expand include size: 98333/2097152 bytes
Template argument size: 2772/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 62636/5000000 bytes
Lua time usage: 0.335/10.000 seconds
Lua memory usage: 7308846/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  534.751      1 -total
 35.64%  190.572      1 Template:Reflist
 20.87%  111.587      4 Template:Cite_book
 19.61%  104.877      1 Template:Short_description
 19.12%  102.245      4 Template:Navbox
 19.09%  102.094      1 Template:ProbDistributions
 13.09%   70.021      2 Template:Pagetype
  7.16%   38.279      1 Template:Probability_distribution
  5.41%   28.906      1 Template:Authority_control
  5.12%   27.368      1 Template:Harvnb Saved in parser cache with key enwiki:pcache:1699223:|#|:idhash:canonical and timestamp 20250817043924 and revision id 1284162921. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Continuous_uniform_distribution&oldid=1284162921 " Categories : Continuous distributions Location-scale family probability distributions Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 5 April 2025, at 23:30 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Continuous uniform distribution 44 languages Add topic

