Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Example 2 Uses Toggle Uses subsection 2.1 Increase power 2.2 Adjusting preexisting differences 3 Assumptions Toggle Assumptions subsection 3.1 Assumption 1: linearity of regression 3.2 Assumption 2: homogeneity of error variances 3.3 Assumption 3: independence of error terms 3.4 Assumption 4: normality of error terms 3.5 Assumption 5: homogeneity of regression slopes 4 Conducting an ANCOVA Toggle Conducting an ANCOVA subsection 4.1 Test multicollinearity 4.2 Test the homogeneity of variance assumption 4.3 Test the homogeneity of regression slopes assumption 4.4 Run ANCOVA analysis 4.5 Follow-up analyses 5 Power considerations 6 See also 7 References 8 External links Toggle the table of contents Analysis of covariance 12 languages Català Deutsch Español فارسی Français Latviešu Magyar 日本語 Polski Português Українська 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia General linear model that blends ANOVA and regression "Ancova" redirects here. For the moth genus, see Ancova (moth) .

Analysis of covariance ( ANCOVA ) is a general linear model that blends ANOVA and regression . ANCOVA evaluates whether the means of a dependent variable (DV) are equal across levels of one or more categorical independent variables (IV) and across one or more continuous variables. For example, the categorical variable(s) might describe treatment and the continuous variable(s) might be covariates (CV)'s, typically nuisance variables; or vice versa. Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance. Intuitively, ANCOVA can be thought of as 'adjusting' the DV by the group means of the CV(s).

[ 1 ] The ANCOVA model assumes a linear relationship between the response (DV) and covariate (CV): y i j = μ μ + τ τ i + B ( x i j − − x ¯ ¯ ) + ϵ ϵ i j .

{\displaystyle y_{ij}=\mu +\tau _{i}+\mathrm {B} (x_{ij}-{\overline {x}})+\epsilon _{ij}.} In this equation, the DV, y i j {\displaystyle y_{ij}} is the jth observation under the ith categorical group; the CV, x i j {\displaystyle x_{ij}} is the j th observation of the covariate under the i th group. Variables in the model that are derived from the observed data are μ μ {\displaystyle \mu } (the grand mean) and x ¯ ¯ {\displaystyle {\overline {x}}} (the global mean for covariate x {\displaystyle x} ). The variables to be fitted are τ τ i {\displaystyle \tau _{i}} (the effect of the i th level of the categorical IV), B {\displaystyle B} (the slope of the line) and ϵ ϵ i j {\displaystyle \epsilon _{ij}} (the associated unobserved error term for the j th observation in the i th group).

Under this specification, the categorical treatment effects sum to zero ( ∑ ∑ i a τ τ i = 0 ) .

{\displaystyle \left(\sum _{i}^{a}\tau _{i}=0\right).} The standard assumptions of the linear regression model are also assumed to hold, as discussed below.

[ 2 ] Example [ edit ] In an agricultural study, ANCOVA can be used to analyze the effect of different fertilizers ( τ τ i {\displaystyle \tau _{i}} ) on crop yield ( y i j {\displaystyle y_{ij}} ), while accounting for soil quality ( x i j {\displaystyle x_{ij}} ) as a covariate. Soil quality, a continuous variable, influences crop yield and may vary across plots, potentially confounding the results.

The model adjusts yield measurements for soil quality differences and evaluates whether fertilizer types differ significantly. Mathematically, this can be expressed as: y i j = μ μ + τ τ i + β β ( x i j − − x ¯ ¯ ) + ϵ ϵ i j , {\displaystyle y_{ij}=\mu +\tau _{i}+\beta (x_{ij}-{\overline {x}})+\epsilon _{ij},} where: y i j {\displaystyle y_{ij}} is the crop yield for the j {\displaystyle j} -th plot under the i {\displaystyle i} -th fertilizer type, μ μ {\displaystyle \mu } is the grand mean crop yield, τ τ i {\displaystyle \tau _{i}} represents the effect of the i {\displaystyle i} -th fertilizer type, subject to the constraint ∑ ∑ i τ τ i = 0 {\displaystyle \sum _{i}\tau _{i}=0} for identifiability, β β {\displaystyle \beta } is the slope of the regression line representing the relationship between soil quality and crop yield, x i j {\displaystyle x_{ij}} is the soil quality for the j {\displaystyle j} -th plot under the i {\displaystyle i} -th fertilizer type, and x ¯ ¯ {\displaystyle {\overline {x}}} is the global mean soil quality, ϵ ϵ i j {\displaystyle \epsilon _{ij}} is the residual error term, assumed to be normally distributed with mean 0 and variance σ σ 2 {\displaystyle \sigma ^{2}} .

In this setup, ANCOVA partitions the total variance in crop yield into variance explained by soil quality (covariate), variance explained by fertilizer type (categorical IV), and residual variance. By adjusting for soil quality, ANCOVA provides a more precise estimate of the fertilizer effect on crop yield. The constraint ∑ ∑ i τ τ i = 0 {\displaystyle \sum _{i}\tau _{i}=0} ensures that the categorical variable's effects are centered around zero, allowing for meaningful interpretation of group differences. It is standard in ANOVA and ANCOVA with categorical variables.

Uses [ edit ] Increase power [ edit ] ANCOVA can be used to increase statistical power (the probability a significant difference is found between groups when one exists) by reducing the within-group error variance .

[ 3 ] In order to understand this, it is necessary to understand the test used to evaluate differences between groups, the F-test . The F -test is computed by dividing the explained variance between groups (e.g., medical recovery differences) by the unexplained variance within the groups. Thus, F = M S b e t w e e n M S w i t h i n {\displaystyle F={\frac {MS_{between}}{MS_{within}}}} If this value is larger than a critical value, we conclude that there is a significant difference between groups. Unexplained variance includes error variance (e.g., individual differences), as well as the influence of other factors. Therefore, the influence of CVs is grouped in the denominator. When we control for the effect of CVs on the DV, we remove it from the denominator making F larger, thereby increasing our power to find a significant effect if one exists at all.

Partitioning variance Adjusting preexisting differences [ edit ] Another use of ANCOVA is to adjust for preexisting differences in nonequivalent (intact) groups. This controversial application aims at correcting for initial group differences (prior to group assignment) that exists on DV among several intact groups. In this situation, participants cannot be made equal through random assignment, so CVs are used to adjust scores and make participants more similar than without the CV. However, even with the use of covariates, there are no statistical techniques that can equate unequal groups. Furthermore, the CV may be so intimately related to the categorical IV that removing the variance on the DV associated with the CV would remove considerable variance on the DV, rendering the results meaningless.

[ 4 ] Assumptions [ edit ] There are several key assumptions that underlie the use of ANCOVA and affect interpretation of the results.

[ 2 ] The standard linear regression assumptions hold; further we assume that the slope of the covariate is equal across all treatment groups (homogeneity of regression slopes).

Assumption 1: linearity of regression [ edit ] The regression relationship between the dependent variable and concomitant variables must be linear.

Assumption 2: homogeneity of error variances [ edit ] The error is a random variable with conditional zero mean and equal variances for different treatment classes and observations.

Assumption 3: independence of error terms [ edit ] The errors are uncorrelated. That is, the error covariance matrix is diagonal.

Assumption 4: normality of error terms [ edit ] The error terms should be normally distributed ϵ ϵ i j {\displaystyle \epsilon _{ij}} ~ N ( 0 , σ σ 2 ) {\displaystyle N(0,\sigma ^{2})} .

Assumption 5: homogeneity of regression slopes [ edit ] The slopes of the different regression lines should be equivalent, i.e., regression lines should be parallel among groups.

The fifth issue, concerning the homogeneity of different treatment regression slopes is particularly important in evaluating the appropriateness of ANCOVA model. Also note that we only need the error terms to be normally distributed. In fact both the independent variable and the concomitant variables will not be normally distributed in most cases.

Conducting an ANCOVA [ edit ] Test multicollinearity [ edit ] If a CV is highly related to another CV (at a correlation of 0.5 or more), then it will not adjust the DV over and above the other CV. One or the other should be removed since they are statistically redundant.

Test the homogeneity of variance assumption [ edit ] Tested by Levene's test of equality of error variances.
This is most important after adjustments have been made, but if you have it before adjustment you are likely to have it afterwards.

Test the homogeneity of regression slopes assumption [ edit ] To see if the CV significantly interacts with the categorical IV, run an ANCOVA model including both the IV and the CVxIV interaction term. 
If the CVxIV interaction is significant, ANCOVA should not be performed. Instead, Green & Salkind [ 5 ] suggest assessing group differences on the DV at particular levels of the CV. Also consider using a moderated regression analysis , treating the CV and its interaction as another IV. Alternatively, one could use mediation analyses to determine if the CV accounts for the IV's effect on the DV [ citation needed ] .

Run ANCOVA analysis [ edit ] If the CV×IV interaction is not significant, rerun the ANCOVA without the CV×IV interaction term.  
In this analysis, you need to use the adjusted means and adjusted mean squared error . The adjusted means (also referred to as least squares means, LS means, estimated marginal means, or EMM) refer to the group means after controlling for the influence of the CV on the DV.

Simple main effects plot showing a small Interaction between the two levels of the independent variable.

Follow-up analyses [ edit ] If there was a significant main effect , it means that there is a significant difference between the levels of one categorical IV, ignoring all other factors.

[ 6 ] To find exactly which levels are significantly different from one another, one can use the same follow-up tests as for the ANOVA.
If there are two or more IVs, there may be a significant interaction , which means that the effect of one IV on the DV changes depending on the level of another factor. One can investigate the simple main effects using the same methods as in a factorial ANOVA .

Power considerations [ edit ] While the inclusion of a covariate into an ANOVA generally increases statistical power [ 7 ] by accounting for some of the variance in the dependent variable and thus increasing the ratio of variance explained by the independent variables, adding a covariate into ANOVA also reduces the degrees of freedom . Accordingly, adding a covariate which accounts for very little variance in the dependent variable might actually reduce power.

See also [ edit ] MANCOVA (Multivariate analysis of covariance) References [ edit ] ^ Keppel, G. (1991).

Design and analysis: A researcher's handbook (3rd ed.). Englewood Cliffs: Prentice-Hall, Inc.

^ a b Montgomery, Douglas C. "Design and analysis of experiments" (8th Ed.). John Wiley & Sons, 2012.

^ Tabachnick, B. G.; Fidell, L. S. (2007).

Using Multivariate Statistics (5th ed.). Boston: Pearson Education.

^ Miller, G. A.; Chapman, J. P. (2001). "Misunderstanding Analysis of Covariance".

Journal of Abnormal Psychology .

110 (1): 40– 48.

doi : 10.1037/0021-843X.110.1.40 .

PMID 11261398 .

^ Green, S. B., & Salkind, N. J. (2011).

Using SPSS for Windows and Macintosh: Analyzing and Understanding Data (6th ed.). Upper Saddle River, NJ: Prentice Hall.

^ Howell, D. C. (2009) Statistical methods for psychology (7th ed.). Belmont: Cengage Wadsworth.

^ Barrett, Timothy J. (2011).

"Computations using analysis of covariance" .

WIREs Computational Statistics .

3 (3): 260– 268.

doi : 10.1002/wics.165 .

ISSN 1939-0068 .

External links [ edit ] Wikiversity has learning resources about ANCOVA Examples of all ANOVA and ANCOVA models with up to three treatment factors, including randomized block, split plot, repeated measures, and Latin squares, and their analysis in R (University of Southampton) One-Way Analysis of Covariance for Independent Samples What is analysis of covariance used for?

Use of covariates in randomized controlled trials by G.J.P. Van Breukelen and K.R.A. Van Dijk (2007) v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Design of experiments Scientific method Scientific experiment Statistical design Control Internal and external validity Experimental unit Blinding Optimal design : Bayesian Random assignment Randomization Restricted randomization Replication versus subsampling Sample size Treatment and blocking Treatment Effect size Contrast Interaction Confounding Orthogonality Blocking Covariate Nuisance variable Models and inference Linear regression Ordinary least squares Bayesian Random effect Mixed model Hierarchical model: Bayesian Analysis of variance (Anova) Cochran's theorem Manova ( multivariate ) Ancova ( covariance ) Compare means Multiple comparison Designs Completely randomized Factorial Fractional factorial Plackett–Burman Taguchi Response surface methodology Polynomial and rational modeling Box–Behnken Central composite Block Generalized randomized block design (GRBD) Latin square Graeco-Latin square Orthogonal array Latin hypercube Repeated measures design Crossover study Randomized controlled trial Sequential analysis Sequential probability ratio test Glossary Category Mathematics portal Statistical outline Statistical topics v t e Least squares and regression analysis Computational statistics Least squares Linear least squares Non-linear least squares Iteratively reweighted least squares Correlation and dependence Pearson product-moment correlation Rank correlation ( Spearman's rho Kendall's tau ) Partial correlation Confounding variable Regression analysis Ordinary least squares Partial least squares Total least squares Ridge regression Regression as a statistical model Linear regression Simple linear regression Ordinary least squares Generalized least squares Weighted least squares General linear model Predictor structure Polynomial regression Growth curve (statistics) Segmented regression Local regression Non-standard Nonlinear regression Nonparametric Semiparametric Robust Quantile Isotonic Non-normal errors Generalized linear model Binomial Poisson Logistic Decomposition of variance Analysis of variance Analysis of covariance Multivariate AOV Model exploration Stepwise regression Model selection Mallows's C p AIC BIC Model specification Regression validation Background Mean and predicted response Gauss–Markov theorem Errors and residuals Goodness of fit Studentized residual Minimum mean-square error Frisch–Waugh–Lovell theorem Design of experiments Response surface methodology Optimal design Bayesian design Numerical approximation Numerical analysis Approximation theory Numerical integration Gaussian quadrature Orthogonal polynomials Chebyshev polynomials Chebyshev nodes Applications Curve fitting Calibration curve Numerical smoothing and differentiation System identification Moving least squares Regression analysis category Statistics category Mathematics portal Statistics outline Statistics topics Retrieved from " https://en.wikipedia.org/w/index.php?title=Analysis_of_covariance&oldid=1294864624 " Categories : Analysis of variance Covariance and correlation Hidden categories: Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from December 2022 This page was last edited on 10 June 2025, at 07:19 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Analysis of covariance 12 languages Add topic

