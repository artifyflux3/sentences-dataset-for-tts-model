Title: Curse of dimensionality

URL Source: https://en.wikipedia.org/wiki/Curse_of_dimensionality

Published Time: 2004-07-06T08:31:52Z

Markdown Content:
The **curse of dimensionality** refers to various phenomena that arise when analyzing and organizing data in [high-dimensional spaces](https://en.wikipedia.org/wiki/High-dimensional_space "High-dimensional space") that do not occur in low-dimensional settings such as the [three-dimensional](https://en.wikipedia.org/wiki/Three-dimensional_space "Three-dimensional space")[physical space](https://en.wikipedia.org/wiki/Physical_space "Physical space") of everyday experience. The expression was coined by [Richard E. Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman "Richard E. Bellman") when considering problems in [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming "Dynamic programming").[[1]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-1)[[2]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-2) The curse generally refers to issues that arise when the number of datapoints is small (in a suitably defined sense) relative to the intrinsic dimension of the data.

Dimensionally cursed phenomena occur in domains such as [numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis "Numerical analysis"), [sampling](https://en.wikipedia.org/wiki/Sampling_(statistics) "Sampling (statistics)"), [combinatorics](https://en.wikipedia.org/wiki/Combinatorics "Combinatorics"), [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning"), [data mining](https://en.wikipedia.org/wiki/Data_mining "Data mining") and [databases](https://en.wikipedia.org/wiki/Database "Database"). The common theme of these problems is that when the dimensionality increases, the [volume](https://en.wikipedia.org/wiki/Volume "Volume") of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.

In some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the [combinatorial explosion](https://en.wikipedia.org/wiki/Combinatorial_explosion "Combinatorial explosion"). Even in the simplest case of ![Image 1: {\displaystyle d}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab)[binary variables](https://en.wikipedia.org/wiki/Binary_data "Binary data"), the number of possible combinations already is ![Image 2: {\displaystyle 2^{d}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e78df590f3fc81f0201082eaaa6844c145c8bdf3), exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.

There is an exponential increase in volume associated with adding extra dimensions to a [mathematical space](https://en.wikipedia.org/wiki/Space_(mathematics) "Space (mathematics)"). For example, 10 2= 100 evenly spaced sample points suffice to sample a [unit interval](https://en.wikipedia.org/wiki/Unit_interval "Unit interval") (try to visualize a "1-dimensional" cube, i.e. a line) with no more than 10−2 = 0.01 distance between points; an equivalent sampling of a 10-dimensional [unit hypercube](https://en.wikipedia.org/wiki/Unit_hypercube "Unit hypercube") with a lattice that has a spacing of 10−2 = 0.01 between adjacent points would require 10 20 = [(10 2)10] sample points. In general, with a spacing distance of 10−_n_ the 10-dimensional hypercube appears to be a factor of 10 _n_(10−1) = [(10 _n_)10/(10 _n_)] "larger" than the 1-dimensional hypercube, which is the unit interval. In the above example _n_ = 2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 10 18 "larger" than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.

When solving dynamic [optimization](https://en.wikipedia.org/wiki/Optimization_(mathematics) "Optimization (mathematics)") problems by numerical [backward induction](https://en.wikipedia.org/wiki/Backward_induction "Backward induction"), the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the "state variable" is large.[[3]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-3)

In [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") problems that involve learning a "state-of-nature" from a finite number of data samples in a high-dimensional [feature space](https://en.wikipedia.org/wiki/Feature_space "Feature space") with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. In an abstract sense, as the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.[[4]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-4)

A typical rule of thumb is that there should be at least 5 training examples for each dimension in the representation.[[5]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Pattern_recog-5) In [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") and insofar as predictive performance is concerned, the _curse of dimensionality_ is used interchangeably with the _peaking phenomenon_,[[5]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Pattern_recog-5) which is also known as _Hughes phenomenon_.[[6]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-6) This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as the number of dimensions or features used is increased but beyond a certain dimensionality it starts deteriorating instead of improving steadily.[[7]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-7)[[8]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-8)[[9]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-McLachlan:2004-9)

Nevertheless, in the context of a _simple_ classifier (e.g., [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis "Linear discriminant analysis") in the multivariate Gaussian model under the assumption of a common known covariance matrix), Zollanvari, _et al._, showed both analytically and empirically that as long as the relative cumulative efficacy of an additional feature set (with respect to features that are already part of the classifier) is greater (or less) than the size of this additional feature set, the expected error of the classifier constructed using these additional features will be less (or greater) than the expected error of the classifier constructed without them. In other words, both the size of additional features and their (relative) cumulative discriminatory effect are important in observing a decrease or increase in the average predictive power.[[10]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-zollanvari-10)

In [metric learning](https://en.wikipedia.org/wiki/Similarity_learning "Similarity learning"), higher dimensions can sometimes allow a model to achieve better performance. After normalizing embeddings to the surface of a hypersphere, FaceNet achieves the best performance using 128 dimensions as opposed to 64, 256, or 512 dimensions in one ablation study.[[11]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-11) A loss function for unitary-invariant dissimilarity between word embeddings was found to be minimized in high dimensions.[[12]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-12)

Genetic mutations in individuals data set | Individual name | Gene 1 | Gene 2 | ... | Gene 2000 |
| --- | --- | --- | --- | --- |
| Individual 1 | 1 | 0 | ... | 1 |
| ... | ... | ... | ... | ... |
| Individual 200 | 0 | 1 | ... | 1 |

In [data mining](https://en.wikipedia.org/wiki/Data_mining "Data mining"), the curse of dimensionality refers to a data set with too many features.

Consider the first table, which depicts 200 individuals and 2000 genes (features) with a 1 or 0 denoting whether or not they have a genetic mutation in that gene. A data mining application to this data set may be finding the correlation between specific genetic mutations and creating a classification algorithm such as a [decision tree](https://en.wikipedia.org/wiki/Decision_tree "Decision tree") to determine whether an individual has cancer or not.

Growth of association pair permutations as pair size grows | Number of pairs | Calculation for permutations | Number of permutations calculated for each row |
| --- | --- | --- |
| 2 | ![Image 3: {\displaystyle 2000!/(2000-2)!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6919bac120998dc9a118c7db6db2290680f6d7eb) | 3 998 000 |
| 3 | ![Image 4: {\displaystyle 2000!/(2000-3)!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8dd346f2ea647c92b300969f4ef89101b8e06807) | 7 988 004 000 |
| 4 | ![Image 5: {\displaystyle 2000!/(2000-4)!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43a91339cf75cd19c8c2652c5d0c1f227c0a71da) | 15 952 043 988 000 |
| 5 | ![Image 6: {\displaystyle 2000!/(2000-5)!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2757f6d06ff55c3203d7ea4d22eec94d6c8787d8) | 31 840 279 800 048 000 |

A common practice of data mining in this domain would be to create [association rules](https://en.wikipedia.org/wiki/Association_rules "Association rules") between genetic mutations that lead to the development of cancers. To do this, one would have to loop through each genetic mutation of each individual and find other genetic mutations that occur over a desired threshold and create pairs. They would start with pairs of two, then three, then four until they result in an empty set of pairs. The complexity of this algorithm can lead to calculating all permutations of gene pairs for each individual or row. Given the formula for calculating the permutations of n items with a group size of r is: ![Image 7: {\displaystyle {\frac {n!}{(n-r)!}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a3815bff4f871b3966ff933b696081a299831549), calculating the number of three pair permutations of any given individual would be 7988004000 different pairs of genes to evaluate for each individual. The number of pairs created will grow by an order of factorial as the size of the pairs increase. The growth is depicted in the permutation table (see right).

As we can see from the permutation table above, one of the major problems data miners face regarding the curse of dimensionality is that the space of possible parameter values grows exponentially or factorially as the number of features in the data set grows. This problem critically affects both computational time and space when searching for associations or optimal features to consider.

Another problem data miners may face when dealing with too many features is that the number of false predictions or classifications tends to increase as the number of features grows in the data set. In terms of the classification problem discussed above, keeping every data point could lead to a higher number of [false positives and false negatives](https://en.wikipedia.org/wiki/False_positives_and_false_negatives "False positives and false negatives") in the model.

This may seem counterintuitive, but consider the genetic mutation table from above, depicting all genetic mutations for each individual. Each genetic mutation, whether they correlate with cancer or not, will have some input or weight in the model that guides the decision-making process of the algorithm. There may be mutations that are [outliers](https://en.wikipedia.org/wiki/Outliers "Outliers") or ones that dominate the overall distribution of genetic mutations when in fact they do not correlate with cancer. These features may be working against one's model, making it more difficult to obtain optimal results.

This problem is up to the data miner to solve, and there is no universal solution. The first step any data miner should take is to explore the data, in an attempt to gain an understanding of how it can be used to solve the problem. One must first understand what the data means, and what they are trying to discover before they can decide if anything must be removed from the data set. Then they can create or use a [feature selection](https://en.wikipedia.org/wiki/Feature_selection "Feature selection") or [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction "Dimensionality reduction") algorithm to remove samples or features from the data set if they deem it necessary. One example of such methods is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range "Interquartile range") method, used to remove [outliers](https://en.wikipedia.org/wiki/Outliers "Outliers") in a data set by calculating the standard deviation of a feature or occurrence.

When a measure such as a [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance "Euclidean distance") is defined using many coordinates, there is little difference in the distances between different pairs of points.

One way to illustrate the "vastness" of high-dimensional Euclidean space is to compare the proportion of an inscribed [hypersphere](https://en.wikipedia.org/wiki/Hypersphere "Hypersphere") with radius ![Image 8: {\displaystyle r}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538) and dimension ![Image 9: {\displaystyle d}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab), to that of a [hypercube](https://en.wikipedia.org/wiki/Hypercube "Hypercube") with edges of length ![Image 10: {\displaystyle 2r.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f7e9c3fbe39aa463f573b1cbec01883275295ff2) The volume of such a sphere is ![Image 11: {\displaystyle {\frac {2r^{d}\pi ^{d/2}}{d\;\Gamma (d/2)}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/34c8e9ef702e36d4b6c3caa01ce2812bdc017b18), where [![Image 12: {\displaystyle \Gamma }](https://wikimedia.org/api/rest_v1/media/math/render/svg/4cfde86a3f7ec967af9955d0988592f0693d2b19)](https://en.wikipedia.org/wiki/Gamma_function "Gamma function") is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function "Gamma function"), while the volume of the cube is ![Image 13: {\displaystyle (2r)^{d}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e4b70ddaebe60763b3fc154e45d328f835de17f9). As the dimension ![Image 14: {\displaystyle d}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab) of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be [seen](https://commons.wikimedia.org/wiki/File:Ball-cube-volume-ratio-semilog.png "commons:File:Ball-cube-volume-ratio-semilog.png") by comparing the proportions as the dimension ![Image 15: {\displaystyle d}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab) goes to infinity:

![Image 16: {\displaystyle {\frac {V_{\mathrm {hypersphere} }}{V_{\mathrm {hypercube} }}}={\frac {\pi ^{d/2}}{d2^{d-1}\Gamma (d/2)}}\rightarrow 0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0b1e17f55f56082ada1da60e44dee2bb1a10495d) as ![Image 17: {\displaystyle d\rightarrow \infty }](https://wikimedia.org/api/rest_v1/media/math/render/svg/bce9ca5152257151f074c2018a2e2f8f504d65f6).
Furthermore, the distance between the center and the corners is ![Image 18: {\displaystyle r{\sqrt {d}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7d1e8509527b454d4708c554ef2ab34288cca085), which increases without bound for fixed r.

In this sense when points are [uniformly generated](https://en.wikipedia.org/wiki/Continuous_uniform_distribution "Continuous uniform distribution") in a high-dimensional hypercube, almost all points are much farther than ![Image 19: {\displaystyle r}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538) units away from the center. In high dimensions, the volume of the _d_-dimensional unit hypercube (with coordinates of the vertices ![Image 20: {\displaystyle \pm 1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0bfeaa85da53ad1947d8000926cfea33827ef1e0)) is concentrated near a sphere with the radius ![Image 21: {\displaystyle {\sqrt {d}}/{\sqrt {3}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3215169b3a7e3aba80fc854897a71df728f07cf0) for large dimension _d_. Indeed, for each coordinate ![Image 22: {\displaystyle x_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158) the average value of ![Image 23: {\displaystyle x_{i}^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ea4ea44d82d72c37a4f22ba288ffef326f925d4) in the cube is[[13]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Bailey2006-13)

![Image 24: {\displaystyle \left\langle x_{i}^{2}\right\rangle ={\frac {1}{2}}\int _{-1}^{1}x^{2}dx={\frac {1}{3}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/92b967d2b42096d412ad0f0bd481192291250442).
The variance of ![Image 25: {\displaystyle x_{i}^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ea4ea44d82d72c37a4f22ba288ffef326f925d4) for uniform distribution in the cube is

![Image 26: {\displaystyle {\frac {1}{2}}\int _{-1}^{1}x^{4}dx-\left\langle x_{i}^{2}\right\rangle ^{2}={\frac {4}{45}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bb48a5499b810222bb6b211a09552af534a57f63)
Therefore, the squared distance from the origin, ![Image 27: {\textstyle r^{2}=\sum _{i}x_{i}^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a91a8a16040c7f09d939aaa36193c003323644a5) has the average value _d_/3 and variance 4 _d_/45. For large _d_, distribution of ![Image 28: {\displaystyle r^{2}/d}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86d46a7da0f9016cbdec0a29e4168f51a1cf0ca5) is close to the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution "Normal distribution") with the mean 1/3 and the standard deviation ![Image 29: {\displaystyle 2/{\sqrt {45d}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ae7f1180e91bce0ab5d800718e6e35a58d0e01a) according to the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem "Central limit theorem"). Thus, when uniformly generating points in high dimensions, both the "middle" of the hypercube, and the corners are empty, and all the volume is concentrated near the surface of a sphere of "intermediate" radius ![Image 30: {\textstyle {\sqrt {d/3}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/373ee55483ca044ad6ed460bcb087cee106ac7f9).

This also helps to understand the [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution "Chi-squared distribution"). Indeed, the (non-central) chi-squared distribution associated to a random point in the [interval](https://en.wikipedia.org/wiki/Interval_(mathematics) "Interval (mathematics)") [-1, 1] is the same as the distribution of the length-squared of a random point in the _d_-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around _d_ times the standard deviation squared (σ 2) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the _d_-cube concentrates near the boundary of a sphere of radius ![Image 31: {\displaystyle \sigma {\sqrt {d}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6865a3b591f15b618aa4b236e6df88676095a134).

A further development of this phenomenon is as follows. Any fixed distribution on the [real numbers](https://en.wikipedia.org/wiki/Real_number "Real number") induces a product distribution on points in ![Image 32: {\displaystyle \mathbb {R} ^{d}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685). For any fixed _n_, it turns out that the difference between the minimum and the maximum distance between a random reference point _Q_ and a list of _n_ random data points _P_ 1,...,_P_ _n_ become indiscernible compared to the minimum distance:[[14]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-14)

![Image 33: {\displaystyle \lim _{d\to \infty }E\left({\frac {\operatorname {dist} _{\max }(d)-\operatorname {dist} _{\min }(d)}{\operatorname {dist} _{\min }(d)}}\right)\to 0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b13e3be054e337e51b4e1e37fe8953d03b22a272).
This is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions ![Image 34: {\displaystyle \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/786849c765da7a84dbc3cce43e96aad58a5868dc) are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables "Independent and identically distributed random variables").[[15]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-survey-15) When attributes are correlated, data can become easier and provide higher distance contrast and the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio "Signal-to-noise ratio") was found to play an important role, thus [feature selection](https://en.wikipedia.org/wiki/Feature_selection "Feature selection") should be used.[[15]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-survey-15)

More recently, it has been suggested that there may be a conceptual flaw in the argument that contrast-loss creates a curse in high dimensions. Machine learning can be understood as the problem of assigning instances to their respective generative process of origin, with class labels acting as symbolic representations of individual generative processes. The curse's derivation assumes all instances are independent, identical outcomes of a single high dimensional generative process. If there is only one generative process, there would exist only one (naturally occurring) class and machine learning would be conceptually ill-defined in both high and low dimensions. Thus, the traditional argument that contrast-loss creates a curse, may be fundamentally inappropriate. In addition, it has been shown that when the generative model is modified to accommodate multiple generative processes, contrast-loss can morph from a curse to a blessing, as it ensures that the nearest-neighbor of an instance is almost-surely its most closely related instance. From this perspective, contrast-loss makes high dimensional distances especially meaningful and not especially non-meaningful as is often argued.[[16]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-16)

### Nearest neighbor search

[[edit](https://en.wikipedia.org/w/index.php?title=Curse_of_dimensionality&action=edit&section=8 "Edit section: Nearest neighbor search")]

The effect complicates [nearest neighbor search](https://en.wikipedia.org/wiki/Nearest_neighbor_search "Nearest neighbor search") in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.[[17]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-17)[[18]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-18)

However, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties,[[19]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-houle-ssdbm10-19) since _relevant_ additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant ("noise") dimensions, however, reduce the contrast in the manner described above. In [time series analysis](https://en.wikipedia.org/wiki/Time_series_analysis "Time series analysis"), where the data are inherently high-dimensional, distance functions also work reliably as long as the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio "Signal-to-noise ratio") is high enough.[[20]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-houle-sstd11-20)

#### _k_-nearest neighbor classification

[[edit](https://en.wikipedia.org/w/index.php?title=Curse_of_dimensionality&action=edit&section=9 "Edit section: k-nearest neighbor classification")]

Another effect of high dimensionality on distance functions concerns _k_-nearest neighbor (_k_-NN) [graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics) "Graph (discrete mathematics)") constructed from a [data set](https://en.wikipedia.org/wiki/Data_set "Data set") using a distance function. As the dimension increases, the [indegree](https://en.wikipedia.org/wiki/Indegree "Indegree") distribution of the _k_-NN [digraph](https://en.wikipedia.org/wiki/Directed_graph "Directed graph") becomes [skewed](https://en.wikipedia.org/wiki/Skewness "Skewness") with a peak on the right because of the emergence of a disproportionate number of **hubs**, that is, data-points that appear in many more _k_-NN lists of other data-points than the average.[[21]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-21) This phenomenon can have a considerable impact on various techniques for [classification](https://en.wikipedia.org/wiki/Classification_(machine_learning) "Classification (machine learning)") (including the [_k_-NN classifier](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm "K-nearest neighbor algorithm")), [semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning "Semi-supervised learning"), and [clustering](https://en.wikipedia.org/wiki/Cluster_analysis "Cluster analysis"),[[22]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-22) and it also affects [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval "Information retrieval").[[23]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-23)

In a 2012 survey, Zimek et al. identified the following problems when searching for [anomalies](https://en.wikipedia.org/wiki/Anomaly_detection "Anomaly detection") in high-dimensional data:[[15]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-survey-15)

1.   Concentration of scores and distances: derived values such as distances become numerically similar
2.   Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant
3.   Definition of reference sets: for local methods, reference sets are often nearest-neighbor based
4.   Incomparable scores for different dimensionalities: different subspaces produce incomparable scores
5.   Interpretability of scores: the scores often no longer convey a semantic meaning
6.   Exponential search space: the search space can no longer be systematically scanned
7.   [Data snooping](https://en.wikipedia.org/wiki/Data_snooping "Data snooping") bias: given the large search space, for every desired significance a hypothesis can be found
8.   Hubness: certain objects occur more frequently in neighbor lists than others.

Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions.

### Blessing of dimensionality

[[edit](https://en.wikipedia.org/w/index.php?title=Curse_of_dimensionality&action=edit&section=11 "Edit section: Blessing of dimensionality")]

Surprisingly and despite the expected "curse of dimensionality" difficulties, common-sense heuristics based on the most straightforward methods "can yield results which are almost surely optimal" for high-dimensional problems.[[24]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Kainen1997-24) The term "blessing of dimensionality" was introduced in the late 1990s.[[24]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Kainen1997-24)[Donoho](https://en.wikipedia.org/wiki/David_Donoho "David Donoho") in his "Millennium manifesto" clearly explained why the "blessing of dimensionality" will form a basis of future data mining.[[25]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Donoho2000-25) The effects of the blessing of dimensionality were discovered in many applications and found their foundation in the [concentration of measure phenomena](https://en.wikipedia.org/wiki/Concentration_of_measure "Concentration of measure").[[26]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-GorbanEntr2020-26) One example of the blessing of dimensionality phenomenon is linear separability of a random point from a large finite random set with high probability even if this set is exponentially large: the number of elements in this random set can grow exponentially with dimension. Moreover, this linear functional can be selected in the form of the simplest linear [Fisher discriminant](https://en.wikipedia.org/wiki/Linear_discriminant_analysis "Linear discriminant analysis"). This separability theorem was proven for a wide class of probability distributions: general uniformly log-concave distributions, product distributions in a cube and many other families (reviewed recently in [[26]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-GorbanEntr2020-26)).

"The blessing of dimensionality and the curse of dimensionality are two sides of the same coin."[[27]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-GorTyukPhTranRS2018-27) For example, the typical property of essentially high-dimensional probability distributions in a high-dimensional space is: the squared distance of random points to a selected point is, with high probability, close to the average (or median) squared distance. This property significantly simplifies the expected geometry of data and indexing of high-dimensional data (blessing),[[28]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Hecht1994-28) but, at the same time, it makes the similarity search in high dimensions difficult and even useless (curse).[[29]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-PestovCamwa2013-29)

Zimek et al.[[15]](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-survey-15) noted that while the typical formalizations of the curse of dimensionality affect [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables "Independent and identically distributed random variables") data, having data that is separated in each attribute becomes easier even in high dimensions, and argued that the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio "Signal-to-noise ratio") matters: data becomes easier with each attribute that adds signal, and harder with attributes that only add noise (irrelevant error) to the data. In particular for unsupervised data analysis this effect is known as swamping.

*   [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation "Bellman equation")
*   [Clustering high-dimensional data](https://en.wikipedia.org/wiki/Clustering_high-dimensional_data "Clustering high-dimensional data")
*   [Concentration of measure](https://en.wikipedia.org/wiki/Concentration_of_measure "Concentration of measure")
*   [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction "Dimensionality reduction")
*   [Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming "Dynamic programming")
*   [Fourier-related transforms](https://en.wikipedia.org/wiki/Fourier-related_transforms "Fourier-related transforms")
*   [Grand Tour](https://en.wikipedia.org/wiki/Grand_Tour_(data_visualisation) "Grand Tour (data visualisation)")
*   [Linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics) "Linear least squares (mathematics)")
*   [Model order reduction](https://en.wikipedia.org/wiki/Model_order_reduction "Model order reduction")
*   [Multilinear PCA](https://en.wikipedia.org/wiki/Multilinear_principal_component_analysis "Multilinear principal component analysis")
*   [Multilinear subspace learning](https://en.wikipedia.org/wiki/Multilinear_subspace_learning "Multilinear subspace learning")
*   [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis "Principal component analysis")
*   [Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition "Singular value decomposition")

1.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-1)**Bellman, Richard Ernest; Rand Corporation (1957). [_Dynamic programming_](https://books.google.com/books?id=wdtoPwAACAAJ). Princeton University Press. p.ix. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-691-07951-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-07951-6 "Special:BookSources/978-0-691-07951-6"). ,

Republished: Bellman, Richard Ernest (2003). [_Dynamic Programming_](https://books.google.com/books?id=fyVtp3EMxasC). Courier Dover Publications. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-42809-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-42809-3 "Special:BookSources/978-0-486-42809-3").
2.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-2)**Bellman, Richard Ernest (1961). [_Adaptive control processes: a guided tour_](https://books.google.com/books?id=POAmAAAAMAAJ). Princeton University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[9780691079011](https://en.wikipedia.org/wiki/Special:BookSources/9780691079011 "Special:BookSources/9780691079011").
3.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-3)**Taylor, C. Robert (1993). ["Dynamic Programming and the Curses of Dimensionality"](https://books.google.com/books?id=71SsDwAAQBAJ&pg=PA1). _Applications Of Dynamic Programming To Agricultural Decision Problems_. Westview Press. pp.1–10. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-8133-8641-1](https://en.wikipedia.org/wiki/Special:BookSources/0-8133-8641-1 "Special:BookSources/0-8133-8641-1").
4.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-4)**Udacity (2015-02-23). ["Curse of Dimensionality - Georgia Tech - Machine Learning"](https://www.youtube.com/watch?v=QZ0DtNFdDko). _[YouTube](https://en.wikipedia.org/wiki/YouTube "YouTube")_. Retrieved 2022-06-29.
5.   ^ [_**a**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Pattern_recog_5-0)[_**b**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Pattern_recog_5-1)Koutroumbas, Konstantinos; Theodoridis, Sergios (2008). [_Pattern Recognition_](https://www.elsevier.com/books/pattern-recognition/theodoridis/978-1-59749-272-0) (4th ed.). Burlington. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-59749-272-0](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59749-272-0 "Special:BookSources/978-1-59749-272-0"). Retrieved 2018-01-08.`{{cite book}}`: CS1 maint: location missing publisher ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_location_missing_publisher "Category:CS1 maint: location missing publisher"))
6.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-6)**Hughes, G.F. (January 1968). "On the mean accuracy of statistical pattern recognizers". _IEEE Transactions on Information Theory_. **14** (1): 55–63. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/TIT.1968.1054102](https://doi.org/10.1109%2FTIT.1968.1054102). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[206729491](https://api.semanticscholar.org/CorpusID:206729491).
7.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-7)**Trunk, G. V. (July 1979). "A Problem of Dimensionality: A Simple Example". _IEEE Transactions on Pattern Analysis and Machine Intelligence_. PAMI-1 (3): 306–307. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/TPAMI.1979.4766926](https://doi.org/10.1109%2FTPAMI.1979.4766926). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")[21868861](https://pubmed.ncbi.nlm.nih.gov/21868861). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[13086902](https://api.semanticscholar.org/CorpusID:13086902).
8.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-8)**B. Chandrasekaran; A. K. Jain (1974). "Quantization Complexity and Independent Measurements". _IEEE Transactions on Computers_. **23** (8): 102–106. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/T-C.1974.223789](https://doi.org/10.1109%2FT-C.1974.223789). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[35360973](https://api.semanticscholar.org/CorpusID:35360973).
9.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-McLachlan:2004_9-0)**McLachlan, G. J. (2004). _Discriminant Analysis and Statistical Pattern Recognition_. Wiley Interscience. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-471-69115-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-69115-0 "Special:BookSources/978-0-471-69115-0"). [MR](https://en.wikipedia.org/wiki/MR_(identifier) "MR (identifier)")[1190469](https://mathscinet.ams.org/mathscinet-getitem?mr=1190469).
10.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-zollanvari_10-0)**Zollanvari, A.; James, A. P.; Sameni, R. (2020). "A Theoretical Analysis of the Peaking Phenomenon in Classification". _Journal of Classification_. **37** (2): 421–434. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/s00357-019-09327-3](https://doi.org/10.1007%2Fs00357-019-09327-3). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[253851666](https://api.semanticscholar.org/CorpusID:253851666).
11.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-11)**Schroff, Florian; Kalenichenko, Dmitry; Philbin, James (June 2015). ["FaceNet: A unified embedding for face recognition and clustering"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)(PDF). _2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. pp.815–823. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1503.03832](https://arxiv.org/abs/1503.03832). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/CVPR.2015.7298682](https://doi.org/10.1109%2FCVPR.2015.7298682). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4673-6964-0](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4673-6964-0 "Special:BookSources/978-1-4673-6964-0"). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[206592766](https://api.semanticscholar.org/CorpusID:206592766).
12.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-12)**Yin, Zi; Shen, Yuanyuan (2018). ["On the Dimensionality of Word Embedding"](https://proceedings.neurips.cc/paper_files/paper/2018/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf)(PDF). _Advances in Neural Information Processing Systems_. **31**. Curran Associates, Inc.
13.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Bailey2006_13-0)**Bailey, D.H.; Borwein, J.M.; Crandall, R.E. (2006), "Box integrals", _Journal of Computational and Applied Mathematics_, **206**: 196–208, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.cam.2006.06.010](https://doi.org/10.1016%2Fj.cam.2006.06.010), [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[2763194](https://api.semanticscholar.org/CorpusID:2763194)
14.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-14)**Beyer, K.; Goldstein, J.; Ramakrishnan, R.; Shaft, U. (1999). "When is "Nearest Neighbor" Meaningful?". [_Database Theory — ICDT'99_](http://digital.library.wisc.edu/1793/60174). LNCS. Vol.1540. pp.217–235. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/3-540-49257-7_15](https://doi.org/10.1007%2F3-540-49257-7_15). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-540-65452-0](https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-65452-0 "Special:BookSources/978-3-540-65452-0"). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[206634099](https://api.semanticscholar.org/CorpusID:206634099).
15.   ^ [_**a**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-survey_15-0)[_**b**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-survey_15-1)[_**c**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-survey_15-2)[_**d**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-survey_15-3)Zimek, A.; Schubert, E.; [Kriegel, H.-P.](https://en.wikipedia.org/wiki/Hans-Peter_Kriegel "Hans-Peter Kriegel") (2012). "A survey on unsupervised outlier detection in high-dimensional numerical data". _Statistical Analysis and Data Mining_. **5** (5): 363–387. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1002/sam.11161](https://doi.org/10.1002%2Fsam.11161). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[6724536](https://api.semanticscholar.org/CorpusID:6724536).
16.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-16)**Lin, Wen-Yan; Liu, Siying; Ren, Changhao; Cheung, Ngai-Man; Li, Hongdong; Matsushita, Yasuyuki (2021). ["Shell Theory: A Statistical Model of Reality"](https://ieeexplore.ieee.org/document/9444188). _IEEE Transactions on Pattern Analysis and Machine Intelligence_. **44** (10): 6438–6453. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/TPAMI.2021.3084598](https://doi.org/10.1109%2FTPAMI.2021.3084598). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[1939-3539](https://search.worldcat.org/issn/1939-3539). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")[34048335](https://pubmed.ncbi.nlm.nih.gov/34048335). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[235242104](https://api.semanticscholar.org/CorpusID:235242104).
17.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-17)**Marimont, R.B.; Shapiro, M.B. (1979). "Nearest Neighbour Searches and the Curse of Dimensionality". _IMA J Appl Math_. **24** (1): 59–70. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1093/imamat/24.1.59](https://doi.org/10.1093%2Fimamat%2F24.1.59).
18.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-18)**Chávez, Edgar; Navarro, Gonzalo; Baeza-Yates, Ricardo; Marroquín, José Luis (2001). "Searching in Metric Spaces". _ACM Computing Surveys_. **33** (3): 273–321. [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)")[10.1.1.100.7845](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.100.7845). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1145/502807.502808](https://doi.org/10.1145%2F502807.502808). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[3201604](https://api.semanticscholar.org/CorpusID:3201604).
19.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-houle-ssdbm10_19-0)**Houle, M. E.; [Kriegel, H. P.](https://en.wikipedia.org/wiki/Hans-Peter_Kriegel "Hans-Peter Kriegel"); Kröger, P.; Schubert, E.; Zimek, A. (2010). [_Can Shared-Neighbor Distances Defeat the Curse of Dimensionality?_](http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf)(PDF). Scientific and Statistical Database Management. Lecture Notes in Computer Science. Vol.6187. p.482. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-3-642-13818-8_34](https://doi.org/10.1007%2F978-3-642-13818-8_34). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-642-13817-1](https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-13817-1 "Special:BookSources/978-3-642-13817-1").
20.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-houle-sstd11_20-0)**Bernecker, T.; Houle, M. E.; [Kriegel, H. P.](https://en.wikipedia.org/wiki/Hans-Peter_Kriegel "Hans-Peter Kriegel"); Kröger, P.; Renz, M.; Schubert, E.; Zimek, A. (2011). _Quality of Similarity Rankings in Time Series_. Symposium on Spatial and Temporal Databases. Lecture Notes in Computer Science. Vol.6849. p.422. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-3-642-22922-0_25](https://doi.org/10.1007%2F978-3-642-22922-0_25). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-642-22921-3](https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-22921-3 "Special:BookSources/978-3-642-22921-3").
21.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-21)**James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2021). [_An introduction to statistical learning: with applications in R_](https://link.springer.com/book/10.1007/978-1-0716-1418-1) (Second ed.). New York, NY: Springer. p.122. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-1-0716-1418-1](https://doi.org/10.1007%2F978-1-0716-1418-1). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-0716-1418-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-0716-1418-1 "Special:BookSources/978-1-0716-1418-1"). Retrieved 1 November 2024.
22.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-22)**Radovanović, Miloš; Nanopoulos, Alexandros; Ivanović, Mirjana (2010). ["Hubs in space: Popular nearest neighbors in high-dimensional data"](http://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf)(PDF). _Journal of Machine Learning Research_. **11**: 2487–2531.
23.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-23)**Radovanović, M.; Nanopoulos, A.; Ivanović, M. (2010). _On the existence of obstinate results in vector space models_. 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10. p.186. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1145/1835449.1835482](https://doi.org/10.1145%2F1835449.1835482). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[9781450301534](https://en.wikipedia.org/wiki/Special:BookSources/9781450301534 "Special:BookSources/9781450301534").
24.   ^ [_**a**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Kainen1997_24-0)[_**b**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Kainen1997_24-1)Kainen, Paul C. (1997), "Utilizing Geometric Anomalies of High Dimension: When Complexity Makes Computation Easier", in Kárný, M.; Warwick, K. (eds.), _Computer Intensive Methods in Control and Signal Processing_, pp.283–294, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-1-4612-1996-5_18](https://doi.org/10.1007%2F978-1-4612-1996-5_18), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4612-7373-8](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4612-7373-8 "Special:BookSources/978-1-4612-7373-8")
25.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Donoho2000_25-0)**[Donoho, David L.](https://en.wikipedia.org/wiki/David_Donoho "David Donoho") (2000), "High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality", _Invited lecture at Mathematical Challenges of the 21st Century, AMS National Meeting, Los Angeles, CA, USA, August 6-12, 2000_, [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)")[10.1.1.329.3392](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.329.3392)
26.   ^ [_**a**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-GorbanEntr2020_26-0)[_**b**_](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-GorbanEntr2020_26-1)[Gorban, Alexander N.](https://en.wikipedia.org/wiki/Aleksandr_Gorban "Aleksandr Gorban"); Makarov, Valery A.; Tyukin, Ivan Y. (2020). ["High-Dimensional Brain in a High-Dimensional World: Blessing of Dimensionality"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7516518). _Entropy_. **22** (1): 82. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[2001.04959](https://arxiv.org/abs/2001.04959). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2020Entrp..22...82G](https://ui.adsabs.harvard.edu/abs/2020Entrp..22...82G). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.3390/e22010082](https://doi.org/10.3390%2Fe22010082). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)")[7516518](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7516518). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")[33285855](https://pubmed.ncbi.nlm.nih.gov/33285855).
27.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-GorTyukPhTranRS2018_27-0)**Gorban, Alexander N.; Tyukin, Ivan Y. (2018). ["Blessing of dimensionality: mathematical foundations of the statistical physics of data"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5869543). _Phil. Trans. R. Soc. A_. **376** (2118): 20170237. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1801.03421](https://arxiv.org/abs/1801.03421). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2018RSPTA.37670237G](https://ui.adsabs.harvard.edu/abs/2018RSPTA.37670237G). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1098/rsta.2017.0237](https://doi.org/10.1098%2Frsta.2017.0237). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)")[5869543](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5869543). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")[29555807](https://pubmed.ncbi.nlm.nih.gov/29555807).
28.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-Hecht1994_28-0)**[Hecht-Nielsen, Robert](https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen "Robert Hecht-Nielsen") (1994), "Context vectors: general-purpose approximate meaning representations self-organized from raw data", in Zurada, J.M.; Marks, R.J.; Robinson, C.J. (eds.), _Computational intelligence: imitating life; Proceedings of World Congress on Computational Intelligence, Neural Networks; 1994; Orlando; FL_, Piscataway, NJ: [IEEE](https://en.wikipedia.org/wiki/IEEE "IEEE") Press, pp.43–56, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0780311043](https://en.wikipedia.org/wiki/Special:BookSources/0780311043 "Special:BookSources/0780311043")
29.   **[^](https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_ref-PestovCamwa2013_29-0)**Pestov, Vladimir (2013). ["Is the k-NN classifier in high dimensions affected by the curse of dimensionality?"](https://doi.org/10.1016%2Fj.camwa.2012.09.011). _Comput. Math. Appl_. **65** (10): 43–56. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.camwa.2012.09.011](https://doi.org/10.1016%2Fj.camwa.2012.09.011).
