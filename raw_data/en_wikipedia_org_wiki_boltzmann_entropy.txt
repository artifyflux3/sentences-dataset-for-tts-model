Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Introduction of the natural logarithm 3 Generalization 4 Boltzmann entropy excludes statistical dependencies 5 See also 6 References 7 External links Toggle the table of contents Boltzmann's entropy formula 14 languages –ë–µ–ª–∞—Ä—É—Å–∫–∞—è Catal√† Dansk Espa√±ol Euskara Fran√ßais Italiano ◊¢◊ë◊®◊ô◊™ ·É•·Éê·É†·Éó·É£·Éö·Éò Lietuvi≈≥ Nederlands Êó•Êú¨Ë™û Portugu√™s ‰∏≠Êñá Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Boltzmann entropy ) Equation in statistical mechanics Boltzmann's entropy formula ‚Äîcarved on his gravestone.

[ 1 ] In statistical mechanics , Boltzmann's entropy formula (also known as the Boltzmann‚ÄìPlanck equation , not to be confused with the more general Boltzmann equation , which is a partial differential equation ) is a probability equation relating the entropy S {\displaystyle S} , also written as S B {\displaystyle S_{\mathrm {B} }} , of an ideal gas to the multiplicity (commonly denoted as Œ© Œ© {\displaystyle \Omega } or W {\displaystyle W} ), the number of real microstates corresponding to the gas's macrostate : S = k B ln ‚Å° ‚Å° W {\displaystyle S=k_{\mathrm {B} }\ln W} 1 where k B {\displaystyle k_{\mathrm {B} }} is the Boltzmann constant (also written as simply k {\displaystyle k} ) and equal to 1.380649 √ó 10 ‚àí23 J/K, and ln {\displaystyle \ln } is the natural logarithm function (or log base e , as in the image above).

In short, the Boltzmann formula shows the relationship between entropy and the number of ways the atoms or molecules of a certain kind of thermodynamic system can be arranged. What is important to note is that W is not all possible states of the system, but ways the system can be arranged and still have the same properties from perspective of external observer. So for example when system contains 5 particles of gas and given amount of energy distributed between them for example [1,1,2,3,4]. Energy distribution can be realized as [1,2,1,3,4] where index represent a particle, but the distribution can also be realized as[2,1,1,3,4] after swapping first two and so forth. W is measure of all possible way the distribution can be realized. When W is small for given distribution that distribution has small entropy, when W is large for given distribution it has a large the entropy.

History [ edit ] Boltzmann's grave in the Zentralfriedhof , Vienna, with bust and entropy formula.

The equation was originally formulated by Ludwig Boltzmann between 1872 and 1875, but later put into its current form by Max Planck in about 1900.

[ 2 ] [ 3 ] To quote Planck, "the logarithmic connection between entropy and probability was first stated by L. Boltzmann in his kinetic theory of gases".

[ 4 ] A 'microstate' is a state specified in terms of the constituent particles of a body of matter or radiation that has been specified as a macrostate in terms of such variables as internal energy and pressure. A macrostate is experimentally observable, with at least a finite extent in spacetime . A microstate can be instantaneous, or can be a trajectory composed of a temporal progression of instantaneous microstates. In experimental practice, such are scarcely observable. The present account concerns instantaneous microstates.

The value of W was originally intended to be proportional to the Wahrscheinlichkeit (the German word for probability) of a macroscopic state for some probability distribution of possible microstates ‚Äîthe collection of (unobservable microscopic single particle) "ways" in which the (observable macroscopic) thermodynamic state of a system can be realized by assigning different positions and momenta to the respective molecules.

There are many instantaneous microstates that apply to a given macrostate. Boltzmann considered collections of such microstates. For a given macrostate, he called the collection of all possible instantaneous microstates of a certain kind by the name monode , for which Gibbs' term ensemble is used nowadays. For single particle instantaneous microstates, Boltzmann called the collection an ergode . Subsequently, Gibbs called it a microcanonical ensemble , and this name is widely used today, perhaps partly because Bohr was more interested in the writings of Gibbs than of Boltzmann.

[ 5 ] Interpreted in this way, Boltzmann's formula is the most basic formula for the thermodynamic entropy . Boltzmann's paradigm was an ideal gas of N identical particles, of which N i are in the i -th microscopic condition (range) of position and momentum.  For this case, the probability of each microstate of the system is equal, so it was equivalent for Boltzmann to calculate the number of microstates associated with a macrostate.

W was historically misinterpreted as literally meaning the number of microstates, and that is what it usually means today.

W can be counted using the formula for permutations W = N !

‚àè ‚àè i N i !

{\displaystyle W={\frac {N!}{\prod _{i}N_{i}!}}} 2 where i ranges over all possible molecular conditions and " !

" denotes factorial . The "correction" in the denominator is due to the fact that identical particles in the same condition are indistinguishable .

W is sometimes called the "thermodynamic probability" since it is an integer greater than one, while mathematical probabilities are always numbers between zero and one.

Introduction of the natural logarithm [ edit ] In Boltzmann‚Äôs 1877 paper, he clarifies molecular state counting to determine the state distribution number introducing the logarithm to simplify the equation.

Boltzmann writes:
‚ÄúThe first task is to determine the permutation number, previously designated by
ùí´
, for any state distribution. Denoting by J the sum of the permutations
ùí´
for all possible state distributions, the quotient
ùí´
/J is the state distribution‚Äôs probability, henceforth denoted by W. We would first like to calculate the permutations
ùí´
for
the state distribution characterized by w 0 molecules with kinetic energy 0, w 1 molecules with kinetic energy œµ, etc. ‚Ä¶ ‚ÄúThe most likely state distribution will be for those w 0 , w 1 ‚Ä¶ values for which
ùí´
is a maximum or since the numerator is a constant, for which the denominator is a minimum. The values w 0 , w 1 must simultaneously satisfy the two constraints (1) and (2). Since the denominator of
ùí´
is a product, it is easiest to determine the minimum of its logarithm, ‚Ä¶‚Äù Therefore, by making the denominator small, he maximizes the number of states. So to simplify the product of the factorials, he uses their natural logarithm to add them. This is the reason for the natural logarithm in Boltzmann‚Äôs entropy formula.

[ 6 ] Generalization [ edit ] Boltzmann's formula applies to microstates of a system, each possible microstate of which is presumed to be equally probable.

But in thermodynamics, the universe is divided into a system of interest, plus its surroundings; then the entropy of Boltzmann's microscopically specified system can be identified with the system entropy in classical thermodynamics.  The microstates of such a thermodynamic system are not equally probable‚Äîfor example, high energy microstates are less probable than low energy microstates for a thermodynamic system kept at a fixed temperature by allowing contact with a heat bath.
For thermodynamic systems where microstates of the system may not have equal probabilities, the appropriate generalization, called the Gibbs entropy formula , is: S G = ‚àí ‚àí k B ‚àë ‚àë p i ln ‚Å° ‚Å° p i {\displaystyle S_{\mathrm {G} }=-k_{\mathrm {B} }\sum p_{i}\ln p_{i}} 3 This reduces to equation ( 1 ) if the probabilities p i are all equal.

Boltzmann used a œÅ œÅ ln ‚Å° ‚Å° œÅ œÅ {\displaystyle \rho \ln \rho } formula as early as 1866.

[ 7 ] He interpreted œÅ as a density in phase space‚Äîwithout mentioning probability‚Äîbut since this satisfies the axiomatic definition of a probability measure we can retrospectively interpret it as a probability anyway.

Gibbs gave an explicitly probabilistic interpretation in 1878.

Boltzmann himself used an expression equivalent to ( 3 ) in his later work [ 8 ] and recognized it as more general than equation ( 1 ).  That is, equation ( 1 ) is a corollary of
equation ( 3 )‚Äîand not vice versa.  In every situation where equation ( 1 ) is valid,
equation ( 3 ) is valid also‚Äîand not vice versa.

Boltzmann entropy excludes statistical dependencies [ edit ] The term Boltzmann entropy is also sometimes used to indicate entropies calculated based on the approximation that the overall probability can be factored into an identical separate term for each particle‚Äîi.e., assuming each particle has an identical independent probability distribution, and ignoring interactions and correlations between the particles.  This is exact for an ideal gas of identical particles that move independently apart from instantaneous collisions, and is an approximation, possibly a poor one, for other systems.

[ 9 ] The Boltzmann entropy is obtained if one assumes one can treat all the component particles of a thermodynamic system as statistically independent.  The probability distribution of the system as a whole then factorises into the product of N separate identical terms, one term for each particle; and when the summation is taken over each possible state in the 6-dimensional phase space of a single particle (rather than the 6 N -dimensional phase space of the system as a whole), the Gibbs entropy formula S G = ‚àí ‚àí N k B ‚àë ‚àë i p i ln ‚Å° ‚Å° p i {\displaystyle S_{\mathrm {G} }=-Nk_{\mathrm {B} }\sum _{i}p_{i}\ln p_{i}} 4 simplifies to the Boltzmann entropy S B {\displaystyle S_{\mathrm {B} }} .

This reflects the original statistical entropy function introduced by Ludwig Boltzmann in 1872.  For the special case of an ideal gas it exactly corresponds to the proper thermodynamic entropy .

For anything but the most dilute of real gases, S B {\displaystyle S_{\mathrm {B} }} leads to increasingly wrong predictions of entropies and physical behaviours, by ignoring the interactions and correlations between different molecules.  Instead one must consider the ensemble of states of the system as a whole, called by Boltzmann a holode , rather than single particle states.

[ 10 ] Gibbs considered several such kinds of ensembles; relevant here is the canonical one.

[ 9 ] See also [ edit ] History of entropy H theorem Gibbs entropy formula nat (unit) Shannon entropy von Neumann entropy References [ edit ] ^ See: photo of Boltzmann's grave in the Zentralfriedhof , Vienna, with bust and entropy formula.

^ Boltzmann equation . Eric Weisstein's World of Physics (states the year was 1872).

^ Perrot, Pierre (1998).

A to Z of Thermodynamics . Oxford University Press.

ISBN 0-19-856552-6 .

(states the year was 1875) ^ Max Planck (1914) The theory of heat radiation equation 164, p.119 ^ Cercignani, C. (1998).

Ludwig Boltzmann: the Man who Trusted Atoms , Oxford University Press, Oxford UK, ISBN 9780198501541 , p. 134, pp. 141‚Äì142.

^ Sharp, K.; Matschinsky, F. Translation of Ludwig Boltzmann‚Äôs Paper ‚ÄúOn the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium‚Äù Sitzungberichte der Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen Classe. Abt. II, LXXVI 1877, pp 373-435 (Wien. Ber. 1877, 76:373-435). Reprinted in Wiss. Abhandlungen, Vol. II, reprint 42, p. 164-223, Barth, Leipzig, 1909. Entropy 2015, 17, 1971-2009.

https://doi.org/10.3390/e17041971 This article incorporates text from this source, which is available under the CC BY 3.0 license.

^ Ludwig Boltzmann (1866). "√úber die Mechanische Bedeutung des Zweiten Hauptsatzes der W√§rmetheorie".

Wiener Berichte .

53 : 195‚Äì 220.

^ Ludwig Boltzmann (1896).

Vorlesungen √ºber Gastheorie, vol. I . J.A. Barth, Leipzig.

; Ludwig Boltzmann (1898).

Vorlesungen √ºber Gastheorie, vol. II . J.A. Barth, Leipzig.

^ a b Jaynes, E. T.

(1965).

Gibbs vs Boltzmann entropies .

American Journal of Physics , 33 , 391-8.

^ Cercignani, C. (1998).

Ludwig Boltzmann: the Man who Trusted Atoms , Oxford University Press, Oxford UK, ISBN 9780198501541 , p. 134.

External links [ edit ] Introduction to Boltzmann's Equation Vorlesungen √ºber Gastheorie, Ludwig Boltzmann (1896) vol. I, J.A. Barth, Leipzig Vorlesungen √ºber Gastheorie, Ludwig Boltzmann (1898) vol. II. J.A. Barth, Leipzig.

v t e Statistical mechanics Theory Principle of maximum entropy ergodic theory Statistical thermodynamics Ensembles partition functions equations of state thermodynamic potential : U H F G Maxwell relations Models Ferromagnetism models Ising Potts Heisenberg percolation Particles with force field depletion force Lennard-Jones potential Mathematical approaches Boltzmann equation H-theorem Vlasov equation BBGKY hierarchy stochastic process mean-field theory and conformal field theory Critical phenomena Phase transition Critical exponents correlation length size scaling Entropy Boltzmann Shannon Tsallis R√©nyi von Neumann Applications Statistical field theory elementary particle superfluidity Condensed matter physics Complex system chaos information theory Boltzmann machine NewPP limit report
Parsed by mw‚Äêweb.codfw.main‚Äê6cc77c66b8‚Äêc5fpk
Cached time: 20250812013549
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1, show‚Äêtoc]
CPU time usage: 0.264 seconds
Real time usage: 0.371 seconds
Preprocessor visited node count: 1114/1000000
Revision size: 13945/2097152 bytes
Post‚Äêexpand include size: 23192/2097152 bytes
Template argument size: 1512/2097152 bytes
Highest expansion depth: 11/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‚Äêexpand size: 41494/5000000 bytes
Lua time usage: 0.137/10.000 seconds
Lua memory usage: 4565952/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  269.289      1 -total
 37.96%  102.217      1 Template:Reflist
 26.37%   71.009      3 Template:Cite_book
 23.68%   63.771      1 Template:Statistical_mechanics_topics
 23.20%   62.486      1 Template:Navbox
 21.13%   56.907      1 Template:Short_description
 13.43%   36.164      4 Template:NumBlk
 11.92%   32.101      2 Template:Pagetype
  6.30%   16.975      5 Template:Main_other
  5.66%   15.255      1 Template:SDcat Saved in parser cache with key enwiki:pcache:9328562:|#|:idhash:canonical and timestamp 20250812013549 and revision id 1303838685. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Boltzmann%27s_entropy_formula&oldid=1303838685 " Categories : Eponymous equations of physics Thermodynamic entropy Thermodynamic equations Ludwig Boltzmann Hidden categories: Articles with imported Creative Commons Attribution 3.0 text Articles with short description Short description matches Wikidata This page was last edited on 2 August 2025, at 10:20 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia¬Æ is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Boltzmann's entropy formula 14 languages Add topic

