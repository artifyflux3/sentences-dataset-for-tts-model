Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Single-parameter test Toggle Single-parameter test subsection 1.1 The statistic 1.1.1 Note on notation 1.2 As most powerful test for small deviations 1.3 Relationship with other hypothesis tests 2 Multiple parameters 3 Special cases 4 See also 5 References 6 Further reading Toggle the table of contents Score test 4 languages Català فارسی Français Русский Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical test based on the gradient of the likelihood function Not to be confused with Test score .

In statistics , the score test assesses constraints on statistical parameters based on the gradient of the likelihood function —known as the score —evaluated at the hypothesized parameter value under the null hypothesis . Intuitively, if the restricted estimator is near the maximum of the likelihood function, the score should not differ from zero by more than sampling error . While the finite sample distributions of score tests are generally unknown, they have an asymptotic χ 2 -distribution under the null hypothesis as first proved by C. R. Rao in 1948, [ 1 ] a fact that can be used to determine statistical significance .

Since function maximization subject to equality constraints is most conveniently done using a Lagrangean expression of the problem, the score test can be equivalently understood as a test of the magnitude of the Lagrange multipliers associated with the constraints where, again, if the constraints are non-binding at the maximum likelihood, the vector of Lagrange multipliers should not differ from zero by more than sampling error. The equivalence of these two approaches was first shown by S. D. Silvey in 1959, [ 2 ] which led to the name Lagrange Multiplier (LM) test that has become more commonly used, particularly in econometrics, since Breusch and Pagan 's much-cited 1980 paper.

[ 3 ] The main advantage of the score test over the Wald test and likelihood-ratio test is that the score test only requires the computation of the restricted estimator.

[ 4 ] This makes testing feasible when the unconstrained maximum likelihood estimate is a boundary point in the parameter space .

[ citation needed ] Further, because the score test only requires the estimation of the likelihood function under the null hypothesis, it is less specific than the likelihood ratio test about the alternative hypothesis.

[ 5 ] Single-parameter test [ edit ] The statistic [ edit ] Let L {\displaystyle L} be the likelihood function which depends on a univariate parameter θ θ {\displaystyle \theta } and let x {\displaystyle x} be the data. The score U ( θ θ ) {\displaystyle U(\theta )} is defined as U ( θ θ ) = ∂ ∂ log ⁡ ⁡ L ( θ θ ∣ ∣ x ) ∂ ∂ θ θ .

{\displaystyle U(\theta )={\frac {\partial \log L(\theta \mid x)}{\partial \theta }}.} The Fisher information is [ 6 ] I ( θ θ ) = − − E ⁡ ⁡ [ ∂ ∂ 2 ∂ ∂ θ θ 2 log ⁡ ⁡ f ( X ; θ θ ) | θ θ ] , {\displaystyle I(\theta )=-\operatorname {E} \left[\left.{\frac {\partial ^{2}}{\partial \theta ^{2}}}\log f(X;\theta )\,\right|\,\theta \right]\,,} where ƒ is the probability density.

The statistic to test H 0 : θ θ = θ θ 0 {\displaystyle {\mathcal {H}}_{0}:\theta =\theta _{0}} is S ( θ θ 0 ) = U ( θ θ 0 ) 2 I ( θ θ 0 ) {\displaystyle S(\theta _{0})={\frac {U(\theta _{0})^{2}}{I(\theta _{0})}}} which has an asymptotic distribution of χ χ 1 2 {\displaystyle \chi _{1}^{2}} , when H 0 {\displaystyle {\mathcal {H}}_{0}} is true. While asymptotically identical, calculating the LM statistic using the outer-gradient-product estimator of the Fisher information matrix can lead to bias in small samples.

[ 7 ] Note on notation [ edit ] Note that some texts use an alternative notation, in which the statistic S ∗ ∗ ( θ θ ) = S ( θ θ ) {\displaystyle S^{*}(\theta )={\sqrt {S(\theta )}}} is tested against a normal distribution.  This approach is equivalent and gives identical results.

As most powerful test for small deviations [ edit ] ( ∂ ∂ log ⁡ ⁡ L ( θ θ ∣ ∣ x ) ∂ ∂ θ θ ) θ θ = θ θ 0 ≥ ≥ C {\displaystyle \left({\frac {\partial \log L(\theta \mid x)}{\partial \theta }}\right)_{\theta =\theta _{0}}\geq C} where L {\displaystyle L} is the likelihood function , θ θ 0 {\displaystyle \theta _{0}} is the value of the parameter of interest under the null hypothesis, and C {\displaystyle C} is a constant set depending on the size of the test desired (i.e. the probability of rejecting H 0 {\displaystyle H_{0}} if H 0 {\displaystyle H_{0}} is true; see Type I error ).

The score test is the most powerful test for small deviations from H 0 {\displaystyle H_{0}} . To see this, consider testing θ θ = θ θ 0 {\displaystyle \theta =\theta _{0}} versus θ θ = θ θ 0 + h {\displaystyle \theta =\theta _{0}+h} .  By the Neyman–Pearson lemma , the most powerful test has the form L ( θ θ 0 + h ∣ ∣ x ) L ( θ θ 0 ∣ ∣ x ) ≥ ≥ K ; {\displaystyle {\frac {L(\theta _{0}+h\mid x)}{L(\theta _{0}\mid x)}}\geq K;} Taking the log of both sides yields log ⁡ ⁡ L ( θ θ 0 + h ∣ ∣ x ) − − log ⁡ ⁡ L ( θ θ 0 ∣ ∣ x ) ≥ ≥ log ⁡ ⁡ K .

{\displaystyle \log L(\theta _{0}+h\mid x)-\log L(\theta _{0}\mid x)\geq \log K.} The score test follows making the substitution (by Taylor series expansion) log ⁡ ⁡ L ( θ θ 0 + h ∣ ∣ x ) ≈ ≈ log ⁡ ⁡ L ( θ θ 0 ∣ ∣ x ) + h × × ( ∂ ∂ log ⁡ ⁡ L ( θ θ ∣ ∣ x ) ∂ ∂ θ θ ) θ θ = θ θ 0 {\displaystyle \log L(\theta _{0}+h\mid x)\approx \log L(\theta _{0}\mid x)+h\times \left({\frac {\partial \log L(\theta \mid x)}{\partial \theta }}\right)_{\theta =\theta _{0}}} and identifying the C {\displaystyle C} above with log ⁡ ⁡ ( K ) {\displaystyle \log(K)} .

Relationship with other hypothesis tests [ edit ] If the null hypothesis is true, the likelihood ratio test , the Wald test , and the score test are asymptotically equivalent tests of hypotheses.

[ 8 ] [ 9 ] When testing nested models , the statistics for each test then converge to a Chi-squared distribution with degrees of freedom equal to the difference in degrees of freedom in the two models. If the null hypothesis is not true, however, the statistics converge to a noncentral chi-squared distribution with possibly different noncentrality parameters.

Multiple parameters [ edit ] A more general score test can be derived when there is more than one parameter.  Suppose that θ θ ^ ^ 0 {\displaystyle {\widehat {\theta }}_{0}} is the maximum likelihood estimate of θ θ {\displaystyle \theta } under the null hypothesis H 0 {\displaystyle H_{0}} while U {\displaystyle U} and I {\displaystyle I} are respectively, the score vector and the Fisher information matrix. Then U T ( θ θ ^ ^ 0 ) I − − 1 ( θ θ ^ ^ 0 ) U ( θ θ ^ ^ 0 ) ∼ ∼ χ χ k 2 {\displaystyle U^{T}({\widehat {\theta }}_{0})I^{-1}({\widehat {\theta }}_{0})U({\widehat {\theta }}_{0})\sim \chi _{k}^{2}} asymptotically under H 0 {\displaystyle H_{0}} , where k {\displaystyle k} is the number of constraints imposed by the null hypothesis and U ( θ θ ^ ^ 0 ) = ∂ ∂ log ⁡ ⁡ L ( θ θ ^ ^ 0 ∣ ∣ x ) ∂ ∂ θ θ {\displaystyle U({\widehat {\theta }}_{0})={\frac {\partial \log L({\widehat {\theta }}_{0}\mid x)}{\partial \theta }}} and I ( θ θ ^ ^ 0 ) = − − E ⁡ ⁡ ( ∂ ∂ 2 log ⁡ ⁡ L ( θ θ ^ ^ 0 ∣ ∣ x ) ∂ ∂ θ θ ∂ ∂ θ θ ′ ) .

{\displaystyle I({\widehat {\theta }}_{0})=-\operatorname {E} \left({\frac {\partial ^{2}\log L({\widehat {\theta }}_{0}\mid x)}{\partial \theta \,\partial \theta '}}\right).} This can be used to test H 0 {\displaystyle H_{0}} .

The actual formula for the test statistic depends on which estimator of the Fisher information matrix is being used.

[ 10 ] Special cases [ edit ] In many situations, the score statistic reduces to another commonly used statistic.

[ 11 ] In linear regression , the Lagrange multiplier test can be expressed as a function of the F -test .

[ 12 ] When the data follows a normal distribution, the score statistic is the same as the t statistic .

[ clarification needed ] When the data consists of binary observations, the score statistic is the same as the chi-squared statistic in the Pearson's chi-squared test .

See also [ edit ] Fisher information Uniformly most powerful test Score (statistics) Sup-LM test References [ edit ] ^ Rao, C. Radhakrishna (1948). "Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation".

Mathematical Proceedings of the Cambridge Philosophical Society .

44 (1): 50– 57.

Bibcode : 1948PCPS...44...50R .

doi : 10.1017/S0305004100023987 .

^ Silvey, S. D. (1959).

"The Lagrangian Multiplier Test" .

Annals of Mathematical Statistics .

30 (2): 389– 407.

doi : 10.1214/aoms/1177706259 .

JSTOR 2237089 .

^ Breusch, T. S.

; Pagan, A. R.

(1980). "The Lagrange Multiplier Test and its Applications to Model Specification in Econometrics".

Review of Economic Studies .

47 (1): 239– 253.

doi : 10.2307/2297111 .

JSTOR 2297111 .

^ Fahrmeir, Ludwig; Kneib, Thomas; Lang, Stefan; Marx, Brian (2013).

Regression : Models, Methods and Applications . Berlin: Springer. pp.

663 –664.

ISBN 978-3-642-34332-2 .

^ Kennedy, Peter (1998).

A Guide to Econometrics (Fourth ed.). Cambridge: MIT Press. p. 68.

ISBN 0-262-11235-3 .

^ Lehmann and Casella, eq. (2.5.16).

^ Davidson, Russel; MacKinnon, James G. (1983). "Small sample properties of alternative forms of the Lagrange Multiplier test".

Economics Letters .

12 ( 3– 4): 269– 275.

doi : 10.1016/0165-1765(83)90048-4 .

^ Engle, Robert F. (1983). "Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics". In Intriligator, M. D.; Griliches, Z. (eds.).

Handbook of Econometrics . Vol. II. Elsevier. pp.

796– 801.

ISBN 978-0-444-86185-6 .

^ Burzykowski, Andrzej Gałecki, Tomasz (2013).

Linear mixed-effects models using R : a step-by-step approach . New York, NY: Springer.

ISBN 978-1-4614-3899-1 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Taboga, Marco.

"Lectures on Probability Theory and Mathematical Statistics" .

statlect.com . Retrieved 31 May 2022 .

^ Cook, T. D.; DeMets, D. L., eds. (2007).

Introduction to Statistical Methods for Clinical Trials . Chapman and Hall. pp.

296– 297.

ISBN 978-1-58488-027-1 .

^ Vandaele, Walter (1981). "Wald, likelihood ratio, and Lagrange multiplier tests as an F test".

Economics Letters .

8 (4): 361– 365.

doi : 10.1016/0165-1765(81)90026-4 .

Further reading [ edit ] Buse, A. (1982). "The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note".

The American Statistician .

36 (3a): 153– 157.

doi : 10.1080/00031305.1982.10482817 .

Godfrey, L. G.

(1988). "The Lagrange Multiplier Test and Testing for Misspecification : An Extended Analysis".

Misspecification Tests in Econometrics . New York: Cambridge University Press. pp.

69– 99.

ISBN 0-521-26616-5 .

Ma, Jun; Nelson, Charles R. (2016). "The superiority of the LM test in a class of econometric models where the Wald test performs poorly".

Unobserved Components and Time Series Econometrics . Oxford University Press. pp.

310– 330.

doi : 10.1093/acprof:oso/9780199683666.003.0014 .

ISBN 978-0-19-968366-6 .

Rao, C. R. (2005). "Score Test: Historical Review and Recent Developments".

Advances in Ranking and Selection, Multiple Comparisons, and Reliability . Boston: Birkhäuser. pp.

3– 20.

ISBN 978-0-8176-3232-8 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Score_test&oldid=1298412882 " Category : Statistical tests Hidden categories: CS1 maint: multiple names: authors list Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from March 2019 Wikipedia articles needing clarification from March 2011 This page was last edited on 2 July 2025, at 09:50 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Score test 4 languages Add topic

