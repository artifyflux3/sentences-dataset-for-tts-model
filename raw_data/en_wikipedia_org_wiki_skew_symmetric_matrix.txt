Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Example 2 Properties Toggle Properties subsection 2.1 Vector space structure 2.2 Determinant 2.3 Cross product 2.4 Spectral theory 3 Skew-symmetric and alternating forms 4 Infinitesimal rotations 5 Coordinate-free 6 Skew-symmetrizable matrix 7 See also 8 References 9 Further reading 10 External links Toggle the table of contents Skew-symmetric matrix 31 languages العربية Български Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Italiano עברית Magyar Nederlands 日本語 Polski Português Русский Simple English Slovenščina Svenska தமிழ் ไทย Türkçe Українська اردو 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Form of a matrix For matrices with antisymmetry over the complex number field, see Skew-Hermitian matrix .

This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( November 2009 ) ( Learn how and when to remove this message ) In mathematics , particularly in linear algebra , a skew-symmetric (or antisymmetric or antimetric [ 1 ] ) matrix is a square matrix whose transpose equals its negative. That is, it satisfies the condition [ 2 ] A skew-symmetric ⟺ ⟺ A T = − − A .

{\displaystyle A{\text{ skew-symmetric}}\quad \iff \quad A^{\textsf {T}}=-A.} In terms of the entries of the matrix, if a i j {\textstyle a_{ij}} denotes the entry in the i {\textstyle i} -th row and j {\textstyle j} -th column, then the skew-symmetric condition is equivalent to A skew-symmetric ⟺ ⟺ a i j = − − a j i .

{\displaystyle A{\text{ skew-symmetric}}\quad \iff \quad a_{ij}=-a_{ji}.} Example [ edit ] The matrix A = [ 0 2 − − 45 − − 2 0 − − 4 45 4 0 ] {\displaystyle A={\begin{bmatrix}0&2&-45\\-2&0&-4\\45&4&0\end{bmatrix}}} is skew-symmetric because A T = [ 0 − − 2 45 2 0 4 − − 45 − − 4 0 ] = − − A .

{\displaystyle A^{\textsf {T}}={\begin{bmatrix}0&-2&45\\2&0&4\\-45&-4&0\end{bmatrix}}=-A.} Properties [ edit ] Throughout, we assume that all matrix entries belong to a field F {\textstyle \mathbb {F} } whose characteristic is not equal to 2. That is, we assume that 1 + 1 ≠ 0 , where 1 denotes the multiplicative identity and 0 the additive identity of the given field. If the characteristic of the field is 2, then a skew-symmetric matrix is the same thing as a symmetric matrix .

The sum of two skew-symmetric matrices is skew-symmetric.

A scalar multiple of a skew-symmetric matrix is skew-symmetric.

The elements on the diagonal of a skew-symmetric matrix are zero, and therefore its trace equals zero.

If A {\textstyle A} is a real skew-symmetric matrix and λ λ {\textstyle \lambda } is a real eigenvalue , then λ λ = 0 {\textstyle \lambda =0} , i.e. the nonzero eigenvalues of a skew-symmetric matrix are non-real.

If A {\textstyle A} is a real skew-symmetric matrix, then I + A {\textstyle I+A} is invertible , where I {\textstyle I} is the identity matrix.

If A {\textstyle A} is a skew-symmetric matrix then A 2 {\textstyle A^{2}} is a symmetric negative semi-definite matrix .

Vector space structure [ edit ] As a result of the first two properties above, the set of all skew-symmetric matrices of a fixed size forms a vector space . The space of n × × n {\textstyle n\times n} skew-symmetric matrices has dimension 1 2 n ( n − − 1 ) .

{\textstyle {\frac {1}{2}}n(n-1).} Let Mat n {\displaystyle {\mbox{Mat}}_{n}} denote the space of n × × n {\textstyle n\times n} matrices. A skew-symmetric matrix is determined by 1 2 n ( n − − 1 ) {\textstyle {\frac {1}{2}}n(n-1)} scalars (the number of entries above the main diagonal ); a symmetric matrix is determined by 1 2 n ( n + 1 ) {\textstyle {\frac {1}{2}}n(n+1)} scalars (the number of entries on or above the main diagonal). Let Skew n {\textstyle {\mbox{Skew}}_{n}} denote the space of n × × n {\textstyle n\times n} skew-symmetric matrices and Sym n {\textstyle {\mbox{Sym}}_{n}} denote the space of n × × n {\textstyle n\times n} symmetric matrices. If A ∈ ∈ Mat n {\textstyle A\in {\mbox{Mat}}_{n}} then A = 1 2 ( A − − A T ) + 1 2 ( A + A T ) .

{\displaystyle A={\tfrac {1}{2}}\left(A-A^{\mathsf {T}}\right)+{\tfrac {1}{2}}\left(A+A^{\mathsf {T}}\right).} Notice that 1 2 ( A − − A T ) ∈ ∈ Skew n {\textstyle {\frac {1}{2}}\left(A-A^{\textsf {T}}\right)\in {\mbox{Skew}}_{n}} and 1 2 ( A + A T ) ∈ ∈ Sym n .

{\textstyle {\frac {1}{2}}\left(A+A^{\textsf {T}}\right)\in {\mbox{Sym}}_{n}.} This is true for every square matrix A {\textstyle A} with entries from any field whose characteristic is different from 2. Then, since Mat n = Skew n + Sym n {\textstyle {\mbox{Mat}}_{n}={\mbox{Skew}}_{n}+{\mbox{Sym}}_{n}} and Skew n ∩ ∩ Sym n = { 0 } , {\textstyle {\mbox{Skew}}_{n}\cap {\mbox{Sym}}_{n}=\{0\},} Mat n = Skew n ⊕ ⊕ Sym n , {\displaystyle {\mbox{Mat}}_{n}={\mbox{Skew}}_{n}\oplus {\mbox{Sym}}_{n},} where ⊕ ⊕ {\displaystyle \oplus } denotes the direct sum .

Denote by ⟨ ⟨ ⋅ ⋅ , ⋅ ⋅ ⟩ ⟩ {\textstyle \langle \cdot ,\cdot \rangle } the standard inner product on R n .

{\displaystyle \mathbb {R} ^{n}.} The real n × × n {\displaystyle n\times n} matrix A {\textstyle A} is skew-symmetric if and only if ⟨ ⟨ A x , y ⟩ ⟩ = − − ⟨ ⟨ x , A y ⟩ ⟩ for all x , y ∈ ∈ R n .

{\displaystyle \langle Ax,y\rangle =-\langle x,Ay\rangle \quad {\text{ for all }}x,y\in \mathbb {R} ^{n}.} This is also equivalent to ⟨ ⟨ x , A x ⟩ ⟩ = 0 {\textstyle \langle x,Ax\rangle =0} for all x ∈ ∈ R n {\displaystyle x\in \mathbb {R} ^{n}} (one implication being obvious, the other a plain consequence of ⟨ ⟨ x + y , A ( x + y ) ⟩ ⟩ = 0 {\textstyle \langle x+y,A(x+y)\rangle =0} for all x {\displaystyle x} and y {\displaystyle y} ).

Since this definition is independent of the choice of basis , skew-symmetry is a property that depends only on the linear operator A {\displaystyle A} and a choice of inner product .

3 × × 3 {\displaystyle 3\times 3} skew symmetric matrices can be used to represent cross products as matrix multiplications.

Furthermore, if A {\displaystyle A} is a skew-symmetric (or skew-Hermitian ) matrix, then x T A x = 0 {\displaystyle x^{T}Ax=0} for all x ∈ ∈ C n {\displaystyle x\in \mathbb {C} ^{n}} .

Determinant [ edit ] Let A {\displaystyle A} be a n × × n {\displaystyle n\times n} skew-symmetric matrix. The determinant of A {\displaystyle A} satisfies det ( A ) = det ( A T ) = det ( − − A ) = ( − − 1 ) n det ( A ) .

{\displaystyle \det(A)=\det \left(A^{\textsf {T}}\right)=\det(-A)={\left(-1\right)}^{n}\det(A).} In particular, if n {\displaystyle n} is odd, and since the underlying field is not of characteristic 2, the determinant vanishes. Hence, all odd dimension skew symmetric matrices are singular as their determinants are always zero. This result is called Jacobi’s theorem , after Carl Gustav Jacobi (Eves, 1980).

The even-dimensional case is more interesting. It turns out that the determinant of A {\displaystyle A} for n {\displaystyle n} even can be written as the square of a polynomial in the entries of A {\displaystyle A} , which was first proved by Cayley: [ 3 ] det ( A ) = Pf ⁡ ⁡ ( A ) 2 .

{\displaystyle \det(A)=\operatorname {Pf} (A)^{2}.} This polynomial is called the Pfaffian of A {\displaystyle A} and is denoted Pf ⁡ ⁡ ( A ) {\displaystyle \operatorname {Pf} (A)} . Thus the determinant of a real skew-symmetric matrix is always non-negative. However this last fact can be proved in an elementary way as follows: the eigenvalues of a real skew-symmetric matrix are purely imaginary (see below) and to every eigenvalue there corresponds the conjugate eigenvalue with the same multiplicity; therefore, as the determinant is the product of the eigenvalues, each one repeated according to its multiplicity, it follows at once that the determinant, if it is not 0, is a positive real number.

The number of distinct terms s ( n ) {\displaystyle s(n)} in the expansion of the determinant of a skew-symmetric matrix of order n {\displaystyle n} was considered already by Cayley, Sylvester, and Pfaff. Due to cancellations, this number is quite small as compared the number of terms of the determinant of a generic matrix of order n {\displaystyle n} , which is n !

{\displaystyle n!} . The sequence s ( n ) {\displaystyle s(n)} (sequence A002370 in the OEIS ) is 1, 0, 1, 0, 6, 0, 120, 0, 5250, 0, 395010, 0, … and it is encoded in the exponential generating function ∑ ∑ n = 0 ∞ ∞ s ( n ) n !

x n = ( 1 − − x 2 ) − − 1 4 exp ⁡ ⁡ ( x 2 4 ) .

{\displaystyle \sum _{n=0}^{\infty }{\frac {s(n)}{n!}}x^{n}=\left(1-x^{2}\right)^{-{\frac {1}{4}}}\exp \left({\frac {x^{2}}{4}}\right).} The latter yields to the asymptotics (for n {\displaystyle n} even) s ( n ) = 2 3 4 π π 1 2 Γ Γ ( 3 4 ) ( n e ) n − − 1 4 ( 1 + O ( n − − 1 ) ) .

{\displaystyle s(n)={\frac {2^{\frac {3}{4}}}{\pi ^{\frac {1}{2}}}}\,\Gamma {\left({\frac {3}{4}}\right)}{\left({\frac {n}{e}}\right)}^{n-{\frac {1}{4}}}\left(1+O{\left(n^{-1}\right)}\right).} The number of positive and negative terms are approximatively a half of the total, although their difference takes larger and larger positive and negative values as n {\displaystyle n} increases (sequence A167029 in the OEIS ).

Cross product [ edit ] Three-by-three skew-symmetric matrices can be used to represent cross products as matrix multiplications. Consider two vectors a = ( a 1 , a 2 , a 3 ) {\displaystyle \mathbf {a} =\left(a_{1},a_{2},a_{3}\right)} and b = ( b 1 , b 2 , b 3 ) .

{\displaystyle \mathbf {b} =\left(b_{1},b_{2},b_{3}\right).} The cross product a × × b {\displaystyle \mathbf {a} \times \mathbf {b} } is a bilinear map , which means that by fixing one of the two arguments, for example a {\displaystyle \mathbf {a} } , it induces a linear map with an associated transformation matrix [ a ] × × {\displaystyle [\mathbf {a} ]_{\times }} , such that a × × b = [ a ] × × b , {\displaystyle \mathbf {a} \times \mathbf {b} =[\mathbf {a} ]_{\times }\mathbf {b} ,} where [ a ] × × {\displaystyle [\mathbf {a} ]_{\times }} is [ a ] × × = [ 0 − − a 3 a 2 a 3 0 − − a 1 − − a 2 a 1 0 ] .

{\displaystyle [\mathbf {a} ]_{\times }={\begin{bmatrix}\,\,0&\!-a_{3}&\,\,\,a_{2}\\\,\,\,a_{3}&0&\!-a_{1}\\\!-a_{2}&\,\,a_{1}&\,\,0\end{bmatrix}}.} This can be immediately verified by computing both sides of the previous equation and comparing each corresponding element of the results.

See also: Plücker matrix One actually has [ a × × b ] × × = [ a ] × × [ b ] × × − − [ b ] × × [ a ] × × ; {\displaystyle [\mathbf {a\times b} ]_{\times }=[\mathbf {a} ]_{\times }[\mathbf {b} ]_{\times }-[\mathbf {b} ]_{\times }[\mathbf {a} ]_{\times };} i.e., the commutator of skew-symmetric three-by-three matrices can be identified with the cross-product of two vectors.  Since the skew-symmetric three-by-three matrices are the Lie algebra of the rotation group S O ( 3 ) {\textstyle SO(3)} this elucidates the relation between three-space R 3 {\textstyle \mathbb {R} ^{3}} , the cross product and three-dimensional rotations.  More on infinitesimal rotations can be found below.

Spectral theory [ edit ] Since a matrix is similar to its own transpose, they must have the same eigenvalues. It follows that the eigenvalues of a skew-symmetric matrix always come in pairs ±λ (except in the odd-dimensional case where there is an additional unpaired 0 eigenvalue). From the spectral theorem , for a real skew-symmetric matrix the nonzero eigenvalues are all pure imaginary and thus are of the form λ λ 1 i , − − λ λ 1 i , λ λ 2 i , − − λ λ 2 i , … … {\displaystyle \lambda _{1}i,-\lambda _{1}i,\lambda _{2}i,-\lambda _{2}i,\ldots } where each of the λ λ k {\displaystyle \lambda _{k}} are real.

Real skew-symmetric matrices are normal matrices (they commute with their adjoints ) and are thus subject to the spectral theorem , which states that any real skew-symmetric matrix can be diagonalized by a unitary matrix . Since the eigenvalues of a real skew-symmetric matrix are imaginary, it is not possible to diagonalize one by a real matrix. However, it is possible to bring every skew-symmetric matrix to a block diagonal form by a special orthogonal transformation .

[ 4 ] [ 5 ] Specifically, every 2 n × × 2 n {\displaystyle 2n\times 2n} real skew-symmetric matrix can be written in the form A = Q Σ Σ Q T {\displaystyle A=Q\Sigma Q^{\textsf {T}}} where Q {\displaystyle Q} is orthogonal and Σ Σ = [ 0 λ λ 1 − − λ λ 1 0 0 ⋯ ⋯ 0 0 0 λ λ 2 − − λ λ 2 0 0 ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 0 ⋯ ⋯ 0 λ λ r − − λ λ r 0 0 ⋱ ⋱ 0 ] {\displaystyle \Sigma ={\begin{bmatrix}{\begin{matrix}0&\lambda _{1}\\-\lambda _{1}&0\end{matrix}}&0&\cdots &0\\0&{\begin{matrix}0&\lambda _{2}\\-\lambda _{2}&0\end{matrix}}&&0\\\vdots &&\ddots &\vdots \\0&0&\cdots &{\begin{matrix}0&\lambda _{r}\\-\lambda _{r}&0\end{matrix}}\\&&&&{\begin{matrix}0\\&\ddots \\&&0\end{matrix}}\end{bmatrix}}} for real positive-definite λ λ k {\displaystyle \lambda _{k}} . The nonzero eigenvalues of this matrix are ±λ k i . In the odd-dimensional case Σ always has at least one row and column of zeros.

More generally, every complex skew-symmetric matrix can be written in the form A = U Σ Σ U T {\displaystyle A=U\Sigma U^{\mathrm {T} }} where U {\displaystyle U} is  unitary and Σ Σ {\displaystyle \Sigma } has the block-diagonal form given above with λ λ k {\displaystyle \lambda _{k}} still real positive-definite. This is an example of the Youla decomposition of a complex square matrix.

[ 6 ] Skew-symmetric and alternating forms [ edit ] A skew-symmetric form φ φ {\displaystyle \varphi } on a vector space V {\displaystyle V} over a field K {\displaystyle K} of arbitrary characteristic is defined to be a bilinear form φ φ : V × × V ↦ ↦ K {\displaystyle \varphi :V\times V\mapsto K} such that for all v , w {\displaystyle v,w} in V , {\displaystyle V,} φ φ ( v , w ) = − − φ φ ( w , v ) .

{\displaystyle \varphi (v,w)=-\varphi (w,v).} This defines a form with desirable properties for vector spaces over fields of characteristic not equal to 2, but in a vector space over a field of characteristic 2, the definition is equivalent to that of a symmetric form, as every element is its own additive inverse.

Where the vector space V {\displaystyle V} is over a field of arbitrary characteristic including characteristic 2, we may define an alternating form as a bilinear form φ φ {\displaystyle \varphi } such that for all vectors v {\displaystyle v} in V {\displaystyle V} φ φ ( v , v ) = 0.

{\displaystyle \varphi (v,v)=0.} This is equivalent to a skew-symmetric form when the field is not of characteristic 2, as seen from 0 = φ φ ( v + w , v + w ) = φ φ ( v , v ) + φ φ ( v , w ) + φ φ ( w , v ) + φ φ ( w , w ) = φ φ ( v , w ) + φ φ ( w , v ) , {\displaystyle 0=\varphi (v+w,v+w)=\varphi (v,v)+\varphi (v,w)+\varphi (w,v)+\varphi (w,w)=\varphi (v,w)+\varphi (w,v),} whence φ φ ( v , w ) = − − φ φ ( w , v ) .

{\displaystyle \varphi (v,w)=-\varphi (w,v).} A bilinear form φ φ {\displaystyle \varphi } will be represented by a matrix A {\displaystyle A} such that φ φ ( v , w ) = v T A w {\displaystyle \varphi (v,w)=v^{\textsf {T}}Aw} , once a basis of V {\displaystyle V} is chosen, and conversely an n × × n {\displaystyle n\times n} matrix A {\displaystyle A} on K n {\displaystyle K^{n}} gives rise to a form sending ( v , w ) {\displaystyle (v,w)} to v T A w .

{\displaystyle v^{\textsf {T}}Aw.} For each of symmetric, skew-symmetric and alternating forms, the representing matrices are symmetric, skew-symmetric and alternating respectively.

Infinitesimal rotations [ edit ] This section is an excerpt from Infinitesimal rotation matrix § Relationship to skew-symmetric matrices .

[ edit ] Skew-symmetric matrices over the field of real numbers form the tangent space to the real orthogonal group O ( n ) {\displaystyle \mathrm {O} (n)} at the identity matrix; formally, the special orthogonal Lie algebra . In this sense, then, skew-symmetric matrices can be thought of as infinitesimal rotations .

Another way of saying this is that the space of skew-symmetric matrices forms the Lie algebra o ( n ) {\displaystyle {\mathfrak {o}}(n)} of the Lie group ⁠ O ( n ) {\displaystyle \mathrm {O} (n)} ⁠ . The Lie bracket on this space is given by the commutator : [ A , B ] = A B − − B A .

{\displaystyle [A,B]=AB-BA.\,} It is easy to check that the commutator of two skew-symmetric matrices is again skew-symmetric: [ A , B ] T = B T A T − − A T B T = ( − − B ) ( − − A ) − − ( − − A ) ( − − B ) = B A − − A B = − − [ A , B ] .

{\displaystyle {\begin{aligned}{[}A,B{]}^{\textsf {T}}&=B^{\textsf {T}}A^{\textsf {T}}-A^{\textsf {T}}B^{\textsf {T}}\\&=(-B)(-A)-(-A)(-B)=BA-AB=-[A,B]\,.\end{aligned}}} The matrix exponential of a skew-symmetric matrix A {\displaystyle A} is then an orthogonal matrix R {\displaystyle R} : R = exp ⁡ ⁡ ( A ) = ∑ ∑ n = 0 ∞ ∞ A n n !

.

{\displaystyle R=\exp(A)=\sum _{n=0}^{\infty }{\frac {A^{n}}{n!}}.} The image of the exponential map of a Lie algebra always lies in the connected component of the Lie group that contains the identity element. In the case of the Lie group ⁠ O ( n ) {\displaystyle \mathrm {O} (n)} ⁠ , this connected component is the special orthogonal group ⁠ S O ( n ) {\displaystyle \mathrm {SO} (n)} ⁠ , consisting of all orthogonal matrices with determinant 1. So R = exp ⁡ ⁡ ( A ) {\displaystyle R=\exp(A)} will have determinant +1. Moreover, since the exponential map of a connected compact Lie group is always surjective, it turns out that every orthogonal matrix with unit determinant can be written as the exponential of some skew-symmetric matrix.

In the particular important case of dimension n = 2 , {\displaystyle n=2,} the exponential representation for an orthogonal matrix reduces to  the well-known polar form of a complex number of unit modulus. Indeed, if ⁠ n = 2 {\displaystyle n=2} ⁠ , a special orthogonal matrix has the form [ a − − b b a ] , {\displaystyle {\begin{bmatrix}a&-b\\b&\,a\end{bmatrix}},} with ⁠ a 2 + b 2 = 1 {\displaystyle a^{2}+b^{2}=1} ⁠ . Therefore, putting a = cos ⁡ ⁡ θ θ {\displaystyle a=\cos \theta } and ⁠ b = sin ⁡ ⁡ θ θ {\displaystyle b=\sin \theta } ⁠ , it can be written [ cos θ θ − − sin θ θ sin θ θ cos θ θ ] = exp ⁡ ⁡ ( θ θ [ 0 − − 1 1 0 ] ) , {\displaystyle {\begin{bmatrix}\cos \,\theta &-\sin \,\theta \\\sin \,\theta &\,\cos \,\theta \end{bmatrix}}=\exp \left(\theta {\begin{bmatrix}0&-1\\1&\,0\end{bmatrix}}\right),} which corresponds exactly to the polar form cos ⁡ ⁡ θ θ + i sin ⁡ ⁡ θ θ = exp ⁡ ⁡ ( i θ θ ) {\displaystyle \cos \theta +i\sin \theta =\exp(i\theta )} of a complex number of unit modulus.

In 3 dimensions, the matrix exponential is Rodrigues' rotation formula in matrix notation , and when expressed via the Euler-Rodrigues formula , the algebra of its four parameters gives rise to quaternions .

The exponential representation of an orthogonal matrix of order n {\displaystyle n} can also be obtained starting from the fact that in dimension n {\displaystyle n} any special orthogonal matrix R {\displaystyle R} can be written as ⁠ R = Q S Q T {\displaystyle R=QSQ^{\textsf {T}}} ⁠ , where Q {\displaystyle Q} is orthogonal and S is a block diagonal matrix with ⌊ ⌊ n / 2 ⌋ ⌋ {\textstyle \lfloor n/2\rfloor } blocks of order 2, plus one of order 1 if n {\displaystyle n} is odd; since each single block of order 2 is also an orthogonal matrix, it admits an exponential form. Correspondingly, the matrix S writes as exponential of a skew-symmetric block matrix Σ Σ {\displaystyle \Sigma } of the form above, ⁠ S = exp ⁡ ⁡ ( Σ Σ ) {\displaystyle S=\exp(\Sigma )} ⁠ , so that ⁠ R = Q exp ⁡ ⁡ ( Σ Σ ) Q T = exp ⁡ ⁡ ( Q Σ Σ Q T ) {\displaystyle R=Q\exp(\Sigma )Q^{\textsf {T}}=\exp(Q\Sigma Q^{\textsf {T}})} ⁠ , exponential of the skew-symmetric matrix ⁠ Q Σ Σ Q T {\displaystyle Q\Sigma Q^{\textsf {T}}} ⁠ . Conversely, the surjectivity of the exponential map, together with the above-mentioned block-diagonalization for skew-symmetric matrices, implies the block-diagonalization for orthogonal matrices.

Coordinate-free [ edit ] More intrinsically (i.e., without using coordinates), skew-symmetric linear transformations on a vector space V {\displaystyle V} with an inner product may be defined as the bivectors on the space, which are sums of simple bivectors ( 2-blades ) v ∧ ∧ w .

{\textstyle v\wedge w.} The correspondence is given by the map v ∧ ∧ w ↦ ↦ v ⊗ ⊗ w − − w ⊗ ⊗ v {\textstyle v\wedge w\mapsto v\otimes w-w\otimes v} ; in orthonormal coordinates these are exactly the elementary skew-symmetric matrices. This characterization is used in interpreting the curl of a vector field (naturally a 2-vector) as an infinitesimal rotation or "curl", hence the name.

Skew-symmetrizable matrix [ edit ] An n × × n {\displaystyle n\times n} matrix A {\displaystyle A} is said to be skew-symmetrizable if there exists an invertible diagonal matrix D {\displaystyle D} such that D A {\displaystyle DA} is skew-symmetric. For real n × × n {\displaystyle n\times n} matrices, sometimes the condition for D {\displaystyle D} to have positive entries is added.

[ 7 ] See also [ edit ] Cayley transform Symmetric matrix Skew-Hermitian matrix Symplectic matrix Symmetry in mathematics References [ edit ] ^ Richard A. Reyment; K. G. Jöreskog ; Leslie F. Marcus (1996).

Applied Factor Analysis in the Natural Sciences . Cambridge University Press. p. 68.

ISBN 0-521-57556-7 .

^ Lipschutz, Seymour; Lipson, Marc (September 2005).

Schaum's Outline of Theory and Problems of Linear Algebra . McGraw-Hill. p. 38.

ISBN 9780070605022 .

^ Cayley, Arthur (1847). "Sur les determinants gauches" [On skew determinants].

Crelle's Journal .

38 : 93– 96.

Reprinted in Cayley, A. (2009). "Sur les Déterminants Gauches".

The Collected Mathematical Papers . Vol. 1. pp.

410– 413.

doi : 10.1017/CBO9780511703676.070 .

ISBN 978-0-511-70367-6 .

^ Duplij, S.; Nikitin, A.; Galkin, A.; Sergyeyev, A.; Dayi, O.F.; Mohapatra, R.; Lipatov, L.; Dunne, G.; Feinberg, J.; Aoyama, H.; Voronov, T. (2004).

"Pfaffian" . In Duplij, S.; Siegel, W.; Bagger, J. (eds.).

Concise Encyclopedia of Supersymmetry . Springer. p. 298.

doi : 10.1007/1-4020-4522-0_393 .

ISBN 978-1-4020-1338-6 .

^ Zumino, Bruno (1962). "Normal Forms of Complex Matrices".

Journal of Mathematical Physics .

3 (5): 1055– 7.

Bibcode : 1962JMP.....3.1055Z .

doi : 10.1063/1.1724294 .

^ Youla, D. C. (1961).

"A normal form for a matrix under the unitary congruence group" .

Can. J. Math .

13 : 694– 704.

doi : 10.4153/CJM-1961-059-8 .

^ Fomin, Sergey; Zelevinsky, Andrei (2001). "Cluster algebras I: Foundations".

arXiv : math/0104151v1 .

Further reading [ edit ] Eves, Howard (1980).

Elementary Matrix Theory . Dover Publications.

ISBN 978-0-486-63946-8 .

Suprunenko, D. A. (2001) [1994], "Skew-symmetric matrix" , Encyclopedia of Mathematics , EMS Press Aitken, A. C. (1944).

"On the number of distinct terms in the expansion of symmetric and skew determinants" .

Edinburgh Math. Notes .

34 : 1– 5.

doi : 10.1017/S0950184300000070 .

External links [ edit ] "Antisymmetric matrix" .

Wolfram Mathworld .

Benner, Peter; Kressner, Daniel .

"HAPACK – Software for (Skew-)Hamiltonian Eigenvalue Problems" .

Ward, R. C.; Gray, L. J. (1978).

"Algorithm 530: An Algorithm for Computing the Eigensystem of Skew-Symmetric Matrices and a Class of Symmetric Matrices [F2]" .

ACM Transactions on Mathematical Software .

4 (3): 286.

doi : 10.1145/355791.355799 .

S2CID 8575785 .

Fortran Fortran90 v t e Matrix classes Explicitly constrained entries Alternant Anti-diagonal Anti-Hermitian Anti-symmetric Arrowhead Band Bidiagonal Bisymmetric Block-diagonal Block Block tridiagonal Boolean Cauchy Centrosymmetric Conference Complex Hadamard Copositive Diagonally dominant Diagonal Discrete Fourier Transform Elementary Equivalent Frobenius Generalized permutation Hadamard Hankel Hermitian Hessenberg Hollow Integer Logical Matrix unit Metzler Moore Nonnegative Pentadiagonal Permutation Persymmetric Polynomial Quaternionic Signature Skew-Hermitian Skew-symmetric Skyline Sparse Sylvester Symmetric Toeplitz Triangular Tridiagonal Vandermonde Walsh Z Constant Exchange Hilbert Identity Lehmer Of ones Pascal Pauli Redheffer Shift Zero Conditions on eigenvalues or eigenvectors Companion Convergent Defective Definite Diagonalizable Hurwitz-stable Positive-definite Stieltjes Satisfying conditions on products or inverses Congruent Idempotent or Projection Invertible Involutory Nilpotent Normal Orthogonal Unimodular Unipotent Unitary Totally unimodular Weighing With specific applications Adjugate Alternating sign Augmented Bézout Carleman Cartan Circulant Cofactor Commutation Confusion Coxeter Distance Duplication and elimination Euclidean distance Fundamental (linear differential equation) Generator Gram Hessian Householder Jacobian Moment Payoff Pick Random Rotation Routh-Hurwitz Seifert Shear Similarity Symplectic Totally positive Transformation Used in statistics Centering Correlation Covariance Design Doubly stochastic Fisher information Hat Precision Stochastic Transition Used in graph theory Adjacency Biadjacency Degree Edmonds Incidence Laplacian Seidel adjacency Tutte Used in science and engineering Cabibbo–Kobayashi–Maskawa Density Fundamental (computer vision) Fuzzy associative Gamma Gell-Mann Hamiltonian Irregular Overlap S State transition Substitution Z (chemistry) Related terms Jordan normal form Linear independence Matrix exponential Matrix representation of conic sections Perfect matrix Pseudoinverse Row echelon form Wronskian Mathematics portal List of matrices Category:Matrices (mathematics) Authority control databases : National Germany NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐sn5h6
Cached time: 20250812014249
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.526 seconds
Real time usage: 0.804 seconds
Preprocessor visited node count: 2172/1000000
Revision size: 19402/2097152 bytes
Post‐expand include size: 73657/2097152 bytes
Template argument size: 991/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 7/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 63124/5000000 bytes
Lua time usage: 0.296/10.000 seconds
Lua memory usage: 6586029/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  464.344      1 -total
 26.04%  120.935      1 Template:Reflist
 19.69%   91.438      5 Template:Cite_book
 17.33%   80.467      1 Template:Matrix_classes
 16.94%   78.677      1 Template:Navbox
 14.16%   65.737      1 Template:Excerpt
 14.06%   65.266      1 Template:Short_description
  9.45%   43.869      2 Template:Pagetype
  9.05%   42.036      1 Template:More_footnotes
  7.76%   36.056      1 Template:Ambox Saved in parser cache with key enwiki:pcache:174055:|#|:idhash:canonical and timestamp 20250812014249 and revision id 1295615914. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Skew-symmetric_matrix&oldid=1295615914 " Category : Matrices (mathematics) Hidden categories: Articles with short description Short description is different from Wikidata Articles lacking in-text citations from November 2009 All articles lacking in-text citations Articles with excerpts Pages that use a deprecated format of the math tags This page was last edited on 14 June 2025, at 21:44 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Skew-symmetric matrix 31 languages Add topic

