Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Etymology 2 Information theory 3 Correlation between statistical negentropy and Gibbs' free energy 4 Brillouin's negentropy principle of information 5 See also 6 References Toggle the table of contents Negentropy 17 languages Boarisch Català Deutsch Español Français Italiano Кыргызча Lietuvių Nederlands 日本語 Norsk bokmål Norsk nynorsk Polski Português Русский Simple English Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Measure of distance to normality Not to be confused with Negative entropy [ clarification needed Should this be " For biological contexts..."?

] .

"Syntropy" redirects here. For other uses, see Syntropy (software) .

This article's factual accuracy is disputed .

Relevant discussion may be found on the talk page . Please help to ensure that disputed statements are reliably sourced .

( May 2025 ) ( Learn how and when to remove this message ) In information theory and statistics , negentropy is used as a measure of distance to normality. It is also known as negative entropy or syntropy .

Etymology [ edit ] The concept and phrase " negative entropy " was introduced by Erwin Schrödinger in his 1944 book What is Life?

.

[ 1 ] Later, French physicist Léon Brillouin shortened the phrase to néguentropie ( transl.

negentropy ).

[ 2 ] [ 3 ] In 1974, Albert Szent-Györgyi proposed replacing the term negentropy with syntropy . That term may have originated in the 1940s with the Italian mathematician Luigi Fantappiè , who tried to construct a unified theory of biology and physics .

Buckminster Fuller tried to popularize this usage, but negentropy remains common.

[ citation needed ] In a note to What is Life?, Schrödinger explained his use of this phrase: ... if I had been catering for them [physicists] alone I should have let the discussion turn on free energy instead. It is the more familiar notion in this context. But this highly technical term seemed linguistically too near to energy for making the average reader alive to the contrast between the two things.

Information theory [ edit ] See also: Maximum entropy probability distribution This section is missing information about the mathematical treatment of negentropy in information theory.

Please expand the section to include this information. Further details may exist on the talk page .

( December 2024 ) In information theory and statistics , negentropy is used as a measure of distance to normality.

[ 4 ] [ 5 ] [ 6 ] Out of all probability distributions with a given mean and variance , the Gaussian or normal distribution is the one with the highest entropy .

[ clarification needed ] [ citation needed ] Negentropy measures the difference in entropy between a given distribution and the Gaussian distribution with the same mean and variance. Thus, negentropy is always nonnegative, is invariant by any linear invertible change of coordinates, and vanishes if and only if the signal is Gaussian.

[ citation needed ] Negentropy is defined as J ( Y ) = h ( Y G ) − − h ( Y ) , {\displaystyle J(Y)=h(Y_{G})-h(Y),} where h ( Y G ) = 1 2 log ⁡ ⁡ ( 2 π π e ⋅ ⋅ σ σ 2 ) {\displaystyle h(Y_{G})={\tfrac {1}{2}}\log \left(2\pi \mathrm {e} \cdot \sigma ^{2}\right)} is the differential entropy of a normal distribution Y G ∼ ∼ N ( μ μ , σ σ 2 ) {\displaystyle Y_{G}\sim N(\mu ,\sigma ^{2})} with the same mean μ μ {\displaystyle \mu } and variance σ σ 2 {\displaystyle \sigma ^{2}} as Y {\displaystyle Y} , and h ( Y ) {\displaystyle h(Y)} is the differential entropy of Y {\displaystyle Y} , with p Y {\displaystyle p_{Y}} as its probability density function : h ( Y ) = − − ∫ ∫ p Y ( u ) log ⁡ ⁡ p Y ( u ) d u {\displaystyle h(Y)=-\int p_{Y}(u)\log p_{Y}(u)\,\mathrm {d} u} Negentropy is used in statistics and signal processing . It is related to network entropy , which is used in independent component analysis .

[ 7 ] [ 8 ] The negentropy of a distribution is equal to the Kullback–Leibler divergence between Y {\displaystyle Y} and a Gaussian distribution with the same mean and variance as Y {\displaystyle Y} (see Differential entropy § Maximization in the normal distribution for a proof): J ( Y ) = D K L ( Y ‖ ‖ Y G ) {\displaystyle J(Y)=D_{KL}(Y\ \Vert \ Y_{G})} In particular, it is always nonnegative (unlike differential entropy, which can be negative).

Correlation between statistical negentropy and Gibbs' free energy [ edit ] This section may be too technical for most readers to understand .

Please help improve it to make it understandable to non-experts , without removing the technical details.

( June 2025 ) ( Learn how and when to remove this message ) Willard Gibbs ' 1873 available energy ( free energy ) graph, which shows a plane perpendicular to the axis of v ( volume ) and passing through point A, which represents the initial state of the body. MN is the section of the surface of dissipated energy . Qε and Qη are sections of the planes η = 0 and ε = 0, and therefore parallel to the axes of ε ( internal energy ) and η ( entropy ) respectively. AD and AE are the energy and entropy of the body in its initial state, AB and AC its available energy ( Gibbs energy ) and its capacity for entropy (the amount by which the entropy of the body can be increased without changing the energy of the body or increasing its volume) respectively.

There is a physical quantity closely linked to free energy ( free enthalpy ), with a unit of entropy and isomorphic to negentropy known in statistics and information theory. In 1873, Willard Gibbs created a diagram illustrating the concept of free energy corresponding to free enthalpy . On the diagram one can see the quantity called capacity for entropy . This quantity is the amount of entropy that may be increased without changing an internal energy or increasing its volume.

[ 9 ] In other words, it is a difference between maximum possible, under assumed conditions, entropy and its actual entropy. It corresponds exactly to the definition of negentropy adopted in statistics and information theory. A similar physical quantity was introduced in 1869 by Massieu for the isothermal process [ 10 ] [ 11 ] [ 12 ] (both quantities differs just with a figure sign) and by then Planck for the isothermal - isobaric process.

[ 13 ] More recently, the Massieu–Planck thermodynamic potential , known also as free entropy , has been shown to play a great role in the so-called entropic formulation of statistical mechanics , [ 14 ] applied among the others in molecular biology [ 15 ] and thermodynamic non-equilibrium processes.

[ 16 ] J = S max − − S = − − Φ Φ = − − k ln ⁡ ⁡ Z {\displaystyle J=S_{\max }-S=-\Phi =-k\ln Z\,} where: S {\displaystyle S} is entropy J {\displaystyle J} is negentropy (Gibbs "capacity for entropy") Φ Φ {\displaystyle \Phi } is the Massieu potential Z {\displaystyle Z} is the partition function k {\displaystyle k} the Boltzmann constant In particular, mathematically the negentropy (the negative entropy function, in physics interpreted as free entropy) is the convex conjugate of LogSumExp (in physics interpreted as the free energy).

Brillouin's negentropy principle of information [ edit ] In 1953, Léon Brillouin derived a general equation [ 17 ] stating that the changing of an information bit value requires at least k T ln ⁡ ⁡ 2 {\displaystyle kT\ln 2} energy. This is the same energy as the work Leó Szilárd 's engine produces in the idealistic case. In his book, [ 18 ] he further explored this problem concluding that any cause of this bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount of energy.

See also [ edit ] Exergy Free entropy Entropy in thermodynamics and information theory References [ edit ] ^ Schrödinger, Erwin, What is Life – the Physical Aspect of the Living Cell , Cambridge University Press, 1944 ^ Brillouin, Leon: (1953) "Negentropy Principle of Information", J. of Applied Physics , v.

24(9) , pp. 1152–1163 ^ Léon Brillouin, La science et la théorie de l'information , Masson, 1959 ^ Hyvärinen, Aapo.

"Survey on Independent Component Analysis, node32: Negentropy" .

cis.legacy.ics.tkk.fi . Helsinki University of Technology Laboratory of Computer and Information Science . Retrieved 2025-06-09 .

^ Hyvärinen, Aapo; Oja, Erkki.

"Independent Component Analysis: A Tutorial, node14: Negentropy" .

cis.legacy.ics.tkk.fi . Helsinki University of Technology Laboratory of Computer and Information Science.

Archived from the original on 2025-04-21 . Retrieved 2025-06-09 .

^ Wang, Ruye.

"Independent Component Analysis, node4: Measures of Non-Gaussianity" . Archived from the original on 2021-03-22 . Retrieved 2025-06-09 .

^ P. Comon, Independent Component Analysis – a new concept?, Signal Processing , 36 287–314, 1994.

^ Didier G. Leibovici and Christian Beckmann, An introduction to Multiway Methods for Multi-Subject fMRI experiment , FMRIB Technical Report 2001, Oxford Centre for Functional Magnetic Resonance Imaging of the Brain (FMRIB), Department of Clinical Neurology, University of Oxford, John Radcliffe Hospital, Headley Way, Headington, Oxford, UK.

^ Willard Gibbs, A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces , Transactions of the Connecticut Academy , 382–404 (1873) ^ Massieu, M. F. (1869a). Sur les fonctions caractéristiques des divers fluides.

C. R. Acad. Sci.

LXIX:858–862.

^ Massieu, M. F. (1869b). Addition au precedent memoire sur les fonctions caractéristiques.

C. R. Acad. Sci.

LXIX:1057–1061.

^ Massieu, M. F. (1869), Compt. Rend.

69 (858): 1057.

^ Planck, M. (1945).

Treatise on Thermodynamics . Dover, New York.

^ Antoni Planes, Eduard Vives, Entropic Formulation of Statistical Mechanics Archived 2008-10-11 at the Wayback Machine , Entropic variables and Massieu–Planck functions 2000-10-24 Universitat de Barcelona ^ John A. Scheilman, Temperature, Stability, and the Hydrophobic Interaction , Biophysical Journal 73 (December 1997), 2960–2964, Institute of Molecular Biology, University of Oregon, Eugene, Oregon 97403 USA ^ Z. Hens and X. de Hemptinne, Non-equilibrium Thermodynamics approach to Transport Processes in Gas Mixtures , Department of Chemistry, Catholic University of Leuven, Celestijnenlaan 200 F, B-3001 Heverlee, Belgium ^ Leon Brillouin, The negentropy principle of information, J. Applied Physics 24 , 1152–1163 1953 ^ Leon Brillouin, Science and Information theory , Dover, 1956 Look up negentropy in Wiktionary, the free dictionary.

NewPP limit report
Parsed by mw‐web.codfw.canary‐5777f5b8f9‐8wb6m
Cached time: 20250814220707
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.357 seconds
Real time usage: 0.561 seconds
Preprocessor visited node count: 2087/1000000
Revision size: 11444/2097152 bytes
Post‐expand include size: 36392/2097152 bytes
Template argument size: 5227/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 11/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 27716/5000000 bytes
Lua time usage: 0.207/10.000 seconds
Lua memory usage: 21654043/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  385.893      1 -total
 24.02%   92.690      1 Template:Reflist
 19.58%   75.571      1 Template:Lang
 18.55%   71.580      3 Template:Cite_web
 14.61%   56.368      1 Template:Short_description
 13.39%   51.655      1 Template:Distinguish
 10.04%   38.747      2 Template:Clarify
  9.11%   35.155      2 Template:Fix-span
  8.66%   33.427      2 Template:Pagetype
  6.67%   25.736     10 Template:Category_handler Saved in parser cache with key enwiki:pcache:361356:|#|:idhash:canonical and timestamp 20250814220707 and revision id 1304301400. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Negentropy&oldid=1304301400 " Categories : Entropy and information Statistical deviation and dispersion Thermodynamic entropy Hidden categories: Webarchive template wayback links Articles with short description Short description is different from Wikidata Wikipedia articles needing clarification from December 2024 Accuracy disputes from May 2025 All accuracy disputes Articles containing French-language text All articles with unsourced statements Articles with unsourced statements from June 2025 Articles to be expanded from December 2024 Wikipedia articles needing clarification from June 2025 Wikipedia articles that are too technical from June 2025 All articles that are too technical This page was last edited on 5 August 2025, at 05:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Negentropy 17 languages Add topic

