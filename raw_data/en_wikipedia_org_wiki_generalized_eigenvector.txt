Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Overview and definition 2 Examples Toggle Examples subsection 2.1 Example 1 2.2 Example 2 3 Jordan chains 4 Canonical basis 5 Computation of generalized eigenvectors Toggle Computation of generalized eigenvectors subsection 5.1 Example 3 6 Generalized modal matrix 7 Jordan normal form Toggle Jordan normal form subsection 7.1 Example 4 7.2 Example 5 8 Applications Toggle Applications subsection 8.1 Matrix functions 8.2 Differential equations 9 Notes 10 References Toggle the table of contents Generalized eigenvector 9 languages Català Deutsch 한국어 日本語 Polski Português Русский Svenska Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Vector satisfying some of the criteria of an eigenvector Not to be confused with Generalized eigenvalue problem .

In linear algebra , a generalized eigenvector of an n × × n {\displaystyle n\times n} matrix A {\displaystyle A} is a vector which satisfies certain criteria which are more relaxed than those for an (ordinary) eigenvector .

[ 1 ] Let V {\displaystyle V} be an n {\displaystyle n} -dimensional vector space and let A {\displaystyle A} be the matrix representation of a linear map from V {\displaystyle V} to V {\displaystyle V} with respect to some ordered basis .

There may not always exist a full set of n {\displaystyle n} linearly independent eigenvectors of A {\displaystyle A} that form a complete basis for V {\displaystyle V} .  That is, the matrix A {\displaystyle A} may not be diagonalizable .

[ 2 ] [ 3 ] This happens when the algebraic multiplicity of at least one eigenvalue λ λ i {\displaystyle \lambda _{i}} is greater than its geometric multiplicity (the nullity of the matrix ( A − − λ λ i I ) {\displaystyle (A-\lambda _{i}I)} , or the dimension of its nullspace ).  In this case, λ λ i {\displaystyle \lambda _{i}} is called a defective eigenvalue and A {\displaystyle A} is called a defective matrix .

[ 4 ] A generalized eigenvector x i {\displaystyle x_{i}} corresponding to λ λ i {\displaystyle \lambda _{i}} , together with the matrix ( A − − λ λ i I ) {\displaystyle (A-\lambda _{i}I)} generate a Jordan chain of linearly independent generalized eigenvectors which form a basis for an invariant subspace of V {\displaystyle V} .

[ 5 ] [ 6 ] [ 7 ] Using generalized eigenvectors, a set of linearly independent eigenvectors of A {\displaystyle A} can be extended, if necessary, to a complete basis for V {\displaystyle V} .

[ 8 ] This basis can be used to determine an "almost diagonal matrix" J {\displaystyle J} in Jordan normal form , similar to A {\displaystyle A} , which is useful in computing certain matrix functions of A {\displaystyle A} .

[ 9 ] The matrix J {\displaystyle J} is also useful in solving the system of linear differential equations x ′ = A x , {\displaystyle \mathbf {x} '=A\mathbf {x} ,} where A {\displaystyle A} need not be diagonalizable.

[ 10 ] [ 11 ] The dimension of the generalized eigenspace corresponding to a given eigenvalue λ λ {\displaystyle \lambda } is the algebraic multiplicity of λ λ {\displaystyle \lambda } .

[ 12 ] Overview and definition [ edit ] There are several equivalent ways to define an ordinary eigenvector .

[ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] [ 19 ] [ 20 ] For our purposes, an eigenvector u {\displaystyle \mathbf {u} } associated with an eigenvalue λ λ {\displaystyle \lambda } of an n {\displaystyle n} × n {\displaystyle n} matrix A {\displaystyle A} is a nonzero vector for which ( A − − λ λ I ) u = 0 {\displaystyle (A-\lambda I)\mathbf {u} =\mathbf {0} } , where I {\displaystyle I} is the n {\displaystyle n} × n {\displaystyle n} identity matrix and 0 {\displaystyle \mathbf {0} } is the zero vector of length n {\displaystyle n} .

[ 21 ] That is, u {\displaystyle \mathbf {u} } is in the kernel of the transformation ( A − − λ λ I ) {\displaystyle (A-\lambda I)} .  If A {\displaystyle A} has n {\displaystyle n} linearly independent eigenvectors, then A {\displaystyle A} is similar to a diagonal matrix D {\displaystyle D} .  That is, there exists an invertible matrix M {\displaystyle M} such that A {\displaystyle A} is diagonalizable through the similarity transformation D = M − − 1 A M {\displaystyle D=M^{-1}AM} .

[ 22 ] [ 23 ] The matrix D {\displaystyle D} is called a spectral matrix for A {\displaystyle A} .  The matrix M {\displaystyle M} is called a modal matrix for A {\displaystyle A} .

[ 24 ] Diagonalizable matrices are of particular interest since matrix functions of them can be computed easily.

[ 25 ] On the other hand, if A {\displaystyle A} does not have n {\displaystyle n} linearly independent eigenvectors associated with it, then A {\displaystyle A} is not diagonalizable.

[ 26 ] [ 27 ] Definition: A vector x m {\displaystyle \mathbf {x} _{m}} is a generalized eigenvector of rank m of the matrix A {\displaystyle A} and corresponding to the eigenvalue λ λ {\displaystyle \lambda } if ( A − − λ λ I ) m x m = 0 {\displaystyle (A-\lambda I)^{m}\mathbf {x} _{m}=\mathbf {0} } but ( A − − λ λ I ) m − − 1 x m ≠ ≠ 0 .

{\displaystyle (A-\lambda I)^{m-1}\mathbf {x} _{m}\neq \mathbf {0} .} [ 28 ] Clearly, a generalized eigenvector of rank 1 is an ordinary eigenvector.

[ 29 ] Every n {\displaystyle n} × n {\displaystyle n} matrix A {\displaystyle A} has n {\displaystyle n} linearly independent generalized eigenvectors associated with it and can be shown to be similar to an "almost diagonal" matrix J {\displaystyle J} in Jordan normal form.

[ 30 ] That is, there exists an invertible matrix M {\displaystyle M} such that J = M − − 1 A M {\displaystyle J=M^{-1}AM} .

[ 31 ] The matrix M {\displaystyle M} in this case is called a generalized modal matrix for A {\displaystyle A} .

[ 32 ] If λ λ {\displaystyle \lambda } is an eigenvalue of algebraic multiplicity μ μ {\displaystyle \mu } , then A {\displaystyle A} will have μ μ {\displaystyle \mu } linearly independent generalized eigenvectors corresponding to λ λ {\displaystyle \lambda } .

[ 33 ] These results, in turn, provide a straightforward method for computing certain matrix functions of A {\displaystyle A} .

[ 34 ] Note:  For an n × × n {\displaystyle n\times n} matrix A {\displaystyle A} over a field F {\displaystyle F} to be expressed in Jordan normal form, all eigenvalues of A {\displaystyle A} must be in F {\displaystyle F} .  That is, the characteristic polynomial f ( x ) {\displaystyle f(x)} must factor completely into linear factors; F {\displaystyle F} must be an algebraically closed field.  For example, if A {\displaystyle A} has real-valued elements, then it may be necessary for the eigenvalues and the components of the eigenvectors to have complex values .

[ 35 ] [ 36 ] [ 37 ] The set spanned by all generalized eigenvectors for a given λ λ {\displaystyle \lambda } forms the generalized eigenspace for λ λ {\displaystyle \lambda } .

[ 38 ] Examples [ edit ] Here are some examples to illustrate the concept of generalized eigenvectors.  Some of the details will be described later.

Example 1 [ edit ] This example is simple but clearly illustrates the point. This type of matrix is used frequently in textbooks.

[ 39 ] [ 40 ] [ 41 ] Suppose A = ( 1 1 0 1 ) .

{\displaystyle A={\begin{pmatrix}1&1\\0&1\end{pmatrix}}.} Then there is only one eigenvalue, λ λ = 1 {\displaystyle \lambda =1} , and its algebraic multiplicity is m = 2 {\displaystyle m=2} .

Notice that this matrix is in Jordan normal form but is not diagonal . Hence, this matrix is not diagonalizable. Since there is one superdiagonal entry, there will be one generalized eigenvector of rank greater than 1 (or one could note that the vector space V {\displaystyle V} is of dimension 2, so there can be at most one generalized eigenvector of rank greater than 1). Alternatively, one could compute the dimension of the nullspace of A − − λ λ I {\displaystyle A-\lambda I} to be p = 1 {\displaystyle p=1} , and thus there are m − − p = 1 {\displaystyle m-p=1} generalized eigenvectors of rank greater than 1.

The ordinary eigenvector v 1 = ( 1 0 ) {\displaystyle \mathbf {v} _{1}={\begin{pmatrix}1\\0\end{pmatrix}}} is computed as usual (see the eigenvector page for examples). Using this eigenvector, we compute the generalized eigenvector v 2 {\displaystyle \mathbf {v} _{2}} by solving ( A − − λ λ I ) v 2 = v 1 .

{\displaystyle (A-\lambda I)\mathbf {v} _{2}=\mathbf {v} _{1}.} Writing out the values: ( ( 1 1 0 1 ) − − 1 ( 1 0 0 1 ) ) ( v 21 v 22 ) = ( 0 1 0 0 ) ( v 21 v 22 ) = ( 1 0 ) .

{\displaystyle \left({\begin{pmatrix}1&1\\0&1\end{pmatrix}}-1{\begin{pmatrix}1&0\\0&1\end{pmatrix}}\right){\begin{pmatrix}v_{21}\\v_{22}\end{pmatrix}}={\begin{pmatrix}0&1\\0&0\end{pmatrix}}{\begin{pmatrix}v_{21}\\v_{22}\end{pmatrix}}={\begin{pmatrix}1\\0\end{pmatrix}}.} This simplifies to v 22 = 1.

{\displaystyle v_{22}=1.} The element v 21 {\displaystyle v_{21}} has no restrictions. The generalized eigenvector of rank 2 is then v 2 = ( a 1 ) {\displaystyle \mathbf {v} _{2}={\begin{pmatrix}a\\1\end{pmatrix}}} , where a can have any scalar value.  The choice of a = 0 is usually the simplest.

Note that ( A − − λ λ I ) v 2 = ( 0 1 0 0 ) ( a 1 ) = ( 1 0 ) = v 1 , {\displaystyle (A-\lambda I)\mathbf {v} _{2}={\begin{pmatrix}0&1\\0&0\end{pmatrix}}{\begin{pmatrix}a\\1\end{pmatrix}}={\begin{pmatrix}1\\0\end{pmatrix}}=\mathbf {v} _{1},} so that v 2 {\displaystyle \mathbf {v} _{2}} is a generalized eigenvector, because ( A − − λ λ I ) 2 v 2 = ( A − − λ λ I ) [ ( A − − λ λ I ) v 2 ] = ( A − − λ λ I ) v 1 = ( 0 1 0 0 ) ( 1 0 ) = ( 0 0 ) = 0 , {\displaystyle (A-\lambda I)^{2}\mathbf {v} _{2}=(A-\lambda I)[(A-\lambda I)\mathbf {v} _{2}]=(A-\lambda I)\mathbf {v} _{1}={\begin{pmatrix}0&1\\0&0\end{pmatrix}}{\begin{pmatrix}1\\0\end{pmatrix}}={\begin{pmatrix}0\\0\end{pmatrix}}=\mathbf {0} ,} so that v 1 {\displaystyle \mathbf {v} _{1}} is an ordinary eigenvector, and that v 1 {\displaystyle \mathbf {v} _{1}} and v 2 {\displaystyle \mathbf {v} _{2}} are linearly independent and hence constitute a basis for the vector space V {\displaystyle V} .

Example 2 [ edit ] This example is more complex than Example 1 .  Unfortunately, it is a little difficult to construct an interesting example of low order.

[ 42 ] The matrix A = ( 1 0 0 0 0 3 1 0 0 0 6 3 2 0 0 10 6 3 2 0 15 10 6 3 2 ) {\displaystyle A={\begin{pmatrix}1&0&0&0&0\\3&1&0&0&0\\6&3&2&0&0\\10&6&3&2&0\\15&10&6&3&2\end{pmatrix}}} has eigenvalues λ λ 1 = 1 {\displaystyle \lambda _{1}=1} and λ λ 2 = 2 {\displaystyle \lambda _{2}=2} with algebraic multiplicities μ μ 1 = 2 {\displaystyle \mu _{1}=2} and μ μ 2 = 3 {\displaystyle \mu _{2}=3} , but geometric multiplicities γ γ 1 = 1 {\displaystyle \gamma _{1}=1} and γ γ 2 = 1 {\displaystyle \gamma _{2}=1} .

The generalized eigenspaces of A {\displaystyle A} are calculated below.

x 1 {\displaystyle \mathbf {x} _{1}} is the ordinary eigenvector associated with λ λ 1 {\displaystyle \lambda _{1}} .

x 2 {\displaystyle \mathbf {x} _{2}} is a generalized eigenvector associated with λ λ 1 {\displaystyle \lambda _{1}} .

y 1 {\displaystyle \mathbf {y} _{1}} is the ordinary eigenvector associated with λ λ 2 {\displaystyle \lambda _{2}} .

y 2 {\displaystyle \mathbf {y} _{2}} and y 3 {\displaystyle \mathbf {y} _{3}} are generalized eigenvectors associated with λ λ 2 {\displaystyle \lambda _{2}} .

( A − − 1 I ) x 1 = ( 0 0 0 0 0 3 0 0 0 0 6 3 1 0 0 10 6 3 1 0 15 10 6 3 1 ) ( 0 3 − − 9 9 − − 3 ) = ( 0 0 0 0 0 ) = 0 , {\displaystyle (A-1I)\mathbf {x} _{1}={\begin{pmatrix}0&0&0&0&0\\3&0&0&0&0\\6&3&1&0&0\\10&6&3&1&0\\15&10&6&3&1\end{pmatrix}}{\begin{pmatrix}0\\3\\-9\\9\\-3\end{pmatrix}}={\begin{pmatrix}0\\0\\0\\0\\0\end{pmatrix}}=\mathbf {0} ,} ( A − − 1 I ) x 2 = ( 0 0 0 0 0 3 0 0 0 0 6 3 1 0 0 10 6 3 1 0 15 10 6 3 1 ) ( 1 − − 15 30 − − 1 − − 45 ) = ( 0 3 − − 9 9 − − 3 ) = x 1 , {\displaystyle (A-1I)\mathbf {x} _{2}={\begin{pmatrix}0&0&0&0&0\\3&0&0&0&0\\6&3&1&0&0\\10&6&3&1&0\\15&10&6&3&1\end{pmatrix}}{\begin{pmatrix}1\\-15\\30\\-1\\-45\end{pmatrix}}={\begin{pmatrix}0\\3\\-9\\9\\-3\end{pmatrix}}=\mathbf {x} _{1},} ( A − − 2 I ) y 1 = ( − − 1 0 0 0 0 3 − − 1 0 0 0 6 3 0 0 0 10 6 3 0 0 15 10 6 3 0 ) ( 0 0 0 0 9 ) = ( 0 0 0 0 0 ) = 0 , {\displaystyle (A-2I)\mathbf {y} _{1}={\begin{pmatrix}-1&0&0&0&0\\3&-1&0&0&0\\6&3&0&0&0\\10&6&3&0&0\\15&10&6&3&0\end{pmatrix}}{\begin{pmatrix}0\\0\\0\\0\\9\end{pmatrix}}={\begin{pmatrix}0\\0\\0\\0\\0\end{pmatrix}}=\mathbf {0} ,} ( A − − 2 I ) y 2 = ( − − 1 0 0 0 0 3 − − 1 0 0 0 6 3 0 0 0 10 6 3 0 0 15 10 6 3 0 ) ( 0 0 0 3 0 ) = ( 0 0 0 0 9 ) = y 1 , {\displaystyle (A-2I)\mathbf {y} _{2}={\begin{pmatrix}-1&0&0&0&0\\3&-1&0&0&0\\6&3&0&0&0\\10&6&3&0&0\\15&10&6&3&0\end{pmatrix}}{\begin{pmatrix}0\\0\\0\\3\\0\end{pmatrix}}={\begin{pmatrix}0\\0\\0\\0\\9\end{pmatrix}}=\mathbf {y} _{1},} ( A − − 2 I ) y 3 = ( − − 1 0 0 0 0 3 − − 1 0 0 0 6 3 0 0 0 10 6 3 0 0 15 10 6 3 0 ) ( 0 0 1 − − 2 0 ) = ( 0 0 0 3 0 ) = y 2 .

{\displaystyle (A-2I)\mathbf {y} _{3}={\begin{pmatrix}-1&0&0&0&0\\3&-1&0&0&0\\6&3&0&0&0\\10&6&3&0&0\\15&10&6&3&0\end{pmatrix}}{\begin{pmatrix}0\\0\\1\\-2\\0\end{pmatrix}}={\begin{pmatrix}0\\0\\0\\3\\0\end{pmatrix}}=\mathbf {y} _{2}.} This results in a basis for each of the generalized eigenspaces of A {\displaystyle A} .
Together the two chains of generalized eigenvectors span the space of all 5-dimensional column vectors.

{ x 1 , x 2 } = { ( 0 3 − − 9 9 − − 3 ) , ( 1 − − 15 30 − − 1 − − 45 ) } , { y 1 , y 2 , y 3 } = { ( 0 0 0 0 9 ) , ( 0 0 0 3 0 ) , ( 0 0 1 − − 2 0 ) } .

{\displaystyle \left\{\mathbf {x} _{1},\mathbf {x} _{2}\right\}=\left\{{\begin{pmatrix}0\\3\\-9\\9\\-3\end{pmatrix}},{\begin{pmatrix}1\\-15\\30\\-1\\-45\end{pmatrix}}\right\},\left\{\mathbf {y} _{1},\mathbf {y} _{2},\mathbf {y} _{3}\right\}=\left\{{\begin{pmatrix}0\\0\\0\\0\\9\end{pmatrix}},{\begin{pmatrix}0\\0\\0\\3\\0\end{pmatrix}},{\begin{pmatrix}0\\0\\1\\-2\\0\end{pmatrix}}\right\}.} An "almost diagonal" matrix J {\displaystyle J} in Jordan normal form , similar to A {\displaystyle A} is obtained as follows: M = ( x 1 x 2 y 1 y 2 y 3 ) = ( 0 1 0 0 0 3 − − 15 0 0 0 − − 9 30 0 0 1 9 − − 1 0 3 − − 2 − − 3 − − 45 9 0 0 ) , {\displaystyle M={\begin{pmatrix}\mathbf {x} _{1}&\mathbf {x} _{2}&\mathbf {y} _{1}&\mathbf {y} _{2}&\mathbf {y} _{3}\end{pmatrix}}={\begin{pmatrix}0&1&0&0&0\\3&-15&0&0&0\\-9&30&0&0&1\\9&-1&0&3&-2\\-3&-45&9&0&0\end{pmatrix}},} J = ( 1 1 0 0 0 0 1 0 0 0 0 0 2 1 0 0 0 0 2 1 0 0 0 0 2 ) , {\displaystyle J={\begin{pmatrix}1&1&0&0&0\\0&1&0&0&0\\0&0&2&1&0\\0&0&0&2&1\\0&0&0&0&2\end{pmatrix}},} where M {\displaystyle M} is a generalized modal matrix for A {\displaystyle A} , the columns of M {\displaystyle M} are a canonical basis for A {\displaystyle A} , and A M = M J {\displaystyle AM=MJ} .

[ 43 ] Jordan chains [ edit ] Definition: Let x m {\displaystyle \mathbf {x} _{m}} be a generalized eigenvector of rank m corresponding to the matrix A {\displaystyle A} and the eigenvalue λ λ {\displaystyle \lambda } .  The chain generated by x m {\displaystyle \mathbf {x} _{m}} is a set of vectors { x m , x m − − 1 , … … , x 1 } {\displaystyle \left\{\mathbf {x} _{m},\mathbf {x} _{m-1},\dots ,\mathbf {x} _{1}\right\}} given by x m − − 1 = ( A − − λ λ I ) x m , {\displaystyle \mathbf {x} _{m-1}=(A-\lambda I)\mathbf {x} _{m},} x m − − 2 = ( A − − λ λ I ) 2 x m = ( A − − λ λ I ) x m − − 1 , {\displaystyle \mathbf {x} _{m-2}=(A-\lambda I)^{2}\mathbf {x} _{m}=(A-\lambda I)\mathbf {x} _{m-1},} x m − − 3 = ( A − − λ λ I ) 3 x m = ( A − − λ λ I ) x m − − 2 , {\displaystyle \mathbf {x} _{m-3}=(A-\lambda I)^{3}\mathbf {x} _{m}=(A-\lambda I)\mathbf {x} _{m-2},} ⋮ ⋮ {\displaystyle \vdots } x 1 = ( A − − λ λ I ) m − − 1 x m = ( A − − λ λ I ) x 2 .

{\displaystyle \mathbf {x} _{1}=(A-\lambda I)^{m-1}\mathbf {x} _{m}=(A-\lambda I)\mathbf {x} _{2}.} 1 where x 1 {\displaystyle \mathbf {x} _{1}} is always an ordinary eigenvector with a given eigenvalue λ λ {\displaystyle \lambda } . Thus, in general, x j = ( A − − λ λ I ) m − − j x m = ( A − − λ λ I ) x j + 1 ( j = 1 , 2 , … … , m − − 1 ) .

{\displaystyle \mathbf {x} _{j}=(A-\lambda I)^{m-j}\mathbf {x} _{m}=(A-\lambda I)\mathbf {x} _{j+1}\qquad (j=1,2,\dots ,m-1).} 2 The vector x j {\displaystyle \mathbf {x} _{j}} , given by ( 2 ), is a generalized eigenvector of rank j corresponding to the eigenvalue λ λ {\displaystyle \lambda } .  A chain is a linearly independent set of vectors.

[ 44 ] Canonical basis [ edit ] Main article: Canonical basis § Linear algebra Definition: A set of n linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.

Thus, once we have determined that a generalized eigenvector of rank m is in a canonical basis, it follows that the m − 1 vectors x m − − 1 , x m − − 2 , … … , x 1 {\displaystyle \mathbf {x} _{m-1},\mathbf {x} _{m-2},\ldots ,\mathbf {x} _{1}} that are in the Jordan chain generated by x m {\displaystyle \mathbf {x} _{m}} are also in the canonical basis.

[ 45 ] Let λ λ i {\displaystyle \lambda _{i}} be an eigenvalue of A {\displaystyle A} of algebraic multiplicity μ μ i {\displaystyle \mu _{i}} .  First, find the ranks (matrix ranks) of the matrices ( A − − λ λ i I ) , ( A − − λ λ i I ) 2 , … … , ( A − − λ λ i I ) m i {\displaystyle (A-\lambda _{i}I),(A-\lambda _{i}I)^{2},\ldots ,(A-\lambda _{i}I)^{m_{i}}} .  The integer m i {\displaystyle m_{i}} is determined to be the first integer for which ( A − − λ λ i I ) m i {\displaystyle (A-\lambda _{i}I)^{m_{i}}} has rank n − − μ μ i {\displaystyle n-\mu _{i}} ( n being the number of rows or columns of A {\displaystyle A} , that is, A {\displaystyle A} is n × n ).

Now define ρ ρ k = rank ⁡ ⁡ ( A − − λ λ i I ) k − − 1 − − rank ⁡ ⁡ ( A − − λ λ i I ) k ( k = 1 , 2 , … … , m i ) .

{\displaystyle \rho _{k}=\operatorname {rank} (A-\lambda _{i}I)^{k-1}-\operatorname {rank} (A-\lambda _{i}I)^{k}\qquad (k=1,2,\ldots ,m_{i}).} The variable ρ ρ k {\displaystyle \rho _{k}} designates the number of linearly independent generalized eigenvectors of rank k corresponding to the eigenvalue λ λ i {\displaystyle \lambda _{i}} that will appear in a canonical basis for A {\displaystyle A} .  Note that rank ⁡ ⁡ ( A − − λ λ i I ) 0 = rank ⁡ ⁡ ( I ) = n {\displaystyle \operatorname {rank} (A-\lambda _{i}I)^{0}=\operatorname {rank} (I)=n} .

[ 46 ] Computation of generalized eigenvectors [ edit ] In the preceding sections we have seen techniques for obtaining the n {\displaystyle n} linearly independent generalized eigenvectors of a canonical basis for the vector space V {\displaystyle V} associated with an n × × n {\displaystyle n\times n} matrix A {\displaystyle A} .  These techniques can be combined into a procedure: Solve the characteristic equation of A {\displaystyle A} for eigenvalues λ λ i {\displaystyle \lambda _{i}} and their algebraic multiplicities μ μ i {\displaystyle \mu _{i}} ; For each λ λ i : {\displaystyle \lambda _{i}:} Determine n − − μ μ i {\displaystyle n-\mu _{i}} ; Determine m i {\displaystyle m_{i}} ; Determine ρ ρ k {\displaystyle \rho _{k}} for ( k = 1 , … … , m i ) {\displaystyle (k=1,\ldots ,m_{i})} ; Determine each Jordan chain for λ λ i {\displaystyle \lambda _{i}} ; Example 3 [ edit ] The matrix A = ( 5 1 − − 2 4 0 5 2 2 0 0 5 3 0 0 0 4 ) {\displaystyle A={\begin{pmatrix}5&1&-2&4\\0&5&2&2\\0&0&5&3\\0&0&0&4\end{pmatrix}}} has an eigenvalue λ λ 1 = 5 {\displaystyle \lambda _{1}=5} of algebraic multiplicity μ μ 1 = 3 {\displaystyle \mu _{1}=3} and an eigenvalue λ λ 2 = 4 {\displaystyle \lambda _{2}=4} of algebraic multiplicity μ μ 2 = 1 {\displaystyle \mu _{2}=1} .  We also have n = 4 {\displaystyle n=4} .  For λ λ 1 {\displaystyle \lambda _{1}} we have n − − μ μ 1 = 4 − − 3 = 1 {\displaystyle n-\mu _{1}=4-3=1} .

( A − − 5 I ) = ( 0 1 − − 2 4 0 0 2 2 0 0 0 3 0 0 0 − − 1 ) , rank ⁡ ⁡ ( A − − 5 I ) = 3.

{\displaystyle (A-5I)={\begin{pmatrix}0&1&-2&4\\0&0&2&2\\0&0&0&3\\0&0&0&-1\end{pmatrix}},\qquad \operatorname {rank} (A-5I)=3.} ( A − − 5 I ) 2 = ( 0 0 2 − − 8 0 0 0 4 0 0 0 − − 3 0 0 0 1 ) , rank ⁡ ⁡ ( A − − 5 I ) 2 = 2.

{\displaystyle (A-5I)^{2}={\begin{pmatrix}0&0&2&-8\\0&0&0&4\\0&0&0&-3\\0&0&0&1\end{pmatrix}},\qquad \operatorname {rank} (A-5I)^{2}=2.} ( A − − 5 I ) 3 = ( 0 0 0 14 0 0 0 − − 4 0 0 0 3 0 0 0 − − 1 ) , rank ⁡ ⁡ ( A − − 5 I ) 3 = 1.

{\displaystyle (A-5I)^{3}={\begin{pmatrix}0&0&0&14\\0&0&0&-4\\0&0&0&3\\0&0&0&-1\end{pmatrix}},\qquad \operatorname {rank} (A-5I)^{3}=1.} The first integer m 1 {\displaystyle m_{1}} for which ( A − − 5 I ) m 1 {\displaystyle (A-5I)^{m_{1}}} has rank n − − μ μ 1 = 1 {\displaystyle n-\mu _{1}=1} is m 1 = 3 {\displaystyle m_{1}=3} .

We now define ρ ρ 3 = rank ⁡ ⁡ ( A − − 5 I ) 2 − − rank ⁡ ⁡ ( A − − 5 I ) 3 = 2 − − 1 = 1 , {\displaystyle \rho _{3}=\operatorname {rank} (A-5I)^{2}-\operatorname {rank} (A-5I)^{3}=2-1=1,} ρ ρ 2 = rank ⁡ ⁡ ( A − − 5 I ) 1 − − rank ⁡ ⁡ ( A − − 5 I ) 2 = 3 − − 2 = 1 , {\displaystyle \rho _{2}=\operatorname {rank} (A-5I)^{1}-\operatorname {rank} (A-5I)^{2}=3-2=1,} ρ ρ 1 = rank ⁡ ⁡ ( A − − 5 I ) 0 − − rank ⁡ ⁡ ( A − − 5 I ) 1 = 4 − − 3 = 1.

{\displaystyle \rho _{1}=\operatorname {rank} (A-5I)^{0}-\operatorname {rank} (A-5I)^{1}=4-3=1.} Consequently, there will be three linearly independent generalized eigenvectors; one each of ranks 3, 2 and 1.  Since λ λ 1 {\displaystyle \lambda _{1}} corresponds to a single chain of three linearly independent generalized eigenvectors, we know that there is a generalized eigenvector x 3 {\displaystyle \mathbf {x} _{3}} of rank 3 corresponding to λ λ 1 {\displaystyle \lambda _{1}} such that ( A − − 5 I ) 3 x 3 = 0 {\displaystyle (A-5I)^{3}\mathbf {x} _{3}=\mathbf {0} } 3 but ( A − − 5 I ) 2 x 3 ≠ ≠ 0 .

{\displaystyle (A-5I)^{2}\mathbf {x} _{3}\neq \mathbf {0} .} 4 Equations ( 3 ) and ( 4 ) represent linear systems that can be solved for x 3 {\displaystyle \mathbf {x} _{3}} .  Let x 3 = ( x 31 x 32 x 33 x 34 ) .

{\displaystyle \mathbf {x} _{3}={\begin{pmatrix}x_{31}\\x_{32}\\x_{33}\\x_{34}\end{pmatrix}}.} Then ( A − − 5 I ) 3 x 3 = ( 0 0 0 14 0 0 0 − − 4 0 0 0 3 0 0 0 − − 1 ) ( x 31 x 32 x 33 x 34 ) = ( 14 x 34 − − 4 x 34 3 x 34 − − x 34 ) = ( 0 0 0 0 ) {\displaystyle (A-5I)^{3}\mathbf {x} _{3}={\begin{pmatrix}0&0&0&14\\0&0&0&-4\\0&0&0&3\\0&0&0&-1\end{pmatrix}}{\begin{pmatrix}x_{31}\\x_{32}\\x_{33}\\x_{34}\end{pmatrix}}={\begin{pmatrix}14x_{34}\\-4x_{34}\\3x_{34}\\-x_{34}\end{pmatrix}}={\begin{pmatrix}0\\0\\0\\0\end{pmatrix}}} and ( A − − 5 I ) 2 x 3 = ( 0 0 2 − − 8 0 0 0 4 0 0 0 − − 3 0 0 0 1 ) ( x 31 x 32 x 33 x 34 ) = ( 2 x 33 − − 8 x 34 4 x 34 − − 3 x 34 x 34 ) ≠ ≠ ( 0 0 0 0 ) .

{\displaystyle (A-5I)^{2}\mathbf {x} _{3}={\begin{pmatrix}0&0&2&-8\\0&0&0&4\\0&0&0&-3\\0&0&0&1\end{pmatrix}}{\begin{pmatrix}x_{31}\\x_{32}\\x_{33}\\x_{34}\end{pmatrix}}={\begin{pmatrix}2x_{33}-8x_{34}\\4x_{34}\\-3x_{34}\\x_{34}\end{pmatrix}}\neq {\begin{pmatrix}0\\0\\0\\0\end{pmatrix}}.} Thus, in order to satisfy the conditions ( 3 ) and ( 4 ), we must have x 34 = 0 {\displaystyle x_{34}=0} and x 33 ≠ ≠ 0 {\displaystyle x_{33}\neq 0} .  No restrictions are placed on x 31 {\displaystyle x_{31}} and x 32 {\displaystyle x_{32}} .  By choosing x 31 = x 32 = x 34 = 0 , x 33 = 1 {\displaystyle x_{31}=x_{32}=x_{34}=0,x_{33}=1} , we obtain x 3 = ( 0 0 1 0 ) {\displaystyle \mathbf {x} _{3}={\begin{pmatrix}0\\0\\1\\0\end{pmatrix}}} as a generalized eigenvector of rank 3 corresponding to λ λ 1 = 5 {\displaystyle \lambda _{1}=5} .  Note that it is possible to obtain infinitely many other generalized eigenvectors of rank 3 by choosing different values of x 31 {\displaystyle x_{31}} , x 32 {\displaystyle x_{32}} and x 33 {\displaystyle x_{33}} , with x 33 ≠ ≠ 0 {\displaystyle x_{33}\neq 0} .  Our first choice, however, is the simplest.

[ 47 ] Now using equations ( 1 ), we obtain x 2 {\displaystyle \mathbf {x} _{2}} and x 1 {\displaystyle \mathbf {x} _{1}} as generalized eigenvectors of rank 2 and 1, respectively, where x 2 = ( A − − 5 I ) x 3 = ( − − 2 2 0 0 ) , {\displaystyle \mathbf {x} _{2}=(A-5I)\mathbf {x} _{3}={\begin{pmatrix}-2\\2\\0\\0\end{pmatrix}},} and x 1 = ( A − − 5 I ) x 2 = ( 2 0 0 0 ) .

{\displaystyle \mathbf {x} _{1}=(A-5I)\mathbf {x} _{2}={\begin{pmatrix}2\\0\\0\\0\end{pmatrix}}.} The simple eigenvalue λ λ 2 = 4 {\displaystyle \lambda _{2}=4} can be dealt with using standard techniques and has an ordinary eigenvector y 1 = ( − − 14 4 − − 3 1 ) .

{\displaystyle \mathbf {y} _{1}={\begin{pmatrix}-14\\4\\-3\\1\end{pmatrix}}.} A canonical basis for A {\displaystyle A} is { x 3 , x 2 , x 1 , y 1 } = { ( 0 0 1 0 ) ( − − 2 2 0 0 ) ( 2 0 0 0 ) ( − − 14 4 − − 3 1 ) } .

{\displaystyle \left\{\mathbf {x} _{3},\mathbf {x} _{2},\mathbf {x} _{1},\mathbf {y} _{1}\right\}=\left\{{\begin{pmatrix}0\\0\\1\\0\end{pmatrix}}{\begin{pmatrix}-2\\2\\0\\0\end{pmatrix}}{\begin{pmatrix}2\\0\\0\\0\end{pmatrix}}{\begin{pmatrix}-14\\4\\-3\\1\end{pmatrix}}\right\}.} x 1 , x 2 {\displaystyle \mathbf {x} _{1},\mathbf {x} _{2}} and x 3 {\displaystyle \mathbf {x} _{3}} are generalized eigenvectors associated with λ λ 1 {\displaystyle \lambda _{1}} , while y 1 {\displaystyle \mathbf {y} _{1}} is the ordinary eigenvector associated with λ λ 2 {\displaystyle \lambda _{2}} .

This is a fairly simple example.  In general, the numbers ρ ρ k {\displaystyle \rho _{k}} of linearly independent generalized eigenvectors of rank k {\displaystyle k} will not always be equal.  That is, there may be several chains of different lengths corresponding to a particular eigenvalue.

[ 48 ] Generalized modal matrix [ edit ] Main article: Generalized modal matrix Let A {\displaystyle A} be an n × n matrix.  A generalized modal matrix M {\displaystyle M} for A {\displaystyle A} is an n × n matrix whose columns, considered as vectors, form a canonical basis for A {\displaystyle A} and appear in M {\displaystyle M} according to the following rules: All Jordan chains consisting of one vector (that is, one vector in length) appear in the first columns of M {\displaystyle M} .

All vectors of one chain appear together in adjacent columns of M {\displaystyle M} .

Each chain appears in M {\displaystyle M} in order of increasing rank (that is, the generalized eigenvector of rank 1 appears before the generalized eigenvector of rank 2 of the same chain, which appears before the generalized eigenvector of rank 3 of the same chain, etc.).

[ 49 ] Jordan normal form [ edit ] [ ⌜ ⌜ λ λ 1 1 λ λ 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌞ ⌞ λ λ 1 1 λ λ 1 λ λ 1 ⌟ ⌟ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 n ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌟ ⌟ ⌞ ⌞ λ λ 2 λ λ 2 ⌟ ⌟ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 λ λ n ⌝ ⌝ ⌜ ⌜ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌜ ⌜ λ λ n 1 n ⌝ ⌝ ⌞ ⌞ λ λ 1 1 λ λ 1 1 λ λ 1 ⌝ ⌝ ⌜ ⌜ λ λ 2 1 λ λ 2 ⌝ ⌝ [ λ λ 3 ] ⋱ ⋱ ⌞ ⌞ λ λ n λ λ n ⌟ ⌟ ] {\displaystyle {\begin{bmatrix}{\color {red}\ulcorner }\lambda _{1}1{\hphantom {\lambda _{1}\lambda _{1}}}{\color {red}\urcorner }{\hphantom {\ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1}}\lambda _{1}1{\hphantom {\lambda _{1}\urcorner \ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\color {red}\llcorner }{\hphantom {\lambda _{1}1\lambda _{1}}}\lambda _{1}{\color {red}\lrcorner }{\hphantom {\ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\urcorner }}{\color {red}\ulcorner }\lambda _{2}1{\hphantom {n}}{\color {red}\urcorner }{\hphantom {[\lambda _{3}]\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\lrcorner }}{\color {red}\llcorner }{\hphantom {\lambda _{2}}}\lambda _{2}{\color {red}\lrcorner }{\hphantom {[\lambda _{3}]\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\urcorner \ulcorner \lambda _{2}1\lambda _{2}\urcorner }}{\color {red}[}\lambda _{3}{\color {red}]}{\hphantom {\ddots \ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\urcorner \ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]}}\ddots {\hphantom {\ulcorner \lambda _{n}1\lambda _{n}\urcorner }}\\{\hphantom {\ulcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\urcorner \ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]\ddots }}{\color {red}\ulcorner }\lambda _{n}1{\hphantom {n}}{\color {red}\urcorner }\\{\hphantom {\llcorner \lambda _{1}1\lambda _{1}1\lambda _{1}\urcorner \ulcorner \lambda _{2}1\lambda _{2}\urcorner [\lambda _{3}]\ddots }}{\color {red}\llcorner }{\hphantom {\lambda _{n}}}\lambda _{n}{\color {red}\lrcorner }\end{bmatrix}}} An example of a matrix in Jordan normal form.

The red blocks are called Jordan blocks.

Main article: Jordan normal form Let V {\displaystyle V} be an n -dimensional vector space; let ϕ ϕ {\displaystyle \phi } be a linear map in L ( V ) , the set of all linear maps from V {\displaystyle V} into itself; and let A {\displaystyle A} be the matrix representation of ϕ ϕ {\displaystyle \phi } with respect to some ordered basis.  It can be shown that if the characteristic polynomial f ( λ λ ) {\displaystyle f(\lambda )} of A {\displaystyle A} factors into linear factors, so that f ( λ λ ) {\displaystyle f(\lambda )} has the form f ( λ λ ) = ± ± ( λ λ − − λ λ 1 ) μ μ 1 ( λ λ − − λ λ 2 ) μ μ 2 ⋯ ⋯ ( λ λ − − λ λ r ) μ μ r , {\displaystyle f(\lambda )=\pm (\lambda -\lambda _{1})^{\mu _{1}}(\lambda -\lambda _{2})^{\mu _{2}}\cdots (\lambda -\lambda _{r})^{\mu _{r}},} where λ λ 1 , λ λ 2 , … … , λ λ r {\displaystyle \lambda _{1},\lambda _{2},\ldots ,\lambda _{r}} are the distinct eigenvalues of A {\displaystyle A} , then each μ μ i {\displaystyle \mu _{i}} is the algebraic multiplicity of its corresponding eigenvalue λ λ i {\displaystyle \lambda _{i}} and A {\displaystyle A} is similar to a matrix J {\displaystyle J} in Jordan normal form , where each λ λ i {\displaystyle \lambda _{i}} appears μ μ i {\displaystyle \mu _{i}} consecutive times on the diagonal, and the entry directly above each λ λ i {\displaystyle \lambda _{i}} (that is, on the superdiagonal ) is either 0 or 1: in each block the entry above the first occurrence of each λ λ i {\displaystyle \lambda _{i}} is always 0 (except in the first block); all other entries on the superdiagonal are 1.  All other entries (that is, off the diagonal and superdiagonal) are 0. (But no ordering is imposed among the eigenvalues, or among the blocks for a given eigenvalue.) The matrix J {\displaystyle J} is as close as one can come to a diagonalization of A {\displaystyle A} .  If A {\displaystyle A} is diagonalizable, then all entries above the diagonal are zero.

[ 50 ] Note that some textbooks have the ones on the subdiagonal , that is, immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.

[ 51 ] [ 52 ] Every n × n matrix A {\displaystyle A} is similar to a matrix J {\displaystyle J} in Jordan normal form, obtained through the similarity transformation J = M − − 1 A M {\displaystyle J=M^{-1}AM} , where M {\displaystyle M} is a generalized modal matrix for A {\displaystyle A} .

[ 53 ] (See Note above.) Example 4 [ edit ] Find a matrix in Jordan normal form that is similar to A = ( 0 4 2 − − 3 8 3 4 − − 8 − − 2 ) .

{\displaystyle A={\begin{pmatrix}0&4&2\\-3&8&3\\4&-8&-2\end{pmatrix}}.} Solution: The characteristic equation of A {\displaystyle A} is ( λ λ − − 2 ) 3 = 0 {\displaystyle (\lambda -2)^{3}=0} , hence, λ λ = 2 {\displaystyle \lambda =2} is an eigenvalue of algebraic multiplicity three.  Following the procedures of the previous sections, we find that rank ⁡ ⁡ ( A − − 2 I ) = 1 {\displaystyle \operatorname {rank} (A-2I)=1} and rank ⁡ ⁡ ( A − − 2 I ) 2 = 0 = n − − μ μ .

{\displaystyle \operatorname {rank} (A-2I)^{2}=0=n-\mu .} Thus, ρ ρ 2 = 1 {\displaystyle \rho _{2}=1} and ρ ρ 1 = 2 {\displaystyle \rho _{1}=2} , which implies that a canonical basis for A {\displaystyle A} will contain one linearly independent generalized eigenvector of rank 2 and two linearly independent generalized eigenvectors of rank 1, or equivalently, one chain of two vectors { x 2 , x 1 } {\displaystyle \left\{\mathbf {x} _{2},\mathbf {x} _{1}\right\}} and one chain of one vector { y 1 } {\displaystyle \left\{\mathbf {y} _{1}\right\}} .  Designating M = ( y 1 x 1 x 2 ) {\displaystyle M={\begin{pmatrix}\mathbf {y} _{1}&\mathbf {x} _{1}&\mathbf {x} _{2}\end{pmatrix}}} , we find that M = ( 2 2 0 1 3 0 0 − − 4 1 ) , {\displaystyle M={\begin{pmatrix}2&2&0\\1&3&0\\0&-4&1\end{pmatrix}},} and J = ( 2 0 0 0 2 1 0 0 2 ) , {\displaystyle J={\begin{pmatrix}2&0&0\\0&2&1\\0&0&2\end{pmatrix}},} where M {\displaystyle M} is a generalized modal matrix for A {\displaystyle A} , the columns of M {\displaystyle M} are a canonical basis for A {\displaystyle A} , and A M = M J {\displaystyle AM=MJ} .

[ 54 ] Note that since generalized eigenvectors themselves are not unique, and since some of the columns of both M {\displaystyle M} and J {\displaystyle J} may be interchanged, it follows that both M {\displaystyle M} and J {\displaystyle J} are not unique.

[ 55 ] Example 5 [ edit ] In Example 3 , we found a canonical basis of linearly independent generalized eigenvectors for a matrix A {\displaystyle A} .  A generalized modal matrix for A {\displaystyle A} is M = ( y 1 x 1 x 2 x 3 ) = ( − − 14 2 − − 2 0 4 0 2 0 − − 3 0 0 1 1 0 0 0 ) .

{\displaystyle M={\begin{pmatrix}\mathbf {y} _{1}&\mathbf {x} _{1}&\mathbf {x} _{2}&\mathbf {x} _{3}\end{pmatrix}}={\begin{pmatrix}-14&2&-2&0\\4&0&2&0\\-3&0&0&1\\1&0&0&0\end{pmatrix}}.} A matrix in Jordan normal form, similar to A {\displaystyle A} is J = ( 4 0 0 0 0 5 1 0 0 0 5 1 0 0 0 5 ) , {\displaystyle J={\begin{pmatrix}4&0&0&0\\0&5&1&0\\0&0&5&1\\0&0&0&5\end{pmatrix}},} so that A M = M J {\displaystyle AM=MJ} .

Applications [ edit ] Matrix functions [ edit ] Main article: Matrix function Three of the most fundamental operations which can be performed on square matrices are matrix addition, multiplication by a scalar, and matrix multiplication.

[ 56 ] These are exactly those operations necessary for defining a polynomial function of an n × n matrix A {\displaystyle A} .

[ 57 ] If we recall from basic calculus that many functions can be written as a Maclaurin series , then we can define more general functions of matrices quite easily.

[ 58 ] If A {\displaystyle A} is diagonalizable, that is D = M − − 1 A M , {\displaystyle D=M^{-1}AM,} with D = ( λ λ 1 0 ⋯ ⋯ 0 0 λ λ 2 ⋯ ⋯ 0 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 0 ⋯ ⋯ λ λ n ) , {\displaystyle D={\begin{pmatrix}\lambda _{1}&0&\cdots &0\\0&\lambda _{2}&\cdots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\cdots &\lambda _{n}\end{pmatrix}},} then D k = ( λ λ 1 k 0 ⋯ ⋯ 0 0 λ λ 2 k ⋯ ⋯ 0 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ 0 0 ⋯ ⋯ λ λ n k ) {\displaystyle D^{k}={\begin{pmatrix}\lambda _{1}^{k}&0&\cdots &0\\0&\lambda _{2}^{k}&\cdots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\cdots &\lambda _{n}^{k}\end{pmatrix}}} and the evaluation of the Maclaurin series for functions of A {\displaystyle A} is greatly simplified.

[ 59 ] For example, to obtain any power k of A {\displaystyle A} , we need only compute D k {\displaystyle D^{k}} , premultiply D k {\displaystyle D^{k}} by M {\displaystyle M} , and postmultiply the result by M − − 1 {\displaystyle M^{-1}} .

[ 60 ] Using generalized eigenvectors, we can obtain the Jordan normal form for A {\displaystyle A} and these results can be generalized to a straightforward method for computing functions of nondiagonalizable matrices.

[ 61 ] (See Matrix function#Jordan decomposition .) Differential equations [ edit ] Main article: Ordinary differential equation Consider the problem of solving the system of linear ordinary differential equations x ′ = A x , {\displaystyle \mathbf {x} '=A\mathbf {x} ,} 5 where x = ( x 1 ( t ) x 2 ( t ) ⋮ ⋮ x n ( t ) ) , x ′ = ( x 1 ′ ( t ) x 2 ′ ( t ) ⋮ ⋮ x n ′ ( t ) ) , {\displaystyle \mathbf {x} ={\begin{pmatrix}x_{1}(t)\\x_{2}(t)\\\vdots \\x_{n}(t)\end{pmatrix}},\quad \mathbf {x} '={\begin{pmatrix}x_{1}'(t)\\x_{2}'(t)\\\vdots \\x_{n}'(t)\end{pmatrix}},} and A = ( a i j ) .

{\displaystyle A=(a_{ij}).} If the matrix A {\displaystyle A} is a diagonal matrix so that a i j = 0 {\displaystyle a_{ij}=0} for i ≠ ≠ j {\displaystyle i\neq j} , then the system ( 5 ) reduces to a system of n equations which take the form x 1 ′ = a 11 x 1 {\displaystyle x_{1}'=a_{11}x_{1}} x 2 ′ = a 22 x 2 {\displaystyle x_{2}'=a_{22}x_{2}} ⋮ ⋮ {\displaystyle \vdots } x n ′ = a n n x n .

{\displaystyle x_{n}'=a_{nn}x_{n}.} 6 In this case, the general solution is given by x 1 = k 1 e a 11 t {\displaystyle x_{1}=k_{1}e^{a_{11}t}} x 2 = k 2 e a 22 t {\displaystyle x_{2}=k_{2}e^{a_{22}t}} ⋮ ⋮ {\displaystyle \vdots } x n = k n e a n n t .

{\displaystyle x_{n}=k_{n}e^{a_{nn}t}.} In the general case, we try to diagonalize A {\displaystyle A} and reduce the system ( 5 ) to a system like ( 6 ) as follows.  If A {\displaystyle A} is diagonalizable, we have D = M − − 1 A M {\displaystyle D=M^{-1}AM} , where M {\displaystyle M} is a modal matrix for A {\displaystyle A} .  Substituting A = M D M − − 1 {\displaystyle A=MDM^{-1}} , equation ( 5 ) takes the form M − − 1 x ′ = D ( M − − 1 x ) {\displaystyle M^{-1}\mathbf {x} '=D(M^{-1}\mathbf {x} )} , or y ′ = D y , {\displaystyle \mathbf {y} '=D\mathbf {y} ,} 7 where x = M y .

{\displaystyle \mathbf {x} =M\mathbf {y} .} 8 The solution of ( 7 ) is y 1 = k 1 e λ λ 1 t {\displaystyle y_{1}=k_{1}e^{\lambda _{1}t}} y 2 = k 2 e λ λ 2 t {\displaystyle y_{2}=k_{2}e^{\lambda _{2}t}} ⋮ ⋮ {\displaystyle \vdots } y n = k n e λ λ n t .

{\displaystyle y_{n}=k_{n}e^{\lambda _{n}t}.} The solution x {\displaystyle \mathbf {x} } of ( 5 ) is then obtained using the relation ( 8 ).

[ 62 ] On the other hand, if A {\displaystyle A} is not diagonalizable, we choose M {\displaystyle M} to be a generalized modal matrix for A {\displaystyle A} , such that J = M − − 1 A M {\displaystyle J=M^{-1}AM} is the Jordan normal form of A {\displaystyle A} .  The system y ′ = J y {\displaystyle \mathbf {y} '=J\mathbf {y} } has the form y 1 ′ = λ λ 1 y 1 + ϵ ϵ 1 y 2 ⋮ ⋮ y n − − 1 ′ = λ λ n − − 1 y n − − 1 + ϵ ϵ n − − 1 y n y n ′ = λ λ n y n , {\displaystyle {\begin{aligned}y_{1}'&=\lambda _{1}y_{1}+\epsilon _{1}y_{2}\\&\vdots \\y_{n-1}'&=\lambda _{n-1}y_{n-1}+\epsilon _{n-1}y_{n}\\y_{n}'&=\lambda _{n}y_{n},\end{aligned}}} 9 where the λ λ i {\displaystyle \lambda _{i}} are the eigenvalues from the main diagonal of J {\displaystyle J} and the ϵ ϵ i {\displaystyle \epsilon _{i}} are the ones and zeros from the superdiagonal of J {\displaystyle J} .  The system ( 9 ) is often more easily solved than ( 5 ).  We may solve the last equation in ( 9 ) for y n {\displaystyle y_{n}} , obtaining y n = k n e λ λ n t {\displaystyle y_{n}=k_{n}e^{\lambda _{n}t}} .  We then substitute this solution for y n {\displaystyle y_{n}} into the next to last equation in ( 9 ) and solve for y n − − 1 {\displaystyle y_{n-1}} .  Continuing this procedure, we work through ( 9 ) from the last equation to the first, solving the entire system for y {\displaystyle \mathbf {y} } .  The solution x {\displaystyle \mathbf {x} } is then obtained using the relation ( 8 ).

[ 63 ] Lemma: Given the following chain of generalized eigenvectors of length r , {\displaystyle r,} X 1 = v 1 e λ λ t {\displaystyle X_{1}=v_{1}e^{\lambda t}} X 2 = ( t v 1 + v 2 ) e λ λ t {\displaystyle X_{2}=(tv_{1}+v_{2})e^{\lambda t}} X 3 = ( t 2 2 v 1 + t v 2 + v 3 ) e λ λ t {\displaystyle X_{3}=\left({\frac {t^{2}}{2}}v_{1}+tv_{2}+v_{3}\right)e^{\lambda t}} ⋮ ⋮ {\displaystyle \vdots } X r = ( t r − − 1 ( r − − 1 ) !

v 1 + .

.

.

+ t 2 2 v r − − 2 + t v r − − 1 + v r ) e λ λ t {\displaystyle X_{r}=\left({\frac {t^{r-1}}{(r-1)!}}v_{1}+...+{\frac {t^{2}}{2}}v_{r-2}+tv_{r-1}+v_{r}\right)e^{\lambda t}} , these functions solve the system of equations, X ′ = A X .

{\displaystyle X'=AX.} Proof: Define v 0 = 0 {\displaystyle v_{0}=0} X j ( t ) = e λ λ t ∑ ∑ i = 1 j t j − − i ( j − − i ) !

v i .

{\displaystyle X_{j}(t)=e^{\lambda t}\sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}v_{i}.} Then, as t 0 = 1 {\displaystyle {t^{0}}=1} and 1 ′ = 0 {\displaystyle 1'=0} , X j ′ ( t ) = e λ λ t ∑ ∑ i = 1 j − − 1 t j − − i − − 1 ( j − − i − − 1 ) !

v i + e λ λ t λ λ ∑ ∑ i = 1 j t j − − i ( j − − i ) !

v i {\displaystyle X'_{j}(t)=e^{\lambda t}\sum _{i=1}^{j-1}{\frac {t^{j-i-1}}{(j-i-1)!}}v_{i}+e^{\lambda t}\lambda \sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}v_{i}} .

On the other hand we have, v 0 = 0 {\displaystyle v_{0}=0} and so A X j ( t ) = e λ λ t ∑ ∑ i = 1 j t j − − i ( j − − i ) !

A v i {\displaystyle AX_{j}(t)=e^{\lambda t}\sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}Av_{i}} = e λ λ t ∑ ∑ i = 1 j t j − − i ( j − − i ) !

( v i − − 1 + λ λ v i ) {\displaystyle =e^{\lambda t}\sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}(v_{i-1}+\lambda v_{i})} = e λ λ t ∑ ∑ i = 2 j t j − − i ( j − − i ) !

v i − − 1 + e λ λ t λ λ ∑ ∑ i = 1 j t j − − i ( j − − i ) !

v i {\displaystyle =e^{\lambda t}\sum _{i=2}^{j}{\frac {t^{j-i}}{(j-i)!}}v_{i-1}+e^{\lambda t}\lambda \sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}v_{i}} = e λ λ t ∑ ∑ i = 1 j − − 1 t j − − i − − 1 ( j − − i − − 1 ) !

v i + e λ λ t λ λ ∑ ∑ i = 1 j t j − − i ( j − − i ) !

v i {\displaystyle =e^{\lambda t}\sum _{i=1}^{j-1}{\frac {t^{j-i-1}}{(j-i-1)!}}v_{i}+e^{\lambda t}\lambda \sum _{i=1}^{j}{\frac {t^{j-i}}{(j-i)!}}v_{i}} = X j ′ ( t ) {\displaystyle =X'_{j}(t)} as required.

Notes [ edit ] ^ Bronson (1970 , p. 189) ^ Beauregard & Fraleigh (1973 , p. 310) ^ Nering (1970 , p. 118) ^ Golub & Van Loan (1996 , p. 316) ^ Beauregard & Fraleigh (1973 , p. 319) ^ Bronson (1970 , pp. 194–195) ^ Golub & Van Loan (1996 , p. 311) ^ Bronson (1970 , p. 196) ^ Bronson (1970 , p. 189) ^ Beauregard & Fraleigh (1973 , pp. 316–318) ^ Nering (1970 , p. 118) ^ Bronson (1970 , p. 196) ^ Anton (1987 , pp. 301–302) ^ Beauregard & Fraleigh (1973 , p. 266) ^ Burden & Faires (1993 , p. 401) ^ Golub & Van Loan (1996 , pp. 310–311) ^ Harper (1976 , p. 58) ^ Herstein (1964 , p. 225) ^ Kreyszig (1972 , pp. 273, 684) ^ Nering (1970 , p. 104) ^ Burden & Faires (1993 , p. 401) ^ Beauregard & Fraleigh (1973 , pp. 270–274) ^ Bronson (1970 , pp. 179–183) ^ Bronson (1970 , p. 181) ^ Bronson (1970 , p. 179) ^ Beauregard & Fraleigh (1973 , pp. 270–274) ^ Bronson (1970 , pp. 179–183) ^ Bronson (1970 , p. 189) ^ Bronson (1970 , pp. 190, 202) ^ Bronson (1970 , pp. 189, 203) ^ Bronson (1970 , pp. 206–207) ^ Bronson (1970 , p. 205) ^ Bronson (1970 , p. 196) ^ Bronson (1970 , pp. 189, 209–215) ^ Golub & Van Loan (1996 , p. 316) ^ Herstein (1964 , p. 259) ^ Nering (1970 , p. 118) ^ Nering (1970 , p. 118) ^ Nering (1970 , p. 118) ^ Herstein (1964 , p. 261) ^ Beauregard & Fraleigh (1973 , p. 310) ^ Nering (1970 , pp. 122, 123) ^ Bronson (1970 , pp. 189–209) ^ Bronson (1970 , pp. 194–195) ^ Bronson (1970 , pp. 196, 197) ^ Bronson (1970 , pp. 197, 198) ^ Bronson (1970 , pp. 190–191) ^ Bronson (1970 , pp. 197–198) ^ Bronson (1970 , p. 205) ^ Beauregard & Fraleigh (1973 , p. 311) ^ Cullen (1966 , p. 114) ^ Franklin (1968 , p. 122) ^ Bronson (1970 , p. 207) ^ Bronson (1970 , pp. 208) ^ Bronson (1970 , p. 206) ^ Beauregard & Fraleigh (1973 , pp. 57–61) ^ Bronson (1970 , p. 104) ^ Bronson (1970 , p. 105) ^ Bronson (1970 , p. 184) ^ Bronson (1970 , p. 185) ^ Bronson (1970 , pp. 209–218) ^ Beauregard & Fraleigh (1973 , pp. 274–275) ^ Beauregard & Fraleigh (1973 , p. 317) References [ edit ] Anton, Howard (1987), Elementary Linear Algebra (5th ed.), New York: Wiley , ISBN 0-471-84819-0 Axler, Sheldon (1997).

Linear Algebra Done Right (2nd ed.). Springer.

ISBN 978-0-387-98258-8 .

Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields , Boston: Houghton Mifflin Co.

, ISBN 0-395-14017-X Bronson, Richard (1970), Matrix Methods: An Introduction , New York: Academic Press , LCCN 70097490 Burden, Richard L.; Faires, J. Douglas (1993), Numerical Analysis (5th ed.), Boston: Prindle, Weber and Schmidt , ISBN 0-534-93219-3 Cullen, Charles G. (1966), Matrices and Linear Transformations , Reading: Addison-Wesley , LCCN 66021267 Franklin, Joel N. (1968), Matrix Theory , Englewood Cliffs: Prentice-Hall , LCCN 68016345 Golub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins University Press , ISBN 0-8018-5414-8 Harper, Charlie (1976), Introduction to Mathematical Physics , New Jersey: Prentice-Hall , ISBN 0-13-487538-9 Herstein, I. N. (1964), Topics In Algebra , Waltham: Blaisdell Publishing Company , ISBN 978-1114541016 {{ citation }} : ISBN / Date incompatibility ( help ) Kreyszig, Erwin (1972), Advanced Engineering Mathematics (3rd ed.), New York: Wiley , ISBN 0-471-50728-8 Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley , LCCN 76091646 v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category v t e Major mathematics areas History Timeline Future Lists Glossary Foundations Category theory Information theory Mathematical logic Philosophy of mathematics Set theory Type theory Algebra Abstract Commutative Elementary Group theory Linear Multilinear Universal Homological Analysis Calculus Real analysis Complex analysis Hypercomplex analysis Differential equations Functional analysis Harmonic analysis Measure theory Discrete Combinatorics Graph theory Order theory Geometry Algebraic Analytic Arithmetic Differential Discrete Euclidean Finite Number theory Arithmetic Algebraic number theory Analytic number theory Diophantine geometry Topology General Algebraic Differential Geometric Homotopy theory Applied Engineering mathematics Mathematical biology Mathematical chemistry Mathematical economics Mathematical finance Mathematical physics Mathematical psychology Mathematical sociology Mathematical statistics Probability Statistics Systems science Control theory Game theory Operations research Computational Computer science Theory of computation Computational complexity theory Numerical analysis Optimization Computer algebra Related topics Mathematicians lists Informal mathematics Films about mathematicians Recreational mathematics Mathematics and art Mathematics education Mathematics portal Category Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐c4s52
Cached time: 20250816234135
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.837 seconds
Real time usage: 1.212 seconds
Preprocessor visited node count: 6406/1000000
Revision size: 40667/2097152 bytes
Post‐expand include size: 69254/2097152 bytes
Template argument size: 3590/2097152 bytes
Highest expansion depth: 10/100
Expensive parser function count: 8/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 104611/5000000 bytes
Lua time usage: 0.310/10.000 seconds
Lua memory usage: 7103983/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  565.005      1 -total
 20.23%  114.314      1 Template:Reflist
 19.87%  112.255     11 Template:Citation
 18.71%  105.692      1 Template:Short_description
 16.07%   90.778      2 Template:Navbox
 15.47%   87.383     63 Template:Harvtxt
 14.32%   80.904      2 Template:Pagetype
 13.92%   78.661      1 Template:Linear_algebra
  7.45%   42.088      9 Template:NumBlk
  4.40%   24.849      1 Template:Distinguish Saved in parser cache with key enwiki:pcache:1137612:|#|:idhash:canonical and timestamp 20250816234135 and revision id 1306284419. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Generalized_eigenvector&oldid=1306284419 " Categories : Linear algebra Matrix theory Hidden categories: Articles with short description Short description matches Wikidata CS1 errors: ISBN date This page was last edited on 16 August 2025, at 23:40 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Generalized eigenvector 9 languages Add topic

