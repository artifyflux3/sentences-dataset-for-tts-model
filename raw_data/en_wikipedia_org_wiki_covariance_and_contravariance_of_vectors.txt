Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Introduction 2 Definition Toggle Definition subsection 2.1 Contravariant transformation 2.2 Covariant transformation 3 Coordinates 4 Covariant and contravariant components of a vector with a metric Toggle Covariant and contravariant components of a vector with a metric subsection 4.1 Euclidean plane 4.1.1 Example 4.2 Three-dimensional Euclidean space 4.3 Vector spaces of any dimension 4.3.1 Definitions 4.3.2 Corollaries 4.3.3 Historical and geometrical meaning 5 Use in tensor analysis 6 Algebra and geometry 7 See also 8 Notes 9 Citations 10 References 11 External links Toggle the table of contents Covariance and contravariance of vectors 19 languages Català Ελληνικά Español Français Galego 한국어 Հայերեն Italiano עברית Nederlands 日本語 ਪੰਜਾਬੀ Português Русский Slovenščina Svenska Türkçe Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Vector behavior under coordinate changes For use of "covariance" in the context of special relativity, see Lorentz covariance . For other uses of "covariant" or "contravariant", see Covariance and contravariance (disambiguation) .

A vector, v , represented in terms of tangent basis e 1 , e 2 , e 3 to the coordinate curves ( left ), dual basis, covector basis, or reciprocal basis e 1 , e 2 , e 3 to coordinate surfaces ( right ), in 3-d general curvilinear coordinates ( q 1 , q 2 , q 3 ) , a tuple of numbers to define a point in a position space . Note the basis and cobasis coincide only when the basis is orthonormal .

[ 1 ] [ specify ] In physics , especially in multilinear algebra and tensor analysis , covariance and contravariance describe how the quantitative description of certain geometric or physical entities changes with a change of basis .

[ 2 ] Briefly, a contravariant vector is a list of numbers that transforms oppositely to a change of basis, and a covariant vector is a list of numbers that transforms in the same way.  Contravariant vectors are often just called vectors and covariant vectors are called covectors or dual vectors .  The terms covariant and contravariant were introduced by James Joseph Sylvester in 1851.

[ 3 ] [ 4 ] Curvilinear coordinate systems , such as cylindrical or spherical coordinates , are often used in physical and geometric problems.  Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another.

Tensors are objects in multilinear algebra that can have aspects of both covariance and contravariance.

Introduction [ edit ] In physics, a vector typically arises as the outcome of a measurement or series of measurements, and is represented as a list (or tuple ) of numbers such as ( v 1 , v 2 , v 3 ) .

{\displaystyle (v_{1},v_{2},v_{3}).} The numbers in the list depend on the choice of coordinate system .  For instance, if the vector represents position with respect to an observer ( position vector ), then the coordinate system may be obtained from a system of rigid rods, or reference axes, along which the components v 1 , v 2 , and v 3 are measured.  For a vector to represent a geometric object, it must be possible to describe how it looks in any other coordinate system.  That is to say, the components of the vectors will transform in a certain way in passing from one coordinate system to another.

A simple illustrative case is that of a Euclidean vector .  For a vector, once a set of basis vectors has been defined, then the components of that vector will always vary opposite to that of the basis vectors.  That vector is therefore defined as a contravariant tensor.  Take a standard position vector for example. By changing the scale of the reference axes from meters to centimeters (that is, dividing the scale of the reference axes by 100, so that the basis vectors now are .01 {\displaystyle .01} meters long), the components of the measured position vector are multiplied by 100.  A vector's components change scale inversely to changes in scale to the reference axes, and consequently a vector is called a contravariant tensor.

A vector , which is an example of a contravariant tensor, has components that transform inversely to the transformation of the reference axes, (with example transformations including rotation and dilation ).

The vector itself does not change under these operations ; instead, the components of the vector change in a way that cancels the change in the spatial axes.  In other words, if the reference axes were rotated in one direction, the component representation of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, would reduce in an exactly compensating way.  Mathematically, if the coordinate system undergoes a transformation described by an n × × n {\displaystyle n\times n} invertible matrix M , so that the basis vectors transform according to [ e 1 ′ ′ e 2 ′ ′ .

.

.

e n ′ ′ ] = [ e 1 e 2 .

.

.

e n ] M {\displaystyle {\begin{bmatrix}\mathbf {e} _{1}^{\prime }\ \mathbf {e} _{2}^{\prime }\ ...\ \mathbf {e} _{n}^{\prime }\end{bmatrix}}={\begin{bmatrix}\mathbf {e} _{1}\ \mathbf {e} _{2}\ ...\ \mathbf {e} _{n}\end{bmatrix}}M} , then the components of a vector v in the original basis ( v i {\displaystyle v^{i}} ) must be similarly transformed via [ v 1 ′ ′ v 2 ′ ′ .

.

.

v n ′ ′ ] = M − − 1 [ v 1 v 2 .

.

.

v n ] {\displaystyle {\begin{bmatrix}v^{1}{^{\prime }}\\v^{2}{^{\prime }}\\...\\v^{n}{^{\prime }}\end{bmatrix}}=M^{-1}{\begin{bmatrix}v^{1}\\v^{2}\\...\\v^{n}\end{bmatrix}}} .  The components of a vector are often represented arranged in a column.

By contrast, a covector has components that transform like the reference axes.  It lives in the dual vector space, and represents a linear map from vectors to scalars. The dot product operator involving vectors is a good example of a covector.  To illustrate, assume we have a covector defined as v ⋅ ⋅ {\displaystyle \mathbf {v} \ \cdot } , where v {\displaystyle \mathbf {v} } is a vector.  The components of this covector in some arbitrary basis are [ v ⋅ ⋅ e 1 v ⋅ ⋅ e 2 .

.

.

v ⋅ ⋅ e n ] {\displaystyle {\begin{bmatrix}\mathbf {v} \cdot \mathbf {e} _{1}&\mathbf {v} \cdot \mathbf {e} _{2}&...&\mathbf {v} \cdot \mathbf {e} _{n}\end{bmatrix}}} , with [ e 1 e 2 .

.

.

e n ] {\displaystyle {\begin{bmatrix}\mathbf {e} _{1}\ \mathbf {e} _{2}\ ...\ \mathbf {e} _{n}\end{bmatrix}}} being the basis vectors in the corresponding vector space. (This can be derived by noting that we want to get the correct answer for the dot product operation when multiplying by an arbitrary vector w {\displaystyle \mathbf {w} } , with components [ w 1 w 2 .

.

.

w n ] {\displaystyle {\begin{bmatrix}w^{1}\\w^{2}\\...\\w^{n}\end{bmatrix}}} ).  The covariance of these covector components is then seen by noting that if a transformation described by an n × × n {\displaystyle n\times n} invertible matrix M were to be applied to the basis vectors in the corresponding vector space, [ e 1 ′ ′ e 2 ′ ′ .

.

.

e n ′ ′ ] = [ e 1 e 2 .

.

.

e n ] M {\displaystyle {\begin{bmatrix}\mathbf {e} _{1}^{\prime }\ \mathbf {e} _{2}^{\prime }\ ...\ \mathbf {e} _{n}^{\prime }\end{bmatrix}}={\begin{bmatrix}\mathbf {e} _{1}\ \mathbf {e} _{2}\ ...\ \mathbf {e} _{n}\end{bmatrix}}M} , then the components of the covector v ⋅ ⋅ {\displaystyle \mathbf {v} \ \cdot } will transform with the same matrix M {\displaystyle M} , namely, [ v ⋅ ⋅ e 1 ′ ′ v ⋅ ⋅ e 2 ′ ′ .

.

.

v ⋅ ⋅ e n ′ ′ ] = [ v ⋅ ⋅ e 1 v ⋅ ⋅ e 2 .

.

.

v ⋅ ⋅ e n ] M {\displaystyle {\begin{bmatrix}\mathbf {v} \cdot \mathbf {e} _{1}^{\prime }&\mathbf {v} \cdot \mathbf {e} _{2}^{\prime }&...&\mathbf {v} \cdot \mathbf {e} _{n}^{\prime }\end{bmatrix}}={\begin{bmatrix}\mathbf {v} \cdot \mathbf {e} _{1}&\mathbf {v} \cdot \mathbf {e} _{2}&...&\mathbf {v} \cdot \mathbf {e} _{n}\end{bmatrix}}M} .  The components of a covector are often represented arranged in a row.

A third concept related to covariance and contravariance is invariance .  A scalar (also called type-0 or rank-0 tensor) is an object that does not vary with the change in basis.  An example of a physical observable that is a scalar is the mass of a particle.  The single, scalar value of mass is independent to changes in basis vectors and consequently is called invariant . The magnitude of a vector (such as distance ) is another example of an invariant, because it remains fixed even if geometrical vector components vary.  (For example, for a position vector of length 3 {\displaystyle 3} meters, if all Cartesian basis vectors are changed from 1 {\displaystyle 1} meters in length to .01 {\displaystyle .01} meters in length, the length of the position vector remains unchanged at 3 {\displaystyle 3} meters, although the vector components will all increase by a factor of 100 {\displaystyle 100} ).  The scalar product of a vector and a covector is invariant, because one has components that vary with the base change, and the other has components that vary oppositely, and the two effects cancel out.  One thus says that covectors are dual to vectors.

Thus, to summarize: A vector or tangent vector , has components that contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be contravariant . In Einstein notation (implicit summation over repeated index), contravariant components are denoted with upper indices as in v = v i e i {\displaystyle \mathbf {v} =v^{i}\mathbf {e} _{i}} A covector or cotangent vector has components that co-vary with a change of basis in the corresponding (initial) vector space. That is, the components must be transformed by the same matrix as the change of basis matrix in the corresponding (initial) vector space. The components of covectors (as opposed to those of vectors) are said to be covariant . In Einstein notation , covariant components are denoted with lower indices as in w = w i e i .

{\displaystyle \mathbf {w} =w_{i}\mathbf {e} ^{i}.} The scalar product of a vector and covector is the scalar v i w i {\displaystyle v^{i}w_{i}} , which is invariant.  It is the duality pairing of vectors and covectors.

Definition [ edit ] The general formulation of covariance and contravariance refers to how the components of a coordinate vector transform under a change of basis ( passive transformation ).

[ 5 ] Thus let V be a vector space of dimension n over a field of scalars S , and let each of f = ( X 1 , ..., X n ) and f ′ = ( Y 1 , ..., Y n ) be a basis of V .

[ note 1 ] Also, let the change of basis from f to f ′ be given by f ↦ ↦ f ′ = ( ∑ ∑ i a 1 i X i , … … , ∑ ∑ i a n i X i ) = f A {\displaystyle \mathbf {f} \mapsto \mathbf {f} '={\biggl (}\sum _{i}a_{1}^{i}X_{i},\dots ,\sum _{i}a_{n}^{i}X_{i}{\biggr )}=\mathbf {f} A} 1 for some invertible n × n matrix A with entries a j i {\displaystyle a_{j}^{i}} .
Here, each vector Y j of the f ′ basis is a linear combination of the vectors X i of the f basis, so that Y j = ∑ ∑ i a j i X i , {\displaystyle Y_{j}=\sum _{i}a_{j}^{i}X_{i},} which are the columns of the matrix product f A {\displaystyle \mathbf {f} A} .

Contravariant transformation [ edit ] Main article: Contravariant transformation A vector v {\displaystyle v} in V is expressed uniquely as a linear combination of the elements X i {\displaystyle X_{i}} of the f basis as v = ∑ ∑ i v i [ f ] X i , {\displaystyle v=\sum _{i}v^{i}[\mathbf {f} ]X_{i},} 2 where v i [ f ] are elements of the field S known as the components of v in the f basis. Denote the column vector of components of v by v [ f ]: v [ f ] = [ v 1 [ f ] v 2 [ f ] ⋮ ⋮ v n [ f ] ] {\displaystyle \mathbf {v} [\mathbf {f} ]={\begin{bmatrix}v^{1}[\mathbf {f} ]\\v^{2}[\mathbf {f} ]\\\vdots \\v^{n}[\mathbf {f} ]\end{bmatrix}}} so that ( 2 ) can be rewritten as a matrix product v = f v [ f ] .

{\displaystyle v=\mathbf {f} \,\mathbf {v} [\mathbf {f} ].} The vector v may also be expressed in terms of the f ′ basis, so that v = f ′ v [ f ′ ] .

{\displaystyle v=\mathbf {f'} \,\mathbf {v} [\mathbf {f'} ].} However, since the vector v itself is invariant under the choice of basis, f v [ f ] = v = f ′ v [ f ′ ] .

{\displaystyle \mathbf {f} \,\mathbf {v} [\mathbf {f} ]=v=\mathbf {f'} \,\mathbf {v} [\mathbf {f'} ].} The invariance of v combined with the relationship ( 1 ) between f and f ′ implies that f v [ f ] = f A v [ f A ] , {\displaystyle \mathbf {f} \,\mathbf {v} [\mathbf {f} ]=\mathbf {f} A\,\mathbf {v} [\mathbf {f} A],} giving the transformation rule v [ f ′ ] = v [ f A ] = A − − 1 v [ f ] .

{\displaystyle \mathbf {v} [\mathbf {f'} ]=\mathbf {v} [\mathbf {f} A]=A^{-1}\mathbf {v} [\mathbf {f} ].} In terms of components, v i [ f A ] = ∑ ∑ j a ~ ~ j i v j [ f ] {\displaystyle v^{i}[\mathbf {f} A]=\sum _{j}{\tilde {a}}_{j}^{i}v^{j}[\mathbf {f} ]} where the coefficients a ~ ~ j i {\displaystyle {\tilde {a}}_{j}^{i}} are the entries of the inverse matrix of A .

Because the components of the vector v transform with the inverse of the matrix A , these components are said to transform contravariantly under a change of basis.

The way A relates the two  pairs is depicted in the following informal diagram using an arrow.  The reversal of the arrow indicates a contravariant change: f ⟶ ⟶ f ′ v [ f ] ⟵ ⟵ v [ f ′ ] {\displaystyle {\begin{aligned}\mathbf {f} &\longrightarrow \mathbf {f'} \\v[\mathbf {f} ]&\longleftarrow v[\mathbf {f'} ]\end{aligned}}} Covariant transformation [ edit ] Main article: Covariant transformation A linear functional α on V is expressed uniquely in terms of its components (elements in S ) in the f basis as α α ( X i ) = α α i [ f ] , i = 1 , 2 , … … , n .

{\displaystyle \alpha (X_{i})=\alpha _{i}[\mathbf {f} ],\quad i=1,2,\dots ,n.} These components are the action of α on the basis vectors X i of the f basis.

Under the change of basis from f to f ′ (via 1 ), the components transform so that α α i [ f A ] = α α ( Y i ) = α α ( ∑ ∑ j a i j X j ) = ∑ ∑ j a i j α α ( X j ) = ∑ ∑ j a i j α α j [ f ] .

{\displaystyle {\begin{aligned}\alpha _{i}[\mathbf {f} A]&=\alpha (Y_{i})\\&=\alpha {\biggl (}\sum _{j}a_{i}^{j}X_{j}{\biggr )}\\&=\sum _{j}a_{i}^{j}\alpha (X_{j})\\&=\sum _{j}a_{i}^{j}\alpha _{j}[\mathbf {f} ].\end{aligned}}} 3 Denote the row vector of components of α by α [ f ]: α α [ f ] = [ α α 1 [ f ] , α α 2 [ f ] , … … , α α n [ f ] ] {\displaystyle \mathbf {\alpha } [\mathbf {f} ]={\begin{bmatrix}\alpha _{1}[\mathbf {f} ],\alpha _{2}[\mathbf {f} ],\dots ,\alpha _{n}[\mathbf {f} ]\end{bmatrix}}} so that ( 3 ) can be rewritten as the matrix product α α [ f A ] = α α [ f ] A .

{\displaystyle \alpha [\mathbf {f} A]=\alpha [\mathbf {f} ]A.} Because the components of the linear functional α transform with the matrix A , these components are said to transform covariantly under a change of basis.

The way A relates the two  pairs is depicted in the following informal diagram using an arrow.  A covariant relationship is indicated since the arrows travel in the same direction: f ⟶ ⟶ f ′ α α [ f ] ⟶ ⟶ α α [ f ′ ] {\displaystyle {\begin{aligned}\mathbf {f} &\longrightarrow \mathbf {f'} \\\alpha [\mathbf {f} ]&\longrightarrow \alpha [\mathbf {f'} ]\end{aligned}}} Had a column vector representation been used instead, the transformation law would be the transpose α α T [ f A ] = A T α α T [ f ] .

{\displaystyle \alpha ^{\mathrm {T} }[\mathbf {f} A]=A^{\mathrm {T} }\alpha ^{\mathrm {T} }[\mathbf {f} ].} Coordinates [ edit ] The choice of basis f on the vector space V defines uniquely a set of coordinate functions on V , by means of x i [ f ] ( v ) = v i [ f ] .

{\displaystyle x^{i}[\mathbf {f} ](v)=v^{i}[\mathbf {f} ].} The coordinates on V are therefore contravariant in the sense that x i [ f A ] = ∑ ∑ k = 1 n a ~ ~ k i x k [ f ] .

{\displaystyle x^{i}[\mathbf {f} A]=\sum _{k=1}^{n}{\tilde {a}}_{k}^{i}x^{k}[\mathbf {f} ].} Conversely, a system of n quantities v i that transform like the coordinates x i on V defines a contravariant vector (or simply vector).  A system of n quantities that transform oppositely to the coordinates is then a covariant vector (or covector).

This formulation of contravariance and covariance is often more natural in applications in which there is a coordinate space (a manifold ) on which vectors live as tangent vectors or cotangent vectors .  Given a local coordinate system x i on the manifold, the reference axes for the coordinate system are the vector fields X 1 = ∂ ∂ ∂ ∂ x 1 , … … , X n = ∂ ∂ ∂ ∂ x n .

{\displaystyle X_{1}={\frac {\partial }{\partial x^{1}}},\dots ,X_{n}={\frac {\partial }{\partial x^{n}}}.} This gives rise to the frame f = ( X 1 , ..., X n ) at every point of the coordinate patch.

If y i is a different coordinate system and Y 1 = ∂ ∂ ∂ ∂ y 1 , … … , Y n = ∂ ∂ ∂ ∂ y n , {\displaystyle Y_{1}={\frac {\partial }{\partial y^{1}}},\dots ,Y_{n}={\frac {\partial }{\partial y^{n}}},} then the frame f' is related to the frame f by the inverse of the Jacobian matrix of the coordinate transition: f ′ = f J − − 1 , J = ( ∂ ∂ y i ∂ ∂ x j ) i , j = 1 n .

{\displaystyle \mathbf {f} '=\mathbf {f} J^{-1},\quad J=\left({\frac {\partial y^{i}}{\partial x^{j}}}\right)_{i,j=1}^{n}.} Or, in indices, ∂ ∂ ∂ ∂ y i = ∑ ∑ j = 1 n ∂ ∂ x j ∂ ∂ y i ∂ ∂ ∂ ∂ x j .

{\displaystyle {\frac {\partial }{\partial y^{i}}}=\sum _{j=1}^{n}{\frac {\partial x^{j}}{\partial y^{i}}}{\frac {\partial }{\partial x^{j}}}.} A tangent vector is by definition a vector that is a linear combination of the coordinate partials ∂ ∂ / ∂ ∂ x i {\displaystyle \partial /\partial x^{i}} .  Thus a tangent vector is defined by v = ∑ ∑ i = 1 n v i [ f ] X i = f v [ f ] .

{\displaystyle v=\sum _{i=1}^{n}v^{i}[\mathbf {f} ]X_{i}=\mathbf {f} \ \mathbf {v} [\mathbf {f} ].} Such a vector is contravariant with respect to change of frame.  Under changes in the coordinate system, one has v [ f ′ ] = v [ f J − − 1 ] = J v [ f ] .

{\displaystyle \mathbf {v} \left[\mathbf {f} '\right]=\mathbf {v} \left[\mathbf {f} J^{-1}\right]=J\,\mathbf {v} [\mathbf {f} ].} Therefore, the components of a tangent vector transform via v i [ f ′ ] = ∑ ∑ j = 1 n ∂ ∂ y i ∂ ∂ x j v j [ f ] .

{\displaystyle v^{i}\left[\mathbf {f} '\right]=\sum _{j=1}^{n}{\frac {\partial y^{i}}{\partial x^{j}}}v^{j}[\mathbf {f} ].} Accordingly, a system of n quantities v i depending on the coordinates that transform in this way on passing from one coordinate system to another is called a contravariant vector.

Covariant and contravariant components of a vector with a metric [ edit ] Covariant and contravariant components of a vector when the basis is not orthogonal.

In a finite-dimensional vector space V over a field K with a non-degenerate symmetric bilinear form g : V × V → K (which may be referred to as the metric tensor ), there is little distinction between covariant and contravariant vectors, because the bilinear form allows covectors to be identified with vectors.  That is, a vector v uniquely determines a covector α via α α ( w ) = g ( v , w ) {\displaystyle \alpha (w)=g(v,w)} for all vectors w .  Conversely, each covector α determines a unique vector v by this equation.  Because of this identification of vectors with covectors, one may speak of the covariant components or contravariant components of a vector, that is, they are just representations of the same vector using the reciprocal basis .

Given a basis f = ( X 1 , ..., X n ) of V , there is a unique reciprocal basis f # = ( Y 1 , ..., Y n ) of V determined by requiring that g ( Y i , X j ) = δ δ j i , {\displaystyle g(Y^{i},X_{j})=\delta _{j}^{i},} the Kronecker delta .  In terms of these bases, any vector v can be written in two ways: v = ∑ ∑ i v i [ f ] X i = f v [ f ] = ∑ ∑ i v i [ f ♯ ♯ ] Y i = f ♯ ♯ v ♯ ♯ [ f ] .

{\displaystyle {\begin{aligned}v&=\sum _{i}v^{i}[\mathbf {f} ]X_{i}=\mathbf {f} \,\mathbf {v} [\mathbf {f} ]\\&=\sum _{i}v_{i}[\mathbf {f^{\sharp }} ]Y^{i}=\mathbf {f} ^{\sharp }\mathbf {v} ^{\sharp }[\mathbf {f} ].\end{aligned}}} The components v i [ f ] are the contravariant components of the vector v in the basis f , and the components v i [ f ] are the covariant components of v in the basis f .  The terminology is justified because under a change of basis, v [ f A ] = A − − 1 v [ f ] , v ♯ ♯ [ f A ] = A T v ♯ ♯ [ f ] {\displaystyle \mathbf {v} [\mathbf {f} A]=A^{-1}\mathbf {v} [\mathbf {f} ],\quad \mathbf {v} ^{\sharp }[\mathbf {f} A]=A^{T}\mathbf {v} ^{\sharp }[\mathbf {f} ]} where A {\displaystyle A} is an invertible n × × n {\displaystyle n\times n} matrix, and the matrix transpose has its usual meaning.

Euclidean plane [ edit ] In the Euclidean plane, the dot product allows for vectors to be identified with covectors.  If e 1 , e 2 {\displaystyle \mathbf {e} _{1},\mathbf {e} _{2}} is a basis, then the dual basis e 1 , e 2 {\displaystyle \mathbf {e} ^{1},\mathbf {e} ^{2}} satisfies e 1 ⋅ ⋅ e 1 = 1 , e 1 ⋅ ⋅ e 2 = 0 e 2 ⋅ ⋅ e 1 = 0 , e 2 ⋅ ⋅ e 2 = 1.

{\displaystyle {\begin{aligned}\mathbf {e} ^{1}\cdot \mathbf {e} _{1}=1,&\quad \mathbf {e} ^{1}\cdot \mathbf {e} _{2}=0\\\mathbf {e} ^{2}\cdot \mathbf {e} _{1}=0,&\quad \mathbf {e} ^{2}\cdot \mathbf {e} _{2}=1.\end{aligned}}} Thus, e 1 and e 2 are perpendicular to each other, as are e 2 and e 1 , and the lengths of e 1 and e 2 normalized against e 1 and e 2 , respectively.

Example [ edit ] For example, [ 6 ] suppose that we are given a basis e 1 , e 2 consisting of a pair of vectors making a 45° angle with one another, such that e 1 has length 2 and e 2 has length 1.  Then the dual basis vectors are given as follows: e 2 is the result of rotating e 1 through an angle of 90° (where the sense is measured by assuming the pair e 1 , e 2 to be positively oriented), and then rescaling so that e 2 ⋅ e 2 = 1 holds.

e 1 is the result of rotating e 2 through an angle of 90°, and then rescaling so that e 1 ⋅ e 1 = 1 holds.

Applying these rules, we find e 1 = 1 2 e 1 − − 1 2 e 2 {\displaystyle \mathbf {e} ^{1}={\frac {1}{2}}\mathbf {e} _{1}-{\frac {1}{\sqrt {2}}}\mathbf {e} _{2}} and e 2 = − − 1 2 e 1 + 2 e 2 .

{\displaystyle \mathbf {e} ^{2}=-{\frac {1}{\sqrt {2}}}\mathbf {e} _{1}+2\mathbf {e} _{2}.} Thus the change of basis matrix in going from the original basis to the reciprocal basis is R = [ 1 2 − − 1 2 − − 1 2 2 ] , {\displaystyle R={\begin{bmatrix}{\frac {1}{2}}&-{\frac {1}{\sqrt {2}}}\\-{\frac {1}{\sqrt {2}}}&2\end{bmatrix}},} since [ e 1 e 2 ] = [ e 1 e 2 ] [ 1 2 − − 1 2 − − 1 2 2 ] .

{\displaystyle [\mathbf {e} ^{1}\ \mathbf {e} ^{2}]=[\mathbf {e} _{1}\ \mathbf {e} _{2}]{\begin{bmatrix}{\frac {1}{2}}&-{\frac {1}{\sqrt {2}}}\\-{\frac {1}{\sqrt {2}}}&2\end{bmatrix}}.} For instance, the vector v = 3 2 e 1 + 2 e 2 {\displaystyle v={\frac {3}{2}}\mathbf {e} _{1}+2\mathbf {e} _{2}} is a vector with contravariant components v 1 = 3 2 , v 2 = 2.

{\displaystyle v^{1}={\frac {3}{2}},\quad v^{2}=2.} The covariant components are obtained by equating the two expressions for the vector v : v = v 1 e 1 + v 2 e 2 = v 1 e 1 + v 2 e 2 {\displaystyle v=v_{1}\mathbf {e} ^{1}+v_{2}\mathbf {e} ^{2}=v^{1}\mathbf {e} _{1}+v^{2}\mathbf {e} _{2}} so [ v 1 v 2 ] = R − − 1 [ v 1 v 2 ] = [ 4 2 2 1 ] [ v 1 v 2 ] = [ 6 + 2 2 2 + 3 2 ] .

{\displaystyle {\begin{aligned}{\begin{bmatrix}v_{1}\\v_{2}\end{bmatrix}}&=R^{-1}{\begin{bmatrix}v^{1}\\v^{2}\end{bmatrix}}\\&={\begin{bmatrix}4&{\sqrt {2}}\\{\sqrt {2}}&1\end{bmatrix}}{\begin{bmatrix}v^{1}\\v^{2}\end{bmatrix}}\\&={\begin{bmatrix}6+2{\sqrt {2}}\\2+{\frac {3}{\sqrt {2}}}\end{bmatrix}}\end{aligned}}.} Three-dimensional Euclidean space [ edit ] In the three-dimensional Euclidean space , one can also determine explicitly the dual basis to a given set of basis vectors e 1 , e 2 , e 3 of E 3 that are not necessarily assumed to be orthogonal nor of unit norm. The dual basis vectors are: e 1 = e 2 × × e 3 e 1 ⋅ ⋅ ( e 2 × × e 3 ) ; e 2 = e 3 × × e 1 e 2 ⋅ ⋅ ( e 3 × × e 1 ) ; e 3 = e 1 × × e 2 e 3 ⋅ ⋅ ( e 1 × × e 2 ) .

{\displaystyle \mathbf {e} ^{1}={\frac {\mathbf {e} _{2}\times \mathbf {e} _{3}}{\mathbf {e} _{1}\cdot (\mathbf {e} _{2}\times \mathbf {e} _{3})}};\qquad \mathbf {e} ^{2}={\frac {\mathbf {e} _{3}\times \mathbf {e} _{1}}{\mathbf {e} _{2}\cdot (\mathbf {e} _{3}\times \mathbf {e} _{1})}};\qquad \mathbf {e} ^{3}={\frac {\mathbf {e} _{1}\times \mathbf {e} _{2}}{\mathbf {e} _{3}\cdot (\mathbf {e} _{1}\times \mathbf {e} _{2})}}.} Even when the e i and e i are not orthonormal , they are still mutually reciprocal: e i ⋅ ⋅ e j = δ δ j i , {\displaystyle \mathbf {e} ^{i}\cdot \mathbf {e} _{j}=\delta _{j}^{i},} Then the contravariant components of any vector v can be obtained by the dot product of v with the dual basis vectors: q 1 = v ⋅ ⋅ e 1 ; q 2 = v ⋅ ⋅ e 2 ; q 3 = v ⋅ ⋅ e 3 .

{\displaystyle q^{1}=\mathbf {v} \cdot \mathbf {e} ^{1};\qquad q^{2}=\mathbf {v} \cdot \mathbf {e} ^{2};\qquad q^{3}=\mathbf {v} \cdot \mathbf {e} ^{3}.} Likewise, the covariant components of v can be obtained from the dot product of v with basis vectors, viz.

q 1 = v ⋅ ⋅ e 1 ; q 2 = v ⋅ ⋅ e 2 ; q 3 = v ⋅ ⋅ e 3 .

{\displaystyle q_{1}=\mathbf {v} \cdot \mathbf {e} _{1};\qquad q_{2}=\mathbf {v} \cdot \mathbf {e} _{2};\qquad q_{3}=\mathbf {v} \cdot \mathbf {e} _{3}.} Then v can be expressed in two (reciprocal) ways, viz.

v = q i e i = q 1 e 1 + q 2 e 2 + q 3 e 3 .

{\displaystyle \mathbf {v} =q^{i}\mathbf {e} _{i}=q^{1}\mathbf {e} _{1}+q^{2}\mathbf {e} _{2}+q^{3}\mathbf {e} _{3}.} or v = q i e i = q 1 e 1 + q 2 e 2 + q 3 e 3 {\displaystyle \mathbf {v} =q_{i}\mathbf {e} ^{i}=q_{1}\mathbf {e} ^{1}+q_{2}\mathbf {e} ^{2}+q_{3}\mathbf {e} ^{3}} Combining the above relations, we have v = ( v ⋅ ⋅ e i ) e i = ( v ⋅ ⋅ e i ) e i {\displaystyle \mathbf {v} =(\mathbf {v} \cdot \mathbf {e} ^{i})\mathbf {e} _{i}=(\mathbf {v} \cdot \mathbf {e} _{i})\mathbf {e} ^{i}} and we can convert between the basis and dual basis with q i = v ⋅ ⋅ e i = ( q j e j ) ⋅ ⋅ e i = ( e j ⋅ ⋅ e i ) q j {\displaystyle q_{i}=\mathbf {v} \cdot \mathbf {e} _{i}=(q^{j}\mathbf {e} _{j})\cdot \mathbf {e} _{i}=(\mathbf {e} _{j}\cdot \mathbf {e} _{i})q^{j}} and q i = v ⋅ ⋅ e i = ( q j e j ) ⋅ ⋅ e i = ( e j ⋅ ⋅ e i ) q j .

{\displaystyle q^{i}=\mathbf {v} \cdot \mathbf {e} ^{i}=(q_{j}\mathbf {e} ^{j})\cdot \mathbf {e} ^{i}=(\mathbf {e} ^{j}\cdot \mathbf {e} ^{i})q_{j}.} If the basis vectors are orthonormal , then they are the same as the dual basis vectors.

Vector spaces of any dimension [ edit ] The following applies to any vector space of dimension n equipped with a non-degenerate commutative and distributive dot product, and thus also to the Euclidean spaces of any dimension.

All indices in the formulas run from 1 to n . The Einstein notation for the implicit summation of the terms with the same upstairs (contravariant) and downstairs (covariant) indices is followed.

The historical and geometrical meaning of the terms contravariant and covariant will be explained at the end of this section.

Definitions [ edit ] Covariant basis of a vector space of dimension n : e j ≜ ≜ {\displaystyle \mathbf {e_{j}} \triangleq } {any linearly independent basis for which in general is e i ⋅ ⋅ e j ≠ ≠ δ δ i j {\displaystyle \mathbf {e_{i}} \cdot \mathbf {e_{j}} \neq \delta _{ij}} }, i.e. not necessarily orthonormal (D.1).

Contravariant components of a vector v {\displaystyle \mathbf {v} } : v i ≜ ≜ { v i ∣ ∣ v = v i e i } {\displaystyle v^{i}\triangleq \{v^{i}\mid \mathbf {v} =v^{i}\mathbf {e_{i}} \}} (D.2).

Dual (contravariant) basis of a vector space of dimension n : e i ≜ ≜ { e i : e i ⋅ ⋅ e j = δ δ j i } {\displaystyle \mathbf {e^{i}} \triangleq \{\mathbf {e^{i}} :\mathbf {e^{i}} \cdot \mathbf {e_{j}} =\delta _{j}^{i}\}} (D.3).

Covariant components of a vector v {\displaystyle \mathbf {v} } : v i ≜ ≜ { v i ∣ ∣ v = v i e i } {\displaystyle v_{i}\triangleq \{v_{i}\mid \mathbf {v} =v_{i}\mathbf {e^{i}} \}} (D.4).

Components of the covariant metric tensor : g i j ≜ ≜ e i ⋅ ⋅ e j {\displaystyle g_{ij}\triangleq \mathbf {e_{i}} \cdot \mathbf {e_{j}} } ; the metric tensor can be considered a square matrix, since it only has two covariant indices: G ≜ ≜ { g i j } {\displaystyle G\triangleq \{g_{ij}\}} ; for the commutative property of the dot product, the g i j {\displaystyle g_{ij}} are symmetric (D.5).

Components of the contravariant metric tensor : g i j ≜ ≜ { h i j : G − − 1 = { h i j } } {\displaystyle g^{ij}\triangleq \{h_{ij}:G^{-1}=\{h_{ij}\}\}} ; these are the elements of the inverse of the covariant metric tensor/matrix G − − 1 {\displaystyle G^{-1}} , and for the properties of the inverse of a symmetric matrix, they're also symmetric (D.6).

Corollaries [ edit ] g i j g j k = δ δ k i {\displaystyle g^{ij}g_{jk}=\delta _{k}^{i}} (1).

Proof: from the properties of the inverse matrix (D.6).

e i = g i j e j {\displaystyle \mathbf {e^{i}} =g^{ij}\mathbf {e_{j}} } (2).

Proof: let's suppose that { A i j ∣ ∣ e i = A i j e j } {\displaystyle \{A^{ij}\mid \mathbf {e^{i}} =A^{ij}\mathbf {e_{j}} \}} ; we will show that A i j = g i j {\displaystyle A^{ij}=g^{ij}} . Taking the dot product of both sides with e k {\displaystyle \mathbf {e_{k}} } : e i ⋅ ⋅ e k = ( A i j e j ) ⋅ ⋅ e k → → (D.3,D.5) A i j g j k = δ δ k i {\displaystyle \mathbf {e^{i}} \cdot \mathbf {e_{k}} =(A^{ij}\mathbf {e_{j}} )\cdot \mathbf {e_{k}} {\stackrel {\text{(D.3,D.5)}}{\to }}A^{ij}g_{jk}=\delta _{k}^{i}} ; multiplying both sides by g m k {\displaystyle g^{mk}} : g m k A i j g j k = g m k δ δ k i → → (D.5) g m k g k j A i j = g m i → → (1) δ δ j m A i j = g m i → → A i m = g m i = (D.6) g i m , Q.E.D.

◼ ◼ {\displaystyle g^{mk}A^{ij}g_{jk}=g^{mk}\delta _{k}^{i}{\stackrel {\text{(D.5)}}{\to }}g^{mk}g_{kj}A^{ij}=g^{mi}{\stackrel {\text{(1)}}{\to }}\delta _{j}^{m}A^{ij}=g^{mi}\to A^{im}=g^{mi}{\stackrel {\text{(D.6)}}{=}}g^{im},\ {\text{Q.E.D.}}\quad \blacksquare } e i ⋅ ⋅ e j = g i j {\displaystyle \mathbf {e^{i}} \cdot \mathbf {e^{j}} =g^{ij}} (3).

Proof: e i = (2) g i k e k ; e j = (2) g j m e m → → e i ⋅ ⋅ e j = g i k g j m ( e k ⋅ ⋅ e m ) = (D.5) g i k g j m g k m = (1) δ δ m i g j m = g j i = (D.6) g i j , Q.E.D.

◼ ◼ {\displaystyle \mathbf {e^{i}} {\stackrel {\text{(2)}}{=}}g^{ik}\mathbf {e_{k}} ;\mathbf {e^{j}} {\stackrel {\text{(2)}}{=}}g^{jm}\mathbf {e_{m}} \to \mathbf {e^{i}} \cdot \mathbf {e^{j}} =g^{ik}g^{jm}(\mathbf {e_{k}} \cdot \mathbf {e_{m}} ){\stackrel {\text{(D.5)}}{=}}g^{ik}g^{jm}g_{km}{\stackrel {\text{(1)}}{=}}\delta _{m}^{i}g^{jm}=g^{ji}{\stackrel {\text{(D.6)}}{=}}g^{ij},\ {\text{Q.E.D.}}\quad \blacksquare } e i = g i j e j {\displaystyle \mathbf {e_{i}} =g_{ij}\mathbf {e^{j}} } (4).

Proof: let's suppose that { B i j ∣ ∣ e i = B i j e j } {\displaystyle \{B_{ij}\mid \mathbf {e_{i}} =B_{ij}\mathbf {e^{j}} \}} ; we will show that B i j = g i j {\displaystyle B_{ij}=g_{ij}} . Taking the dot product of both sides with e k {\displaystyle \mathbf {e^{k}} } : e i ⋅ ⋅ e k = ( B i j e j ) ⋅ ⋅ e k → → (D.3,3) B i j g j k = δ δ i k {\displaystyle \mathbf {e_{i}} \cdot \mathbf {e^{k}} =(B_{ij}\mathbf {e^{j}} )\cdot \mathbf {e^{k}} {\stackrel {\text{(D.3,3)}}{\to }}B_{ij}g^{jk}=\delta _{i}^{k}} ; multiplying both sides by g m k {\displaystyle g_{mk}} : g m k B i j g j k = g m k δ δ i k → → (D.5) g m k g k j B i j = g m i → → (1) δ δ m j B i j = g m i → → B i m = g m i = (D.5) g i m , Q.E.D.

◼ ◼ {\displaystyle g_{mk}B_{ij}g^{jk}=g_{mk}\delta _{i}^{k}{\stackrel {\text{(D.5)}}{\to }}g_{mk}g^{kj}B_{ij}=g_{mi}{\stackrel {\text{(1)}}{\to }}\delta _{m}^{j}B_{ij}=g_{mi}\to B_{im}=g_{mi}{\stackrel {\text{(D.5)}}{=}}g_{im},\ {\text{Q.E.D.}}\quad \blacksquare } v i = g i j v j {\displaystyle v^{i}=g^{ij}v_{j}} (5).

Proof: v = (D.2) v i e i ; v = (D.4) v j e j = (2) v j ( g j i e i ) → → v i e i = v j g j i e i , ∀ ∀ i → → v i = g j i v j = (D.6) g i j v j , Q.E.D.

◼ ◼ {\displaystyle \mathbf {v} {\stackrel {\text{(D.2)}}{=}}v^{i}\mathbf {e_{i}} ;\mathbf {v} {\stackrel {\text{(D.4)}}{=}}v_{j}\mathbf {e^{j}} {\stackrel {\text{(2)}}{=}}v_{j}(g^{ji}\mathbf {e_{i}} )\to v^{i}\mathbf {e_{i}} =v_{j}g^{ji}\mathbf {e_{i}} ,\forall i\to v^{i}=g^{ji}v_{j}{\stackrel {\text{(D.6)}}{=}}g^{ij}v_{j},\ {\text{Q.E.D.}}\quad \blacksquare } v i = g i j v j {\displaystyle v_{i}=g_{ij}v^{j}} (6).

Proof: specular to (5).

v i = v ⋅ ⋅ e i {\displaystyle v_{i}=\mathbf {v} \cdot \mathbf {e_{i}} } (7).

Proof: v ⋅ ⋅ e i = (D.4) ( v j e j ) ⋅ ⋅ e i = (D.3) v j δ δ i j = v i , Q.E.D.

◼ ◼ {\displaystyle \mathbf {v} \cdot \mathbf {e_{i}} {\stackrel {\text{(D.4)}}{=}}(v_{j}\mathbf {e^{j}} )\cdot \mathbf {e_{i}} {\stackrel {\text{(D.3)}}{=}}v_{j}\delta _{i}^{j}=v_{i},\ {\text{Q.E.D.}}\quad \blacksquare } v i = v ⋅ ⋅ e i {\displaystyle v^{i}=\mathbf {v} \cdot \mathbf {e^{i}} } (8).

Proof: specular to (7).

u ⋅ ⋅ v = g i j u i v j {\displaystyle \mathbf {u} \cdot \mathbf {v} =g_{ij}u^{i}v^{j}} (9).

Proof: u ⋅ ⋅ v = (D.2) ( u i e i ) ⋅ ⋅ ( v j e j ) = ( e i ⋅ ⋅ e j ) u i v j = (D.5) g i j u i v j , Q.E.D.

◼ ◼ {\displaystyle \mathbf {u} \cdot \mathbf {v} {\stackrel {\text{(D.2)}}{=}}(u^{i}\mathbf {e_{i}} )\cdot (v^{j}\mathbf {e_{j}} )=(\mathbf {e_{i}} \cdot \mathbf {e_{j}} )u^{i}v^{j}{\stackrel {\text{(D.5)}}{=}}g_{ij}u^{i}v^{j},\ {\text{Q.E.D.}}\quad \blacksquare } .

u ⋅ ⋅ v = g i j u i v j {\displaystyle \mathbf {u} \cdot \mathbf {v} =g^{ij}u_{i}v_{j}} (10).

Proof: specular to (9).

Historical and geometrical meaning [ edit ] Aid for explaining the geometrical meaning of covariant and contravariant vector components.

Considering this figure for the case of an Euclidean space with n = 2 {\displaystyle n=2} , since v = O A + O B {\displaystyle \mathbf {v} =\mathbf {OA} +\mathbf {OB} } , if we want to express v {\displaystyle \mathbf {v} } in terms of the covariant basis, we have to multiply the basis vectors by the coefficients v 1 = | O A | | e 1 | , v 2 = | O B | | e 2 | {\displaystyle v^{1}={\frac {\vert \mathbf {OA} \vert }{\vert \mathbf {e_{1}} \vert }},v^{2}={\frac {\vert \mathbf {OB} \vert }{\vert \mathbf {e_{2}} \vert }}} .

With v {\displaystyle \mathbf {v} } and thus O A {\displaystyle \mathbf {OA} } and O B {\displaystyle \mathbf {OB} } fixed, if the module of e i {\displaystyle \mathbf {e_{i}} } increases, the value of the v i {\displaystyle v^{i}} component decreases, and that's why they're called contra -variant (with respect to the variation of the basis vectors module).

Symmetrically, corollary (7) states that the v i {\displaystyle v_{i}} components equal the dot product v ⋅ ⋅ e i {\displaystyle \mathbf {v} \cdot \mathbf {e_{i}} } between the vector and the covariant basis vectors, and since this is directly proportional to the basis vectors module, they're called co -variant.

If we consider the dual (contravariant) basis, the situation is perfectly specular: the covariant components are contra -variant with respect to the module of the dual basis vectors, while the contravariant components are co -variant.

So in the end it all boils down to a matter of convention:  historically the first non- orthonormal basis of the vector space of choice was called "covariant", its dual basis "contravariant", and the corresponding components named specularly.

If the covariant basis becomes orthonormal , the dual contravariant basis aligns with it and the covariant components collapse into the contravariant ones, the most familiar situation when dealing with geometrical Euclidean vectors.

G {\displaystyle G} and G − − 1 {\displaystyle G^{-1}} become the identity matrix I {\displaystyle I} , and: g i j = δ δ i j , g i j = δ δ i j , u ⋅ ⋅ v = δ δ i j u i v j = ∑ ∑ i u i v i = δ δ i j u i v j = ∑ ∑ i u i v i .

{\displaystyle g_{ij}=\delta _{ij},g^{ij}=\delta ^{ij},\mathbf {u} \cdot \mathbf {v} =\delta _{ij}u^{i}v^{j}=\sum _{i}u^{i}v^{i}=\delta ^{ij}u_{i}v_{j}=\sum _{i}u_{i}v_{i}.} If the metric is non-Euclidean, but for instance Minkowskian like in the special relativity and general relativity theories, the basis are never orthonormal, even in the case of special relativity where G {\displaystyle G} and G − − 1 {\displaystyle G^{-1}} become, for n = 4 , η η ≜ ≜ d i a g ( 1 , − − 1 , − − 1 , − − 1 ) {\displaystyle n=4,\ \eta \triangleq diag(1,-1,-1,-1)} . In this scenario, the covariant and contravariant components always differ.

Use in tensor analysis [ edit ] The distinction between covariance and contravariance is particularly important for computations with tensors , which often have mixed variance . This means that they have both covariant and contravariant components, or both vector and covector components. The valence of a tensor is the number of covariant and contravariant terms, and in Einstein notation , covariant components have lower indices, while contravariant components have upper indices.  The duality between covariance and contravariance intervenes whenever a vector or tensor quantity is represented by its components, although modern differential geometry uses more sophisticated index-free methods to represent tensors .

In tensor analysis , a covariant vector varies more or less reciprocally to a corresponding contravariant vector.  Expressions for lengths, areas and volumes of objects in the vector space can then be given in terms of tensors with covariant and contravariant indices. Under simple expansions and contractions of the coordinates, the reciprocity is exact; under affine transformations the components of a vector intermingle on going between covariant and contravariant expression.

On a manifold , a tensor field will typically have multiple, upper and lower indices, where Einstein notation is widely used. When the manifold is equipped with a metric , covariant and contravariant indices become very closely related to one another. Contravariant indices can be turned into covariant indices by contracting with the metric tensor. The reverse is possible by contracting with the (matrix) inverse of the metric tensor. Note that in general, no such relation exists in spaces not endowed with a metric tensor. Furthermore, from a more abstract standpoint, a tensor is simply "there" and its components of either kind are only calculational artifacts whose values depend on the chosen coordinates.

The explanation in geometric terms is that a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the tangent bundle as well as the cotangent bundle .

A contravariant vector is one which transforms like d x μ μ d τ τ {\displaystyle {\frac {dx^{\mu }}{d\tau }}} , where x μ μ {\displaystyle x^{\mu }\!} are the coordinates of a particle at its proper time τ τ {\displaystyle \tau } . A covariant vector is one which transforms like ∂ ∂ φ φ ∂ ∂ x μ μ {\displaystyle {\frac {\partial \varphi }{\partial x^{\mu }}}} , where φ φ {\displaystyle \varphi } is a scalar field.

Algebra and geometry [ edit ] In category theory , there are covariant functors and contravariant functors . The assignment of the dual space to a vector space is a standard example of a contravariant functor. Contravariant (resp. covariant) vectors are contravariant (resp. covariant) functors from a GL ( n ) {\displaystyle {\text{GL}}(n)} - torsor to the fundamental representation of GL ( n ) {\displaystyle {\text{GL}}(n)} .  Similarly, tensors of higher degree are functors with values in other representations of GL ( n ) {\displaystyle {\text{GL}}(n)} .  However, some constructions of multilinear algebra are of "mixed" variance, which prevents them from being functors.

In differential geometry , the components of a vector relative to a basis of the tangent bundle are covariant if they change with the same linear transformation as a change of basis.  They are contravariant if they change by the inverse transformation.  This is sometimes a source of confusion for two distinct but related reasons.  The first is that vectors whose components are covariant (called covectors or 1-forms ) actually pull back under smooth functions, meaning that the operation assigning the space of covectors to a smooth manifold is actually a contravariant functor.  Likewise, vectors whose components are contravariant push forward under smooth mappings, so the operation assigning the space of (contravariant) vectors to a smooth manifold is a covariant functor.  Secondly, in the classical approach to differential geometry, it is not bases of the tangent bundle that are the most primitive object, but rather changes in the coordinate system.  Vectors with contravariant components transform in the same way as changes in the coordinates (because these actually change oppositely to the induced change of basis).  Likewise, vectors with covariant components transform in the opposite way as changes in the coordinates.

See also [ edit ] Active and passive transformation Mixed tensor Two-point tensor , a generalization allowing indices to reference multiple vector bases Notes [ edit ] ^ A basis f may here profitably be viewed as a linear isomorphism from R n to V .  Regarding f as a row vector whose entries are the elements of the basis, the associated linear isomorphism is then x ↦ ↦ f x .

{\displaystyle \mathbf {x} \mapsto \mathbf {f} \mathbf {x} .} Citations [ edit ] ^ Misner, C.; Thorne, K.S.; Wheeler, J.A. (1973).

Gravitation . W.H. Freeman.

ISBN 0-7167-0344-0 .

^ Frankel, Theodore (2012).

The geometry of physics : an introduction . Cambridge: Cambridge University Press. p. 42.

ISBN 978-1-107-60260-1 .

OCLC 739094283 .

^ Sylvester, J.J. (1851).

"On the general theory of associated algebraical forms" .

Cambridge and Dublin Mathematical Journal . Vol. 6. pp.

289– 293.

^ Sylvester, J.J. University Press (16 February 2012).

The collected mathematical papers of James Joseph Sylvester . Vol. 3, 1870– 1883. Cambridge University Press.

ISBN 978-1107661431 .

OCLC 758983870 .

^ J A Schouten (1954).

Ricci calculus (2 ed.). Springer. p. 6.

^ Bowen, Ray; Wang, C.-C. (2008) [1976].

"§3.14 Reciprocal Basis and Change of Basis" .

Introduction to Vectors and Tensors . Dover. pp. 78, 79, 81.

ISBN 9780486469140 .

References [ edit ] Kusse, Bruce R.; Westwig, Erik A. (2010), Mathematical Physics: Applied Mathematics for Scientists and Engineers (2nd ed.), Wiley, ISBN 978-3-527-61814-9 .

Arfken, George B.

; Weber, Hans J. (2005), Mathematical Methods for Physicists (6th ed.), Harcourt, ISBN 0-12-059876-0 .

Dodson, C. T. J.; Poston, T. (1991), Tensor geometry , Graduate Texts in Mathematics, vol. 130 (2nd ed.), Springer, ISBN 978-3-540-52018-4 , MR 1223091 .

Greub, Werner Hildbert (1967), Multilinear algebra , Die Grundlehren der Mathematischen Wissenschaften, Band 136, Springer, Bibcode : 1967mual.book.....G , ISBN 9780387038278 , MR 0224623 .

Sternberg, Shlomo (1983), Lectures on differential geometry , Chelsea, ISBN 978-0-8284-0316-0 .

Sylvester, J.J. (1853), "On a Theory of the Syzygetic Relations of Two Rational Integral Functions, Comprising an Application to the Theory of Sturm's Functions, and That of the Greatest Algebraical Common Measure" , Philosophical Transactions of the Royal Society of London , 143 : 407– 548, doi : 10.1098/rstl.1853.0018 , JSTOR 108572 .

Weinreich, Gabriel (1998), Geometrical Vectors , Chicago Lectures in Physics, The University of Chicago Press, p. 126, ISBN 9780226890487 External links [ edit ] "Covariant tensor" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] "Contravariant tensor" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Weisstein, Eric W.

"Covariant Tensor" .

MathWorld .

Weisstein, Eric W.

"Contravariant Tensor" .

MathWorld .

Invariance, Contravariance, and Covariance Dullemond, Kees; Peeters, Kasper (2010).

"Introduction to tensor calculus" (PDF) .

v t e Tensors Glossary of tensor theory Scope Mathematics Coordinate system Differential geometry Dyadic algebra Euclidean geometry Exterior calculus Multilinear algebra Tensor algebra Tensor calculus Physics Engineering Computer vision Continuum mechanics Electromagnetism General relativity Transport phenomena Notation Abstract index notation Einstein notation Index notation Multi-index notation Penrose graphical notation Ricci calculus Tetrad (index notation) Van der Waerden notation Voigt notation Tensor definitions Tensor (intrinsic definition) Tensor field Tensor density Tensors in curvilinear coordinates Mixed tensor Antisymmetric tensor Symmetric tensor Tensor operator Tensor bundle Two-point tensor Operations Covariant derivative Exterior covariant derivative Exterior derivative Exterior product Hodge star operator Lie derivative Raising and lowering indices Symmetrization Tensor contraction Tensor product Transpose (2nd-order tensors) Related abstractions Affine connection Basis Cartan formalism (physics) Connection form Covariance and contravariance of vectors Differential form Dimension Exterior form Fiber bundle Geodesic Levi-Civita connection Linear map Manifold Matrix Multivector Pseudotensor Spinor Vector Vector space Notable tensors Mathematics Kronecker delta Levi-Civita symbol Metric tensor Nonmetricity tensor Ricci curvature Riemann curvature tensor Torsion tensor Weyl tensor Physics Moment of inertia Angular momentum tensor Spin tensor Cauchy stress tensor stress–energy tensor Einstein tensor EM tensor Gluon field strength tensor Metric tensor (GR) Mathematicians Élie Cartan Augustin-Louis Cauchy Elwin Bruno Christoffel Albert Einstein Leonhard Euler Carl Friedrich Gauss Hermann Grassmann Tullio Levi-Civita Gregorio Ricci-Curbastro Bernhard Riemann Jan Arnoldus Schouten Woldemar Voigt Hermann Weyl NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐gtwkj
Cached time: 20250812003119
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.722 seconds
Real time usage: 0.922 seconds
Preprocessor visited node count: 3147/1000000
Revision size: 43085/2097152 bytes
Post‐expand include size: 65504/2097152 bytes
Template argument size: 3516/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 74511/5000000 bytes
Lua time usage: 0.358/10.000 seconds
Lua memory usage: 6243950/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  613.481      1 -total
 23.97%  147.047      2 Template:Reflist
 21.29%  130.580      6 Template:Cite_book
 21.01%  128.884      3 Template:Navbox
 16.38%  100.475      1 Template:Short_description
 15.60%   95.674      1 Template:Tensors
 11.20%   68.740      2 Template:Pagetype
  8.27%   50.747      7 Template:Citation
  6.66%   40.856      3 Template:NumBlk
  5.69%   34.919      1 Template:About Saved in parser cache with key enwiki:pcache:202886:|#|:idhash:canonical and timestamp 20250812003119 and revision id 1300918192. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Covariance_and_contravariance_of_vectors&oldid=1300918192 " Categories : Tensors Differential geometry Riemannian geometry Vectors (mathematics and physics) Hidden categories: CS1: long volume value Articles with short description Short description is different from Wikidata Articles needing more detailed references This page was last edited on 17 July 2025, at 03:26 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Covariance and contravariance of vectors 19 languages Add topic

