Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Strategies 2 Example 3 Advantages 4 Disadvantages 5 Mean and standard error 6 Sample size allocation 7 See also 8 References 9 Further reading Toggle the table of contents Stratified sampling 20 languages العربية বাংলা Català Deutsch Español فارسی Français 한국어 Hrvatski Italiano 日本語 Norsk bokmål Norsk nynorsk Polski Português Română Русский Türkçe Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Sampling from a population which can be partitioned into subpopulations This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Stratified sampling" – news · newspapers · books · scholar · JSTOR ( December 2020 ) ( Learn how and when to remove this message ) In statistics , stratified sampling is a method of sampling from a population which can be partitioned into subpopulations .

Stratified sampling example In statistical surveys , when subpopulations within an overall population vary, it could be advantageous to sample each subpopulation ( stratum ) independently.

Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should define a partition of the population. That is, it should be collectively exhaustive and mutually exclusive : every element in the population must be assigned to one and only one stratum. Then sampling is done in each stratum, for example: by simple random sampling . The objective is to improve the precision of the sample by reducing sampling error . It can produce a weighted mean that has less variability than the arithmetic mean of a simple random sample of the population.

In computational statistics , stratified sampling is a method of variance reduction when Monte Carlo methods are used to estimate population statistics from a known population.

[ 1 ] Strategies [ edit ] Proportionate allocation uses a sampling fraction in each of the strata that are proportional to that of the total population. For instance, if the population consists of n total individuals, m of which are male and f female (and where m + f = n ), then the relative size of the two samples ( x 1 = m / n males, x 2 = f / n females) should reflect this proportion.

Optimum allocation (or disproportionate allocation ) – The sampling fraction of each stratum is proportionate to both the proportion (as above) and the standard deviation of the distribution of the variable. Larger samples are taken in the strata with the greatest variability to generate the least possible overall sampling variance.

Neyman allocation is a strategy of this type.

A real-world example of using stratified sampling would be for a political survey . If the respondents needed to reflect the diversity of the population, the researcher would specifically seek to include participants of various minority groups such as race or religion, based on their proportionality to the total population as mentioned above. A stratified survey could thus claim to be more representative of the population than a survey of simple random sampling or systematic sampling . Both mean and variance can be corrected for disproportionate sampling costs using stratified sample sizes .

Example [ edit ] Assume that we need to estimate the average number of votes for each candidate in an election. Assume that a country has 3 towns: Town A has 1 million factory workers, Town B has 2 million office workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over the entire population but there is some chance that the resulting random sample is poorly balanced across these towns and hence is biased, causing a significant error in estimation (when the outcome of interest has a different distribution, in terms of the parameter of interest, between the towns). Instead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively, then we can produce a smaller error in estimation for the same total sample size. This method is generally used when a population is not a homogeneous group.

Advantages [ edit ] The reasons to use stratified sampling rather than simple random sampling include [ 2 ] If measurements within strata have a lower standard deviation (as compared to the overall standard deviation in the population), stratification gives a smaller error in estimation.

For many applications, measurements become more manageable and/or cheaper when the population is grouped into strata.

When it is desirable to have estimates of the population parameters for groups within the population – stratified sampling verifies we have enough samples from the strata of interest.

If the population density varies greatly within a region, stratified sampling will ensure that estimates can be made with equal accuracy in different parts of the region, and that comparisons of sub-regions can be made with equal statistical power . For example, in Ontario a survey taken throughout the province might use a larger sampling fraction in the less populated north, since the disparity in population between north and south is so great that a sampling fraction based on the provincial sample as a whole might result in the collection of only a handful of data from the north.

Disadvantages [ edit ] It would be a misapplication of the technique to make subgroups' sample sizes proportional to the amount of data available from the subgroups, rather than scaling sample sizes to subgroup sizes (or to their variances, if known to vary significantly—e.g. using an F test ). Data representing each subgroup are taken to be of equal importance if suspected variation among them warrants stratified sampling. If subgroup variances differ significantly and the data needs to be stratified by variance, it is not possible to simultaneously make each subgroup sample size proportional to subgroup size within the total population. For an efficient way to partition sampling resources among groups that vary in their means, variance and costs, see "optimum allocation" .
The problem of stratified sampling in the case of unknown class priors (ratio of subpopulations in the entire population) can have a deleterious effect on the performance of any analysis on the dataset, e.g. classification.

[ 3 ] In that regard, minimax sampling ratio can be used to make the dataset robust with respect to uncertainty in the underlying data generating process.

[ 3 ] Combining sub-strata to ensure adequate numbers can lead to Simpson's paradox , where trends that exist in different groups of data disappear or even reverse when the groups are combined.

Mean and standard error [ edit ] The mean and variance of stratified random sampling are given by: [ 2 ] x ¯ ¯ = 1 N ∑ ∑ h = 1 L N h x ¯ ¯ h {\displaystyle {\bar {x}}={\frac {1}{N}}\sum _{h=1}^{L}N_{h}{\bar {x}}_{h}} s x ¯ ¯ 2 = ∑ ∑ h = 1 L ( N h N ) 2 ( N h − − n h N h − − 1 ) s h 2 n h {\displaystyle s_{\bar {x}}^{2}=\sum _{h=1}^{L}\left({\frac {N_{h}}{N}}\right)^{2}\left({\frac {N_{h}-n_{h}}{N_{h}-1}}\right){\frac {s_{h}^{2}}{n_{h}}}} where L = {\displaystyle L={}} number of strata N = {\displaystyle N={}} the sum of all stratum sizes N h = {\displaystyle N_{h}={}} size of stratum h {\displaystyle h} x ¯ ¯ h = {\displaystyle {\bar {x}}_{h}={}} sample mean of stratum h {\displaystyle h} n h = {\displaystyle n_{h}={}} number of observations in stratum h {\displaystyle h} s h = {\displaystyle s_{h}={}} sample standard deviation of stratum h {\displaystyle h} Note that the term ( N h − − n h ) / ( N h − − 1 ) {\displaystyle (N_{h}-n_{h})/(N_{h}-1)} , which equals 1 − − n h − − 1 N h − − 1 {\displaystyle 1-{\frac {n_{h}-1}{N_{h}-1}}} , is a finite population correction and N h {\displaystyle N_{h}} must be expressed in "sample units". Forgoing the finite population correction gives: s x ¯ ¯ 2 = ∑ ∑ h = 1 L ( N h N ) 2 s h 2 n h {\displaystyle s_{\bar {x}}^{2}=\sum _{h=1}^{L}\left({\frac {N_{h}}{N}}\right)^{2}{\frac {s_{h}^{2}}{n_{h}}}} where the w h = N h / N {\displaystyle w_{h}=N_{h}/N} is the population weight of stratum h {\displaystyle h} .

Sample size allocation [ edit ] For the proportional allocation strategy, the size of the sample in each stratum is taken in proportion to the size of the stratum. Suppose that in a company there are the following staff: [ 4 ] male, full-time: 90 male, part-time: 18 female, full-time: 9 female, part-time: 63 total: 180 and we are asked to take a sample of 40 staff, stratified according to the above categories.

The first step is to calculate the percentage of each group of the total.

% male, full-time = 90 ÷ 180 = 50% % male, part-time = 18 ÷ 180 = 10% % female, full-time = 9 ÷ 180 = 5% % female, part-time = 63 ÷ 180 = 35% This tells us that of our sample of 40, 50% (20 individuals) should be male, full-time.

10% (4 individuals) should be male, part-time.

5%  (2 individuals) should be female, full-time.

35% (14 individuals) should be female, part-time.

Another easy way without having to calculate the percentage is to multiply each group size by the sample size and divide by the total population size (size of entire staff): male, full-time = 90 × (40 ÷ 180) = 20 male, part-time = 18 × (40 ÷ 180) = 4 female, full-time = 9 × (40 ÷ 180) = 2 female, part-time = 63 × (40 ÷ 180) = 14 See also [ edit ] Mathematics portal Opinion poll Multistage sampling Statistical benchmarking Stratified sample size Stratification (clinical trials) References [ edit ] ^ Botev, Z.; Ridder, A. (2017). "Variance Reduction".

Wiley StatsRef: Statistics Reference Online . pp.

1– 6.

doi : 10.1002/9781118445112.stat07975 .

hdl : 1959.4/unsworks_50616 .

ISBN 9781118445112 .

^ a b "6.1 How to Use Stratified Sampling | STAT 506" .

Pennstate: Statistics Online Courses . Retrieved 2015-07-23 .

^ a b Shahrokh Esfahani, Mohammad; Dougherty, Edward R.

(2014).

"Effect of separate sampling on classification accuracy" .

Bioinformatics .

30 (2): 242– 250.

doi : 10.1093/bioinformatics/btt662 .

PMID 24257187 .

^ Hunt, Neville; Tyrrell, Sidney (2001).

"Stratified Sampling" .

Webpage at Coventry University . Archived from the original on 13 October 2013 . Retrieved 12 July 2012 .

Further reading [ edit ] Särndal, Carl-Erik; et al. (2003). "Stratified Sampling".

Model Assisted Survey Sampling . New York: Springer. pp.

100– 109.

ISBN 0-387-40620-4 .

v t e Seven basic tools of quality Cause-and-effect diagram Check sheet Control chart Histogram Pareto chart Scatter diagram Stratification Quality (business) v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Stratified_sampling&oldid=1303141865 " Categories : Sampling (statistics) Sampling techniques Variance reduction Hidden categories: Articles with short description Short description is different from Wikidata Articles needing additional references from December 2020 All articles needing additional references This page was last edited on 29 July 2025, at 08:46 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Stratified sampling 20 languages Add topic

