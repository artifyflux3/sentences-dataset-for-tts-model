Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Statement of the inequality 2 Essential form of the inequality 3 Relationship to the Brunn–Minkowski inequality 4 Applications in probability and statistics Toggle Applications in probability and statistics subsection 4.1 Log-concave distributions 4.2 Applications to concentration of measure 5 References 6 Further reading Toggle the table of contents Prékopa–Leindler inequality 1 language Italiano Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In mathematics , the Prékopa–Leindler inequality is an integral inequality closely related to the reverse Young's inequality , the Brunn–Minkowski inequality and a number of other important and classical inequalities in analysis . The result is named after the Hungarian mathematicians András Prékopa and László Leindler .

[ 1 ] [ 2 ] Statement of the inequality [ edit ] Let 0 < λ < 1 and let f , g , h : R n → [0, +∞) be non- negative real-valued measurable functions defined on n -dimensional Euclidean space R n . Suppose that these functions satisfy h ( ( 1 − − λ λ ) x + λ λ y ) ≥ ≥ f ( x ) 1 − − λ λ g ( y ) λ λ {\displaystyle h\left((1-\lambda )x+\lambda y\right)\geq f(x)^{1-\lambda }g(y)^{\lambda }} 1 for all x and y in R n . Then ‖ ‖ h ‖ ‖ 1 := ∫ ∫ R n h ( x ) d x ≥ ≥ ( ∫ ∫ R n f ( x ) d x ) 1 − − λ λ ( ∫ ∫ R n g ( x ) d x ) λ λ =: ‖ ‖ f ‖ ‖ 1 1 − − λ λ ‖ ‖ g ‖ ‖ 1 λ λ .

{\displaystyle \|h\|_{1}:=\int _{\mathbb {R} ^{n}}h(x)\,\mathrm {d} x\geq \left(\int _{\mathbb {R} ^{n}}f(x)\,\mathrm {d} x\right)^{1-\lambda }\left(\int _{\mathbb {R} ^{n}}g(x)\,\mathrm {d} x\right)^{\lambda }=:\|f\|_{1}^{1-\lambda }\|g\|_{1}^{\lambda }.} Essential form of the inequality [ edit ] Recall that the essential supremum of a measurable function f : R n → R is defined by e s s s u p x ∈ ∈ R n ⁡ ⁡ f ( x ) = inf { t ∈ ∈ [ − − ∞ ∞ , + ∞ ∞ ] ∣ ∣ f ( x ) ≤ ≤ t for almost all x ∈ ∈ R n } .

{\displaystyle \mathop {\mathrm {ess\,sup} } _{x\in \mathbb {R} ^{n}}f(x)=\inf \left\{t\in [-\infty ,+\infty ]\mid f(x)\leq t{\text{ for almost all }}x\in \mathbb {R} ^{n}\right\}.} This notation allows the following essential form of the Prékopa–Leindler inequality: let 0 < λ < 1 and let f , g ∈ L 1 ( R n ; [0, +∞)) be non-negative absolutely integrable functions. Let s ( x ) = e s s s u p y ∈ ∈ R n ⁡ ⁡ f ( x − − y 1 − − λ λ ) 1 − − λ λ g ( y λ λ ) λ λ .

{\displaystyle s(x)=\mathop {\mathrm {ess\,sup} } _{y\in \mathbb {R} ^{n}}f\left({\frac {x-y}{1-\lambda }}\right)^{1-\lambda }g\left({\frac {y}{\lambda }}\right)^{\lambda }.} Then s is measurable and ‖ ‖ s ‖ ‖ 1 ≥ ≥ ‖ ‖ f ‖ ‖ 1 1 − − λ λ ‖ ‖ g ‖ ‖ 1 λ λ .

{\displaystyle \|s\|_{1}\geq \|f\|_{1}^{1-\lambda }\|g\|_{1}^{\lambda }.} The essential supremum form was given by Herm Brascamp and Elliott Lieb .

[ 3 ] Its use can change the left side of the inequality. For example, a function g that takes the value 1 at exactly one point will not usually yield a zero left side in the "non-essential sup" form but it will always yield a zero left side in the "essential sup" form.

Relationship to the Brunn–Minkowski inequality [ edit ] It can be shown that the usual Prékopa–Leindler inequality implies the Brunn–Minkowski inequality in the following form: if 0 < λ < 1 and A and B are bounded , measurable subsets of R n such that the Minkowski sum (1 − λ ) A + λ B is also measurable, then μ μ ( ( 1 − − λ λ ) A + λ λ B ) ≥ ≥ μ μ ( A ) 1 − − λ λ μ μ ( B ) λ λ , {\displaystyle \mu \left((1-\lambda )A+\lambda B\right)\geq \mu (A)^{1-\lambda }\mu (B)^{\lambda },} where μ denotes n -dimensional Lebesgue measure . Hence, the Prékopa–Leindler inequality can also be used [ 4 ] to prove the Brunn–Minkowski inequality in its more familiar form: if 0 < λ < 1 and A and B are non- empty , bounded , measurable subsets of R n such that (1 − λ ) A + λ B is also measurable, then μ μ ( ( 1 − − λ λ ) A + λ λ B ) 1 / n ≥ ≥ ( 1 − − λ λ ) μ μ ( A ) 1 / n + λ λ μ μ ( B ) 1 / n .

{\displaystyle \mu \left((1-\lambda )A+\lambda B\right)^{1/n}\geq (1-\lambda )\mu (A)^{1/n}+\lambda \mu (B)^{1/n}.} Applications in probability and statistics [ edit ] Log-concave distributions [ edit ] The Prékopa–Leindler inequality is useful in the theory of log-concave distributions , as it can be used to show that log-concavity is preserved by marginalization and independent summation of log-concave distributed random variables. Since, if X , Y {\displaystyle X,Y} have pdf f , g {\displaystyle f,g} , and X , Y {\displaystyle X,Y} are independent, then f ⋆ ⋆ g {\displaystyle f\star g} is the pdf of X + Y {\displaystyle X+Y} , we also have that the convolution of two log-concave functions is log-concave.

Suppose that H ( x , y ) is a log-concave distribution for ( x , y ) ∈ R m × R n , so that by definition we have H ( ( 1 − − λ λ ) ( x 1 , y 1 ) + λ λ ( x 2 , y 2 ) ) ≥ ≥ H ( x 1 , y 1 ) 1 − − λ λ H ( x 2 , y 2 ) λ λ , {\displaystyle H\left((1-\lambda )(x_{1},y_{1})+\lambda (x_{2},y_{2})\right)\geq H(x_{1},y_{1})^{1-\lambda }H(x_{2},y_{2})^{\lambda },} 2 and let M ( y ) denote the marginal distribution obtained by integrating over x : M ( y ) = ∫ ∫ R m H ( x , y ) d x .

{\displaystyle M(y)=\int _{\mathbb {R} ^{m}}H(x,y)\,dx.} Let y 1 , y 2 ∈ R n and 0 < λ < 1 be given. Then equation ( 2 ) satisfies condition ( 1 ) with h ( x ) = H ( x ,(1 − λ )y 1 + λy 2 ), f ( x ) = H ( x , y 1 ) and g ( x ) = H ( x , y 2 ), so the Prékopa–Leindler inequality applies. It can be written in terms of M as M ( ( 1 − − λ λ ) y 1 + λ λ y 2 ) ≥ ≥ M ( y 1 ) 1 − − λ λ M ( y 2 ) λ λ , {\displaystyle M((1-\lambda )y_{1}+\lambda y_{2})\geq M(y_{1})^{1-\lambda }M(y_{2})^{\lambda },} which is the definition of log-concavity for M .

To see how this implies the preservation of log-convexity by independent sums, suppose that X and Y are independent random variables with log-concave distribution. Since the product of two log-concave functions is log-concave, the joint distribution of ( X , Y ) is also log-concave. Log-concavity is preserved by affine changes of coordinates, so the distribution of ( X + Y , X − Y ) is log-concave as well. Since the distribution of X+Y is a marginal over the joint distribution of ( X + Y , X − Y ), we conclude that X + Y has a log-concave distribution.

Applications to concentration of measure [ edit ] The Prékopa–Leindler inequality can be used to prove results about concentration of measure.

Theorem [ citation needed ] Let A ⊆ ⊆ R n {\textstyle A\subseteq \mathbb {R} ^{n}} , and set A ϵ ϵ = { x : d ( x , A ) < ϵ ϵ } {\textstyle A_{\epsilon }=\{x:d(x,A)<\epsilon \}} . Let γ γ ( x ) {\textstyle \gamma (x)} denote the standard Gaussian pdf, and μ μ {\textstyle \mu } its associated measure. Then μ μ ( A ϵ ϵ ) ≥ ≥ 1 − − e − − ϵ ϵ 2 / 4 μ μ ( A ) {\textstyle \mu (A_{\epsilon })\geq 1-{\frac {e^{-\epsilon ^{2}/4}}{\mu (A)}}} .

Proof of concentration of measure The proof of this theorem goes by way of the following lemma: Lemma In the notation of the theorem, ∫ ∫ R n exp ⁡ ⁡ ( d ( x , A ) 2 / 4 ) d μ μ ≤ ≤ 1 / μ μ ( A ) {\textstyle \int _{\mathbb {R} ^{n}}\exp(d(x,A)^{2}/4)d\mu \leq 1/\mu (A)} .

This lemma can be proven from Prékopa–Leindler by taking h ( x ) = γ γ ( x ) , f ( x ) = e d ( x , A ) 2 4 γ γ ( x ) , g ( x ) = 1 A ( x ) γ γ ( x ) {\textstyle h(x)=\gamma (x),f(x)=e^{\frac {d(x,A)^{2}}{4}}\gamma (x),g(x)=1_{A}(x)\gamma (x)} and λ λ = 1 / 2 {\textstyle \lambda =1/2} . To verify the hypothesis of the inequality, h ( x + y 2 ) ≥ ≥ f ( x ) g ( y ) {\textstyle h({\frac {x+y}{2}})\geq {\sqrt {f(x)g(y)}}} , note that we only need to consider y ∈ ∈ A {\textstyle y\in A} , in which case d ( x , A ) ≤ ≤ | | x − − y | | {\textstyle d(x,A)\leq ||x-y||} . This allows us to calculate: ( 2 π π ) n f ( x ) g ( x ) = exp ⁡ ⁡ ( d ( x , A ) 4 − − | | x | | 2 / 2 − − | | y | | 2 / 2 ) ≤ ≤ exp ⁡ ⁡ ( | | x − − y | | 2 4 − − | | x | | 2 / 2 − − | | y | | 2 / 2 ) = exp ⁡ ⁡ ( − − | | x + y 2 | | 2 ) = ( 2 π π ) n h ( x + y 2 ) 2 .

{\displaystyle (2\pi )^{n}f(x)g(x)=\exp({\frac {d(x,A)}{4}}-||x||^{2}/2-||y||^{2}/2)\leq \exp({\frac {||x-y||^{2}}{4}}-||x||^{2}/2-||y||^{2}/2)=\exp(-||{\frac {x+y}{2}}||^{2})=(2\pi )^{n}h({\frac {x+y}{2}})^{2}.} Since ∫ ∫ h ( x ) d x = 1 {\textstyle \int h(x)dx=1} , the PL-inequality immediately gives the lemma.

To conclude the concentration inequality from the lemma, note that on R n ∖ ∖ A ϵ ϵ {\textstyle \mathbb {R} ^{n}\setminus A_{\epsilon }} , d ( x , A ) > ϵ ϵ {\textstyle d(x,A)>\epsilon } , so we have ∫ ∫ R n exp ⁡ ⁡ ( d ( x , A ) 2 / 4 ) d μ μ ≥ ≥ ( 1 − − μ μ ( A ϵ ϵ ) ) exp ⁡ ⁡ ( ϵ ϵ 2 / 4 ) {\textstyle \int _{\mathbb {R} ^{n}}\exp(d(x,A)^{2}/4)d\mu \geq (1-\mu (A_{\epsilon }))\exp(\epsilon ^{2}/4)} . Applying the lemma and rearranging proves the result.

References [ edit ] ^ Prékopa, András (1971).

"Logarithmic concave measures with application to stochastic programming" (PDF) .

Acta Sci. Math.

32 : 301– 316.

^ Prékopa, András (1973).

"On logarithmic concave measures and functions" (PDF) .

Acta Sci. Math.

34 : 335– 343.

^ Herm Jan Brascamp ; Elliott H. Lieb (1976).

"On extensions of the Brunn–Minkowski and Prekopa–Leindler theorems, including inequalities for log concave functions and with an application to the diffusion equation" .

Journal of Functional Analysis .

22 (4): 366– 389.

doi : 10.1016/0022-1236(76)90004-5 .

^ Gardner, Richard J. (2002).

"The Brunn–Minkowski inequality" (PDF) .

Bull. Amer. Math. Soc. (N.S.) .

39 (3): 355–405 (electronic).

doi : 10.1090/S0273-0979-02-00941-2 .

ISSN 0273-0979 .

Further reading [ edit ] Eaton, Morris L. (1987). "Log concavity and related topics".

Lectures on Topics in Probability Inequalities . Amsterdam. pp.

77– 109.

ISBN 90-6196-316-8 .

{{ cite book }} :  CS1 maint: location missing publisher ( link ) Wainwright, Martin J. (2019). "Concentration of Measure".

High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge University Press. pp.

72– 76.

ISBN 978-1-108-49802-9 .

v t e Lp spaces Basic concepts Banach & Hilbert spaces L p spaces Measure Lebesgue Measure space Measurable space / function Minkowski distance Sequence spaces L 1 spaces Integrable function Lebesgue integration Taxicab geometry L 2 spaces Bessel's Cauchy–Schwarz Euclidean distance Hilbert space Parseval's identity Polarization identity Pythagorean theorem Square-integrable function L ∞ ∞ {\displaystyle L^{\infty }} spaces Bounded function Chebyshev distance Infimum and supremum Essential Uniform norm Maps Almost everywhere Convergence almost everywhere Convergence in measure Function space Integral transform Locally integrable function Measurable function Symmetric decreasing rearrangement Inequalities Babenko–Beckner Chebyshev's Clarkson's Hanner's Hausdorff–Young Hölder's Markov's Minkowski Young's convolution Results Marcinkiewicz interpolation theorem Plancherel theorem Riemann–Lebesgue Riesz–Fischer theorem Riesz–Thorin theorem For Lebesgue measure Isoperimetric inequality Brunn–Minkowski theorem Milman's reverse Minkowski–Steiner formula Prékopa–Leindler inequality Vitale's random Brunn–Minkowski inequality Applications & related Bochner space Fourier analysis Lorentz space Probability theory Quasinorm Real analysis Sobolev space *-algebra C*-algebra Von Neumann v t e Measure theory Basic concepts Absolute continuity of measures Lebesgue integration L p spaces Measure Measure space Probability space Measurable space / function Sets Almost everywhere Atom Baire set Borel set equivalence relation Borel space Carathéodory's criterion Cylindrical σ-algebra Cylinder set 𝜆-system Essential range infimum/supremum Locally measurable π -system σ-algebra Non-measurable set Vitali set Null set Support Transverse measure Universally measurable Types of measures Atomic Baire Banach Besov Borel Brown Complex Complete Content ( Logarithmically ) Convex Decomposable Discrete Equivalent Finite Inner ( Quasi- ) Invariant Locally finite Maximising Metric outer Outer Perfect Pre-measure ( Sub- ) Probability Projection-valued Radon Random Regular Borel regular Inner regular Outer regular Saturated Set function σ-finite s-finite Signed Singular Spectral Strictly positive Tight Vector Particular measures Counting Dirac Euler Gaussian Haar Harmonic Hausdorff Intensity Lebesgue Infinite-dimensional Logarithmic Product Projections Pushforward Spherical measure Tangent Trivial Young Maps Measurable function Bochner Strongly Weakly Convergence: almost everywhere of measures in measure of random variables in distribution in probability Cylinder set measure Random: compact set element measure process variable vector Projection-valued measure Main results Carathéodory's extension theorem Convergence theorems Dominated Monotone Vitali Decomposition theorems Hahn Jordan Maharam's Egorov's Fatou's lemma Fubini's Fubini–Tonelli Hölder's inequality Minkowski inequality Radon–Nikodym Riesz–Markov–Kakutani representation theorem Other results Disintegration theorem Lifting theory Lebesgue's density theorem Lebesgue differentiation theorem Sard's theorem Vitali–Hahn–Saks theorem For Lebesgue measure Isoperimetric inequality Brunn–Minkowski theorem Milman's reverse Minkowski–Steiner formula Prékopa–Leindler inequality Vitale's random Brunn–Minkowski inequality Applications & related Convex analysis Descriptive set theory Probability theory Real analysis Spectral theory NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐hk8r7
Cached time: 20250812013003
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.351 seconds
Real time usage: 0.517 seconds
Preprocessor visited node count: 1270/1000000
Revision size: 10360/2097152 bytes
Post‐expand include size: 61327/2097152 bytes
Template argument size: 1447/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 43334/5000000 bytes
Lua time usage: 0.170/10.000 seconds
Lua memory usage: 4718648/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  331.442      1 -total
 34.27%  113.582      1 Template:Reflist
 30.04%   99.556      4 Template:Navbox
 29.53%   97.862      4 Template:Cite_journal
 24.21%   80.250      1 Template:Lp_spaces
 16.04%   53.174      2 Template:NumBlk
 12.48%   41.375      1 Template:Citation_needed
 10.73%   35.563      1 Template:Fix
  7.28%   24.136      2 Template:Category_handler
  4.64%   15.365      1 Template:Measure_theory Saved in parser cache with key enwiki:pcache:10570298:|#|:idhash:canonical and timestamp 20250812013003 and revision id 1286472681. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Prékopa–Leindler_inequality&oldid=1286472681 " Categories : Geometric inequalities Integral geometry Real analysis Theorems in mathematical analysis Theorems in measure theory Hidden categories: All articles with unsourced statements Articles with unsourced statements from September 2020 CS1 maint: location missing publisher This page was last edited on 20 April 2025, at 03:09 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Prékopa–Leindler inequality 1 language Add topic

