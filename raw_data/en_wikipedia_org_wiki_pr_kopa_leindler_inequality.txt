Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Statement of the inequality 2 Essential form of the inequality 3 Relationship to the Brunnâ€“Minkowski inequality 4 Applications in probability and statistics Toggle Applications in probability and statistics subsection 4.1 Log-concave distributions 4.2 Applications to concentration of measure 5 References 6 Further reading Toggle the table of contents PrÃ©kopaâ€“Leindler inequality 1 language Italiano Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia In mathematics , the PrÃ©kopaâ€“Leindler inequality is an integral inequality closely related to the reverse Young's inequality , the Brunnâ€“Minkowski inequality and a number of other important and classical inequalities in analysis . The result is named after the Hungarian mathematicians AndrÃ¡s PrÃ©kopa and LÃ¡szlÃ³ Leindler .

[ 1 ] [ 2 ] Statement of the inequality [ edit ] Let 0Â < Î» <Â 1 and let f , g , h : R n â†’Â [0,Â +âˆ) be non- negative real-valued measurable functions defined on n -dimensional Euclidean space R n . Suppose that these functions satisfy h ( ( 1 âˆ’ âˆ’ Î» Î» ) x + Î» Î» y ) â‰¥ â‰¥ f ( x ) 1 âˆ’ âˆ’ Î» Î» g ( y ) Î» Î» {\displaystyle h\left((1-\lambda )x+\lambda y\right)\geq f(x)^{1-\lambda }g(y)^{\lambda }} 1 for all x and y in R n . Then â€– â€– h â€– â€– 1 := âˆ« âˆ« R n h ( x ) d x â‰¥ â‰¥ ( âˆ« âˆ« R n f ( x ) d x ) 1 âˆ’ âˆ’ Î» Î» ( âˆ« âˆ« R n g ( x ) d x ) Î» Î» =: â€– â€– f â€– â€– 1 1 âˆ’ âˆ’ Î» Î» â€– â€– g â€– â€– 1 Î» Î» .

{\displaystyle \|h\|_{1}:=\int _{\mathbb {R} ^{n}}h(x)\,\mathrm {d} x\geq \left(\int _{\mathbb {R} ^{n}}f(x)\,\mathrm {d} x\right)^{1-\lambda }\left(\int _{\mathbb {R} ^{n}}g(x)\,\mathrm {d} x\right)^{\lambda }=:\|f\|_{1}^{1-\lambda }\|g\|_{1}^{\lambda }.} Essential form of the inequality [ edit ] Recall that the essential supremum of a measurable function f : R n â†’ R is defined by e s s s u p x âˆˆ âˆˆ R n â¡ â¡ f ( x ) = inf { t âˆˆ âˆˆ [ âˆ’ âˆ’ âˆ âˆ , + âˆ âˆ ] âˆ£ âˆ£ f ( x ) â‰¤ â‰¤ t for almost all x âˆˆ âˆˆ R n } .

{\displaystyle \mathop {\mathrm {ess\,sup} } _{x\in \mathbb {R} ^{n}}f(x)=\inf \left\{t\in [-\infty ,+\infty ]\mid f(x)\leq t{\text{ for almost all }}x\in \mathbb {R} ^{n}\right\}.} This notation allows the following essential form of the PrÃ©kopaâ€“Leindler inequality: let 0Â < Î» <Â 1 and let f , g âˆˆ L 1 ( R n ;Â [0,Â +âˆ)) be non-negative absolutely integrable functions. Let s ( x ) = e s s s u p y âˆˆ âˆˆ R n â¡ â¡ f ( x âˆ’ âˆ’ y 1 âˆ’ âˆ’ Î» Î» ) 1 âˆ’ âˆ’ Î» Î» g ( y Î» Î» ) Î» Î» .

{\displaystyle s(x)=\mathop {\mathrm {ess\,sup} } _{y\in \mathbb {R} ^{n}}f\left({\frac {x-y}{1-\lambda }}\right)^{1-\lambda }g\left({\frac {y}{\lambda }}\right)^{\lambda }.} Then s is measurable and â€– â€– s â€– â€– 1 â‰¥ â‰¥ â€– â€– f â€– â€– 1 1 âˆ’ âˆ’ Î» Î» â€– â€– g â€– â€– 1 Î» Î» .

{\displaystyle \|s\|_{1}\geq \|f\|_{1}^{1-\lambda }\|g\|_{1}^{\lambda }.} The essential supremum form was given by Herm Brascamp and Elliott Lieb .

[ 3 ] Its use can change the left side of the inequality. For example, a function g that takes the value 1 at exactly one point will not usually yield a zero left side in the "non-essential sup" form but it will always yield a zero left side in the "essential sup" form.

Relationship to the Brunnâ€“Minkowski inequality [ edit ] It can be shown that the usual PrÃ©kopaâ€“Leindler inequality implies the Brunnâ€“Minkowski inequality in the following form: if 0Â < Î» <Â 1 and A and B are bounded , measurable subsets of R n such that the Minkowski sum (1Â âˆ’ Î» ) A +Â Î» B is also measurable, then Î¼ Î¼ ( ( 1 âˆ’ âˆ’ Î» Î» ) A + Î» Î» B ) â‰¥ â‰¥ Î¼ Î¼ ( A ) 1 âˆ’ âˆ’ Î» Î» Î¼ Î¼ ( B ) Î» Î» , {\displaystyle \mu \left((1-\lambda )A+\lambda B\right)\geq \mu (A)^{1-\lambda }\mu (B)^{\lambda },} where Î¼ denotes n -dimensional Lebesgue measure . Hence, the PrÃ©kopaâ€“Leindler inequality can also be used [ 4 ] to prove the Brunnâ€“Minkowski inequality in its more familiar form: if 0Â < Î» <Â 1 and A and B are non- empty , bounded , measurable subsets of R n such that (1Â âˆ’ Î» ) A +Â Î» B is also measurable, then Î¼ Î¼ ( ( 1 âˆ’ âˆ’ Î» Î» ) A + Î» Î» B ) 1 / n â‰¥ â‰¥ ( 1 âˆ’ âˆ’ Î» Î» ) Î¼ Î¼ ( A ) 1 / n + Î» Î» Î¼ Î¼ ( B ) 1 / n .

{\displaystyle \mu \left((1-\lambda )A+\lambda B\right)^{1/n}\geq (1-\lambda )\mu (A)^{1/n}+\lambda \mu (B)^{1/n}.} Applications in probability and statistics [ edit ] Log-concave distributions [ edit ] The PrÃ©kopaâ€“Leindler inequality is useful in the theory of log-concave distributions , as it can be used to show that log-concavity is preserved by marginalization and independent summation of log-concave distributed random variables. Since, if X , Y {\displaystyle X,Y} have pdf f , g {\displaystyle f,g} , and X , Y {\displaystyle X,Y} are independent, then f â‹† â‹† g {\displaystyle f\star g} is the pdf of X + Y {\displaystyle X+Y} , we also have that the convolution of two log-concave functions is log-concave.

Suppose that H ( x , y ) is a log-concave distribution for ( x , y ) âˆˆ R m Ã— R n , so that by definition we have H ( ( 1 âˆ’ âˆ’ Î» Î» ) ( x 1 , y 1 ) + Î» Î» ( x 2 , y 2 ) ) â‰¥ â‰¥ H ( x 1 , y 1 ) 1 âˆ’ âˆ’ Î» Î» H ( x 2 , y 2 ) Î» Î» , {\displaystyle H\left((1-\lambda )(x_{1},y_{1})+\lambda (x_{2},y_{2})\right)\geq H(x_{1},y_{1})^{1-\lambda }H(x_{2},y_{2})^{\lambda },} 2 and let M ( y ) denote the marginal distribution obtained by integrating over x : M ( y ) = âˆ« âˆ« R m H ( x , y ) d x .

{\displaystyle M(y)=\int _{\mathbb {R} ^{m}}H(x,y)\,dx.} Let y 1 , y 2 âˆˆ R n and 0Â < Î» <Â 1 be given. Then equation ( 2 ) satisfies condition ( 1 ) with h ( x ) = H ( x ,(1Â âˆ’ Î» )y 1 + Î»y 2 ), f ( x ) = H ( x , y 1 ) and g ( x ) = H ( x , y 2 ), so the PrÃ©kopaâ€“Leindler inequality applies. It can be written in terms of M as M ( ( 1 âˆ’ âˆ’ Î» Î» ) y 1 + Î» Î» y 2 ) â‰¥ â‰¥ M ( y 1 ) 1 âˆ’ âˆ’ Î» Î» M ( y 2 ) Î» Î» , {\displaystyle M((1-\lambda )y_{1}+\lambda y_{2})\geq M(y_{1})^{1-\lambda }M(y_{2})^{\lambda },} which is the definition of log-concavity for M .

To see how this implies the preservation of log-convexity by independent sums, suppose that X and Y are independent random variables with log-concave distribution. Since the product of two log-concave functions is log-concave, the joint distribution of ( X , Y ) is also log-concave. Log-concavity is preserved by affine changes of coordinates, so the distribution of ( X + Y , X âˆ’ Y ) is log-concave as well. Since the distribution of X+Y is a marginal over the joint distribution of ( X + Y , X âˆ’ Y ), we conclude that X + Y has a log-concave distribution.

Applications to concentration of measure [ edit ] The PrÃ©kopaâ€“Leindler inequality can be used to prove results about concentration of measure.

Theorem [ citation needed ] Let A âŠ† âŠ† R n {\textstyle A\subseteq \mathbb {R} ^{n}} , and set A Ïµ Ïµ = { x : d ( x , A ) < Ïµ Ïµ } {\textstyle A_{\epsilon }=\{x:d(x,A)<\epsilon \}} . Let Î³ Î³ ( x ) {\textstyle \gamma (x)} denote the standard Gaussian pdf, and Î¼ Î¼ {\textstyle \mu } its associated measure. Then Î¼ Î¼ ( A Ïµ Ïµ ) â‰¥ â‰¥ 1 âˆ’ âˆ’ e âˆ’ âˆ’ Ïµ Ïµ 2 / 4 Î¼ Î¼ ( A ) {\textstyle \mu (A_{\epsilon })\geq 1-{\frac {e^{-\epsilon ^{2}/4}}{\mu (A)}}} .

Proof of concentration of measure The proof of this theorem goes by way of the following lemma: Lemma In the notation of the theorem, âˆ« âˆ« R n exp â¡ â¡ ( d ( x , A ) 2 / 4 ) d Î¼ Î¼ â‰¤ â‰¤ 1 / Î¼ Î¼ ( A ) {\textstyle \int _{\mathbb {R} ^{n}}\exp(d(x,A)^{2}/4)d\mu \leq 1/\mu (A)} .

This lemma can be proven from PrÃ©kopaâ€“Leindler by taking h ( x ) = Î³ Î³ ( x ) , f ( x ) = e d ( x , A ) 2 4 Î³ Î³ ( x ) , g ( x ) = 1 A ( x ) Î³ Î³ ( x ) {\textstyle h(x)=\gamma (x),f(x)=e^{\frac {d(x,A)^{2}}{4}}\gamma (x),g(x)=1_{A}(x)\gamma (x)} and Î» Î» = 1 / 2 {\textstyle \lambda =1/2} . To verify the hypothesis of the inequality, h ( x + y 2 ) â‰¥ â‰¥ f ( x ) g ( y ) {\textstyle h({\frac {x+y}{2}})\geq {\sqrt {f(x)g(y)}}} , note that we only need to consider y âˆˆ âˆˆ A {\textstyle y\in A} , in which case d ( x , A ) â‰¤ â‰¤ | | x âˆ’ âˆ’ y | | {\textstyle d(x,A)\leq ||x-y||} . This allows us to calculate: ( 2 Ï€ Ï€ ) n f ( x ) g ( x ) = exp â¡ â¡ ( d ( x , A ) 4 âˆ’ âˆ’ | | x | | 2 / 2 âˆ’ âˆ’ | | y | | 2 / 2 ) â‰¤ â‰¤ exp â¡ â¡ ( | | x âˆ’ âˆ’ y | | 2 4 âˆ’ âˆ’ | | x | | 2 / 2 âˆ’ âˆ’ | | y | | 2 / 2 ) = exp â¡ â¡ ( âˆ’ âˆ’ | | x + y 2 | | 2 ) = ( 2 Ï€ Ï€ ) n h ( x + y 2 ) 2 .

{\displaystyle (2\pi )^{n}f(x)g(x)=\exp({\frac {d(x,A)}{4}}-||x||^{2}/2-||y||^{2}/2)\leq \exp({\frac {||x-y||^{2}}{4}}-||x||^{2}/2-||y||^{2}/2)=\exp(-||{\frac {x+y}{2}}||^{2})=(2\pi )^{n}h({\frac {x+y}{2}})^{2}.} Since âˆ« âˆ« h ( x ) d x = 1 {\textstyle \int h(x)dx=1} , the PL-inequality immediately gives the lemma.

To conclude the concentration inequality from the lemma, note that on R n âˆ– âˆ– A Ïµ Ïµ {\textstyle \mathbb {R} ^{n}\setminus A_{\epsilon }} , d ( x , A ) > Ïµ Ïµ {\textstyle d(x,A)>\epsilon } , so we have âˆ« âˆ« R n exp â¡ â¡ ( d ( x , A ) 2 / 4 ) d Î¼ Î¼ â‰¥ â‰¥ ( 1 âˆ’ âˆ’ Î¼ Î¼ ( A Ïµ Ïµ ) ) exp â¡ â¡ ( Ïµ Ïµ 2 / 4 ) {\textstyle \int _{\mathbb {R} ^{n}}\exp(d(x,A)^{2}/4)d\mu \geq (1-\mu (A_{\epsilon }))\exp(\epsilon ^{2}/4)} . Applying the lemma and rearranging proves the result.

References [ edit ] ^ PrÃ©kopa, AndrÃ¡s (1971).

"Logarithmic concave measures with application to stochastic programming" (PDF) .

Acta Sci. Math.

32 : 301â€“ 316.

^ PrÃ©kopa, AndrÃ¡s (1973).

"On logarithmic concave measures and functions" (PDF) .

Acta Sci. Math.

34 : 335â€“ 343.

^ Herm Jan Brascamp ; Elliott H. Lieb (1976).

"On extensions of the Brunnâ€“Minkowski and Prekopaâ€“Leindler theorems, including inequalities for log concave functions and with an application to the diffusion equation" .

Journal of Functional Analysis .

22 (4): 366â€“ 389.

doi : 10.1016/0022-1236(76)90004-5 .

^ Gardner, Richard J. (2002).

"The Brunnâ€“Minkowski inequality" (PDF) .

Bull. Amer. Math. Soc. (N.S.) .

39 (3): 355â€“405 (electronic).

doi : 10.1090/S0273-0979-02-00941-2 .

ISSN 0273-0979 .

Further reading [ edit ] Eaton, Morris L. (1987). "Log concavity and related topics".

Lectures on Topics in Probability Inequalities . Amsterdam. pp.

77â€“ 109.

ISBN 90-6196-316-8 .

{{ cite book }} :  CS1 maint: location missing publisher ( link ) Wainwright, Martin J. (2019). "Concentration of Measure".

High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge University Press. pp.

72â€“ 76.

ISBN 978-1-108-49802-9 .

v t e Lp spaces Basic concepts Banach & Hilbert spaces L p spaces Measure Lebesgue Measure space Measurable space / function Minkowski distance Sequence spaces L 1 spaces Integrable function Lebesgue integration Taxicab geometry L 2 spaces Bessel's Cauchyâ€“Schwarz Euclidean distance Hilbert space Parseval's identity Polarization identity Pythagorean theorem Square-integrable function L âˆ âˆ {\displaystyle L^{\infty }} spaces Bounded function Chebyshev distance Infimum and supremum Essential Uniform norm Maps Almost everywhere Convergence almost everywhere Convergence in measure Function space Integral transform Locally integrable function Measurable function Symmetric decreasing rearrangement Inequalities Babenkoâ€“Beckner Chebyshev's Clarkson's Hanner's Hausdorffâ€“Young HÃ¶lder's Markov's Minkowski Young's convolution Results Marcinkiewicz interpolation theorem Plancherel theorem Riemannâ€“Lebesgue Rieszâ€“Fischer theorem Rieszâ€“Thorin theorem For Lebesgue measure Isoperimetric inequality Brunnâ€“Minkowski theorem Milman's reverse Minkowskiâ€“Steiner formula PrÃ©kopaâ€“Leindler inequality Vitale's random Brunnâ€“Minkowski inequality ApplicationsÂ &Â related Bochner space Fourier analysis Lorentz space Probability theory Quasinorm Real analysis Sobolev space *-algebra C*-algebra Von Neumann v t e Measure theory Basic concepts Absolute continuity of measures Lebesgue integration L p spaces Measure Measure space Probability space Measurable space / function Sets Almost everywhere Atom Baire set Borel set equivalence relation Borel space CarathÃ©odory's criterion Cylindrical Ïƒ-algebra Cylinder set ğœ†-system Essential range infimum/supremum Locally measurable Ï€ -system Ïƒ-algebra Non-measurable set Vitali set Null set Support Transverse measure Universally measurable Types of measures Atomic Baire Banach Besov Borel Brown Complex Complete Content ( Logarithmically ) Convex Decomposable Discrete Equivalent Finite Inner ( Quasi- ) Invariant Locally finite Maximising Metric outer Outer Perfect Pre-measure ( Sub- ) Probability Projection-valued Radon Random Regular Borel regular Inner regular Outer regular Saturated Set function Ïƒ-finite s-finite Signed Singular Spectral Strictly positive Tight Vector Particular measures Counting Dirac Euler Gaussian Haar Harmonic Hausdorff Intensity Lebesgue Infinite-dimensional Logarithmic Product Projections Pushforward Spherical measure Tangent Trivial Young Maps Measurable function Bochner Strongly Weakly Convergence: almost everywhere of measures in measure of random variables in distribution in probability Cylinder set measure Random: compact set element measure process variable vector Projection-valued measure Main results CarathÃ©odory's extension theorem Convergence theorems Dominated Monotone Vitali Decomposition theorems Hahn Jordan Maharam's Egorov's Fatou's lemma Fubini's Fubiniâ€“Tonelli HÃ¶lder's inequality Minkowski inequality Radonâ€“Nikodym Rieszâ€“Markovâ€“Kakutani representation theorem Other results Disintegration theorem Lifting theory Lebesgue's density theorem Lebesgue differentiation theorem Sard's theorem Vitaliâ€“Hahnâ€“Saks theorem For Lebesgue measure Isoperimetric inequality Brunnâ€“Minkowski theorem Milman's reverse Minkowskiâ€“Steiner formula PrÃ©kopaâ€“Leindler inequality Vitale's random Brunnâ€“Minkowski inequality ApplicationsÂ &Â related Convex analysis Descriptive set theory Probability theory Real analysis Spectral theory NewPP limit report
Parsed by mwâ€web.codfw.mainâ€6cc77c66b8â€hk8r7
Cached time: 20250812013003
Cache expiry: 2592000
Reduced expiry: false
Complications: [varyâ€revisionâ€sha1, showâ€toc]
CPU time usage: 0.351 seconds
Real time usage: 0.517 seconds
Preprocessor visited node count: 1270/1000000
Revision size: 10360/2097152 bytes
Postâ€expand include size: 61327/2097152 bytes
Template argument size: 1447/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip postâ€expand size: 43334/5000000 bytes
Lua time usage: 0.170/10.000 seconds
Lua memory usage: 4718648/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  331.442      1 -total
 34.27%  113.582      1 Template:Reflist
 30.04%   99.556      4 Template:Navbox
 29.53%   97.862      4 Template:Cite_journal
 24.21%   80.250      1 Template:Lp_spaces
 16.04%   53.174      2 Template:NumBlk
 12.48%   41.375      1 Template:Citation_needed
 10.73%   35.563      1 Template:Fix
  7.28%   24.136      2 Template:Category_handler
  4.64%   15.365      1 Template:Measure_theory Saved in parser cache with key enwiki:pcache:10570298:|#|:idhash:canonical and timestamp 20250812013003 and revision id 1286472681. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=PrÃ©kopaâ€“Leindler_inequality&oldid=1286472681 " Categories : Geometric inequalities Integral geometry Real analysis Theorems in mathematical analysis Theorems in measure theory Hidden categories: All articles with unsourced statements Articles with unsourced statements from September 2020 CS1 maint: location missing publisher This page was last edited on 20 April 2025, at 03:09 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents PrÃ©kopaâ€“Leindler inequality 1 language Add topic

