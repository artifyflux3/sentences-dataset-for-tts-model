Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Nomenclature difficulty 2 Definition Toggle Definition subsection 2.1 Examples of exponential family distributions 2.2 Scalar parameter 2.2.1 Support must be independent of θ 2.2.2 Vector valued x and θ 2.2.3 Canonical formulation 2.3 Factorization of the variables involved 2.4 Vector parameter 2.5 Vector parameter, vector variable 2.6 Measure-theoretic formulation 3 Interpretation 4 Properties 5 Examples Toggle Examples subsection 5.1 Normal distribution: unknown mean, known variance 5.2 Normal distribution: unknown mean and unknown variance 5.3 Binomial distribution 6 Table of distributions 7 Moments and cumulants of the sufficient statistic Toggle Moments and cumulants of the sufficient statistic subsection 7.1 Normalization of the distribution 7.2 Moment-generating function of the sufficient statistic 7.2.1 Differential identities for cumulants 7.2.2 Example 1 7.2.3 Example 2 7.2.4 Example 3 8 Entropy Toggle Entropy subsection 8.1 Relative entropy 8.2 Maximum-entropy derivation 9 Role in statistics Toggle Role in statistics subsection 9.1 Classical estimation: sufficiency 9.2 Bayesian estimation: conjugate distributions 9.3 Unbiased estimation 9.4 Hypothesis testing: uniformly most powerful tests 9.5 Generalized linear models 10 See also 11 Footnotes 12 References Toggle References subsection 12.1 Citations 12.2 Sources 13 Further reading 14 External links Toggle the table of contents Exponential family 10 languages Català Deutsch Español فارسی Français 한국어 עברית Nederlands 日本語 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Pitman–Koopman theorem ) This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Exponential family" – news · newspapers · books · scholar · JSTOR ( January 2025 ) ( Learn how and when to remove this message ) Family of probability distributions related to the normal distribution Not to be confused with the exponential distribution .

"Natural parameter" redirects here. For other uses, see Natural parameter (disambiguation) .

In probability and statistics , an exponential family is a parametric set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, including the enabling of the user to calculate expectations, covariances using differentiation based on some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The term exponential class is sometimes used in place of "exponential family", [ 1 ] or the older term Koopman–Darmois family .
Sometimes loosely referred to as the exponential family, this class of distributions is distinct because they all possess a variety of desirable properties, most importantly the existence of a sufficient statistic .

The concept of exponential families is credited to [ 2 ] E. J. G. Pitman , [ 3 ] G. Darmois , [ 4 ] and B. O. Koopman [ 5 ] in 1935–1936. Exponential families of distributions provide a general framework for selecting a possible alternative parameterisation of a parametric family of distributions, in terms of natural parameters, and for defining useful sample statistics , called the natural sufficient statistics of the family.

Nomenclature difficulty [ edit ] The terms "distribution" and "family" are often used loosely: Specifically, an exponential family is a set of distributions, where the specific distribution varies with the parameter; [ a ] however, a parametric family of distributions is often referred to as " a distribution" (like "the normal distribution", meaning "the family of normal distributions"), and the set of all exponential families is sometimes loosely referred to as "the" exponential family.

Definition [ edit ] Most of the commonly used distributions form an exponential family or subset of an exponential family, listed in the subsection below. The subsections following it are a sequence of increasingly more general mathematical definitions of an exponential family. A casual reader may wish to restrict attention to the first and simplest definition, which corresponds to a single-parameter family of discrete or continuous probability distributions.

Examples of exponential family distributions [ edit ] Exponential families include many of the most common distributions. Among many others, exponential families includes the following: [ 6 ] normal exponential gamma chi-squared beta Dirichlet Bernoulli categorical Poisson Wishart inverse Wishart geometric A number of common distributions are exponential families, but only when certain parameters are fixed and known. For example: binomial (with fixed number of trials) multinomial (with fixed number of trials) negative binomial (with fixed number of failures) Note that in each case, the parameters which must be fixed are those that set a limit on the range of values that can possibly be observed.

Examples of common distributions that are not exponential families are Student's t , most mixture distributions , and even the family of uniform distributions when the bounds are not fixed. See the section below on examples for more discussion.

Scalar parameter [ edit ] The value of θ θ {\displaystyle \theta } is called the parameter of the family.

A single-parameter exponential family is a set of probability distributions whose probability density function (or probability mass function , for the case of a discrete distribution ) can be expressed in the form f X ( x | θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) ] {\displaystyle f_{X}{\left(x\,{\big |}\,\theta \right)}=h(x)\,\exp \left[\eta (\theta )\cdot T(x)-A(\theta )\right]} where T ( x ) , h ( x ) , η ( θ ) , and A ( θ ) are known functions. The function h ( x ) must be non-negative.

An alternative, equivalent form often given is f X ( x | θ θ ) = h ( x ) g ( θ θ ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) ] {\displaystyle f_{X}{\left(x\ {\big |}\ \theta \right)}=h(x)\,g(\theta )\,\exp \left[\eta (\theta )\cdot T(x)\right]} or equivalently f X ( x | θ θ ) = exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) + B ( x ) ] .

{\displaystyle f_{X}{\left(x\ {\big |}\ \theta \right)}=\exp \left[\eta (\theta )\cdot T(x)-A(\theta )+B(x)\right].} In terms of log probability , log ⁡ ⁡ ( f X ( x | θ θ ) ) = η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) + B ( x ) .

{\displaystyle \log(f_{X}{\left(x\ {\big |}\ \theta \right)})=\eta (\theta )\cdot T(x)-A(\theta )+B(x).} Note that g ( θ θ ) = e − − A ( θ θ ) {\displaystyle g(\theta )=e^{-A(\theta )}} and h ( x ) = e B ( x ) {\displaystyle h(x)=e^{B(x)}} .

Support must be independent of θ [ edit ] Importantly, the support of f X ( x | θ θ ) {\displaystyle f_{X}{\left(x{\big |}\theta \right)}} (all the possible x {\displaystyle x} values for which f X ( x | θ θ ) {\displaystyle f_{X}\!\left(x{\big |}\theta \right)} is greater than 0 {\displaystyle 0} ) is required to not depend on θ θ .

{\displaystyle \theta ~.} [ 7 ] This requirement can be used to exclude a parametric family distribution from being an exponential family.

For example: The Pareto distribution has a pdf which is defined for x ≥ ≥ x m {\displaystyle x\geq x_{\mathsf {m}}} (the minimum value, x m , {\displaystyle x_{m}\ ,} being the scale parameter) and its support, therefore, has a lower limit of x m .

{\displaystyle x_{\mathsf {m}}~.} Since the support of f α α , x m ( x ) {\displaystyle f_{\alpha ,x_{m}}\!(x)} is dependent on the value of the parameter, the family of Pareto distributions does not form an exponential family of distributions (at least when x m {\displaystyle x_{m}} is unknown).

Another example: Bernoulli-type distributions – binomial , negative binomial , geometric distribution , and similar – can only be included in the exponential class if the number of Bernoulli trials , n , is treated as a fixed constant – excluded from the free parameter(s) θ θ {\displaystyle \theta } – since the allowed number of trials sets the limits for the number of "successes" or "failures" that can be observed in a set of trials.

Vector valued x and θ [ edit ] Often x {\displaystyle x} is a vector of measurements, in which case T ( x ) {\displaystyle T(x)} may be a function from the space of possible values of x {\displaystyle x} to the real numbers.

More generally, η η ( θ θ ) {\displaystyle \eta (\theta )} and T ( x ) {\displaystyle T(x)} can each be vector-valued such that η η ( θ θ ) ⋅ ⋅ T ( x ) {\displaystyle \eta (\theta )\cdot T(x)} is real-valued. However, see the discussion below on vector parameters , regarding the curved exponential family.

Canonical formulation [ edit ] If η η ( θ θ ) = θ θ , {\displaystyle \eta (\theta )=\theta \ ,} then the exponential family is said to be in canonical form . By defining a transformed parameter η η = η η ( θ θ ) , {\displaystyle \eta =\eta (\theta )\ ,} it is always possible to convert an exponential family to canonical form. The canonical form is non-unique, since η η ( θ θ ) {\displaystyle \eta (\theta )} can be multiplied by any nonzero constant, provided that T ( x ) is multiplied by that constant's reciprocal, or a constant c can be added to η η ( θ θ ) {\displaystyle \eta (\theta )} and h ( x ) multiplied by exp ⁡ ⁡ [ − − c ⋅ ⋅ T ( x ) ] {\displaystyle \exp \left[{-c}\cdot T(x)\,\right]} to offset it. In the special case that η η ( θ θ ) = θ θ {\displaystyle \eta (\theta )=\theta } and T ( x ) = x , then the family is called a natural exponential family .

Even when x {\displaystyle x} is a scalar, and there is only a single parameter, the functions η η ( θ θ ) {\displaystyle \eta (\theta )} and T ( x ) {\displaystyle T(x)} can still be vectors, as described below.

The function A ( θ θ ) , {\displaystyle A(\theta )\ ,} or equivalently g ( θ θ ) , {\displaystyle g(\theta )\ ,} is automatically determined once the other functions have been chosen, since it must assume a form that causes the distribution to be normalized (sum or integrate to one over the entire domain). Furthermore, both of these functions can always be written as functions of η η , {\displaystyle \eta \ ,} even when η η ( θ θ ) {\displaystyle \eta (\theta )} is not a one-to-one function, i.e. two or more different values of θ θ {\displaystyle \theta } map to the same value of η η ( θ θ ) , {\displaystyle \eta (\theta )\ ,} and hence η η ( θ θ ) {\displaystyle \eta (\theta )} cannot be inverted. In such a case, all values of θ θ {\displaystyle \theta } mapping to the same η η ( θ θ ) {\displaystyle \eta (\theta )} will also have the same value for A ( θ θ ) {\displaystyle A(\theta )} and g ( θ θ ) .

{\displaystyle g(\theta )~.} Factorization of the variables involved [ edit ] What is important to note, and what characterizes all exponential family variants, is that the parameter(s) and the observation variable(s) must factorize (can be separated into products each of which involves only one type of variable), either directly or within either part (the base or exponent) of an exponentiation operation.  Generally, this means that all of the factors constituting the density or mass function must be of one of the following forms: f ( x ) , c f ( x ) , [ f ( x ) ] c , [ f ( x ) ] g ( θ θ ) , [ f ( x ) ] h ( x ) g ( θ θ ) , g ( θ θ ) , c g ( θ θ ) , [ g ( θ θ ) ] c , [ g ( θ θ ) ] f ( x ) , o r [ g ( θ θ ) ] h ( x ) j ( θ θ ) , {\displaystyle {\begin{aligned}f(x),&&c^{f(x)},&&{[f(x)]}^{c},&&{[f(x)]}^{g(\theta )},&&{[f(x)]}^{h(x)g(\theta )},\\g(\theta ),&&c^{g(\theta )},&&{[g(\theta )]}^{c},&&{[g(\theta )]}^{f(x)},&&~~{\mathsf {or}}~~{[g(\theta )]}^{h(x)j(\theta )},\end{aligned}}} where f and h are arbitrary functions of x , the observed statistical variable; g and j are arbitrary functions of θ θ , {\displaystyle \theta ,} the fixed parameters defining the shape of the distribution; and c is any arbitrary constant expression (i.e. a number or an expression that does not change with either x or θ θ {\displaystyle \theta } ).

There are further restrictions on how many such factors can occur.  For example, the two expressions: [ f ( x ) g ( θ θ ) ] h ( x ) j ( θ θ ) , [ f ( x ) ] h ( x ) j ( θ θ ) [ g ( θ θ ) ] h ( x ) j ( θ θ ) , {\displaystyle {[f(x)g(\theta )]}^{h(x)j(\theta )},\qquad {[f(x)]}^{h(x)j(\theta )}{[g(\theta )]}^{h(x)j(\theta )},} are the same, i.e. a product of two "allowed" factors.  However, when rewritten into the factorized form, [ f ( x ) g ( θ θ ) ] h ( x ) j ( θ θ ) = [ f ( x ) ] h ( x ) j ( θ θ ) [ g ( θ θ ) ] h ( x ) j ( θ θ ) = exp ⁡ ⁡ { [ h ( x ) log ⁡ ⁡ f ( x ) ] j ( θ θ ) + h ( x ) [ j ( θ θ ) log ⁡ ⁡ g ( θ θ ) ] } , {\displaystyle {\begin{aligned}{\left[f(x)g(\theta )\right]}^{h(x)j(\theta )}&={\left[f(x)\right]}^{h(x)j(\theta )}{\left[g(\theta )\right]}^{h(x)j(\theta )}\\[4pt]&=\exp \left\{{[h(x)\log f(x)]j(\theta )+h(x)[j(\theta )\log g(\theta )]}\right\},\end{aligned}}} it can be seen that it cannot be expressed in the required form. (However, a form of this sort is a member of a curved exponential family , which allows multiple factorized terms in the exponent.

[ citation needed ] ) To see why an expression of the form [ f ( x ) ] g ( θ θ ) {\displaystyle {[f(x)]}^{g(\theta )}} qualifies, [ f ( x ) ] g ( θ θ ) = e g ( θ θ ) log ⁡ ⁡ f ( x ) {\displaystyle {[f(x)]}^{g(\theta )}=e^{g(\theta )\log f(x)}} and hence factorizes inside of the exponent. Similarly, [ f ( x ) ] h ( x ) g ( θ θ ) = e h ( x ) g ( θ θ ) log ⁡ ⁡ f ( x ) = e [ h ( x ) log ⁡ ⁡ f ( x ) ] g ( θ θ ) {\displaystyle {[f(x)]}^{h(x)g(\theta )}=e^{h(x)g(\theta )\log f(x)}=e^{[h(x)\log f(x)]g(\theta )}} and again factorizes inside of the exponent.

A factor consisting of a sum where both types of variables are involved (e.g. a factor of the form 1 + f ( x ) g ( θ θ ) {\displaystyle 1+f(x)g(\theta )} ) cannot be factorized in this fashion (except in some cases where occurring directly in an exponent); this is why, for example, the Cauchy distribution and Student's t distribution are not exponential families.

Vector parameter [ edit ] The definition in terms of one real-number parameter can be extended to one real-vector parameter θ θ ≡ ≡ [ θ θ 1 θ θ 2 ⋯ ⋯ θ θ s ] T .

{\displaystyle {\boldsymbol {\theta }}\equiv {\begin{bmatrix}\theta _{1}&\theta _{2}&\cdots &\theta _{s}\end{bmatrix}}^{\mathsf {T}}.} A family of distributions is said to belong to a vector exponential family if the probability density function (or probability mass function, for discrete distributions) can be written as f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ ( ∑ ∑ i = 1 s η η i ( θ θ ) T i ( x ) − − A ( θ θ ) ) , {\displaystyle f_{X}(x\mid {\boldsymbol {\theta }})=h(x)\,\exp \left(\sum _{i=1}^{s}\eta _{i}({\boldsymbol {\theta }})T_{i}(x)-A({\boldsymbol {\theta }})\right)~,} or in a more compact form, f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) ] {\displaystyle f_{X}(x\mid {\boldsymbol {\theta }})=h(x)\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x)-A({\boldsymbol {\theta }})\right]} This form writes the sum as a dot product of vector-valued functions η η ( θ θ ) {\displaystyle {\boldsymbol {\eta }}({\boldsymbol {\theta }})} and T ( x ) .

An alternative, equivalent form often seen is f X ( x ∣ ∣ θ θ ) = h ( x ) g ( θ θ ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) ] {\displaystyle f_{X}(x\mid {\boldsymbol {\theta }})=h(x)\,g({\boldsymbol {\theta }})\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x)\right]} As in the scalar valued case, the exponential family is said to be in canonical form if η η i ( θ θ ) = θ θ i , ∀ ∀ i .

{\displaystyle \eta _{i}({\boldsymbol {\theta }})=\theta _{i}~,\quad \forall i\,.} A vector exponential family is said to be curved if the dimension of θ θ ≡ ≡ [ θ θ 1 θ θ 2 ⋯ ⋯ θ θ d ] T {\displaystyle {\boldsymbol {\theta }}\equiv {\begin{bmatrix}\theta _{1}&\theta _{2}&\cdots &\theta _{d}\end{bmatrix}}^{\mathsf {T}}} is less than the dimension of the vector η η ( θ θ ) ≡ ≡ [ η η 1 ( θ θ ) η η 2 ( θ θ ) ⋯ ⋯ η η s ( θ θ ) ] T .

{\displaystyle {\boldsymbol {\eta }}({\boldsymbol {\theta }})\equiv {\begin{bmatrix}\eta _{1}{\!({\boldsymbol {\theta }})}&\eta _{2}{\!({\boldsymbol {\theta }})}&\cdots &\eta _{s}{\!({\boldsymbol {\theta }})}\end{bmatrix}}^{\mathsf {T}}~.} That is, if the dimension , d , of the parameter vector is less than the number of functions , s , of the parameter vector in the above representation of the probability density function. Most common distributions in the exponential family are not curved, and many algorithms designed to work with any exponential family implicitly or explicitly assume that the distribution is not curved.

Just as in the case of a scalar-valued parameter, the function A ( θ θ ) {\displaystyle A({\boldsymbol {\theta }})} or equivalently g ( θ θ ) {\displaystyle g({\boldsymbol {\theta }})} is automatically determined by the normalization constraint, once the other functions have been chosen. Even if η η ( θ θ ) {\displaystyle {\boldsymbol {\eta }}({\boldsymbol {\theta }})} is not one-to-one, functions A ( η η ) {\displaystyle A({\boldsymbol {\eta }})} and g ( η η ) {\displaystyle g({\boldsymbol {\eta }})} can be defined by requiring that the distribution is normalized for each value of the natural parameter η η {\displaystyle {\boldsymbol {\eta }}} . This yields the canonical form f X ( x ∣ ∣ η η ) = h ( x ) exp ⁡ ⁡ [ η η ⋅ ⋅ T ( x ) − − A ( η η ) ] , {\displaystyle f_{X}(x\mid {\boldsymbol {\eta }})=h(x)\exp \left[{\boldsymbol {\eta }}\cdot \mathbf {T} (x)-A({\boldsymbol {\eta }})\right],} or equivalently f X ( x ∣ ∣ η η ) = h ( x ) g ( η η ) exp ⁡ ⁡ [ η η ⋅ ⋅ T ( x ) ] .

{\displaystyle f_{X}(x\mid {\boldsymbol {\eta }})=h(x)g({\boldsymbol {\eta }})\exp \left[{\boldsymbol {\eta }}\cdot \mathbf {T} (x)\right].} The above forms may sometimes be seen with η η T T ( x ) {\displaystyle {\boldsymbol {\eta }}^{\mathsf {T}}\mathbf {T} (x)} in place of η η ⋅ ⋅ T ( x ) {\displaystyle {\boldsymbol {\eta }}\cdot \mathbf {T} (x)\,} . These are exactly equivalent formulations, merely using different notation for the dot product .

Vector parameter, vector variable [ edit ] The vector-parameter form over a single scalar-valued random variable can be trivially expanded to cover a joint distribution over a vector of random variables. The resulting distribution is simply the same as the above distribution for a scalar-valued random variable with each occurrence of the scalar x replaced by the vector x = [ x 1 x 2 ⋯ ⋯ x k ] T .

{\displaystyle \mathbf {x} ={\begin{bmatrix}x_{1}&x_{2}&\cdots &x_{k}\end{bmatrix}}^{\mathsf {T}}.} The dimensions k of the random variable need not match the dimension d of the parameter vector, nor (in the case of a curved exponential function) the dimension s of the natural parameter η η {\displaystyle {\boldsymbol {\eta }}} and sufficient statistic T ( x ) .

The distribution in this case is written as f X ( x ∣ ∣ θ θ ) = h ( x ) exp [ ∑ ∑ i = 1 s η η i ( θ θ ) T i ( x ) − − A ( θ θ ) ] {\displaystyle f_{X}{\left(\mathbf {x} \mid {\boldsymbol {\theta }}\right)}=h(\mathbf {x} )\,\exp \!\left[\sum _{i=1}^{s}\eta _{i}({\boldsymbol {\theta }})T_{i}(\mathbf {x} )-A({\boldsymbol {\theta }})\right]} Or more compactly as f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) ] {\displaystyle f_{X}{\left(\mathbf {x} \mid {\boldsymbol {\theta }}\right)}=h(\mathbf {x} )\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (\mathbf {x} )-A({\boldsymbol {\theta }})\right]} Or alternatively as f X ( x ∣ ∣ θ θ ) = g ( θ θ ) h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) ] {\displaystyle f_{X}{\left(\mathbf {x} \mid {\boldsymbol {\theta }}\right)}=g({\boldsymbol {\theta }})\,h(\mathbf {x} )\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (\mathbf {x} )\right]} Measure-theoretic formulation [ edit ] We use cumulative distribution functions (CDF) in order to encompass both discrete and continuous distributions.

Suppose H is a non-decreasing function of a real variable. Then Lebesgue–Stieltjes integrals with respect to d H ( x ) {\displaystyle dH(\mathbf {x} )} are integrals with respect to the reference measure of the exponential family generated by H .

Any member of that exponential family has cumulative distribution function d F ( x ∣ ∣ θ θ ) = exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) ] d H ( x ) .

{\displaystyle dF{\left(\mathbf {x} \mid {\boldsymbol {\theta }}\right)}=\exp \left[{\boldsymbol {\eta }}(\theta )\cdot \mathbf {T} (\mathbf {x} )-A({\boldsymbol {\theta }})\right]~dH(\mathbf {x} )\,.} H ( x ) is a Lebesgue–Stieltjes integrator for the reference measure. When the reference measure is finite, it can be normalized and H is actually the cumulative distribution function of a probability distribution. If F is absolutely continuous with a density f ( x ) {\displaystyle f(x)} with respect to a reference measure d x {\displaystyle dx} (typically Lebesgue measure ), one can write d F ( x ) = f ( x ) d x {\displaystyle dF(x)=f(x)\,dx} .
In this case, H is also absolutely continuous and can be written d H ( x ) = h ( x ) d x {\displaystyle dH(x)=h(x)\,dx} so the formulas reduce to that of the previous paragraphs. If F is discrete, then H is a step function (with steps on the support of F ).

Alternatively, we can write the probability measure directly as P ( d x ∣ ∣ θ θ ) = exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( θ θ ) ] μ μ ( d x ) .

{\displaystyle P\left(d\mathbf {x} \mid {\boldsymbol {\theta }}\right)=\exp \left[{\boldsymbol {\eta }}(\theta )\cdot \mathbf {T} (\mathbf {x} )-A({\boldsymbol {\theta }})\right]~\mu (d\mathbf {x} )\,.} for some reference measure μ μ {\displaystyle \mu \,} .

Interpretation [ edit ] In the definitions above, the functions T ( x ) , η ( θ ) , and A ( η ) were arbitrary. However, these functions have important interpretations in the resulting probability distribution.

T ( x ) is a sufficient statistic of the distribution. For exponential families, the sufficient statistic is a function of the data that holds all information the data x provides with regard to the unknown parameter values. This means that, for any data sets x {\displaystyle x} and y {\displaystyle y} , the likelihood ratio is the same, that is f ( x ; θ θ 1 ) f ( x ; θ θ 2 ) = f ( y ; θ θ 1 ) f ( y ; θ θ 2 ) {\displaystyle {\frac {f(x;\theta _{1})}{f(x;\theta _{2})}}={\frac {f(y;\theta _{1})}{f(y;\theta _{2})}}} if T ( x ) = T ( y ) . This is true even if x and y are not equal to each other. The dimension of T ( x ) equals the number of parameters of θ and encompasses all of the information regarding the data related to the parameter θ . The sufficient statistic of a set of independent identically distributed data observations is simply the sum of individual sufficient statistics, and encapsulates all the information needed to describe the posterior distribution of the parameters, given the data (and hence to derive any desired estimate of the parameters). (This important property is discussed further below .) η is called the natural parameter . The set of values of η for which the function f X ( x ; η η ) {\displaystyle f_{X}(x;\eta )} is integrable is called the natural parameter space . It can be shown that the natural parameter space is always convex .

A ( η ) is called the log- partition function [ b ] because it is the logarithm of a normalization factor , without which f X ( x ; θ θ ) {\displaystyle f_{X}(x;\theta )} would not be a probability distribution: A ( η η ) = log ⁡ ⁡ ( ∫ ∫ X h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) ] d x ) {\displaystyle A(\eta )=\log \left(\int _{X}h(x)\,\exp \left[\eta (\theta )\cdot T(x)\right]\,dx\right)} The function A is important in its own right, because the mean , variance and other moments of the sufficient statistic T ( x ) can be derived simply by differentiating A ( η ) . For example, because log( x ) is one of the components of the sufficient statistic of the gamma distribution , E ⁡ ⁡ [ log ⁡ ⁡ x ] {\displaystyle \operatorname {\mathcal {E}} [\log x]} can be easily determined for this distribution using A ( η ) . Technically, this is true because K ( u ∣ ∣ η η ) = A ( η η + u ) − − A ( η η ) , {\displaystyle K{\left(u\mid \eta \right)}=A(\eta +u)-A(\eta )\,,} is the cumulant generating function of the sufficient statistic.

Properties [ edit ] Exponential families have a large number of properties that make them extremely useful for statistical analysis. In many cases, it can be shown that only exponential families have these properties.  Examples: Exponential families are the only families with sufficient statistics that can summarize arbitrary amounts of independent identically distributed data using a fixed number of values. ( Pitman – Koopman – Darmois theorem) Exponential families have conjugate priors , an important property in Bayesian statistics .

The posterior predictive distribution of an exponential-family random variable with a conjugate prior can always be written in closed form (provided that the normalizing factor of the exponential-family distribution can itself be written in closed form).

[ c ] In the mean-field approximation in variational Bayes (used for approximating the posterior distribution in large Bayesian networks ), the best approximating posterior distribution of an exponential-family node (a node is a random variable in the context of Bayesian networks) with a conjugate prior is in the same family as the node.

[ 8 ] Given an exponential family defined by f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ θ θ ⋅ ⋅ T ( x ) − − A ( θ θ ) ] {\displaystyle f_{X}{\!(x\mid \theta )}=h(x)\exp \left[\theta \cdot T(x)-A(\theta )\right]} , where Θ Θ {\displaystyle \Theta } is the parameter space, such that θ θ ∈ ∈ Θ Θ ⊂ ⊂ R k {\displaystyle \theta \in \Theta \subset \mathbb {R} ^{k}} . Then If Θ Θ {\displaystyle \Theta } has nonempty interior in R k {\displaystyle \mathbb {R} ^{k}} , then given any IID samples X 1 , .

.

.

, X n ∼ ∼ f X {\displaystyle X_{1},...,X_{n}\sim f_{X}} , the statistic T ( X 1 , … … , X n ) := ∑ ∑ i = 1 n T ( X i ) {\textstyle T(X_{1},\dots ,X_{n}):=\sum _{i=1}^{n}T(X_{i})} is a complete statistic for θ θ {\displaystyle \theta } .

[ 9 ] [ 10 ] T {\displaystyle T} is a minimal statistic for θ θ {\displaystyle \theta } if and only if for all θ θ 1 , θ θ 2 ∈ ∈ Θ Θ {\displaystyle \theta _{1},\theta _{2}\in \Theta } , and x 1 , x 2 {\displaystyle x_{1},x_{2}} in the support of X {\displaystyle X} , if ( θ θ 1 − − θ θ 2 ) ⋅ ⋅ [ T ( x 1 ) − − T ( x 2 ) ] = 0 {\displaystyle (\theta _{1}-\theta _{2})\cdot [T(x_{1})-T(x_{2})]=0} , then θ θ 1 = θ θ 2 {\displaystyle \theta _{1}=\theta _{2}} or x 1 = x 2 {\displaystyle x_{1}=x_{2}} .

[ 11 ] Examples [ edit ] It is critical, when considering the examples in this section, to remember the discussion above about what it means to say that a "distribution" is an exponential family, and in particular to keep in mind that the set of parameters that are allowed to vary is critical in determining whether a "distribution" is or is not an exponential family.

The normal , exponential , log-normal , gamma , chi-squared , beta , Dirichlet , Bernoulli , categorical , Poisson , geometric , inverse Gaussian , ALAAM , von Mises , and von Mises-Fisher distributions are all exponential families.

Some distributions are exponential families only if some of their parameters are held fixed.  The family of Pareto distributions with a fixed minimum bound x m form an exponential family.  The families of binomial and multinomial distributions with fixed number of trials n but unknown probability parameter(s) are exponential families.  The family of negative binomial distributions with fixed number of failures (a.k.a. stopping-time parameter) r is an exponential family.  However, when any of the above-mentioned fixed parameters are allowed to vary, the resulting family is not an exponential family.

As mentioned above, as a general rule, the support of an exponential family must remain the same across all parameter settings in the family. This is why the above cases (e.g. binomial with varying number of trials, Pareto with varying minimum bound) are not exponential families — in all of the cases, the parameter in question affects the support (particularly, changing the minimum or maximum possible value). For similar reasons, neither the discrete uniform distribution nor continuous uniform distribution are exponential families as one or both bounds vary.

The Weibull distribution with fixed shape parameter k is an exponential family. Unlike in the previous examples, the shape parameter does not affect the support; the fact that allowing it to vary makes the Weibull non-exponential is due rather to the particular form of the Weibull's probability density function ( k appears in the exponent of an exponent).

In general, distributions that result from a finite or infinite mixture of other distributions, e.g.

mixture model densities and compound probability distributions , are not exponential families. Examples are typical Gaussian mixture models as well as many heavy-tailed distributions that result from compounding (i.e. infinitely mixing) a distribution with a prior distribution over one of its parameters, e.g. the Student's t -distribution (compounding a normal distribution over a gamma-distributed precision prior), and the beta-binomial and Dirichlet-multinomial distributions.  Other examples of distributions that are not exponential families are the F-distribution , Cauchy distribution , hypergeometric distribution and logistic distribution .

Following are some detailed examples of the representation of some useful distribution as exponential families.

Normal distribution: unknown mean, known variance [ edit ] As a first example, consider a random variable distributed normally with unknown mean μ and known variance σ 2 . The probability density function is then f σ σ ( x ; μ μ ) = 1 2 π π σ σ 2 e − − ( x − − μ μ ) 2 / 2 σ σ 2 .

{\displaystyle f_{\sigma }(x;\mu )={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-(x-\mu )^{2}/2\sigma ^{2}}.} This is a single-parameter exponential family, as can be seen by setting T σ σ ( x ) = x σ σ , h σ σ ( x ) = 1 2 π π σ σ 2 e − − x 2 / 2 σ σ 2 , A σ σ ( μ μ ) = μ μ 2 2 σ σ 2 , η η σ σ ( μ μ ) = μ μ σ σ .

{\displaystyle {\begin{aligned}T_{\sigma }(x)&={\frac {x}{\sigma }},&h_{\sigma }(x)&={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-x^{2}/2\sigma ^{2}},\\[4pt]A_{\sigma }(\mu )&={\frac {\mu ^{2}}{2\sigma ^{2}}},&\eta _{\sigma }(\mu )&={\frac {\mu }{\sigma }}.\end{aligned}}} If σ = 1 this is in canonical form, as then η ( μ ) = μ .

Normal distribution: unknown mean and unknown variance [ edit ] Next, consider the case of a normal distribution with unknown mean and unknown variance. The probability density function is then f ( y ; μ μ , σ σ 2 ) = 1 2 π π σ σ 2 e − − ( y − − μ μ ) 2 / 2 σ σ 2 .

{\displaystyle f(y;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-(y-\mu )^{2}/2\sigma ^{2}}.} This is an exponential family which can be written in canonical form by defining h ( y ) = 1 2 π π , η η = [ μ μ σ σ 2 , − − 1 2 σ σ 2 ] , T ( y ) = ( y , y 2 ) T , A ( η η ) = μ μ 2 2 σ σ 2 + log ⁡ ⁡ | σ σ | = − − η η 1 2 4 η η 2 + 1 2 log ⁡ ⁡ | 1 2 η η 2 | {\displaystyle {\begin{aligned}h(y)&={\frac {1}{\sqrt {2\pi }}},&{\boldsymbol {\eta }}&=\left[{\frac {\mu }{\sigma ^{2}}},~-{\frac {1}{2\sigma ^{2}}}\right],\\T(y)&=\left(y,y^{2}\right)^{\mathsf {T}},&A({\boldsymbol {\eta }})&={\frac {\mu ^{2}}{2\sigma ^{2}}}+\log |\sigma |=-{\frac {\eta _{1}^{2}}{4\eta _{2}}}+{\frac {1}{2}}\log \left|{\frac {1}{2\eta _{2}}}\right|\end{aligned}}} Binomial distribution [ edit ] As an example of a discrete exponential family, consider the binomial distribution with known number of trials n . The probability mass function for this distribution is f ( x ) = ( n x ) p x ( 1 − − p ) n − − x , x ∈ ∈ { 0 , 1 , 2 , … … , n } .

{\displaystyle f(x)={\binom {n}{x}}p^{x}{\left(1-p\right)}^{n-x},\quad x\in \{0,1,2,\ldots ,n\}.} This can equivalently be written as f ( x ) = ( n x ) exp ⁡ ⁡ [ x log ⁡ ⁡ ( p 1 − − p ) + n log ⁡ ⁡ ( 1 − − p ) ] , {\displaystyle f(x)={\binom {n}{x}}\exp \left[x\log \left({\frac {p}{1-p}}\right)+n\log(1-p)\right],} which shows that the binomial distribution is an exponential family, whose natural parameter is η η = log ⁡ ⁡ p 1 − − p .

{\displaystyle \eta =\log {\frac {p}{1-p}}.} This function of p is known as logit .

Table of distributions [ edit ] The following table shows how to rewrite a number of common distributions as exponential-family distributions with natural parameters. Refer to the flashcards [ 12 ] for main exponential families.

For a scalar variable and scalar parameter, the form is as follows: f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) T ( x ) − − A ( η η ) ] {\displaystyle f_{X}(x\mid \theta )=h(x)\exp \left[\eta ({\theta })T(x)-A(\eta )\right]} For a scalar variable and vector parameter: f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( η η ) ] f X ( x ∣ ∣ θ θ ) = h ( x ) g ( θ θ ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) ] {\displaystyle {\begin{aligned}f_{X}(x\mid {\boldsymbol {\theta }})&=h(x)\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x)-A({\boldsymbol {\eta }})\right]\\[4pt]f_{X}(x\mid {\boldsymbol {\theta }})&=h(x)\,g({\boldsymbol {\theta }})\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x)\right]\end{aligned}}} For a vector variable and vector parameter: f X ( x ∣ ∣ θ θ ) = h ( x ) exp ⁡ ⁡ [ η η ( θ θ ) ⋅ ⋅ T ( x ) − − A ( η η ) ] {\displaystyle f_{X}(\mathbf {x} \mid {\boldsymbol {\theta }})=h(\mathbf {x} )\,\exp \left[{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (\mathbf {x} )-A({\boldsymbol {\eta }})\right]} The above formulas choose the functional form of the exponential-family with a log-partition function A ( η η ) {\displaystyle A({\boldsymbol {\eta }})} . The reason for this is so that the moments of the sufficient statistics can be calculated easily, simply by differentiating this function.  Alternative forms involve either parameterizing this function in terms of the normal parameter θ θ {\displaystyle {\boldsymbol {\theta }}} instead of the natural parameter, and/or using a factor g ( η η ) {\displaystyle g({\boldsymbol {\eta }})} outside of the exponential. The relation between the latter and the former is: A ( η η ) = − − log ⁡ ⁡ g ( η η ) , g ( η η ) = e − − A ( η η ) {\displaystyle {\begin{aligned}A({\boldsymbol {\eta }})&=-\log g({\boldsymbol {\eta }}),\\[2pt]g({\boldsymbol {\eta }})&=e^{-A({\boldsymbol {\eta }})}\end{aligned}}} To convert between the representations involving the two types of parameter, use the formulas below for writing one type of parameter in terms of the other.

Distribution Parameter(s) θ Natural parameter(s) η Inverse parameter mapping Base measure h ( x ) Sufficient statistic T ( x ) Log-partition A ( η ) Log-partition A ( θ ) Bernoulli distribution p {\displaystyle p} log ⁡ ⁡ p 1 − − p {\displaystyle \log {\frac {p}{1-p}}} This is the logit function .

1 1 + e − − η η = e η η 1 + e η η {\displaystyle {\frac {1}{1+e^{-\eta }}}={\frac {e^{\eta }}{1+e^{\eta }}}} This is the logistic function .

1 {\displaystyle 1} x {\displaystyle x} log ⁡ ⁡ ( 1 + e η η ) {\displaystyle \log(1+e^{\eta })} − − log ⁡ ⁡ ( 1 − − p ) {\displaystyle -\log(1-p)} binomial distribution with known number of trials n {\displaystyle n} p {\displaystyle p} log ⁡ ⁡ p 1 − − p {\displaystyle \log {\frac {p}{1-p}}} 1 1 + e − − η η = e η η 1 + e η η {\displaystyle {\frac {1}{1+e^{-\eta }}}={\frac {e^{\eta }}{1+e^{\eta }}}} ( n x ) {\displaystyle {\binom {n}{x}}} x {\displaystyle x} n log ⁡ ⁡ ( 1 + e η η ) {\displaystyle n\log(1+e^{\eta })} − − n log ⁡ ⁡ ( 1 − − p ) {\displaystyle -n\log(1-p)} Poisson distribution λ λ {\displaystyle \lambda } log ⁡ ⁡ λ λ {\displaystyle \log \lambda } e η η {\displaystyle e^{\eta }} 1 x !

{\displaystyle {\frac {1}{x!}}} x {\displaystyle x} e η η {\displaystyle e^{\eta }} λ λ {\displaystyle \lambda } negative binomial distribution with known number of failures r {\displaystyle r} p {\displaystyle p} log ⁡ ⁡ ( 1 − − p ) {\displaystyle \log(1-p)} 1 − − e η η {\displaystyle 1-e^{\eta }} ( x + r − − 1 x ) {\displaystyle {\binom {x{+}r{-}1}{x}}} x {\displaystyle x} − − r log ⁡ ⁡ ( 1 − − e η η ) {\displaystyle -r\log(1-e^{\eta })} − − r log ⁡ ⁡ ( 1 − − p ) {\displaystyle -r\log(1-p)} exponential distribution λ λ {\displaystyle \lambda } − − λ λ {\displaystyle -\lambda } − − η η {\displaystyle -\eta } 1 {\displaystyle 1} x {\displaystyle x} − − log ⁡ ⁡ ( − − η η ) {\displaystyle -\log(-\eta )} − − log ⁡ ⁡ λ λ {\displaystyle -\log \lambda } Pareto distribution with known minimum value x m {\displaystyle x_{m}} α α {\displaystyle \alpha } − − α α − − 1 {\displaystyle -\alpha -1} − − 1 − − η η {\displaystyle -1-\eta } 1 {\displaystyle 1} log ⁡ ⁡ x {\displaystyle \log x} − − log ⁡ ⁡ ( − − 1 − − η η ) + ( 1 + η η ) log ⁡ ⁡ x m {\displaystyle {\begin{aligned}&-\log(-1-\eta )\\&+(1+\eta )\log x_{\mathrm {m} }\end{aligned}}} − − log ⁡ ⁡ ( α α x m α α ) {\displaystyle -\log \left(\alpha x_{\mathrm {m} }^{\alpha }\right)} Weibull distribution with known shape k λ λ {\displaystyle \lambda } − − 1 λ λ k {\displaystyle -{\frac {1}{\lambda ^{k}}}} ( − − η η ) − − 1 / k {\displaystyle (-\eta )^{-1/k}} x k − − 1 {\displaystyle x^{k-1}} x k {\displaystyle x^{k}} log ⁡ ⁡ ( − − 1 η η k ) {\displaystyle \log \left(-{\frac {1}{\eta k}}\right)} log ⁡ ⁡ λ λ k k {\displaystyle \log {\frac {\lambda ^{k}}{k}}} Laplace distribution with known mean μ μ {\displaystyle \mu } b {\displaystyle b} − − 1 b {\displaystyle -{\frac {1}{b}}} − − 1 η η {\displaystyle -{\frac {1}{\eta }}} 1 {\displaystyle 1} | x − − μ μ | {\displaystyle |x-\mu |} log ⁡ ⁡ ( − − 2 η η ) {\displaystyle \log \left(-{\frac {2}{\eta }}\right)} log ⁡ ⁡ 2 b {\displaystyle \log 2b} chi-squared distribution ν ν {\displaystyle \nu } ν ν 2 − − 1 {\displaystyle {\frac {\nu }{2}}-1} 2 ( η η + 1 ) {\displaystyle 2(\eta +1)} e − − x / 2 {\displaystyle e^{-x/2}} log ⁡ ⁡ x {\displaystyle \log x} log ⁡ ⁡ Γ Γ ( η η + 1 ) + ( η η + 1 ) log ⁡ ⁡ 2 {\displaystyle {\begin{aligned}&\log \Gamma (\eta +1)\\&+(\eta +1)\log 2\end{aligned}}} log ⁡ ⁡ Γ Γ ( ν ν 2 ) + ν ν 2 log ⁡ ⁡ 2 {\displaystyle {\begin{aligned}&\log \Gamma {\left({\tfrac {\nu }{2}}\right)}\\&+{\tfrac {\nu }{2}}\log 2\end{aligned}}} normal distribution known variance μ μ {\displaystyle \mu } μ μ σ σ {\displaystyle {\frac {\mu }{\sigma }}} σ σ η η {\displaystyle \sigma \eta } e − − x 2 / ( 2 σ σ 2 ) 2 π π σ σ {\displaystyle {\frac {e^{-x^{2}/(2\sigma ^{2})}}{{\sqrt {2\pi }}\sigma }}} x σ σ {\displaystyle {\frac {x}{\sigma }}} η η 2 2 {\displaystyle {\frac {\eta ^{2}}{2}}} μ μ 2 2 σ σ 2 {\displaystyle {\frac {\mu ^{2}}{2\sigma ^{2}}}} continuous Bernoulli distribution λ λ {\displaystyle \lambda } log ⁡ ⁡ λ λ 1 − − λ λ {\displaystyle \log {\frac {\lambda }{1-\lambda }}} e η η 1 + e η η {\displaystyle {\frac {e^{\eta }}{1+e^{\eta }}}} 1 {\displaystyle 1} x {\displaystyle x} log ⁡ ⁡ e η η − − 1 η η {\displaystyle \log {\frac {e^{\eta }-1}{\eta }}} log ⁡ ⁡ ( 1 − − 2 λ λ 1 − − λ λ ) − − log 2 ⁡ ⁡ ( 1 λ λ − − 1 ) {\displaystyle {\begin{aligned}&\log \left({\tfrac {1-2\lambda }{1-\lambda }}\right)\\[1ex]{}-{}&\log ^{2}\left({\tfrac {1}{\lambda }}-1\right)\end{aligned}}} where log 2 refers to the iterated logarithm normal distribution μ μ , σ σ 2 {\displaystyle \mu ,\ \sigma ^{2}} [ μ μ σ σ 2 − − 1 2 σ σ 2 ] {\displaystyle {\begin{bmatrix}{\dfrac {\mu }{\sigma ^{2}}}\\[1ex]-{\dfrac {1}{2\sigma ^{2}}}\end{bmatrix}}} [ − − η η 1 2 η η 2 − − 1 2 η η 2 ] {\displaystyle {\begin{bmatrix}-{\dfrac {\eta _{1}}{2\eta _{2}}}\\[1ex]-{\dfrac {1}{2\eta _{2}}}\end{bmatrix}}} 1 2 π π {\displaystyle {\frac {1}{\sqrt {2\pi }}}} [ x x 2 ] {\displaystyle {\begin{bmatrix}x\\x^{2}\end{bmatrix}}} − − η η 1 2 4 η η 2 − − 1 2 log ⁡ ⁡ ( − − 2 η η 2 ) {\displaystyle -{\frac {\eta _{1}^{2}}{4\eta _{2}}}-{\frac {1}{2}}\log(-2\eta _{2})} μ μ 2 2 σ σ 2 + log ⁡ ⁡ σ σ {\displaystyle {\frac {\mu ^{2}}{2\sigma ^{2}}}+\log \sigma } log-normal distribution μ μ , σ σ 2 {\displaystyle \mu ,\ \sigma ^{2}} [ μ μ σ σ 2 − − 1 2 σ σ 2 ] {\displaystyle {\begin{bmatrix}{\dfrac {\mu }{\sigma ^{2}}}\\[1ex]-{\dfrac {1}{2\sigma ^{2}}}\end{bmatrix}}} [ − − η η 1 2 η η 2 − − 1 2 η η 2 ] {\displaystyle {\begin{bmatrix}-{\dfrac {\eta _{1}}{2\eta _{2}}}\\[1ex]-{\dfrac {1}{2\eta _{2}}}\end{bmatrix}}} 1 2 π π x {\displaystyle {\frac {1}{{\sqrt {2\pi }}x}}} [ log ⁡ ⁡ x ( log ⁡ ⁡ x ) 2 ] {\displaystyle {\begin{bmatrix}\log x\\(\log x)^{2}\end{bmatrix}}} − − η η 1 2 4 η η 2 − − 1 2 log ⁡ ⁡ ( − − 2 η η 2 ) {\displaystyle -{\frac {\eta _{1}^{2}}{4\eta _{2}}}-{\frac {1}{2}}\log(-2\eta _{2})} μ μ 2 2 σ σ 2 + log ⁡ ⁡ σ σ {\displaystyle {\frac {\mu ^{2}}{2\sigma ^{2}}}+\log \sigma } inverse Gaussian distribution μ μ , λ λ {\displaystyle \mu ,\ \lambda } [ − − λ λ 2 μ μ 2 − − λ λ 2 ] {\displaystyle {\begin{bmatrix}-{\dfrac {\lambda }{2\mu ^{2}}}\\[15pt]-{\dfrac {\lambda }{2}}\end{bmatrix}}} [ η η 2 η η 1 − − 2 η η 2 ] {\displaystyle {\begin{bmatrix}{\sqrt {\dfrac {\eta _{2}}{\eta _{1}}}}\\[15pt]-2\eta _{2}\end{bmatrix}}} 1 2 π π x 3 / 2 {\displaystyle {\frac {1}{{\sqrt {2\pi }}x^{3/2}}}} [ x 1 x ] {\displaystyle {\begin{bmatrix}x\\[5pt]{\dfrac {1}{x}}\end{bmatrix}}} − − 2 η η 1 η η 2 − − 1 2 log ⁡ ⁡ ( − − 2 η η 2 ) {\displaystyle -2{\sqrt {\eta _{1}\eta _{2}}}-{\tfrac {1}{2}}\log(-2\eta _{2})} − − λ λ μ μ − − 1 2 log ⁡ ⁡ λ λ {\displaystyle -{\tfrac {\lambda }{\mu }}-{\tfrac {1}{2}}\log \lambda } gamma distribution α α , β β {\displaystyle \alpha ,\ \beta } [ α α − − 1 − − β β ] {\displaystyle {\begin{bmatrix}\alpha -1\\-\beta \end{bmatrix}}} [ η η 1 + 1 − − η η 2 ] {\displaystyle {\begin{bmatrix}\eta _{1}+1\\-\eta _{2}\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x x ] {\displaystyle {\begin{bmatrix}\log x\\x\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( η η 1 + 1 ) − − ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) {\displaystyle {\begin{aligned}&\log \Gamma (\eta _{1}+1)\\{}-{}&(\eta _{1}+1)\log(-\eta _{2})\end{aligned}}} log ⁡ ⁡ Γ Γ ( α α ) β β α α {\displaystyle \log {\frac {\Gamma (\alpha )}{\beta ^{\alpha }}}} k , θ θ {\displaystyle k,\ \theta } [ k − − 1 − − 1 θ θ ] {\displaystyle {\begin{bmatrix}k-1\\[5pt]-{\dfrac {1}{\theta }}\end{bmatrix}}} [ η η 1 + 1 − − 1 η η 2 ] {\displaystyle {\begin{bmatrix}\eta _{1}+1\\[5pt]-{\dfrac {1}{\eta _{2}}}\end{bmatrix}}} log ⁡ ⁡ ( θ θ k Γ Γ ( k ) ) {\displaystyle \log \left(\theta ^{k}\Gamma (k)\right)} inverse gamma distribution α α , β β {\displaystyle \alpha ,\ \beta } [ − − α α − − 1 − − β β ] {\displaystyle {\begin{bmatrix}-\alpha -1\\-\beta \end{bmatrix}}} [ − − η η 1 − − 1 − − η η 2 ] {\displaystyle {\begin{bmatrix}-\eta _{1}-1\\-\eta _{2}\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x 1 x ] {\displaystyle {\begin{bmatrix}\log x\\{\frac {1}{x}}\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( − − η η 1 − − 1 ) + ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) {\displaystyle {\begin{aligned}&\log \Gamma (-\eta _{1}-1)\\+&\left(\eta _{1}+1\right)\log(-\eta _{2})\end{aligned}}} log ⁡ ⁡ Γ Γ ( α α ) β β α α {\displaystyle \log {\frac {\Gamma (\alpha )}{\beta ^{\alpha }}}} generalized inverse Gaussian distribution p , a , b {\displaystyle p,\ a,\ b} [ p − − 1 − − a / 2 − − b / 2 ] {\displaystyle {\begin{bmatrix}p-1\\-a/2\\-b/2\end{bmatrix}}} [ η η 1 + 1 − − 2 η η 2 − − 2 η η 3 ] {\displaystyle {\begin{bmatrix}\eta _{1}+1\\-2\eta _{2}\\-2\eta _{3}\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x x 1 x ] {\displaystyle {\begin{bmatrix}\log x\\x\\{\frac {1}{x}}\end{bmatrix}}} log ⁡ ⁡ 2 K η η 1 + 1 ( 4 η η 2 η η 3 ) − − η η 1 + 1 2 log ⁡ ⁡ η η 2 η η 3 {\displaystyle {\begin{aligned}&\log 2K_{\eta _{1}+1}{\!\left({\sqrt {4\eta _{2}\eta _{3}}}\right)}\\[2pt]{}-{}&{\frac {\eta _{1}+1}{2}}\log {\frac {\eta _{2}}{\eta _{3}}}\end{aligned}}} log ⁡ ⁡ 2 K p ( a b ) − − p 2 log ⁡ ⁡ a b {\displaystyle {\begin{aligned}&\log 2K_{p}({\sqrt {ab}})\\[2pt]&{}-{\frac {p}{2}}\log {\frac {a}{b}}\end{aligned}}} scaled inverse chi-squared distribution ν ν , σ σ 2 {\displaystyle \nu ,\ \sigma ^{2}} [ − − ν ν 2 − − 1 − − ν ν σ σ 2 2 ] {\displaystyle {\begin{bmatrix}-{\dfrac {\nu }{2}}-1\\[10pt]-{\dfrac {\nu \sigma ^{2}}{2}}\end{bmatrix}}} [ − − 2 ( η η 1 + 1 ) η η 2 η η 1 + 1 ] {\displaystyle {\begin{bmatrix}-2(\eta _{1}+1)\\[10pt]{\dfrac {\eta _{2}}{\eta _{1}+1}}\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x 1 x ] {\displaystyle {\begin{bmatrix}\log x\\{\frac {1}{x}}\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( − − η η 1 − − 1 ) + ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) {\displaystyle {\begin{aligned}&\log \Gamma (-\eta _{1}-1)\\[2pt]+&\left(\eta _{1}+1\right)\log(-\eta _{2})\end{aligned}}} log ⁡ ⁡ Γ Γ ( ν ν 2 ) − − ν ν 2 log ⁡ ⁡ ν ν σ σ 2 2 {\displaystyle {\begin{aligned}&\log \Gamma {\left({\frac {\nu }{2}}\right)}\\[2pt]{}-{}&{\frac {\nu }{2}}\log {\frac {\nu \sigma ^{2}}{2}}\end{aligned}}} beta distribution (variant 1) α α , β β {\displaystyle \alpha ,\ \beta } [ α α β β ] {\displaystyle {\begin{bmatrix}\alpha \\\beta \end{bmatrix}}} [ η η 1 η η 2 ] {\displaystyle {\begin{bmatrix}\eta _{1}\\\eta _{2}\end{bmatrix}}} 1 x ( 1 − − x ) {\displaystyle {\frac {1}{x(1-x)}}} [ log ⁡ ⁡ x log ⁡ ⁡ ( 1 − − x ) ] {\displaystyle {\begin{bmatrix}\log x\\\log(1{-}x)\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( η η 1 ) Γ Γ ( η η 2 ) Γ Γ ( η η 1 + η η 2 ) {\displaystyle \log {\frac {\Gamma (\eta _{1})\,\Gamma (\eta _{2})}{\Gamma (\eta _{1}+\eta _{2})}}} log ⁡ ⁡ Γ Γ ( α α ) Γ Γ ( β β ) Γ Γ ( α α + β β ) {\displaystyle \log {\frac {\Gamma (\alpha )\,\Gamma (\beta )}{\Gamma (\alpha +\beta )}}} beta distribution (variant 2) α α , β β {\displaystyle \alpha ,\ \beta } [ α α − − 1 β β − − 1 ] {\displaystyle {\begin{bmatrix}\alpha -1\\\beta -1\end{bmatrix}}} [ η η 1 + 1 η η 2 + 1 ] {\displaystyle {\begin{bmatrix}\eta _{1}+1\\\eta _{2}+1\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x log ⁡ ⁡ ( 1 − − x ) ] {\displaystyle {\begin{bmatrix}\log x\\\log(1{-}x)\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( η η 1 + 1 ) Γ Γ ( η η 2 + 1 ) Γ Γ ( η η 1 + η η 2 + 2 ) {\displaystyle \log {\frac {\Gamma (\eta _{1}+1)\,\Gamma (\eta _{2}+1)}{\Gamma (\eta _{1}+\eta _{2}+2)}}} log ⁡ ⁡ Γ Γ ( α α ) Γ Γ ( β β ) Γ Γ ( α α + β β ) {\displaystyle \log {\frac {\Gamma (\alpha )\,\Gamma (\beta )}{\Gamma (\alpha +\beta )}}} multivariate normal distribution μ μ , Σ Σ {\displaystyle {\boldsymbol {\mu }},\ {\boldsymbol {\Sigma }}} [ Σ Σ − − 1 μ μ − − 1 2 Σ Σ − − 1 ] {\displaystyle {\begin{bmatrix}{\boldsymbol {\Sigma }}^{-1}{\boldsymbol {\mu }}\\[5pt]-{\frac {1}{2}}{\boldsymbol {\Sigma }}^{-1}\end{bmatrix}}} [ − − 1 2 η η 2 − − 1 η η 1 − − 1 2 η η 2 − − 1 ] {\displaystyle {\begin{bmatrix}-{\frac {1}{2}}{\boldsymbol {\eta }}_{2}^{-1}{\boldsymbol {\eta }}_{1}\\[5pt]-{\frac {1}{2}}{\boldsymbol {\eta }}_{2}^{-1}\end{bmatrix}}} ( 2 π π ) − − k 2 {\displaystyle (2\pi )^{-{\frac {k}{2}}}} [ x x x T ] {\displaystyle {\begin{bmatrix}\mathbf {x} \\[5pt]\mathbf {x} \mathbf {x} ^{\mathsf {T}}\end{bmatrix}}} − − 1 4 η η 1 T η η 2 − − 1 η η 1 − − 1 2 log ⁡ ⁡ | − − 2 η η 2 | {\displaystyle {\begin{aligned}&-{\tfrac {1}{4}}{\boldsymbol {\eta }}_{1}^{\mathsf {T}}{\boldsymbol {\eta }}_{2}^{-1}{\boldsymbol {\eta }}_{1}\\&-{\tfrac {1}{2}}\log \left|-2{\boldsymbol {\eta }}_{2}\right|\end{aligned}}} 1 2 μ μ T Σ Σ − − 1 μ μ + 1 2 log ⁡ ⁡ | Σ Σ | {\displaystyle {\begin{aligned}&{\tfrac {1}{2}}{\boldsymbol {\mu }}^{\mathsf {T}}{\boldsymbol {\Sigma }}^{-1}{\boldsymbol {\mu }}\\+&{\tfrac {1}{2}}\log \left|{\boldsymbol {\Sigma }}\right|\end{aligned}}} categorical distribution (variant 1) p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where ∑ ∑ i = 1 k p i = 1 {\textstyle \sum \limits _{i=1}^{k}p_{i}=1} [ log ⁡ ⁡ p 1 ⋮ ⋮ log ⁡ ⁡ p k ] {\displaystyle {\begin{bmatrix}\log p_{1}\\\vdots \\\log p_{k}\end{bmatrix}}} [ e η η 1 ⋮ ⋮ e η η k ] {\displaystyle {\begin{bmatrix}e^{\eta _{1}}\\\vdots \\e^{\eta _{k}}\end{bmatrix}}} where ∑ ∑ i = 1 k e η η i = 1 {\textstyle \sum \limits _{i=1}^{k}e^{\eta _{i}}=1} 1 {\displaystyle 1} [ [ x = 1 ] ⋮ ⋮ [ x = k ] ] {\displaystyle {\begin{bmatrix}[x=1]\\\vdots \\{[x=k]}\end{bmatrix}}} [ x = i ] {\displaystyle [x=i]} is the Iverson bracket [ i ] 0 {\displaystyle 0} 0 {\displaystyle 0} categorical distribution (variant 2) p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where ∑ ∑ i = 1 k p i = 1 {\textstyle \sum \limits _{i=1}^{k}p_{i}=1} [ log ⁡ ⁡ p 1 + C ⋮ ⋮ log ⁡ ⁡ p k + C ] {\displaystyle {\begin{bmatrix}\log p_{1}+C\\\vdots \\\log p_{k}+C\end{bmatrix}}} 1 C [ e η η 1 ⋮ ⋮ e η η k ] {\displaystyle {\frac {1}{C}}{\begin{bmatrix}e^{\eta _{1}}\\\vdots \\e^{\eta _{k}}\end{bmatrix}}} where C = ∑ ∑ i = 1 k e η η i {\textstyle C=\sum \limits _{i=1}^{k}e^{\eta _{i}}} 1 {\displaystyle 1} [ [ x = 1 ] ⋮ ⋮ [ x = k ] ] {\displaystyle {\begin{bmatrix}[x=1]\\\vdots \\{[x=k]}\end{bmatrix}}} [ x = i ] {\displaystyle [x=i]} is the Iverson bracket [ i ] 0 {\displaystyle 0} 0 {\displaystyle 0} categorical distribution (variant 3) p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where p k = 1 − − ∑ ∑ i = 1 k − − 1 p i {\textstyle p_{k}=1-\sum \limits _{i=1}^{k-1}p_{i}} [ log ⁡ ⁡ p 1 p k ⋮ ⋮ log ⁡ ⁡ p k − − 1 p k 0 ] {\displaystyle {\begin{bmatrix}\log {\dfrac {p_{1}}{p_{k}}}\\[10pt]\vdots \\[5pt]\log {\dfrac {p_{k-1}}{p_{k}}}\\[15pt]0\end{bmatrix}}} This is the inverse softmax function , a generalization of the logit function .

1 C 1 [ e η η 1 ⋮ ⋮ e η η k ] = {\displaystyle {\frac {1}{C_{1}}}{\begin{bmatrix}e^{\eta _{1}}\\[5pt]\vdots \\[5pt]e^{\eta _{k}}\end{bmatrix}}=} 1 C 2 [ e η η 1 ⋮ ⋮ e η η k − − 1 1 ] {\displaystyle {\frac {1}{C_{2}}}{\begin{bmatrix}e^{\eta _{1}}\\[5pt]\vdots \\[5pt]e^{\eta _{k-1}}\\[5pt]1\end{bmatrix}}} where C 1 = ∑ ∑ i = 1 k e η η i {\textstyle C_{1}=\sum \limits _{i=1}^{k}e^{\eta _{i}}} and C 2 = 1 + ∑ ∑ i = 1 k − − 1 e η η i {\textstyle C_{2}=1+\sum \limits _{i=1}^{k-1}e^{\eta _{i}}} .

This is the softmax function , a generalization of the logistic function .

1 {\displaystyle 1} [ [ x = 1 ] ⋮ ⋮ [ x = k ] ] {\displaystyle {\begin{bmatrix}[x=1]\\\vdots \\{[x=k]}\end{bmatrix}}} [ x = i ] {\displaystyle [x=i]} is the Iverson bracket [ i ] log ⁡ ⁡ ( ∑ ∑ i = 1 k e η η i ) = log ⁡ ⁡ ( 1 + ∑ ∑ i = 1 k − − 1 e η η i ) {\displaystyle {\begin{aligned}&\textstyle \log \left(\sum \limits _{i=1}^{k}e^{\eta _{i}}\right)\\={}&\textstyle \log \left(1+\sum \limits _{i=1}^{k-1}e^{\eta _{i}}\right)\end{aligned}}} − − log ⁡ ⁡ p k {\displaystyle -\log p_{k}} multinomial distribution (variant 1) with known number of trials n p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where ∑ ∑ i = 1 k p i = 1 {\textstyle \sum \limits _{i=1}^{k}p_{i}=1} [ log ⁡ ⁡ p 1 ⋮ ⋮ log ⁡ ⁡ p k ] {\displaystyle {\begin{bmatrix}\log p_{1}\\\vdots \\\log p_{k}\end{bmatrix}}} [ e η η 1 ⋮ ⋮ e η η k ] {\displaystyle {\begin{bmatrix}e^{\eta _{1}}\\\vdots \\e^{\eta _{k}}\end{bmatrix}}} where ∑ ∑ i = 1 k e η η i = 1 {\textstyle \sum \limits _{i=1}^{k}e^{\eta _{i}}=1} n !

∏ ∏ i = 1 k x i !

{\displaystyle {\frac {n!}{\prod \limits _{i=1}^{k}x_{i}!}}} [ x 1 ⋮ ⋮ x k ] {\displaystyle {\begin{bmatrix}x_{1}\\\vdots \\x_{k}\end{bmatrix}}} 0 {\displaystyle 0} 0 {\displaystyle 0} multinomial distribution (variant 2) with known number of trials n {\displaystyle n} p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where ∑ ∑ i = 1 k p i = 1 {\textstyle \sum \limits _{i=1}^{k}p_{i}=1} [ log ⁡ ⁡ p 1 + C ⋮ ⋮ log ⁡ ⁡ p k + C ] {\displaystyle {\begin{bmatrix}\log p_{1}+C\\\vdots \\\log p_{k}+C\end{bmatrix}}} 1 C [ e η η 1 ⋮ ⋮ e η η k ] {\displaystyle {\frac {1}{C}}{\begin{bmatrix}e^{\eta _{1}}\\\vdots \\e^{\eta _{k}}\end{bmatrix}}} where C = ∑ ∑ i = 1 k e η η i {\textstyle C=\sum \limits _{i=1}^{k}e^{\eta _{i}}} n !

∏ ∏ i = 1 k x i !

{\displaystyle {\frac {n!}{\prod \limits _{i=1}^{k}x_{i}!}}} [ x 1 ⋮ ⋮ x k ] {\displaystyle {\begin{bmatrix}x_{1}\\\vdots \\x_{k}\end{bmatrix}}} 0 {\displaystyle 0} 0 {\displaystyle 0} multinomial distribution (variant 3) with known number of trials n {\displaystyle n} p 1 , … … , p k {\displaystyle p_{1},\ \ldots ,\,p_{k}} where p k = 1 − − ∑ ∑ i = 1 k − − 1 p i {\textstyle p_{k}=1-\sum \limits _{i=1}^{k-1}p_{i}} [ log ⁡ ⁡ p 1 p k ⋮ ⋮ log ⁡ ⁡ p k − − 1 p k 0 ] {\displaystyle {\begin{bmatrix}\log {\dfrac {p_{1}}{p_{k}}}\\[10pt]\vdots \\[5pt]\log {\dfrac {p_{k-1}}{p_{k}}}\\[15pt]0\end{bmatrix}}} 1 C 1 [ e η η 1 ⋮ ⋮ e η η k ] = {\displaystyle {\frac {1}{C_{1}}}{\begin{bmatrix}e^{\eta _{1}}\\[10pt]\vdots \\[5pt]e^{\eta _{k}}\end{bmatrix}}=} 1 C 2 [ e η η 1 ⋮ ⋮ e η η k − − 1 1 ] {\displaystyle {\frac {1}{C_{2}}}{\begin{bmatrix}e^{\eta _{1}}\\[5pt]\vdots \\[5pt]e^{\eta _{k-1}}\\[5pt]1\end{bmatrix}}} where C 1 = ∑ ∑ i = 1 k e η η i {\textstyle C_{1}=\sum \limits _{i=1}^{k}e^{\eta _{i}}} and C 2 = 1 + ∑ ∑ i = 1 k − − 1 e η η i {\textstyle C_{2}=1+\sum \limits _{i=1}^{k-1}e^{\eta _{i}}} n !

∏ ∏ i = 1 k x i !

{\displaystyle {\frac {n!}{\prod \limits _{i=1}^{k}x_{i}!}}} [ x 1 ⋮ ⋮ x k ] {\displaystyle {\begin{bmatrix}x_{1}\\\vdots \\x_{k}\end{bmatrix}}} n log ⁡ ⁡ ( ∑ ∑ i = 1 k e η η i ) = n log ⁡ ⁡ ( 1 + ∑ ∑ i = 1 k − − 1 e η η i ) {\displaystyle {\begin{aligned}&\textstyle n\log \left(\sum \limits _{i=1}^{k}e^{\eta _{i}}\right)\\[4pt]={}&\textstyle n\log \left(1+\sum \limits _{i=1}^{k-1}e^{\eta _{i}}\right)\end{aligned}}} − − n log ⁡ ⁡ p k {\displaystyle -n\log p_{k}} Dirichlet distribution (variant 1) α α 1 , … … , α α k {\displaystyle \alpha _{1},\ \ldots ,\,\alpha _{k}} [ α α 1 ⋮ ⋮ α α k ] {\displaystyle {\begin{bmatrix}\alpha _{1}\\\vdots \\\alpha _{k}\end{bmatrix}}} [ η η 1 ⋮ ⋮ η η k ] {\displaystyle {\begin{bmatrix}\eta _{1}\\\vdots \\\eta _{k}\end{bmatrix}}} 1 ∏ ∏ i = 1 k x i {\displaystyle {\frac {1}{\prod \limits _{i=1}^{k}x_{i}}}} [ log ⁡ ⁡ x 1 ⋮ ⋮ log ⁡ ⁡ x k ] {\displaystyle {\begin{bmatrix}\log x_{1}\\\vdots \\\log x_{k}\end{bmatrix}}} ∑ ∑ i = 1 k log ⁡ ⁡ Γ Γ ( η η i ) − − log ⁡ ⁡ Γ Γ ( ∑ ∑ i = 1 k η η i ) {\displaystyle {\begin{aligned}\textstyle \sum \limits _{i=1}^{k}\log \Gamma (\eta _{i})\\\textstyle -\log \Gamma {\left(\sum \limits _{i=1}^{k}\eta _{i}\right)}\end{aligned}}} ∑ ∑ i = 1 k log ⁡ ⁡ Γ Γ ( α α i ) − − log ⁡ ⁡ Γ Γ ( ∑ ∑ i = 1 k α α i ) {\displaystyle {\begin{aligned}&\textstyle \sum \limits _{i=1}^{k}\log \Gamma (\alpha _{i})\\{}-{}&\textstyle \log \Gamma {\left(\sum \limits _{i=1}^{k}\alpha _{i}\right)}\end{aligned}}} Dirichlet distribution (variant 2) α α 1 , … … , α α k {\displaystyle \alpha _{1},\ \ldots ,\,\alpha _{k}} [ α α 1 − − 1 ⋮ ⋮ α α k − − 1 ] {\displaystyle {\begin{bmatrix}\alpha _{1}-1\\\vdots \\\alpha _{k}-1\end{bmatrix}}} [ η η 1 + 1 ⋮ ⋮ η η k + 1 ] {\displaystyle {\begin{bmatrix}\eta _{1}+1\\\vdots \\\eta _{k}+1\end{bmatrix}}} 1 {\displaystyle 1} [ log ⁡ ⁡ x 1 ⋮ ⋮ log ⁡ ⁡ x k ] {\displaystyle {\begin{bmatrix}\log x_{1}\\\vdots \\\log x_{k}\end{bmatrix}}} ∑ ∑ i = 1 k log ⁡ ⁡ Γ Γ ( η η i + 1 ) − − log ⁡ ⁡ Γ Γ ( ∑ ∑ i = 1 k ( η η i + 1 ) ) {\displaystyle {\begin{aligned}&\textstyle \sum \limits _{i=1}^{k}\log \Gamma (\eta _{i}+1)\\{}-{}&\textstyle \log \Gamma {\left(\sum \limits _{i=1}^{k}(\eta _{i}+1)\right)}\end{aligned}}} ∑ ∑ i = 1 k log ⁡ ⁡ Γ Γ ( α α i ) − − log ⁡ ⁡ Γ Γ ( ∑ ∑ i = 1 k α α i ) {\displaystyle {\begin{aligned}&\textstyle \sum \limits _{i=1}^{k}\log \Gamma (\alpha _{i})\\{}-{}&\textstyle \log \Gamma {\left(\sum \limits _{i=1}^{k}\alpha _{i}\right)}\end{aligned}}} Wishart distribution V , n {\displaystyle \mathbf {V} ,\ n} [ − − 1 2 V − − 1 n − − p − − 1 2 ] {\displaystyle {\begin{bmatrix}-{\frac {1}{2}}\mathbf {V} ^{-1}\\[5pt]{\dfrac {n{-}p{-}1}{2}}\end{bmatrix}}} [ − − 1 2 η η 1 − − 1 2 η η 2 + p + 1 ] {\displaystyle {\begin{bmatrix}-{\frac {1}{2}}{\boldsymbol {\eta }}_{1}^{-1}\\[5pt]2\eta _{2}{+}p{+}1\end{bmatrix}}} 1 {\displaystyle 1} [ X log ⁡ ⁡ | X | ] {\displaystyle {\begin{bmatrix}\mathbf {X} \\\log |\mathbf {X} |\end{bmatrix}}} − − [ η η 2 + p + 1 2 ] log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( η η 2 + p + 1 2 ) = − − n 2 log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( n 2 ) = [ η η 2 + p + 1 2 ] log ⁡ ⁡ ( 2 p | V | ) + log ⁡ ⁡ Γ Γ p ( η η 2 + p + 1 2 ) {\displaystyle {\begin{aligned}&-\left[\eta _{2}+{\tfrac {p+1}{2}}\right]\log \left|-{\boldsymbol {\eta }}_{1}\right|\\&+\log \Gamma _{p}{\left(\eta _{2}+{\tfrac {p+1}{2}}\right)}\\[1ex]=&-{\tfrac {n}{2}}\log \left|-{\boldsymbol {\eta }}_{1}\right|\\&+\log \Gamma _{p}{\left({\tfrac {n}{2}}\right)}\\[1ex]={}&\left[\eta _{2}+{\tfrac {p+1}{2}}\right]\log \left(2^{p}\left|\mathbf {V} \right|\right)\\&+\log \Gamma _{p}{\left(\eta _{2}+{\tfrac {p+1}{2}}\right)}\end{aligned}}} Three variants with different parameterizations are given, to facilitate computing moments of the sufficient statistics.

n 2 log ⁡ ⁡ ( 2 p | V | ) + log ⁡ ⁡ Γ Γ p ( n 2 ) {\displaystyle {\begin{aligned}&{\frac {n}{2}}\log \left(2^{p}\left|\mathbf {V} \right|\right)\\[2pt]&+\log \Gamma _{p}{\left({\frac {n}{2}}\right)}\end{aligned}}} Note : Uses the fact that tr ⁡ ⁡ ( A T B ) = vec ⁡ ⁡ ( A ) ⋅ ⋅ vec ⁡ ⁡ ( B ) , {\displaystyle \operatorname {tr} (\mathbf {A} ^{\mathsf {T}}\mathbf {B} )=\operatorname {vec} (\mathbf {A} )\cdot \operatorname {vec} (\mathbf {B} ),} i.e. the trace of a matrix product is much like a dot product . The matrix parameters are assumed to be vectorized (laid out in a vector) when inserted into the exponential form. Also, V {\displaystyle \mathbf {V} } and X {\displaystyle \mathbf {X} } are symmetric, so e.g.

V T = V .

{\displaystyle \mathbf {V} ^{\mathsf {T}}=\mathbf {V} \ .} inverse Wishart distribution Ψ Ψ , m {\displaystyle \mathbf {\Psi } ,\,m} − − 1 2 [ Ψ Ψ m + p + 1 ] {\displaystyle -{\frac {1}{2}}{\begin{bmatrix}{\boldsymbol {\Psi }}\\[5pt]m{+}p{+}1\end{bmatrix}}} − − [ 2 η η 1 2 η η 2 + p + 1 ] {\displaystyle -{\begin{bmatrix}2{\boldsymbol {\eta }}_{1}\\[5pt]2\eta _{2}{+}p{+}1\end{bmatrix}}} 1 {\displaystyle 1} [ X − − 1 log ⁡ ⁡ | X | ] {\displaystyle {\begin{bmatrix}\mathbf {X} ^{-1}\\\log |\mathbf {X} |\end{bmatrix}}} [ η η 2 + p + 1 2 ] log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( − − η η 2 − − p + 1 2 ) = − − m 2 log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( m 2 ) = − − [ η η 2 + p + 1 2 ] log ⁡ ⁡ 2 p | Ψ Ψ | + log ⁡ ⁡ Γ Γ p ( − − η η 2 − − p + 1 2 ) {\displaystyle {\begin{aligned}&\left[\eta _{2}+{\tfrac {p+1}{2}}\right]\log \left|-{\boldsymbol {\eta }}_{1}\right|\\&+\log \Gamma _{p}{\left(-\eta _{2}-{\tfrac {p+1}{2}}\right)}\\[1ex]=&-{\tfrac {m}{2}}\log \left|-{\boldsymbol {\eta }}_{1}\right|\\&+\log \Gamma _{p}{\left({\tfrac {m}{2}}\right)}\\[1ex]=&-\left[\eta _{2}+{\tfrac {p+1}{2}}\right]\log {\tfrac {2^{p}}{\left|{\boldsymbol {\Psi }}\right|}}\\&+\log \Gamma _{p}{\left(-\eta _{2}-{\tfrac {p+1}{2}}\right)}\end{aligned}}} m 2 log ⁡ ⁡ 2 p | Ψ Ψ | + log ⁡ ⁡ Γ Γ p ( m 2 ) {\displaystyle {\begin{aligned}{\frac {m}{2}}\log {\frac {2^{p}}{|{\boldsymbol {\Psi }}|}}\\[4pt]+\log \Gamma _{p}{\left({\frac {m}{2}}\right)}\end{aligned}}} normal-gamma distribution α α , β β , μ μ , λ λ {\displaystyle \alpha ,\ \beta ,\ \mu ,\ \lambda } [ α α − − 1 2 − − β β − − λ λ μ μ 2 2 λ λ μ μ − − λ λ 2 ] {\displaystyle {\begin{bmatrix}\alpha -{\frac {1}{2}}\\-\beta -{\dfrac {\lambda \mu ^{2}}{2}}\\\lambda \mu \\-{\dfrac {\lambda }{2}}\end{bmatrix}}} [ η η 1 + 1 2 − − η η 2 + η η 3 2 4 η η 4 − − η η 3 2 η η 4 − − 2 η η 4 ] {\displaystyle {\begin{bmatrix}\eta _{1}+{\frac {1}{2}}\\-\eta _{2}+{\dfrac {\eta _{3}^{2}}{4\eta _{4}}}\\-{\dfrac {\eta _{3}}{2\eta _{4}}}\\-2\eta _{4}\end{bmatrix}}} 1 2 π π {\displaystyle {\dfrac {1}{\sqrt {2\pi }}}} [ log ⁡ ⁡ τ τ τ τ τ τ x τ τ x 2 ] {\displaystyle {\begin{bmatrix}\log \tau \\\tau \\\tau x\\\tau x^{2}\end{bmatrix}}} log ⁡ ⁡ Γ Γ ( η η 1 + 1 2 ) − − 1 2 log ⁡ ⁡ ( − − 2 η η 4 ) − − ( η η 1 + 1 2 ) log ⁡ ⁡ ( η η 3 2 4 η η 4 − − η η 2 ) {\displaystyle {\begin{aligned}&\log \Gamma {\left(\eta _{1}+{\tfrac {1}{2}}\right)}\\[2pt]-{}&{\tfrac {1}{2}}\log \left(-2\eta _{4}\right)\\[2pt]-{}&\left(\eta _{1}+{\tfrac {1}{2}}\right)\log \left({\tfrac {\eta _{3}^{2}}{4\eta _{4}}}-\eta _{2}\right)\end{aligned}}} log ⁡ ⁡ Γ Γ ( α α ) − − α α log ⁡ ⁡ β β − − 1 2 log ⁡ ⁡ λ λ {\displaystyle {\begin{aligned}&\log \Gamma {\left(\alpha \right)}\\[2pt]&-\alpha \log \beta \\[2pt]&-{\tfrac {1}{2}}\log \lambda \end{aligned}}} ^ a b c The Iverson bracket is a generalization of the discrete delta-function: If the bracketed expression is true, the bracket has value 1; if the enclosed statement is false, the Iverson bracket is zero. There are many variant notations, e.g. wavey brackets: ⧙ a = b ⧘ is equivalent to the [ a = b ] notation used above.

The three variants of the categorical distribution and multinomial distribution are due to the fact that the parameters p i {\displaystyle p_{i}} are constrained, such that ∑ ∑ i = 1 k p i = 1 .

{\displaystyle \sum _{i=1}^{k}p_{i}=1\,.} Thus, there are only k − − 1 {\displaystyle k-1} independent parameters.

Variant 1 uses k {\displaystyle k} natural parameters with a simple relation between the standard and natural parameters; however, only k − − 1 {\displaystyle k-1} of the natural parameters are independent, and the set of k {\displaystyle k} natural parameters is nonidentifiable .  The constraint on the usual parameters translates to a similar constraint on the natural parameters.

Variant 2 demonstrates the fact that the entire set of natural parameters is nonidentifiable: Adding any constant value to the natural parameters has no effect on the resulting distribution. However, by using the constraint on the natural parameters, the formula for the normal parameters in terms of the natural parameters can be written in a way that is independent on the constant that is added.

Variant 3 shows how to make the parameters identifiable in a convenient way by setting C = − − log ⁡ ⁡ p k .

{\displaystyle C=-\log p_{k}\ .} This effectively "pivots" around p k {\displaystyle p_{k}} and causes the last natural parameter to have the constant value of 0. All the remaining formulas are written in a way that does not access p k {\displaystyle p_{k}} , so that effectively the model has only k − − 1 {\displaystyle k-1} parameters, both of the usual and natural kind.

Variants 1 and 2 are not actually standard exponential families at all. Rather they are curved exponential families , i.e. there are k − − 1 {\displaystyle k-1} independent parameters embedded in a k {\displaystyle k} -dimensional parameter space.

[ 13 ] Many of the standard results for exponential families do not apply to curved exponential families. An example is the log-partition function A ( x ) {\displaystyle A(x)} , which has the value of 0 in the curved cases. In standard exponential families, the derivatives of this function correspond to the moments (more technically, the cumulants ) of the sufficient statistics, e.g. the mean and variance. However, a value of 0 suggests that the mean and variance of all the sufficient statistics are uniformly 0, whereas in fact the mean of the i {\displaystyle i} th sufficient statistic should be p i {\displaystyle p_{i}} . (This does emerge correctly when using the form of A ( x ) {\displaystyle A(x)} shown in variant 3.) Moments and cumulants of the sufficient statistic [ edit ] Normalization of the distribution [ edit ] We start with the normalization of the probability distribution. In general, any non-negative function f ( x ) that serves as the kernel of a probability distribution (the part encoding all dependence on x ) can be made into a proper distribution by normalizing : i.e.

p ( x ) = 1 Z f ( x ) {\displaystyle p(x)={\frac {1}{Z}}f(x)} where Z = ∫ ∫ x f ( x ) d x .

{\displaystyle Z=\int _{x}f(x)\,dx.} The factor Z is sometimes termed the normalizer or partition function , based on an analogy to statistical physics .

In the case of an exponential family where p ( x ; η η ) = g ( η η ) h ( x ) e η η ⋅ ⋅ T ( x ) , {\displaystyle p(x;{\boldsymbol {\eta }})=g({\boldsymbol {\eta }})h(x)e^{{\boldsymbol {\eta }}\cdot \mathbf {T} (x)},} the kernel is K ( x ) = h ( x ) e η η ⋅ ⋅ T ( x ) {\displaystyle K(x)=h(x)e^{{\boldsymbol {\eta }}\cdot \mathbf {T} (x)}} and the partition function is Z = ∫ ∫ x h ( x ) e η η ⋅ ⋅ T ( x ) d x .

{\displaystyle Z=\int _{x}h(x)e^{{\boldsymbol {\eta }}\cdot \mathbf {T} (x)}\,dx.} Since the distribution must be normalized, we have 1 = ∫ ∫ x g ( η η ) h ( x ) e η η ⋅ ⋅ T ( x ) d x = g ( η η ) ∫ ∫ x h ( x ) e η η ⋅ ⋅ T ( x ) d x = g ( η η ) Z .

{\displaystyle {\begin{aligned}1&=\int _{x}g({\boldsymbol {\eta }})h(x)e^{{\boldsymbol {\eta }}\cdot \mathbf {T} (x)}\,dx\\&=g({\boldsymbol {\eta }})\int _{x}h(x)e^{{\boldsymbol {\eta }}\cdot \mathbf {T} (x)}\,dx\\[1ex]&=g({\boldsymbol {\eta }})Z.\end{aligned}}} In other words, g ( η η ) = 1 Z {\displaystyle g({\boldsymbol {\eta }})={\frac {1}{Z}}} or equivalently A ( η η ) = − − log ⁡ ⁡ g ( η η ) = log ⁡ ⁡ Z .

{\displaystyle A({\boldsymbol {\eta }})=-\log g({\boldsymbol {\eta }})=\log Z.} This justifies calling A the log-normalizer or log-partition function .

Moment-generating function of the sufficient statistic [ edit ] Now, the moment-generating function of T ( x ) is M T ( u ) ≡ ≡ E ⁡ ⁡ [ exp ⁡ ⁡ ( u T T ( x ) ) ∣ ∣ η η ] = ∫ ∫ x h ( x ) exp ⁡ ⁡ [ ( η η + u ) T T ( x ) − − A ( η η ) ] d x = e A ( η η + u ) − − A ( η η ) {\displaystyle {\begin{aligned}M_{T}(u)&\equiv \operatorname {E} \left[\exp \left(u^{\mathsf {T}}T(x)\right)\mid \eta \right]\\&=\int _{x}h(x)\,\exp \left[(\eta +u)^{\mathsf {T}}T(x)-A(\eta )\right]\,dx\\[1ex]&=e^{A(\eta +u)-A(\eta )}\end{aligned}}} proving the earlier statement that K ( u ∣ ∣ η η ) = A ( η η + u ) − − A ( η η ) {\displaystyle K(u\mid \eta )=A(\eta +u)-A(\eta )} is the cumulant generating function for T .

An important subclass of exponential families are the natural exponential families , which have a similar form for the moment-generating function for the distribution of x .

Differential identities for cumulants [ edit ] In particular, using the properties of the cumulant generating function, E ⁡ ⁡ ( T j ) = ∂ ∂ A ( η η ) ∂ ∂ η η j {\displaystyle \operatorname {E} (T_{j})={\frac {\partial A(\eta )}{\partial \eta _{j}}}} and cov ⁡ ⁡ ( T i , T j ) = ∂ ∂ 2 A ( η η ) ∂ ∂ η η i ∂ ∂ η η j .

{\displaystyle \operatorname {cov} \left(T_{i},\,T_{j}\right)={\frac {\partial ^{2}A(\eta )}{\partial \eta _{i}\,\partial \eta _{j}}}.} The first two raw moments and all mixed second moments can be recovered from these two identities. Higher-order moments and cumulants are obtained by higher derivatives. This technique is often useful when T is a complicated function of the data, whose moments are difficult to calculate by integration.

Another way to see this that does not rely on the theory of cumulants is to begin from the fact that the distribution of an exponential family must be normalized, and differentiate.  We illustrate using the simple case of a one-dimensional parameter, but an analogous derivation holds more generally.

In the one-dimensional case, we have p ( x ) = g ( η η ) h ( x ) e η η T ( x ) .

{\displaystyle p(x)=g(\eta )h(x)e^{\eta T(x)}.} This must be normalized, so 1 = ∫ ∫ x p ( x ) d x = ∫ ∫ x g ( η η ) h ( x ) e η η T ( x ) d x = g ( η η ) ∫ ∫ x h ( x ) e η η T ( x ) d x .

{\displaystyle 1=\int _{x}p(x)\,dx=\int _{x}g(\eta )h(x)e^{\eta T(x)}\,dx=g(\eta )\int _{x}h(x)e^{\eta T(x)}\,dx.} Take the derivative of both sides with respect to η : 0 = g ( η η ) d d η η ∫ ∫ x h ( x ) e η η T ( x ) d x + g ′ ( η η ) ∫ ∫ x h ( x ) e η η T ( x ) d x = g ( η η ) ∫ ∫ x h ( x ) ( d d η η e η η T ( x ) ) d x + g ′ ( η η ) ∫ ∫ x h ( x ) e η η T ( x ) d x = g ( η η ) ∫ ∫ x h ( x ) e η η T ( x ) T ( x ) d x + g ′ ( η η ) ∫ ∫ x h ( x ) e η η T ( x ) d x = ∫ ∫ x T ( x ) g ( η η ) h ( x ) e η η T ( x ) d x + g ′ ( η η ) g ( η η ) ∫ ∫ x g ( η η ) h ( x ) e η η T ( x ) d x = ∫ ∫ x T ( x ) p ( x ) d x + g ′ ( η η ) g ( η η ) ∫ ∫ x p ( x ) d x = E ⁡ ⁡ [ T ( x ) ] + g ′ ( η η ) g ( η η ) = E ⁡ ⁡ [ T ( x ) ] + d d η η log ⁡ ⁡ g ( η η ) {\displaystyle {\begin{aligned}0&=g(\eta ){\frac {d}{d\eta }}\int _{x}h(x)e^{\eta T(x)}\,dx+g'(\eta )\int _{x}h(x)e^{\eta T(x)}\,dx\\[1ex]&=g(\eta )\int _{x}h(x)\left({\frac {d}{d\eta }}e^{\eta T(x)}\right)\,dx+g'(\eta )\int _{x}h(x)e^{\eta T(x)}\,dx\\[1ex]&=g(\eta )\int _{x}h(x)e^{\eta T(x)}T(x)\,dx+g'(\eta )\int _{x}h(x)e^{\eta T(x)}\,dx\\[1ex]&=\int _{x}T(x)g(\eta )h(x)e^{\eta T(x)}\,dx+{\frac {g'(\eta )}{g(\eta )}}\int _{x}g(\eta )h(x)e^{\eta T(x)}\,dx\\[1ex]&=\int _{x}T(x)p(x)\,dx+{\frac {g'(\eta )}{g(\eta )}}\int _{x}p(x)\,dx\\[1ex]&=\operatorname {E} [T(x)]+{\frac {g'(\eta )}{g(\eta )}}\\[1ex]&=\operatorname {E} [T(x)]+{\frac {d}{d\eta }}\log g(\eta )\end{aligned}}} Therefore, E ⁡ ⁡ [ T ( x ) ] = − − d d η η log ⁡ ⁡ g ( η η ) = d d η η A ( η η ) .

{\displaystyle \operatorname {E} [T(x)]=-{\frac {d}{d\eta }}\log g(\eta )={\frac {d}{d\eta }}A(\eta ).} Example 1 [ edit ] As an introductory example, consider the gamma distribution , whose distribution is defined by p ( x ) = β β α α Γ Γ ( α α ) x α α − − 1 e − − β β x .

{\displaystyle p(x)={\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}x^{\alpha -1}e^{-\beta x}.} Referring to the above table, we can see that the natural parameter is given by η η 1 = α α − − 1 , η η 2 = − − β β , {\displaystyle {\begin{aligned}\eta _{1}&=\alpha -1,\\\eta _{2}&=-\beta ,\end{aligned}}} the reverse substitutions are α α = η η 1 + 1 , β β = − − η η 2 , {\displaystyle {\begin{aligned}\alpha &=\eta _{1}+1,\\\beta &=-\eta _{2},\end{aligned}}} the sufficient statistics are (log x , x) , and the log-partition function is A ( η η 1 , η η 2 ) = log ⁡ ⁡ Γ Γ ( η η 1 + 1 ) − − ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) .

{\displaystyle A(\eta _{1},\eta _{2})=\log \Gamma (\eta _{1}+1)-(\eta _{1}+1)\log(-\eta _{2}).} We can find the mean of the sufficient statistics as follows.  First, for η 1 : E ⁡ ⁡ [ log ⁡ ⁡ x ] = ∂ ∂ ∂ ∂ η η 1 A ( η η 1 , η η 2 ) = ∂ ∂ ∂ ∂ η η 1 [ log ⁡ ⁡ Γ Γ ( η η 1 + 1 ) − − ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) ] = ψ ψ ( η η 1 + 1 ) − − log ⁡ ⁡ ( − − η η 2 ) = ψ ψ ( α α ) − − log ⁡ ⁡ β β , {\displaystyle {\begin{aligned}\operatorname {E} [\log x]&={\frac {\partial }{\partial \eta _{1}}}A(\eta _{1},\eta _{2})\\[0.5ex]&={\frac {\partial }{\partial \eta _{1}}}\left[\log \Gamma (\eta _{1}+1)-(\eta _{1}+1)\log(-\eta _{2})\right]\\[1ex]&=\psi (\eta _{1}+1)-\log(-\eta _{2})\\[1ex]&=\psi (\alpha )-\log \beta ,\end{aligned}}} Where ψ ψ ( x ) {\displaystyle \psi (x)} is the digamma function (derivative of log gamma), and we used the reverse substitutions in the last step.

Now, for η 2 : E ⁡ ⁡ [ x ] = ∂ ∂ ∂ ∂ η η 2 A ( η η 1 , η η 2 ) = ∂ ∂ ∂ ∂ η η 2 [ log ⁡ ⁡ Γ Γ ( η η 1 + 1 ) − − ( η η 1 + 1 ) log ⁡ ⁡ ( − − η η 2 ) ] = − − ( η η 1 + 1 ) 1 − − η η 2 ( − − 1 ) = η η 1 + 1 − − η η 2 = α α β β , {\displaystyle {\begin{aligned}\operatorname {E} [x]&={\frac {\partial }{\partial \eta _{2}}}A(\eta _{1},\eta _{2})\\[1ex]&={\frac {\partial }{\partial \eta _{2}}}\left[\log \Gamma (\eta _{1}+1)-(\eta _{1}+1)\log(-\eta _{2})\right]\\[1ex]&=-(\eta _{1}+1){\frac {1}{-\eta _{2}}}(-1)={\frac {\eta _{1}+1}{-\eta _{2}}}={\frac {\alpha }{\beta }},\end{aligned}}} again making the reverse substitution in the last step.

To compute the variance of x , we just differentiate again: Var ⁡ ⁡ ( x ) = ∂ ∂ 2 ∂ ∂ η η 2 2 A ( η η 1 , η η 2 ) = ∂ ∂ ∂ ∂ η η 2 η η 1 + 1 − − η η 2 = η η 1 + 1 η η 2 2 = α α β β 2 .

{\displaystyle {\begin{aligned}\operatorname {Var} (x)&={\frac {\partial ^{2}}{\partial \eta _{2}^{2}}}A{\left(\eta _{1},\eta _{2}\right)}={\frac {\partial }{\partial \eta _{2}}}{\frac {\eta _{1}+1}{-\eta _{2}}}\\[1ex]&={\frac {\eta _{1}+1}{\eta _{2}^{2}}}={\frac {\alpha }{\beta ^{2}}}.\end{aligned}}} All of these calculations can be done using integration, making use of various properties of the gamma function , but this requires significantly more work.

Example 2 [ edit ] As another example consider a real valued random variable X with density p θ θ ( x ) = θ θ e − − x ( 1 + e − − x ) θ θ + 1 {\displaystyle p_{\theta }(x)={\frac {\theta e^{-x}}{\left(1+e^{-x}\right)^{\theta +1}}}} indexed by shape parameter θ θ ∈ ∈ ( 0 , ∞ ∞ ) {\displaystyle \theta \in (0,\infty )} (this is called the skew-logistic distribution ). The density can be rewritten as e − − x 1 + e − − x exp ⁡ ⁡ [ − − θ θ log ⁡ ⁡ ( 1 + e − − x ) + log ⁡ ⁡ ( θ θ ) ] {\displaystyle {\frac {e^{-x}}{1+e^{-x}}}\exp[-\theta \log \left(1+e^{-x})+\log(\theta )\right]} Notice this is an exponential family with natural parameter η η = − − θ θ , {\displaystyle \eta =-\theta ,} sufficient statistic T = log ⁡ ⁡ ( 1 + e − − x ) , {\displaystyle T=\log \left(1+e^{-x}\right),} and log-partition function A ( η η ) = − − log ⁡ ⁡ ( θ θ ) = − − log ⁡ ⁡ ( − − η η ) {\displaystyle A(\eta )=-\log(\theta )=-\log(-\eta )} So using the first identity, E ⁡ ⁡ [ log ⁡ ⁡ ( 1 + e − − X ) ] = E ⁡ ⁡ ( T ) = ∂ ∂ A ( η η ) ∂ ∂ η η = ∂ ∂ ∂ ∂ η η [ − − log ⁡ ⁡ ( − − η η ) ] = 1 − − η η = 1 θ θ , {\displaystyle \operatorname {E} \left[\log \left(1+e^{-X}\right)\right]=\operatorname {E} (T)={\frac {\partial A(\eta )}{\partial \eta }}={\frac {\partial }{\partial \eta }}[-\log(-\eta )]={\frac {1}{-\eta }}={\frac {1}{\theta }},} and using the second identity var ⁡ ⁡ [ log ⁡ ⁡ ( 1 + e − − X ) ] = ∂ ∂ 2 A ( η η ) ∂ ∂ η η 2 = ∂ ∂ ∂ ∂ η η [ 1 − − η η ] = 1 ( − − η η ) 2 = 1 θ θ 2 .

{\displaystyle \operatorname {var} \left[\log \left(1+e^{-X}\right)\right]={\frac {\partial ^{2}A(\eta )}{\partial \eta ^{2}}}={\frac {\partial }{\partial \eta }}\left[{\frac {1}{-\eta }}\right]={\frac {1}{{\left(-\eta \right)}^{2}}}={\frac {1}{\theta ^{2}}}.} This example illustrates a case where using this method is very simple, but the direct calculation would be nearly impossible.

Example 3 [ edit ] The final example is one where integration would be extremely difficult.  This is the case of the Wishart distribution , which is defined over matrices.  Even taking derivatives is a bit tricky, as it involves matrix calculus , but the respective identities are listed in that article.

From the above table, we can see that the natural parameter is given by η η 1 = − − 1 2 V − − 1 , η η 2 = − − 1 2 ( n − − p − − 1 ) , {\displaystyle {\begin{aligned}{\boldsymbol {\eta }}_{1}&=-{\tfrac {1}{2}}\mathbf {V} ^{-1},\\\eta _{2}&={\hphantom {-}}{\tfrac {1}{2}}\left(n-p-1\right),\end{aligned}}} the reverse substitutions are V = − − 1 2 η η 1 − − 1 , n = 2 η η 2 + p + 1 , {\displaystyle {\begin{aligned}\mathbf {V} &=-{\tfrac {1}{2}}{\boldsymbol {\eta }}_{1}^{-1},\\n&=2\eta _{2}+p+1,\end{aligned}}} and the sufficient statistics are ( X , log ⁡ ⁡ | X | ) .

{\displaystyle (\mathbf {X} ,\log |\mathbf {X} |).} The log-partition function is written in various forms in the table, to facilitate differentiation and back-substitution. We use the following forms: A ( η η 1 , n ) = − − n 2 log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( n 2 ) , A ( V , η η 2 ) = ( η η 2 + p + 1 2 ) log ⁡ ⁡ ( 2 p | V | ) + log ⁡ ⁡ Γ Γ p ( η η 2 + p + 1 2 ) .

{\displaystyle {\begin{aligned}A({\boldsymbol {\eta }}_{1},n)&=-{\frac {n}{2}}\log \left|-{\boldsymbol {\eta }}_{1}\right|+\log \Gamma _{p}{\left({\frac {n}{2}}\right)},\\[1ex]A(\mathbf {V} ,\eta _{2})&=\left(\eta _{2}+{\frac {p+1}{2}}\right)\log \left(2^{p}\left|\mathbf {V} \right|\right)+\log \Gamma _{p}{\left(\eta _{2}+{\frac {p+1}{2}}\right)}.\end{aligned}}} Expectation of X (associated with η 1 ) To differentiate with respect to η 1 , we need the following matrix calculus identity: ∂ ∂ log ⁡ ⁡ | a X | ∂ ∂ X = ( X − − 1 ) T {\displaystyle {\frac {\partial \log |a\mathbf {X} |}{\partial \mathbf {X} }}=(\mathbf {X} ^{-1})^{\mathsf {T}}} Then: E ⁡ ⁡ [ X ] = ∂ ∂ ∂ ∂ η η 1 A ( η η 1 , … … ) = ∂ ∂ ∂ ∂ η η 1 [ − − n 2 log ⁡ ⁡ | − − η η 1 | + log ⁡ ⁡ Γ Γ p ( n 2 ) ] = − − n 2 ( η η 1 − − 1 ) T = n 2 ( − − η η 1 − − 1 ) T = n ( V ) T = n V {\displaystyle {\begin{aligned}\operatorname {E} [\mathbf {X} ]&={\frac {\partial }{\partial {\boldsymbol {\eta }}_{1}}}A\left({\boldsymbol {\eta }}_{1},\ldots \right)\\[1ex]&={\frac {\partial }{\partial {\boldsymbol {\eta }}_{1}}}\left[-{\frac {n}{2}}\log \left|-{\boldsymbol {\eta }}_{1}\right|+\log \Gamma _{p}{\left({\frac {n}{2}}\right)}\right]\\[1ex]&=-{\frac {n}{2}}({\boldsymbol {\eta }}_{1}^{-1})^{\mathsf {T}}\\[1ex]&={\frac {n}{2}}(-{\boldsymbol {\eta }}_{1}^{-1})^{\mathsf {T}}\\[1ex]&=n(\mathbf {V} )^{\mathsf {T}}\\[1ex]&=n\mathbf {V} \end{aligned}}} The last line uses the fact that V is symmetric, and therefore it is the same when transposed.

Expectation of log | X | (associated with η 2 ) Now, for η 2 , we first need to expand the part of the log-partition function that involves the multivariate gamma function : log ⁡ ⁡ Γ Γ p ( a ) = log ⁡ ⁡ ( π π p ( p − − 1 ) 4 ∏ ∏ j = 1 p Γ Γ ( a + 1 − − j 2 ) ) = p ( p − − 1 ) 4 log ⁡ ⁡ π π + ∑ ∑ j = 1 p log ⁡ ⁡ Γ Γ ( a + 1 − − j 2 ) {\displaystyle {\begin{aligned}\log \Gamma _{p}(a)&=\log \left(\pi ^{\frac {p(p-1)}{4}}\prod _{j=1}^{p}\Gamma {\left(a+{\frac {1-j}{2}}\right)}\right)\\&={\frac {p(p-1)}{4}}\log \pi +\sum _{j=1}^{p}\log \Gamma {\left(a+{\frac {1-j}{2}}\right)}\end{aligned}}} We also need the digamma function : ψ ψ ( x ) = d d x log ⁡ ⁡ Γ Γ ( x ) .

{\displaystyle \psi (x)={\frac {d}{dx}}\log \Gamma (x).} Then: E ⁡ ⁡ [ log ⁡ ⁡ | X | ] = ∂ ∂ ∂ ∂ η η 2 A ( … … , η η 2 ) = ∂ ∂ ∂ ∂ η η 2 [ − − ( η η 2 + p + 1 2 ) log ⁡ ⁡ ( 2 p | V | ) + log ⁡ ⁡ Γ Γ p ( η η 2 + p + 1 2 ) ] = ∂ ∂ ∂ ∂ η η 2 [ ( η η 2 + p + 1 2 ) log ⁡ ⁡ ( 2 p | V | ) ] + ∂ ∂ ∂ ∂ η η 2 [ p ( p − − 1 ) 4 log ⁡ ⁡ π π ] = + ∂ ∂ ∂ ∂ η η 2 ∑ ∑ j = 1 p log ⁡ ⁡ Γ Γ ( η η 2 + p + 1 2 + 1 − − j 2 ) = p log ⁡ ⁡ 2 + log ⁡ ⁡ | V | + ∑ ∑ j = 1 p ψ ψ ( η η 2 + p + 1 2 + 1 − − j 2 ) = p log ⁡ ⁡ 2 + log ⁡ ⁡ | V | + ∑ ∑ j = 1 p ψ ψ ( n − − p − − 1 2 + p + 1 2 + 1 − − j 2 ) = p log ⁡ ⁡ 2 + log ⁡ ⁡ | V | + ∑ ∑ j = 1 p ψ ψ ( n + 1 − − j 2 ) {\displaystyle {\begin{aligned}\operatorname {E} [\log |\mathbf {X} |]&={\frac {\partial }{\partial \eta _{2}}}A\left(\ldots ,\eta _{2}\right)\\[1ex]&={\frac {\partial }{\partial \eta _{2}}}\left[-\left(\eta _{2}+{\frac {p+1}{2}}\right)\log \left(2^{p}\left|\mathbf {V} \right|\right)+\log \Gamma _{p}{\left(\eta _{2}+{\frac {p+1}{2}}\right)}\right]\\[1ex]&={\frac {\partial }{\partial \eta _{2}}}\left[\left(\eta _{2}+{\frac {p+1}{2}}\right)\log \left(2^{p}\left|\mathbf {V} \right|\right)\right]+{\frac {\partial }{\partial \eta _{2}}}\left[{\frac {p(p-1)}{4}}\log \pi \right]\\&{\hphantom {=}}+{\frac {\partial }{\partial \eta _{2}}}\sum _{j=1}^{p}\log \Gamma {\left(\eta _{2}+{\frac {p+1}{2}}+{\frac {1-j}{2}}\right)}\\[1ex]&=p\log 2+\log |\mathbf {V} |+\sum _{j=1}^{p}\psi {\left(\eta _{2}+{\frac {p+1}{2}}+{\frac {1-j}{2}}\right)}\\[1ex]&=p\log 2+\log |\mathbf {V} |+\sum _{j=1}^{p}\psi {\left({\frac {n-p-1}{2}}+{\frac {p+1}{2}}+{\frac {1-j}{2}}\right)}\\[1ex]&=p\log 2+\log |\mathbf {V} |+\sum _{j=1}^{p}\psi {\left({\frac {n+1-j}{2}}\right)}\end{aligned}}} This latter formula is listed in the Wishart distribution article.  Both of these expectations are needed when deriving the variational Bayes update equations in a Bayes network involving a Wishart distribution (which is the conjugate prior of the multivariate normal distribution ).

Computing these formulas using integration would be much more difficult.  The first one, for example, would require matrix integration.

Entropy [ edit ] Relative entropy [ edit ] The relative entropy ( Kullback–Leibler divergence , KL divergence) of two distributions in an exponential family has a simple expression as the Bregman divergence between the natural parameters with respect to the log-normalizer.

[ 14 ] The relative entropy is defined in terms of an integral, while the Bregman divergence is defined in terms of a derivative and inner product, and thus is easier to calculate and has a closed-form expression (assuming the derivative has a closed-form expression). Further, the Bregman divergence in terms of the natural parameters and the log-normalizer equals the Bregman divergence of the dual parameters (expectation parameters), in the opposite order, for the convex conjugate function.

[ 15 ] Fixing an exponential family with log-normalizer ⁠ A {\displaystyle A} ⁠ (with convex conjugate ⁠ A ∗ ∗ {\displaystyle A^{*}} ⁠ ), writing P A , θ θ {\displaystyle P_{A,\theta }} for the distribution in this family corresponding a fixed value of the natural parameter ⁠ θ θ {\displaystyle \theta } ⁠ (writing ⁠ θ θ ′ {\displaystyle \theta '} ⁠ for another value, and with ⁠ η η , η η ′ {\displaystyle \eta ,\eta '} ⁠ for the corresponding dual expectation/moment parameters), writing KL for the KL divergence, and ⁠ B A {\displaystyle B_{A}} ⁠ for the Bregman divergence, the divergences are related as: KL ⁡ ⁡ ( P A , θ θ ∥ ∥ P A , θ θ ′ ) = B A ( θ θ ′ ∥ ∥ θ θ ) = B A ∗ ∗ ( η η ∥ ∥ η η ′ ) .

{\displaystyle \operatorname {KL} (P_{A,\theta }\parallel P_{A,\theta '})=B_{A}(\theta '\parallel \theta )=B_{A^{*}}(\eta \parallel \eta ').} The KL divergence is conventionally written with respect to the first parameter, while the Bregman divergence is conventionally written with respect to the second parameter, and thus this can be read as "the relative entropy is equal to the Bregman divergence defined by the log-normalizer on the swapped natural parameters", or equivalently as "equal to the Bregman divergence defined by the dual to the log-normalizer on the expectation parameters".

Maximum-entropy derivation [ edit ] Exponential families arise naturally as the answer to the following question: what is the maximum-entropy distribution consistent with given constraints on expected values?

The information entropy of a probability distribution dF ( x ) can only be computed with respect to some other probability distribution (or, more generally, a positive measure), and both measures must be mutually absolutely continuous . Accordingly, we need to pick a reference measure dH ( x ) with the same support as dF ( x ) .

The entropy of dF ( x ) relative to dH ( x ) is S [ d F ∣ ∣ d H ] = − − ∫ ∫ d F d H log ⁡ ⁡ d F d H d H {\displaystyle S[dF\mid dH]=-\int {\frac {dF}{dH}}\log {\frac {dF}{dH}}\,dH} or S [ d F ∣ ∣ d H ] = ∫ ∫ log ⁡ ⁡ d H d F d F {\displaystyle S[dF\mid dH]=\int \log {\frac {dH}{dF}}\,dF} where dF / dH and dH / dF are Radon–Nikodym derivatives . The ordinary definition of entropy for a discrete distribution supported on a set I , namely S = − − ∑ ∑ i ∈ ∈ I p i log ⁡ ⁡ p i {\displaystyle S=-\sum _{i\in I}p_{i}\log p_{i}} assumes , though this is seldom pointed out, that dH is chosen to be the counting measure on I .

Consider now a collection of observable quantities (random variables) T i . The probability distribution dF whose entropy with respect to dH is greatest, subject to the conditions that the expected value of T i be equal to t i , is an exponential family with dH as reference measure and ( T 1 , ..., T n ) as sufficient statistic.

The derivation is a simple variational calculation using Lagrange multipliers . Normalization is imposed by letting T 0 = 1 be one of the constraints. The natural parameters of the distribution are the Lagrange multipliers, and the normalization factor is the Lagrange multiplier associated to T 0 .

For examples of such derivations, see Maximum entropy probability distribution .

Role in statistics [ edit ] Classical estimation: sufficiency [ edit ] According to the Pitman – Koopman – Darmois theorem , among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases.

Less tersely, suppose X k , (where k = 1, 2, 3, ...

n ) are independent , identically distributed random variables. Only if their distribution is one of the exponential family of distributions is there a sufficient statistic T ( X 1 , ..., X n ) whose number of scalar components does not increase as the sample size n increases; the statistic T may be a vector or a single scalar number , but whatever it is, its size will neither grow nor shrink when more data are obtained.

As a counterexample if these conditions are relaxed, the family of uniform distributions (either discrete or continuous , with either or both bounds unknown) has a sufficient statistic, namely the sample maximum, sample minimum, and sample size, but does not form an exponential family, as the domain varies with the parameters.

Bayesian estimation: conjugate distributions [ edit ] Exponential families are also important in Bayesian statistics . In Bayesian statistics a prior distribution is multiplied by a likelihood function and then normalised to produce a posterior distribution . In the case of a likelihood which belongs to an exponential family there exists a conjugate prior , which is often also in an exponential family. A conjugate prior π for the parameter η η {\displaystyle {\boldsymbol {\eta }}} of an exponential family f ( x ∣ ∣ η η ) = h ( x ) exp ⁡ ⁡ [ η η T T ( x ) − − A ( η η ) ] {\displaystyle f(x\mid {\boldsymbol {\eta }})=h(x)\,\exp \left[{\boldsymbol {\eta }}^{\mathsf {T}}\mathbf {T} (x)-A({\boldsymbol {\eta }})\right]} is given by p π π ( η η ∣ ∣ χ χ , ν ν ) = f ( χ χ , ν ν ) exp ⁡ ⁡ [ η η T χ χ − − ν ν A ( η η ) ] , {\displaystyle p_{\pi }({\boldsymbol {\eta }}\mid {\boldsymbol {\chi }},\nu )=f({\boldsymbol {\chi }},\nu )\,\exp \left[{\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }}-\nu A({\boldsymbol {\eta }})\right],} or equivalently p π π ( η η ∣ ∣ χ χ , ν ν ) = f ( χ χ , ν ν ) g ( η η ) ν ν exp ⁡ ⁡ ( η η T χ χ ) , χ χ ∈ ∈ R s {\displaystyle p_{\pi }({\boldsymbol {\eta }}\mid {\boldsymbol {\chi }},\nu )=f({\boldsymbol {\chi }},\nu )\,g({\boldsymbol {\eta }})^{\nu }\,\exp \left({\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }}\right),\qquad {\boldsymbol {\chi }}\in \mathbb {R} ^{s}} where s is the dimension of η η {\displaystyle {\boldsymbol {\eta }}} and ν ν > 0 {\displaystyle \nu >0} and χ χ {\displaystyle {\boldsymbol {\chi }}} are hyperparameters (parameters controlling parameters).

ν ν {\displaystyle \nu } corresponds to the effective number of observations that the prior distribution contributes, and χ χ {\displaystyle {\boldsymbol {\chi }}} corresponds to the total amount that these pseudo-observations contribute to the sufficient statistic over all observations and pseudo-observations.

f ( χ χ , ν ν ) {\displaystyle f({\boldsymbol {\chi }},\nu )} is a normalization constant that is automatically determined by the remaining functions and serves to ensure that the given function is a probability density function (i.e. it is normalized ).

A ( η η ) {\displaystyle A({\boldsymbol {\eta }})} and equivalently g ( η η ) {\displaystyle g({\boldsymbol {\eta }})} are the same functions as in the definition of the distribution over which π is the conjugate prior.

A conjugate prior is one which, when combined with the likelihood and normalised, produces a posterior distribution which is of the same type as the prior. For example, if one is estimating the success probability of a binomial distribution, then if one chooses to use a beta distribution as one's prior, the posterior is another beta distribution. This makes the computation of the posterior particularly simple. Similarly, if one is estimating the parameter of a Poisson distribution the use of a gamma prior will lead to another gamma posterior. Conjugate priors are often very flexible and can be very convenient. However, if one's belief about the likely value of the theta parameter of a binomial is represented by (say) a bimodal (two-humped) prior distribution, then this cannot be represented by a beta distribution. It can however be represented by using a mixture density as the prior, here a combination of two beta distributions; this is a form of hyperprior .

An arbitrary likelihood will not belong to an exponential family, and thus in general no conjugate prior exists. The posterior will then have to be computed by numerical methods.

To show that the above prior distribution is a conjugate prior, we can derive the posterior.

First, assume that the probability of a single observation follows an exponential family, parameterized using its natural parameter: p F ( x ∣ ∣ η η ) = h ( x ) g ( η η ) exp ⁡ ⁡ [ η η T T ( x ) ] {\displaystyle p_{F}(x\mid {\boldsymbol {\eta }})=h(x)\,g({\boldsymbol {\eta }})\,\exp \left[{\boldsymbol {\eta }}^{\mathsf {T}}\mathbf {T} (x)\right]} Then, for data X = ( x 1 , … … , x n ) {\displaystyle \mathbf {X} =(x_{1},\ldots ,x_{n})} , the likelihood is computed as follows: p ( X ∣ ∣ η η ) = ( ∏ ∏ i = 1 n h ( x i ) ) g ( η η ) n exp ⁡ ⁡ ( η η T ∑ ∑ i = 1 n T ( x i ) ) {\displaystyle p(\mathbf {X} \mid {\boldsymbol {\eta }})=\left(\prod _{i=1}^{n}h(x_{i})\right)g({\boldsymbol {\eta }})^{n}\exp \left({\boldsymbol {\eta }}^{\mathsf {T}}\sum _{i=1}^{n}\mathbf {T} (x_{i})\right)} Then, for the above conjugate prior: p π π ( η η ∣ ∣ χ χ , ν ν ) = f ( χ χ , ν ν ) g ( η η ) ν ν exp ⁡ ⁡ ( η η T χ χ ) ∝ ∝ g ( η η ) ν ν exp ⁡ ⁡ ( η η T χ χ ) {\displaystyle {\begin{aligned}p_{\pi }({\boldsymbol {\eta }}\mid {\boldsymbol {\chi }},\nu )&=f({\boldsymbol {\chi }},\nu )g({\boldsymbol {\eta }})^{\nu }\exp({\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }})\propto g({\boldsymbol {\eta }})^{\nu }\exp({\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }})\end{aligned}}} We can then compute the posterior as follows: p ( η η ∣ ∣ X , χ χ , ν ν ) ∝ ∝ p ( X ∣ ∣ η η ) p π π ( η η ∣ ∣ χ χ , ν ν ) = ( ∏ ∏ i = 1 n h ( x i ) ) g ( η η ) n exp ⁡ ⁡ ( η η T ∑ ∑ i = 1 n T ( x i ) ) f ( χ χ , ν ν ) g ( η η ) ν ν exp ⁡ ⁡ ( η η T χ χ ) ∝ ∝ g ( η η ) n exp ⁡ ⁡ ( η η T ∑ ∑ i = 1 n T ( x i ) ) g ( η η ) ν ν exp ⁡ ⁡ ( η η T χ χ ) ∝ ∝ g ( η η ) ν ν + n exp ⁡ ⁡ ( η η T ( χ χ + ∑ ∑ i = 1 n T ( x i ) ) ) {\displaystyle {\begin{aligned}p({\boldsymbol {\eta }}\mid \mathbf {X} ,{\boldsymbol {\chi }},\nu )&\propto p(\mathbf {X} \mid {\boldsymbol {\eta }})p_{\pi }({\boldsymbol {\eta }}\mid {\boldsymbol {\chi }},\nu )\\&=\left(\prod _{i=1}^{n}h(x_{i})\right)g({\boldsymbol {\eta }})^{n}\exp \left({\boldsymbol {\eta }}^{\mathsf {T}}\sum _{i=1}^{n}\mathbf {T} (x_{i})\right)f({\boldsymbol {\chi }},\nu )g({\boldsymbol {\eta }})^{\nu }\exp({\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }})\\&\propto g({\boldsymbol {\eta }})^{n}\exp \left({\boldsymbol {\eta }}^{\mathsf {T}}\sum _{i=1}^{n}\mathbf {T} (x_{i})\right)g({\boldsymbol {\eta }})^{\nu }\exp({\boldsymbol {\eta }}^{\mathsf {T}}{\boldsymbol {\chi }})\\&\propto g({\boldsymbol {\eta }})^{\nu +n}\exp \left({\boldsymbol {\eta }}^{\mathsf {T}}\left({\boldsymbol {\chi }}+\sum _{i=1}^{n}\mathbf {T} (x_{i})\right)\right)\end{aligned}}} The last line is the kernel of the posterior distribution, i.e.

p ( η η ∣ ∣ X , χ χ , ν ν ) = p π π ( η η | χ χ + ∑ ∑ i = 1 n T ( x i ) , ν ν + n ) {\displaystyle p({\boldsymbol {\eta }}\mid \mathbf {X} ,{\boldsymbol {\chi }},\nu )=p_{\pi }\left({\boldsymbol {\eta }}\left|~{\boldsymbol {\chi }}+\sum _{i=1}^{n}\mathbf {T} (x_{i}),\nu +n\right.\right)} This shows that the posterior has the same form as the prior.

The data X enters into this equation only in the expression T ( X ) = ∑ ∑ i = 1 n T ( x i ) , {\displaystyle \mathbf {T} (\mathbf {X} )=\sum _{i=1}^{n}\mathbf {T} (x_{i}),} which is termed the sufficient statistic of the data.  That is, the value of the sufficient statistic is sufficient to completely determine the posterior distribution.  The actual data points themselves are not needed, and all sets of data points with the same sufficient statistic will have the same distribution.  This is important because the dimension of the sufficient statistic does not grow with the data size — it has only as many components as the components of η η {\displaystyle {\boldsymbol {\eta }}} (equivalently, the number of parameters of the distribution of a single data point).

The update equations are as follows: χ χ ′ = χ χ + T ( X ) = χ χ + ∑ ∑ i = 1 n T ( x i ) ν ν ′ = ν ν + n {\displaystyle {\begin{aligned}{\boldsymbol {\chi }}'&={\boldsymbol {\chi }}+\mathbf {T} (\mathbf {X} )\\&={\boldsymbol {\chi }}+\sum _{i=1}^{n}\mathbf {T} (x_{i})\\\nu '&=\nu +n\end{aligned}}} This shows that the update equations can be written simply in terms of the number of data points and the sufficient statistic of the data.  This can be seen clearly in the various examples of update equations shown in the conjugate prior page. Because of the way that the sufficient statistic is computed, it necessarily involves sums of components of the data (in some cases disguised as products or other forms — a product can be written in terms of a sum of logarithms ). The cases where the update equations for particular distributions don't exactly match the above forms are cases where the conjugate prior has been expressed using a different parameterization than the one that produces a conjugate prior of the above form — often specifically because the above form is defined over the natural parameter η η {\displaystyle {\boldsymbol {\eta }}} while conjugate priors are usually defined over the actual parameter θ θ .

{\displaystyle {\boldsymbol {\theta }}.} Unbiased estimation [ edit ] If the likelihood z | η η ∼ ∼ e η η z f 1 ( η η ) f 0 ( z ) {\displaystyle z|\eta \sim e^{\eta z}f_{1}(\eta )f_{0}(z)} is an exponential family, then the unbiased estimator of η η {\displaystyle \eta } is − − d d z ln ⁡ ⁡ f 0 ( z ) {\displaystyle -{\frac {d}{dz}}\ln f_{0}(z)} .

[ 16 ] Hypothesis testing: uniformly most powerful tests [ edit ] Further information: Uniformly most powerful test A one-parameter exponential family has a monotone non-decreasing likelihood ratio in the sufficient statistic T ( x ) , provided that η ( θ ) is non-decreasing. As a consequence, there exists a uniformly most powerful test for testing the hypothesis H 0 : θ ≥ θ 0 vs .

H 1 : θ < θ 0 .

Generalized linear models [ edit ] Exponential families form the basis for the distribution functions used in generalized linear models (GLM), a class of model that encompasses many of the commonly used regression models in statistics.  Examples include logistic regression using the binomial family and Poisson regression .

See also [ edit ] Exponential dispersion model Gibbs measure Modified half-normal distribution Natural exponential family Footnotes [ edit ] ^ For example, the family of normal distributions includes the standard normal distribution N (0, 1) with mean 0 and variance 1, as well as other normal distributions with different mean and variance.

^ "Partition function" is often used in statistics as a synonym of "normalization factor".

^ These distributions are often not themselves exponential families. Common examples of non-exponential families arising from exponential ones are the Student's t -distribution , beta-binomial distribution and Dirichlet-multinomial distribution .

References [ edit ] Citations [ edit ] ^ Kupperman, M. (1958).

"Probabilities of hypotheses and information-statistics in sampling from exponential-class populations" .

Annals of Mathematical Statistics .

9 (2): 571– 575.

doi : 10.1214/aoms/1177706633 .

JSTOR 2237349 .

^ Andersen, Erling (September 1970). "Sufficiency and Exponential Families for Discrete Sample Spaces".

Journal of the American Statistical Association .

65 (331). Journal of the American Statistical Association: 1248– 1255.

doi : 10.2307/2284291 .

JSTOR 2284291 .

MR 0268992 .

^ Pitman, E.

; Wishart, J. (1936). "Sufficient statistics and intrinsic accuracy".

Mathematical Proceedings of the Cambridge Philosophical Society .

32 (4): 567– 579.

Bibcode : 1936PCPS...32..567P .

doi : 10.1017/S0305004100019307 .

S2CID 120708376 .

^ Darmois, G. (1935). "Sur les lois de probabilites a estimation exhaustive".

C. R. Acad. Sci. Paris (in French).

200 : 1265– 1266.

^ Koopman, B.

(1936).

"On distribution admitting a sufficient statistic" .

Transactions of the American Mathematical Society .

39 (3).

American Mathematical Society : 399– 409.

doi : 10.2307/1989758 .

JSTOR 1989758 .

MR 1501854 .

^ "General Exponential Families" .

www.randomservices.org . Retrieved 2022-08-30 .

^ Abramovich & Ritov (2013).

Statistical Theory: A concise introduction . Chapman & Hall.

ISBN 978-1439851845 .

^ Blei, David.

"Variational Inference" (PDF) . Princeton U.

^ Casella, George (2002).

Statistical inference . Roger L. Berger (2nd ed.). Australia: Thomson Learning. Theorem 6.2.25.

ISBN 0-534-24312-6 .

OCLC 46538638 .

^ Brown, Lawrence D. (1986).

Fundamentals of statistical exponential families : with applications in statistical decision theory . Hayward, Calif.: Institute of Mathematical Statistics. Theorem 2.12.

ISBN 0-940600-10-2 .

OCLC 15986663 .

^ Keener, Robert W. (2010).

Theoretical statistics : topics for a core course . New York. pp. 47, Example 3.12.

ISBN 978-0-387-93839-4 .

OCLC 676700036 .

{{ cite book }} :  CS1 maint: location missing publisher ( link ) ^ Nielsen, Frank; Garcia, Vincent (2009). "Statistical exponential families: A digest with flash cards".

arXiv : 0911.4863 [ cs.LG ].

^ van Garderen, Kees Jan (1997). "Curved Exponential Models in Econometrics".

Econometric Theory .

13 (6): 771– 790.

doi : 10.1017/S0266466600006253 .

S2CID 122742807 .

^ Nielsen & Nock 2010 , 4. Bregman Divergences and Relative Entropy of Exponential Families.

^ Barndorff-Nielsen 1978 , 9.1 Convex duality and exponential families.

^ Efron, Bradley (December 2011).

"Tweedie's Formula and Selection Bias" .

Journal of the American Statistical Association .

106 (496): 1602– 1614.

doi : 10.1198/jasa.2011.tm11181 .

ISSN 0162-1459 .

PMC 3325056 .

PMID 22505788 .

This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( November 2010 ) ( Learn how and when to remove this message ) Sources [ edit ] Barndorff-Nielsen, Ole (1978).

Information and exponential families in statistical theory . Wiley Series in Probability and Mathematical Statistics. Chichester: John Wiley & Sons, Ltd. pp. ix+238 pp.

ISBN 0-471-99545-2 .

MR 0489333 .

Reprinted as Barndorff-Nielsen, Ole (2014).

Information and exponential families in statistical theory . John Wiley & Sons, Ltd.

doi : 10.1002/9781118857281 .

ISBN 978-111885750-2 .

Nielsen, Frank; Garcia, Vincent (2009). "Statistical exponential families: A digest with flash cards".

arXiv : 0911.4863 .

Bibcode : 2009arXiv0911.4863N .

Nielsen, Frank; Nock, Richard (2010).

Entropies and cross-entropies of exponential families (PDF) . IEEE International Conference on Image Processing.

doi : 10.1109/ICIP.2010.5652054 . Archived from the original (PDF) on 2019-03-31.

Further reading [ edit ] Fahrmeir, Ludwig; Tutz, G. (1994).

Multivariate Statistical Modelling based on Generalized Linear Models . Springer. pp.

18– 22, 345– 349.

ISBN 0-387-94233-5 .

Keener, Robert W. (2006).

Theoretical Statistics: Topics for a Core Course . Springer. pp.

27– 28, 32– 33.

ISBN 978-0-387-93838-7 .

Lehmann, E. L.; Casella, G. (1998).

Theory of Point Estimation (2nd ed.). sec. 1.5.

ISBN 0-387-98502-6 .

External links [ edit ] A primer on the exponential family of distributions Exponential family of distributions on the Earliest known uses of some of the words of mathematics jMEF: A Java library for exponential families Archived 2013-04-11 at archive.today Graphical Models, Exponential Families, and Variational Inference by Wainwright and Jordan (2008) v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐qksm6
Cached time: 20250812014150
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 1.187 seconds
Real time usage: 1.392 seconds
Preprocessor visited node count: 10212/1000000
Revision size: 88084/2097152 bytes
Post‐expand include size: 287587/2097152 bytes
Template argument size: 11213/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 115981/5000000 bytes
Lua time usage: 0.436/10.000 seconds
Lua memory usage: 8996831/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  706.187      1 -total
 22.73%  160.496      3 Template:Reflist
 18.19%  128.483      1 Template:Statistics
 17.96%  126.814      1 Template:Navbox_with_collapsible_groups
 14.78%  104.353      1 Template:Short_description
 14.55%  102.739      7 Template:Cite_journal
 11.52%   81.326      2 Template:Pagetype
  8.91%   62.938     81 Template:Math
  8.83%   62.362      1 Template:Refimprove
  8.47%   59.839     15 Template:Navbox Saved in parser cache with key enwiki:pcache:339174:|#|:idhash:canonical and timestamp 20250812014150 and revision id 1303758854. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Exponential_family&oldid=1303758854#Classical_estimation:_sufficiency " Categories : Exponentials Continuous distributions Discrete distributions Types of probability distributions Hidden categories: CS1 French-language sources (fr) CS1 maint: location missing publisher Articles needing additional references from January 2025 All articles needing additional references Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from June 2011 Articles lacking in-text citations from November 2010 All articles lacking in-text citations Webarchive template archiveis links This page was last edited on 1 August 2025, at 22:38 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Exponential family 10 languages Add topic

