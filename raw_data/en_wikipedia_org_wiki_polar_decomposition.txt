Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Geometric interpretation 2 Properties Toggle Properties subsection 2.1 Relation to the SVD 2.2 Relation to normal matrices 3 Construction and proofs of existence Toggle Construction and proofs of existence subsection 3.1 Derivation for normal matrices 3.2 Derivation for invertible matrices 3.3 General derivation 4 Bounded operators on Hilbert space 5 Unbounded operators 6 Quaternion polar decomposition 7 Alternative planar decompositions 8 Numerical determination of the matrix polar decomposition 9 See also 10 References Toggle the table of contents Polar decomposition 13 languages Català Čeština Deutsch Français 한국어 Italiano עברית Polski Русский Svenska Українська اردو 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Representation of invertible matrices as unitary operator multiplying a Hermitian operator In mathematics , the polar decomposition of a square real or complex matrix A {\displaystyle A} is a factorization of the form A = U P {\displaystyle A=UP} , where U {\displaystyle U} is a unitary matrix , and P {\displaystyle P} is a positive semi-definite Hermitian matrix ( U {\displaystyle U} is an orthogonal matrix , and P {\displaystyle P} is a positive semi-definite symmetric matrix in the real case), both square and of the same size.

[ 1 ] If a real n × × n {\displaystyle n\times n} matrix A {\displaystyle A} is interpreted as a linear transformation of n {\displaystyle n} -dimensional space R n {\displaystyle \mathbb {R} ^{n}} , the polar decomposition separates it into a rotation or reflection U {\displaystyle U} of R n {\displaystyle \mathbb {R} ^{n}} and a scaling of the space along a set of n {\displaystyle n} orthogonal axes.

The polar decomposition of a square matrix A {\displaystyle A} always exists. If A {\displaystyle A} is invertible , the decomposition is unique, and the factor P {\displaystyle P} will be positive-definite . In that case, A {\displaystyle A} can be written uniquely in the form A = U e X {\displaystyle A=Ue^{X}} , where U {\displaystyle U} is unitary, and X {\displaystyle X} is the unique self-adjoint logarithm of the matrix P {\displaystyle P} .

[ 2 ] This decomposition is useful in computing the fundamental group of (matrix) Lie groups .

[ 3 ] The polar decomposition can also be defined as A = P ′ U {\displaystyle A=P'U} , where P ′ = U P U − − 1 {\displaystyle P'=UPU^{-1}} is a symmetric positive-definite matrix with the same eigenvalues as P {\displaystyle P} but different eigenvectors.

The polar decomposition of a matrix can be seen as the matrix analog of the polar form of a complex number z {\displaystyle z} as z = u r {\displaystyle z=ur} , where r {\displaystyle r} is its absolute value (a non-negative real number ), and u {\displaystyle u} is a complex number with unit norm (an element of the circle group ).

The definition A = U P {\displaystyle A=UP} may be extended to rectangular matrices A ∈ ∈ C m × × n {\displaystyle A\in \mathbb {C} ^{m\times n}} by requiring U ∈ ∈ C m × × n {\displaystyle U\in \mathbb {C} ^{m\times n}} to be a semi-unitary matrix, and P ∈ ∈ C n × × n {\displaystyle P\in \mathbb {C} ^{n\times n}} to be a positive-semidefinite Hermitian matrix. The decomposition always exists, and P {\displaystyle P} is always unique. The matrix U {\displaystyle U} is unique if and only if A {\displaystyle A} has full rank.

[ 4 ] Geometric interpretation [ edit ] A real square m × × m {\displaystyle m\times m} matrix A {\displaystyle A} can be interpreted as the linear transformation of R m {\displaystyle \mathbb {R} ^{m}} that takes a column vector x {\displaystyle x} to A x {\displaystyle Ax} .  Then, in the polar decomposition A = R P {\displaystyle A=RP} , the factor R {\displaystyle R} is an m × × m {\displaystyle m\times m} real orthogonal matrix.  The polar decomposition then can be seen as expressing the linear transformation defined by A {\displaystyle A} into a scaling of the space R m {\displaystyle \mathbb {R} ^{m}} along each eigenvector e i {\displaystyle e_{i}} of P {\displaystyle P} by a scale factor σ σ i {\displaystyle \sigma _{i}} (the action of P {\displaystyle P} ), followed by a rotation of R m {\displaystyle \mathbb {R} ^{m}} (the action of R {\displaystyle R} ).

Alternatively, the decomposition A = P R {\displaystyle A=PR} expresses the transformation defined by A {\displaystyle A} as a rotation ( R {\displaystyle R} ) followed by a scaling ( P {\displaystyle P} ) along certain orthogonal directions.  The scale factors are the same, but the directions are different.

Properties [ edit ] The polar decomposition of the complex conjugate of A {\displaystyle A} is given by A ¯ ¯ = U ¯ ¯ P ¯ ¯ .

{\displaystyle {\overline {A}}={\overline {U}}{\overline {P}}.} Note that det A = det U det P = e i θ θ r {\displaystyle \det A=\det U\det P=e^{i\theta }r} gives the corresponding polar decomposition of the determinant of A , since det U = e i θ θ , {\displaystyle \det U=e^{i\theta },} and det P = r = | det A | .

{\displaystyle \det P=r=|\det A|.} In particular, if A {\displaystyle A} has determinant 1, then both U {\displaystyle U} and P {\displaystyle P} have determinant 1.

The positive-semidefinite matrix P is always unique, even if A is singular , and is denoted as P = ( A ∗ ∗ A ) 1 / 2 , {\displaystyle P=(A^{*}A)^{1/2},} where A ∗ ∗ {\displaystyle A^{*}} denotes the conjugate transpose of A {\displaystyle A} . The uniqueness of P ensures that this expression is well-defined. The uniqueness is guaranteed by the fact that A ∗ ∗ A {\displaystyle A^{*}A} is a positive-semidefinite Hermitian matrix and, therefore, has a unique positive-semidefinite Hermitian square root .

[ 5 ] If A is invertible, then P is positive-definite, thus also invertible, and the matrix U is uniquely determined by U = A P − − 1 .

{\displaystyle U=AP^{-1}.} Relation to the SVD [ edit ] In terms of the singular value decomposition (SVD) of A {\displaystyle A} , A = W Σ Σ V ∗ ∗ {\displaystyle A=W\Sigma V^{*}} , one has P = V Σ Σ V ∗ ∗ , U = W V ∗ ∗ , {\displaystyle {\begin{aligned}P&=V\Sigma V^{*},\\U&=WV^{*},\end{aligned}}} where U {\displaystyle U} , V {\displaystyle V} , and W {\displaystyle W} are unitary matrices ( orthogonal if the field is the reals R {\displaystyle \mathbb {R} } ). This confirms that P {\displaystyle P} is positive-definite, and U {\displaystyle U} is unitary. Thus, the existence of the SVD is equivalent to the existence of polar decomposition.

One can also decompose A {\displaystyle A} in the form A = P ′ U .

{\displaystyle A=P'U.} Here U {\displaystyle U} is the same as before, and P ′ {\displaystyle P'} is given by P ′ = U P U − − 1 = ( A A ∗ ∗ ) 1 / 2 = W Σ Σ W ∗ ∗ .

{\displaystyle P'=UPU^{-1}=(AA^{*})^{1/2}=W\Sigma W^{*}.} This is known as the left polar decomposition, whereas the previous decomposition is known as the right polar decomposition. Left polar decomposition is also known as reverse polar decomposition.

The polar decomposition of a square invertible real matrix A {\displaystyle A} is of the form A = [ A ] R , {\displaystyle A=[A]R,} where [ A ] ≡ ≡ ( A A T ) 1 / 2 {\displaystyle [A]\equiv \left(AA^{\mathsf {T}}\right)^{1/2}} is a positive-definite matrix , and R = [ A ] − − 1 A {\displaystyle R=[A]^{-1}A} is an orthogonal matrix.

Relation to normal matrices [ edit ] The matrix A {\displaystyle A} with polar decomposition A = U P {\displaystyle A=UP} is normal if and only if U {\displaystyle U} and P {\displaystyle P} commute ( U P = P U {\displaystyle UP=PU} ), or equivalently, they are simultaneously diagonalizable .

Construction and proofs of existence [ edit ] The core idea behind the construction of the polar decomposition is similar to that used to compute the singular-value decomposition .

Derivation for normal matrices [ edit ] If A {\displaystyle A} is normal , then it is unitarily equivalent to a diagonal matrix: A = V Λ Λ V ∗ ∗ {\displaystyle A=V\Lambda V^{*}} for some unitary matrix V {\displaystyle V} and some diagonal matrix Λ Λ .

{\displaystyle \Lambda ~.} This makes the derivation of its polar decomposition particularly straightforward, as we can then write A = V Φ Φ Λ Λ | Λ Λ | V ∗ ∗ = ( V Φ Φ Λ Λ V ∗ ∗ ) ⏟ ⏟ ≡ ≡ U ( V | Λ Λ | V ∗ ∗ ) ⏟ ⏟ ≡ ≡ P , {\displaystyle A=V\Phi _{\Lambda }|\Lambda |V^{*}=\underbrace {\left(V\Phi _{\Lambda }V^{*}\right)} _{\equiv U}\underbrace {\left(V|\Lambda |V^{*}\right)} _{\equiv P},} where | Λ Λ | {\displaystyle |\Lambda |} is the matrix of absolute diagonal values, and Φ Φ Λ Λ {\displaystyle \Phi _{\Lambda }} is a diagonal matrix containing the phases of the elements of Λ Λ , {\displaystyle \Lambda ,} that is, ( Φ Φ Λ Λ ) i i ≡ ≡ Λ Λ i i / | Λ Λ i i | {\displaystyle (\Phi _{\Lambda })_{ii}\equiv \Lambda _{ii}/|\Lambda _{ii}|} when Λ Λ i i ≠ ≠ 0 , {\displaystyle \Lambda _{ii}\neq 0,} , and ( Φ Φ Λ Λ ) i i = 0 {\displaystyle (\Phi _{\Lambda })_{ii}=0} when Λ Λ i i = 0 .

{\displaystyle \Lambda _{ii}=0~.} The polar decomposition is thus A = U P , {\displaystyle A=UP,} with U {\displaystyle U} and P {\displaystyle P} diagonal in the eigenbasis of A {\displaystyle A} and having eigenvalues equal to the phases and absolute values of those of A , {\displaystyle A,} respectively.

Derivation for invertible matrices [ edit ] From the singular-value decomposition , it can be shown that a matrix A {\displaystyle A} is invertible if and only if A ∗ ∗ A {\displaystyle A^{*}A} (equivalently, A A ∗ ∗ {\displaystyle AA^{*}} ) is. Moreover, this is true if and only if the eigenvalues of A ∗ ∗ A {\displaystyle A^{*}A} are all not zero.

[ 6 ] In this case, the polar decomposition is directly obtained by writing A = A ( A ∗ ∗ A ) − − 1 / 2 ( A ∗ ∗ A ) 1 / 2 , {\displaystyle A=A\left(A^{*}A\right)^{-1/2}\left(A^{*}A\right)^{1/2},} and observing that A ( A ∗ ∗ A ) − − 1 / 2 {\displaystyle A\left(A^{*}A\right)^{-1/2}} is unitary. To see this, we can exploit the spectral decomposition of A ∗ ∗ A {\displaystyle A^{*}A} to write A ( A ∗ ∗ A ) − − 1 / 2 = A V D − − 1 / 2 V ∗ ∗ {\displaystyle A\left(A^{*}A\right)^{-1/2}=AVD^{-1/2}V^{*}} .

In this expression, V ∗ ∗ {\displaystyle V^{*}} is unitary because V {\displaystyle V} is. To show that also A V D − − 1 / 2 {\displaystyle AVD^{-1/2}} is unitary, we can use the SVD to write A = W D 1 / 2 V ∗ ∗ {\displaystyle A=WD^{1/2}V^{*}} , so that A V D − − 1 / 2 = W D 1 / 2 V ∗ ∗ V D − − 1 / 2 = W , {\displaystyle AVD^{-1/2}=WD^{1/2}V^{*}VD^{-1/2}=W,} where again W {\displaystyle W} is unitary by construction.

Yet another way to directly show the unitarity of A ( A ∗ ∗ A ) − − 1 / 2 {\displaystyle A\left(A^{*}A\right)^{-1/2}} is to note that, writing the SVD of A {\displaystyle A} in terms of rank-1 matrices as A = ∑ ∑ k s k v k w k ∗ ∗ {\textstyle A=\sum _{k}s_{k}v_{k}w_{k}^{*}} , where s k {\displaystyle s_{k}} are the singular values of A {\displaystyle A} , we have A ( A ∗ ∗ A ) − − 1 / 2 = ( ∑ ∑ j λ λ j v j w j ∗ ∗ ) ( ∑ ∑ k | λ λ k | − − 1 w k w k ∗ ∗ ) = ∑ ∑ k λ λ k | λ λ k | v k w k ∗ ∗ , {\displaystyle A\left(A^{*}A\right)^{-1/2}=\left(\sum _{j}\lambda _{j}v_{j}w_{j}^{*}\right)\left(\sum _{k}|\lambda _{k}|^{-1}w_{k}w_{k}^{*}\right)=\sum _{k}{\frac {\lambda _{k}}{|\lambda _{k}|}}v_{k}w_{k}^{*},} which directly implies the unitarity of A ( A ∗ ∗ A ) − − 1 / 2 {\displaystyle A\left(A^{*}A\right)^{-1/2}} because a matrix is unitary if and only if its singular values have unitary absolute value.

Note how, from the above construction, it follows that the unitary matrix in the polar decomposition of an invertible matrix is uniquely defined .

General derivation [ edit ] The SVD of a square matrix A {\displaystyle A} reads A = W D 1 / 2 V ∗ ∗ {\displaystyle A=WD^{1/2}V^{*}} , with W , V {\displaystyle W,V} unitary matrices, and D {\displaystyle D} a diagonal, positive semi-definite matrix. By simply inserting an additional pair of W {\displaystyle W} s or V {\displaystyle V} s, we obtain the two forms of the polar decomposition of A {\displaystyle A} : A = W D 1 / 2 V ∗ ∗ = ( W D 1 / 2 W ∗ ∗ ) ⏟ ⏟ P ( W V ∗ ∗ ) ⏟ ⏟ U = ( W V ∗ ∗ ) ⏟ ⏟ U ( V D 1 / 2 V ∗ ∗ ) ⏟ ⏟ P ′ .

{\displaystyle A=WD^{1/2}V^{*}=\underbrace {\left(WD^{1/2}W^{*}\right)} _{P}\underbrace {\left(WV^{*}\right)} _{U}=\underbrace {\left(WV^{*}\right)} _{U}\underbrace {\left(VD^{1/2}V^{*}\right)} _{P'}.} More generally, if A {\displaystyle A} is some rectangular n × × m {\displaystyle n\times m} matrix, its SVD can be written as A = W D 1 / 2 V ∗ ∗ {\displaystyle A=WD^{1/2}V^{*}} where now W {\displaystyle W} and V {\displaystyle V} are isometries with dimensions n × × r {\displaystyle n\times r} and m × × r {\displaystyle m\times r} , respectively, where r ≡ ≡ rank ⁡ ⁡ ( A ) {\displaystyle r\equiv \operatorname {rank} (A)} , and D {\displaystyle D} is again a diagonal positive semi-definite square matrix with dimensions r × × r {\displaystyle r\times r} . We can now apply the same reasoning used in the above equation to write A = P U = U P ′ {\displaystyle A=PU=UP'} , but now U ≡ ≡ W V ∗ ∗ {\displaystyle U\equiv WV^{*}} is not in general unitary. Nonetheless, U {\displaystyle U} has the same support and range as A {\displaystyle A} , and it satisfies U ∗ ∗ U = V V ∗ ∗ {\displaystyle U^{*}U=VV^{*}} and U U ∗ ∗ = W W ∗ ∗ {\displaystyle UU^{*}=WW^{*}} . This makes U {\displaystyle U} into an isometry when its action is restricted onto the support of A {\displaystyle A} , that is, it means that U {\displaystyle U} is a partial isometry .

As an explicit example of this more general case, consider the SVD of the following matrix: A ≡ ≡ ( 1 1 2 − − 2 0 0 ) = ( 1 0 0 1 0 0 ) ⏟ ⏟ ≡ ≡ W ( 2 0 0 8 ) ⏟ ⏟ D ( 1 2 1 2 1 2 − − 1 2 ) ⏟ ⏟ V † † .

{\displaystyle A\equiv {\begin{pmatrix}1&1\\2&-2\\0&0\end{pmatrix}}=\underbrace {\begin{pmatrix}1&0\\0&1\\0&0\end{pmatrix}} _{\equiv W}\underbrace {\begin{pmatrix}{\sqrt {2}}&0\\0&{\sqrt {8}}\end{pmatrix}} _{\sqrt {D}}\underbrace {\begin{pmatrix}{\frac {1}{\sqrt {2}}}&{\frac {1}{\sqrt {2}}}\\{\frac {1}{\sqrt {2}}}&-{\frac {1}{\sqrt {2}}}\end{pmatrix}} _{V^{\dagger }}.} We then have W V † † = 1 2 ( 1 1 1 − − 1 0 0 ) {\displaystyle WV^{\dagger }={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&1\\1&-1\\0&0\end{pmatrix}}} which is an isometry, but not unitary. On the other hand, if we consider the decomposition of A ≡ ≡ ( 1 0 0 0 2 0 ) = ( 1 0 0 1 ) ( 1 0 0 2 ) ( 1 0 0 0 1 0 ) , {\displaystyle A\equiv {\begin{pmatrix}1&0&0\\0&2&0\end{pmatrix}}={\begin{pmatrix}1&0\\0&1\end{pmatrix}}{\begin{pmatrix}1&0\\0&2\end{pmatrix}}{\begin{pmatrix}1&0&0\\0&1&0\end{pmatrix}},} we find W V † † = ( 1 0 0 0 1 0 ) , {\displaystyle WV^{\dagger }={\begin{pmatrix}1&0&0\\0&1&0\end{pmatrix}},} which is a partial isometry (but not an isometry).

Bounded operators on Hilbert space [ edit ] The polar decomposition of any bounded linear operator A between complex Hilbert spaces is a canonical factorization as the product of a partial isometry and a non-negative operator.

The polar decomposition for matrices generalizes as follows: if A is a bounded linear operator then there is a unique factorization of A as a product A = UP where U is a partial isometry, P is a non-negative self-adjoint operator and the initial space of U is the closure of the range of P .

The operator U must be weakened to a partial isometry, rather than unitary, because of the following issues. If A is the one-sided shift on l 2 ( N ), then | A | = { A * A } 1/2 = I . So if A = U | A |, U must be A , which is not unitary.

The existence of a polar decomposition is a consequence of Douglas' lemma : Lemma — If A , B are bounded operators on a Hilbert space H , and A * A ≤ B * B , then there exists a contraction C such that A = CB . Furthermore, C is unique if ker( B * ) ⊂ ker( C ).

The operator C can be defined by C ( Bh ) := Ah for all h in H , extended by continuity to the closure of Ran ( B ), and by zero on the orthogonal complement to all of H . The lemma then follows since A * A ≤ B * B implies ker( B ) ⊂ ker( A ).

In particular. If A * A = B * B , then C is a partial isometry, which is unique if ker( B * ) ⊂ ker( C ).
In general, for any bounded operator A , A ∗ ∗ A = ( A ∗ ∗ A ) 1 / 2 ( A ∗ ∗ A ) 1 / 2 , {\displaystyle A^{*}A=\left(A^{*}A\right)^{1/2}\left(A^{*}A\right)^{1/2},} where ( A * A ) 1/2 is the unique positive square root of A * A given by the usual functional calculus . So by the lemma, we have A = U ( A ∗ ∗ A ) 1 / 2 {\displaystyle A=U\left(A^{*}A\right)^{1/2}} for some partial isometry U , which is unique if ker( A * ) ⊂ ker( U ). Take P to be ( A * A ) 1/2 and one obtains the polar decomposition A = UP . Notice that an analogous argument can be used to show A = P'U ' , where P' is positive and U ' a partial isometry.

When H is finite-dimensional, U can be extended to a unitary operator; this is not true in general (see example above). Alternatively, the polar decomposition can be shown using the operator version of singular value decomposition .

By property of the continuous functional calculus , | A | is in the C*-algebra generated by A . A similar but weaker statement holds for the partial isometry: U is in the von Neumann algebra generated by A . If A is invertible, the polar part U will be in the C*-algebra as well.

Unbounded operators [ edit ] If A is a closed, densely defined unbounded operator between complex Hilbert spaces then it still has a (unique) polar decomposition A = U | A | , {\displaystyle A=U|A|,} where | A | is a (possibly unbounded) non-negative self-adjoint operator with the same domain as A , and U is a partial isometry vanishing on the orthogonal complement of the range ran(| A |).

The proof uses the same lemma as above, which goes through for unbounded operators in general. If dom( A * A ) = dom( B * B ), and A * Ah = B * Bh for all h ∈ dom( A * A ), then there exists a partial isometry U such that A = UB .

U is unique if ran( B ) ⊥ ⊂ ker( U ). The operator A being closed and densely defined ensures that the operator A * A is self-adjoint (with dense domain) and therefore allows one to define ( A * A ) 1/2 . Applying the lemma gives polar decomposition.

If an unbounded operator A is affiliated to a von Neumann algebra M , and A = UP is its polar decomposition, then U is in M and so is the spectral projection of P , 1 B ( P ), for any Borel set B in [0, ∞) .

Quaternion polar decomposition [ edit ] The polar decomposition of quaternions H {\displaystyle \mathbb {H} } with orthonormal basis quaternions 1 , ı ı ^ ^ , ȷ ȷ ^ ^ , k ^ ^ {\displaystyle 1,{\hat {\imath }},{\hat {\jmath }},{\hat {k}}} depends on the unit 2-dimensional sphere r ^ ^ ∈ ∈ { x ı ı ^ ^ + y ȷ ȷ ^ ^ + z k ^ ^ ∈ ∈ H ∖ ∖ R : x 2 + y 2 + z 2 = 1 } {\displaystyle {\hat {r}}\in \{x{\hat {\imath }}+y{\hat {\jmath }}+z{\hat {k}}\in \mathbb {H} \setminus \mathbb {R} :x^{2}+y^{2}+z^{2}=1\}} of square roots of minus one , known as right versors . Given any r ^ ^ {\displaystyle {\hat {r}}} on this sphere and an angle − π < a ≤ π , the versor e a r ^ ^ = cos ⁡ ⁡ a + r ^ ^ sin ⁡ ⁡ a {\displaystyle e^{a{\hat {r}}}=\cos a+{\hat {r}}\sin a} is on the unit 3-sphere of H .

{\displaystyle \mathbb {H} .} For a = 0 and a = π , the versor is 1 or −1, regardless of which r is selected. The norm t of a quaternion q is the Euclidean distance from the origin to q . When a quaternion is not just a real number, then there is a unique polar decomposition: q = t exp ⁡ ⁡ ( a r ^ ^ ) .

{\displaystyle q=t\exp(a{\hat {r}}).} Here r , a , t are all uniquely determined such that r is a right versor ( r 2 = –1 ), a satisfies 0 < a < π , and t > 0 .

Alternative planar decompositions [ edit ] In the Cartesian plane , alternative planar ring decompositions arise as follows: If x ≠ 0 , z = x (1 + ε( y / x )) is a polar decomposition of a dual number z = x + yε , where ε 2 = 0 ; i.e., ε is nilpotent . In this polar decomposition, the unit circle has been replaced by the line x = 1 , the polar angle by the slope y / x , and the radius x is negative in the left half-plane.

If x 2 ≠ y 2 , then the unit hyperbola x 2 − y 2 = 1 , and its conjugate x 2 − y 2 = −1 can be used to form a polar decomposition based on the branch of the unit hyperbola through (1, 0) . This branch is parametrized by the hyperbolic angle a and is written cosh ⁡ ⁡ a + j sinh ⁡ ⁡ a = exp ⁡ ⁡ ( a j ) = e a j , {\displaystyle \cosh a+j\sinh a=\exp(aj)=e^{aj},} where j 2 = +1 , and the arithmetic [ 7 ] of split-complex numbers is used. The branch through (−1, 0) is traced by − e aj . Since the operation of multiplying by j reflects a point across the line y = x , the conjugate hyperbola has branches traced by je aj or − je aj . Therefore a point in one of the quadrants has a polar decomposition in one of the forms: r e a j , − − r e a j , r j e a j , − − r j e a j , r > 0.

{\displaystyle re^{aj},-re^{aj},rje^{aj},-rje^{aj},\quad r>0.} The set {1, −1, j , − j } has products that make it isomorphic to the Klein four-group . Evidently polar decomposition in this case involves an element from that group.

Polar decomposition of an element of the algebra M(2, R) of 2 × 2 real matrices uses these alternative planar decompositions since any planar subalgebra is isomorphic to dual numbers, split-complex numbers, or ordinary complex numbers.

Numerical determination of the matrix polar decomposition [ edit ] To compute an approximation of the polar decomposition A = UP , usually the unitary factor U is approximated.

[ 8 ] [ 9 ] The iteration is based on Heron's method for the square root of 1 and computes, starting from U 0 = A {\displaystyle U_{0}=A} , the sequence U k + 1 = 1 2 ( U k + ( U k ∗ ∗ ) − − 1 ) , k = 0 , 1 , 2 , … … {\displaystyle U_{k+1}={\frac {1}{2}}\left(U_{k}+\left(U_{k}^{*}\right)^{-1}\right),\qquad k=0,1,2,\ldots } The combination of inversion and Hermite conjugation is chosen so that in the singular value decomposition, the unitary factors remain the same and the iteration reduces to Heron's method on the singular values.

This basic iteration may be refined to speed up the process: Every step or in regular intervals, the range of the singular values of U k {\displaystyle U_{k}} is estimated and then the matrix is rescaled to γ γ k U k {\displaystyle \gamma _{k}U_{k}} to center the singular values around 1 . The scaling factor γ γ k {\displaystyle \gamma _{k}} is computed using matrix norms of the matrix and its inverse. Examples of such scale estimates are: γ γ k = ‖ U k − − 1 ‖ 1 ‖ U k − − 1 ‖ ∞ ∞ ‖ U k ‖ 1 ‖ U k ‖ ∞ ∞ 4 {\displaystyle \gamma _{k}={\sqrt[{4}]{\frac {\left\|U_{k}^{-1}\right\|_{1}\left\|U_{k}^{-1}\right\|_{\infty }}{\left\|U_{k}\right\|_{1}\left\|U_{k}\right\|_{\infty }}}}} using the row-sum and column-sum matrix norms or γ γ k = ‖ U k − − 1 ‖ F ‖ U k ‖ F {\displaystyle \gamma _{k}={\sqrt {\frac {\left\|U_{k}^{-1}\right\|_{F}}{\left\|U_{k}\right\|_{F}}}}} using the Frobenius norm . Including the scale factor, the iteration is now U k + 1 = 1 2 ( γ γ k U k + 1 γ γ k ( U k ∗ ∗ ) − − 1 ) , k = 0 , 1 , 2 , … … {\displaystyle U_{k+1}={\frac {1}{2}}\left(\gamma _{k}U_{k}+{\frac {1}{\gamma _{k}}}\left(U_{k}^{*}\right)^{-1}\right),\qquad k=0,1,2,\ldots } The QR decomposition can be used in a preparation step to reduce a singular matrix A to a smaller regular matrix, and inside every step to speed up the computation of the inverse.

Heron's method for computing roots of x 2 − − 1 = 0 {\displaystyle x^{2}-1=0} can be replaced by higher order methods, for instance based on Halley's method of third order, resulting in U k + 1 = U k ( I + 3 U k ∗ ∗ U k ) − − 1 ( 3 I + U k ∗ ∗ U k ) , k = 0 , 1 , 2 , … … {\displaystyle U_{k+1}=U_{k}\left(I+3U_{k}^{*}U_{k}\right)^{-1}\left(3I+U_{k}^{*}U_{k}\right),\qquad k=0,1,2,\ldots } This iteration can again be combined with rescaling. This particular formula has the benefit that it is also applicable to singular or rectangular matrices A .

See also [ edit ] Cartan decomposition Algebraic polar decomposition Polar decomposition of a complex measure Lie group decomposition References [ edit ] ^ Hall 2015 , Section 2.5.

^ Hall 2015 , Theorem 2.17.

^ Hall 2015 , Section 13.3.

^ Higham, Nicholas J.; Schreiber, Robert S. (1990). "Fast polar decomposition of an arbitrary matrix".

SIAM J. Sci. Stat. Comput .

11 (4). Philadelphia, PA, USA: Society for Industrial and Applied Mathematics: 648– 655.

CiteSeerX 10.1.1.111.9239 .

doi : 10.1137/0911038 .

ISSN 0196-5204 .

S2CID 14268409 .

^ Hall 2015 , Lemma 2.18.

^ Note how this implies, by the positivity of A ∗ ∗ A {\displaystyle A^{*}A} , that the eigenvalues are all real and strictly positive.

^ Sobczyk, G. (1995) "Hyperbolic Number Plane", College Mathematics Journal 26:268–280.

^ Higham, Nicholas J. (1986). "Computing the polar decomposition with applications".

SIAM J. Sci. Stat. Comput .

7 (4). Philadelphia, PA, USA: Society for Industrial and Applied Mathematics: 1160– 1174.

CiteSeerX 10.1.1.137.7354 .

doi : 10.1137/0907079 .

ISSN 0196-5204 .

^ Byers, Ralph; Hongguo Xu (2008). "A New Scaling for Newton's Iteration for the Polar Decomposition and its Backward Stability".

SIAM J. Matrix Anal. Appl .

30 (2). Philadelphia, PA, USA: Society for Industrial and Applied Mathematics: 822– 843.

CiteSeerX 10.1.1.378.6737 .

doi : 10.1137/070699895 .

ISSN 0895-4798 .

Conway, J. B.

(1990).

A Course in Functional Analysis .

Graduate Texts in Mathematics . New York: Springer.

doi : 10.1007/978-1-4757-4383-8 .

Douglas, R. G.

(1966). "On Majorization, Factorization, and Range Inclusion of Operators on Hilbert Space".

Proc. Amer. Math. Soc.

17 : 413– 415.

doi : 10.1090/S0002-9939-1966-0203464-1 .

Hall, Brian C. (2015).

Lie Groups, Lie Algebras, and Representations: An Elementary Introduction . Graduate Texts in Mathematics. Vol. 222 (2nd ed.). Springer.

ISBN 978-3319134666 .

Helgason, Sigurdur (1978).

Differential geometry, Lie groups, and symmetric spaces . Academic Press.

ISBN 0-8218-2848-7 .

v t e Functional analysis ( topics – glossary ) Spaces Banach Besov Fréchet Hilbert Hölder Nuclear Orlicz Schwartz Sobolev Topological vector Properties Barrelled Complete Dual ( Algebraic / Topological ) Locally convex Reflexive Separable Theorems Hahn–Banach Riesz representation Closed graph Uniform boundedness principle Kakutani fixed-point Krein–Milman Min–max Gelfand–Naimark Banach–Alaoglu Operators Adjoint Bounded Compact Hilbert–Schmidt Normal Nuclear Trace class Transpose Unbounded Unitary Algebras Banach algebra C*-algebra Spectrum of a C*-algebra Operator algebra Group algebra of a locally compact group Von Neumann algebra Open problems Invariant subspace problem Mahler's conjecture Applications Hardy space Spectral theory of ordinary differential equations Heat kernel Index theorem Calculus of variations Functional calculus Integral linear operator Jones polynomial Topological quantum field theory Noncommutative geometry Riemann hypothesis Distribution (or Generalized functions ) Advanced topics Approximation property Balanced set Choquet theory Weak topology Banach–Mazur distance Tomita–Takesaki theory Category v t e Spectral theory and * -algebras Basic concepts Involution/*-algebra Banach algebra B*-algebra C*-algebra Noncommutative topology Projection-valued measure Spectrum Spectrum of a C*-algebra Spectral radius Operator space Main results Gelfand–Mazur theorem Gelfand–Naimark theorem Gelfand representation Polar decomposition Singular value decomposition Spectral theorem Spectral theory of normal C*-algebras Special Elements/Operators Isospectral Normal operator Hermitian/Self-adjoint operator Unitary operator Unit Spectrum Krein–Rutman theorem Normal eigenvalue Spectrum of a C*-algebra Spectral radius Spectral asymmetry Spectral gap Decomposition Decomposition of a spectrum Continuous Point Residual Approximate point Compression Direct integral Discrete Spectral abscissa Spectral Theorem Borel functional calculus Min-max theorem Positive operator-valued measure Projection-valued measure Riesz projector Rigged Hilbert space Spectral theorem Spectral theory of compact operators Spectral theory of normal C*-algebras Special algebras Amenable Banach algebra With an Approximate identity Banach function algebra Disk algebra Nuclear C*-algebra Uniform algebra Von Neumann algebra Tomita–Takesaki theory Finite-Dimensional Alon–Boppana bound Bauer–Fike theorem Numerical range Schur–Horn theorem Generalizations Dirac spectrum Essential spectrum Pseudospectrum Structure space ( Shilov boundary ) Miscellaneous Abstract index group Banach algebra cohomology Cohen–Hewitt factorization theorem Extensions of symmetric operators Fredholm theory Limiting absorption principle Schröder–Bernstein theorems for operator algebras Sherman–Takeda theorem Unbounded operator Examples Wiener algebra Applications Almost Mathieu operator Corona theorem Hearing the shape of a drum ( Dirichlet eigenvalue ) Heat kernel Kuznetsov trace formula Lax pair Proto-value function Ramanujan graph Rayleigh–Faber–Krahn inequality Spectral geometry Spectral method Spectral theory of ordinary differential equations Sturm–Liouville theory Superstrong approximation Transfer operator Transform theory Weyl law Wiener–Khinchin theorem Retrieved from " https://en.wikipedia.org/w/index.php?title=Polar_decomposition&oldid=1287465916 " Categories : Lie groups Operator theory Matrix theory Matrix decompositions Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 26 April 2025, at 13:01 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Polar decomposition 13 languages Add topic

