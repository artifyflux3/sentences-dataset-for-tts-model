Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Density 2 Theorems Toggle Theorems subsection 2.1 Distribution of the inverse of a Wishart-distributed matrix 2.2 Marginal and conditional distributions from an inverse Wishart-distributed matrix 2.3 Conjugate distribution 2.4 Moments 3 Related distributions 4 See also 5 References Toggle the table of contents Inverse-Wishart distribution 4 languages Català فارسی Français 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Inverse-Wishart Notation W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle {\mathcal {W}}^{-1}({\mathbf {\Psi } },\nu )} Parameters ν ν > p − − 1 {\displaystyle \nu >p-1} degrees of freedom ( real ) Ψ Ψ > 0 {\displaystyle \mathbf {\Psi } >0} , p × × p {\displaystyle p\times p} scale matrix ( pos. def.

) Support X {\displaystyle \mathbf {X} } is p × p positive definite PDF | Ψ Ψ | ν ν / 2 2 ν ν p / 2 Γ Γ p ( ν ν 2 ) | X | − − ( ν ν + p + 1 ) / 2 e − − 1 2 tr ⁡ ⁡ ( Ψ Ψ X − − 1 ) {\displaystyle {\frac {\left|\mathbf {\Psi } \right|^{\nu /2}}{2^{\nu p/2}\Gamma _{p}({\frac {\nu }{2}})}}\left|\mathbf {X} \right|^{-(\nu +p+1)/2}e^{-{\frac {1}{2}}\operatorname {tr} (\mathbf {\Psi } \mathbf {X} ^{-1})}} Γ Γ p {\displaystyle \Gamma _{p}} is the multivariate gamma function tr {\displaystyle \operatorname {tr} } is the trace function Mean Ψ Ψ ν ν − − p − − 1 {\displaystyle {\frac {\mathbf {\Psi } }{\nu -p-1}}} For ν ν > p + 1 {\displaystyle \nu >p+1} Mode Ψ Ψ ν ν + p + 1 {\displaystyle {\frac {\mathbf {\Psi } }{\nu +p+1}}} [ 1 ] : 406 Variance see below In statistics , the inverse Wishart distribution , also called the inverted Wishart distribution , is a probability distribution defined on real-valued positive-definite matrices .  In Bayesian statistics it is used as the conjugate prior for the covariance matrix of a multivariate normal distribution.

We say X {\displaystyle \mathbf {X} } follows an inverse Wishart distribution, denoted as X ∼ ∼ W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle \mathbf {X} \sim {\mathcal {W}}^{-1}(\mathbf {\Psi } ,\nu )} , if its inverse X − − 1 {\displaystyle \mathbf {X} ^{-1}} has a Wishart distribution W ( Ψ Ψ − − 1 , ν ν ) {\displaystyle {\mathcal {W}}(\mathbf {\Psi } ^{-1},\nu )} . Important identities have been derived for the inverse-Wishart distribution.

[ 2 ] Density [ edit ] The probability density function of the inverse Wishart is: [ 3 ] f X ( X ; Ψ Ψ , ν ν ) = | Ψ Ψ | ν ν / 2 2 ν ν p / 2 Γ Γ p ( ν ν 2 ) | X | − − ( ν ν + p + 1 ) / 2 e − − 1 2 tr ⁡ ⁡ ( Ψ Ψ X − − 1 ) {\displaystyle f_{\mathbf {X} }({\mathbf {X} };{\mathbf {\Psi } },\nu )={\frac {\left|{\mathbf {\Psi } }\right|^{\nu /2}}{2^{\nu p/2}\Gamma _{p}({\frac {\nu }{2}})}}\left|\mathbf {X} \right|^{-(\nu +p+1)/2}e^{-{\frac {1}{2}}\operatorname {tr} (\mathbf {\Psi } \mathbf {X} ^{-1})}} where X {\displaystyle \mathbf {X} } and Ψ Ψ {\displaystyle {\mathbf {\Psi } }} are p × × p {\displaystyle p\times p} positive definite matrices, | ⋅ ⋅ | {\displaystyle |\cdot |} is the determinant, and Γ Γ p ( ⋅ ⋅ ) {\displaystyle \Gamma _{p}(\cdot )} is the multivariate gamma function .

Theorems [ edit ] Distribution of the inverse of a Wishart-distributed matrix [ edit ] If X ∼ ∼ W ( Σ Σ , ν ν ) {\displaystyle {\mathbf {X} }\sim {\mathcal {W}}({\mathbf {\Sigma } },\nu )} and Σ Σ {\displaystyle {\mathbf {\Sigma } }} is of size p × × p {\displaystyle p\times p} , then A = X − − 1 {\displaystyle \mathbf {A} ={\mathbf {X} }^{-1}} has an inverse Wishart distribution A ∼ ∼ W − − 1 ( Σ Σ − − 1 , ν ν ) {\displaystyle \mathbf {A} \sim {\mathcal {W}}^{-1}({\mathbf {\Sigma } }^{-1},\nu )} .

[ 4 ] Marginal and conditional distributions from an inverse Wishart-distributed matrix [ edit ] Suppose A ∼ ∼ W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle {\mathbf {A} }\sim {\mathcal {W}}^{-1}({\mathbf {\Psi } },\nu )} has an inverse Wishart distribution. Partition the matrices A {\displaystyle {\mathbf {A} }} and Ψ Ψ {\displaystyle {\mathbf {\Psi } }} conformably with each other A = [ A 11 A 12 A 21 A 22 ] , Ψ Ψ = [ Ψ Ψ 11 Ψ Ψ 12 Ψ Ψ 21 Ψ Ψ 22 ] {\displaystyle {\mathbf {A} }={\begin{bmatrix}\mathbf {A} _{11}&\mathbf {A} _{12}\\\mathbf {A} _{21}&\mathbf {A} _{22}\end{bmatrix}},\;{\mathbf {\Psi } }={\begin{bmatrix}\mathbf {\Psi } _{11}&\mathbf {\Psi } _{12}\\\mathbf {\Psi } _{21}&\mathbf {\Psi } _{22}\end{bmatrix}}} where A i j {\displaystyle {\mathbf {A} _{ij}}} and Ψ Ψ i j {\displaystyle {\mathbf {\Psi } _{ij}}} are p i × × p j {\displaystyle p_{i}\times p_{j}} matrices, then we have A 11 {\displaystyle \mathbf {A} _{11}} is independent of A 11 − − 1 A 12 {\displaystyle \mathbf {A} _{11}^{-1}\mathbf {A} _{12}} and A 22 ⋅ ⋅ 1 {\displaystyle {\mathbf {A} }_{22\cdot 1}} , where A 22 ⋅ ⋅ 1 = A 22 − − A 21 A 11 − − 1 A 12 {\displaystyle {\mathbf {A} _{22\cdot 1}}={\mathbf {A} }_{22}-{\mathbf {A} }_{21}{\mathbf {A} }_{11}^{-1}{\mathbf {A} }_{12}} is the Schur complement of A 11 {\displaystyle {\mathbf {A} _{11}}} in A {\displaystyle {\mathbf {A} }} ; A 11 ∼ ∼ W − − 1 ( Ψ Ψ 11 , ν ν − − p 2 ) {\displaystyle {\mathbf {A} _{11}}\sim {\mathcal {W}}^{-1}({\mathbf {\Psi } _{11}},\nu -p_{2})} ; A 11 − − 1 A 12 ∣ ∣ A 22 ⋅ ⋅ 1 ∼ ∼ M N p 1 × × p 2 ( Ψ Ψ 11 − − 1 Ψ Ψ 12 , A 22 ⋅ ⋅ 1 ⊗ ⊗ Ψ Ψ 11 − − 1 ) {\displaystyle {\mathbf {A} }_{11}^{-1}{\mathbf {A} }_{12}\mid {\mathbf {A} }_{22\cdot 1}\sim MN_{p_{1}\times p_{2}}({\mathbf {\Psi } }_{11}^{-1}{\mathbf {\Psi } }_{12},{\mathbf {A} }_{22\cdot 1}\otimes {\mathbf {\Psi } }_{11}^{-1})} , where M N p × × q ( ⋅ ⋅ , ⋅ ⋅ ) {\displaystyle MN_{p\times q}(\cdot ,\cdot )} is a matrix normal distribution ; A 22 ⋅ ⋅ 1 ∼ ∼ W − − 1 ( Ψ Ψ 22 ⋅ ⋅ 1 , ν ν ) {\displaystyle {\mathbf {A} }_{22\cdot 1}\sim {\mathcal {W}}^{-1}({\mathbf {\Psi } }_{22\cdot 1},\nu )} , where Ψ Ψ 22 ⋅ ⋅ 1 = Ψ Ψ 22 − − Ψ Ψ 21 Ψ Ψ 11 − − 1 Ψ Ψ 12 {\displaystyle {\mathbf {\Psi } _{22\cdot 1}}={\mathbf {\Psi } }_{22}-{\mathbf {\Psi } }_{21}{\mathbf {\Psi } }_{11}^{-1}{\mathbf {\Psi } }_{12}} ; Conjugate distribution [ edit ] Suppose we wish to make inference about a covariance matrix Σ Σ {\displaystyle {\mathbf {\Sigma } }} whose prior p ( Σ Σ ) {\displaystyle {p(\mathbf {\Sigma } )}} has a W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle {\mathcal {W}}^{-1}({\mathbf {\Psi } },\nu )} distribution.  If the observations X = [ x 1 , … … , x n ] {\displaystyle \mathbf {X} =[\mathbf {x} _{1},\ldots ,\mathbf {x} _{n}]} are independent p-variate Gaussian variables drawn from a N ( 0 , Σ Σ ) {\displaystyle N(\mathbf {0} ,{\mathbf {\Sigma } })} distribution, then the conditional distribution p ( Σ Σ ∣ ∣ X ) {\displaystyle {p(\mathbf {\Sigma } \mid \mathbf {X} )}} has a W − − 1 ( A + Ψ Ψ , n + ν ν ) {\displaystyle {\mathcal {W}}^{-1}({\mathbf {A} }+{\mathbf {\Psi } },n+\nu )} distribution, where A = X X T {\displaystyle {\mathbf {A} }=\mathbf {X} \mathbf {X} ^{T}} .

Because the prior and posterior distributions are the same family, we say the inverse Wishart distribution is conjugate to the multivariate Gaussian.

Due to its conjugacy to the multivariate Gaussian, it is possible to marginalize out (integrate out) the Gaussian's parameter Σ Σ {\displaystyle \mathbf {\Sigma } } , using the formula p ( x ) = p ( x | Σ Σ ) p ( Σ Σ ) p ( Σ Σ | x ) {\displaystyle p(x)={\frac {p(x|\Sigma )p(\Sigma )}{p(\Sigma |x)}}} and the linear algebra identity v T Ω Ω v = tr ( Ω Ω v v T ) {\displaystyle v^{T}\Omega v={\text{tr}}(\Omega vv^{T})} : f X ∣ ∣ Ψ Ψ , ν ν ( x ) = ∫ ∫ f X ∣ ∣ Σ Σ = σ σ ( x ) f Σ Σ ∣ ∣ Ψ Ψ , ν ν ( σ σ ) d σ σ = | Ψ Ψ | ν ν / 2 Γ Γ p ( ν ν + n 2 ) π π n p / 2 | Ψ Ψ + A | ( ν ν + n ) / 2 Γ Γ p ( ν ν 2 ) {\displaystyle f_{\mathbf {X} \,\mid \,\Psi ,\nu }(\mathbf {x} )=\int f_{\mathbf {X} \,\mid \,\mathbf {\Sigma } \,=\,\sigma }(\mathbf {x} )f_{\mathbf {\Sigma } \,\mid \,\mathbf {\Psi } ,\nu }(\sigma )\,d\sigma ={\frac {|\mathbf {\Psi } |^{\nu /2}\Gamma _{p}\left({\frac {\nu +n}{2}}\right)}{\pi ^{np/2}|\mathbf {\Psi } +\mathbf {A} |^{(\nu +n)/2}\Gamma _{p}({\frac {\nu }{2}})}}} (this is useful because the variance matrix Σ Σ {\displaystyle \mathbf {\Sigma } } is not known in practice, but because Ψ Ψ {\displaystyle {\mathbf {\Psi } }} is known a priori , and A {\displaystyle {\mathbf {A} }} can be obtained from the data, the right hand side can be evaluated directly). The inverse-Wishart distribution as a prior can be constructed via existing transferred prior knowledge .

[ 5 ] Moments [ edit ] The following is based on Press, S. J. (1982) "Applied Multivariate Analysis", 2nd ed. (Dover Publications, New York), after reparameterizing the degree of freedom to be consistent with the p.d.f. definition above.

Let W ∼ ∼ W ( Ψ Ψ − − 1 , ν ν ) {\displaystyle W\sim {\mathcal {W}}(\mathbf {\Psi } ^{-1},\nu )} with ν ν ≥ ≥ p {\displaystyle \nu \geq p} and X ≐ ≐ W − − 1 {\displaystyle X\doteq W^{-1}} , so that X ∼ ∼ W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle X\sim {\mathcal {W}}^{-1}(\mathbf {\Psi } ,\nu )} .

The mean, for ν ν ≥ ≥ p + 2 {\displaystyle \nu \geq p+2} : [ 4 ] : 91 E ⁡ ⁡ ( X ) = Ψ Ψ ν ν − − p − − 1 .

{\displaystyle \operatorname {E} (\mathbf {X} )={\frac {\mathbf {\Psi } }{\nu -p-1}}.} The variance of each element of X {\displaystyle \mathbf {X} } : Var ⁡ ⁡ ( x i j ) = ( ν ν − − p + 1 ) ψ ψ i j 2 + ( ν ν − − p − − 1 ) ψ ψ i i ψ ψ j j ( ν ν − − p ) ( ν ν − − p − − 1 ) 2 ( ν ν − − p − − 3 ) {\displaystyle \operatorname {Var} (x_{ij})={\frac {(\nu -p+1)\psi _{ij}^{2}+(\nu -p-1)\psi _{ii}\psi _{jj}}{(\nu -p)(\nu -p-1)^{2}(\nu -p-3)}}} The variance of the diagonal uses the same formula as above with i = j {\displaystyle i=j} , which simplifies to: Var ⁡ ⁡ ( x i i ) = 2 ψ ψ i i 2 ( ν ν − − p − − 1 ) 2 ( ν ν − − p − − 3 ) .

{\displaystyle \operatorname {Var} (x_{ii})={\frac {2\psi _{ii}^{2}}{(\nu -p-1)^{2}(\nu -p-3)}}.} The covariance of elements of X {\displaystyle \mathbf {X} } are given by: Cov ⁡ ⁡ ( x i j , x k ℓ ℓ ) = 2 ψ ψ i j ψ ψ k ℓ ℓ + ( ν ν − − p − − 1 ) ( ψ ψ i k ψ ψ j ℓ ℓ + ψ ψ i ℓ ℓ ψ ψ k j ) ( ν ν − − p ) ( ν ν − − p − − 1 ) 2 ( ν ν − − p − − 3 ) {\displaystyle \operatorname {Cov} (x_{ij},x_{k\ell })={\frac {2\psi _{ij}\psi _{k\ell }+(\nu -p-1)(\psi _{ik}\psi _{j\ell }+\psi _{i\ell }\psi _{kj})}{(\nu -p)(\nu -p-1)^{2}(\nu -p-3)}}} The same results are expressed in Kronecker product form by von Rosen [ 6 ] as follows: E ( W − − 1 ⊗ ⊗ W − − 1 ) = c 1 Ψ Ψ ⊗ ⊗ Ψ Ψ + c 2 V e c ( Ψ Ψ ) V e c ( Ψ Ψ ) T + c 2 K p p Ψ Ψ ⊗ ⊗ Ψ Ψ C o v ⊗ ⊗ ( W − − 1 , W − − 1 ) = ( c 1 − − c 3 ) Ψ Ψ ⊗ ⊗ Ψ Ψ + c 2 V e c ( Ψ Ψ ) V e c ( Ψ Ψ ) T + c 2 K p p Ψ Ψ ⊗ ⊗ Ψ Ψ {\displaystyle {\begin{aligned}\mathbf {E} \left(W^{-1}\otimes W^{-1}\right)&=c_{1}\Psi \otimes \Psi +c_{2}Vec(\Psi )Vec(\Psi )^{T}+c_{2}K_{pp}\Psi \otimes \Psi \\\mathbf {Cov} _{\otimes }\left(W^{-1},W^{-1}\right)&=(c_{1}-c_{3})\Psi \otimes \Psi +c_{2}Vec(\Psi )Vec(\Psi )^{T}+c_{2}K_{pp}\Psi \otimes \Psi \end{aligned}}} where c 2 = [ ( ν ν − − p ) ( ν ν − − p − − 1 ) ( ν ν − − p − − 3 ) ] − − 1 c 1 = ( ν ν − − p − − 2 ) c 2 c 3 = ( ν ν − − p − − 1 ) − − 2 , {\displaystyle {\begin{aligned}c_{2}&=\left[(\nu -p)(\nu -p-1)(\nu -p-3)\right]^{-1}\\c_{1}&=(\nu -p-2)c_{2}\\c_{3}&=(\nu -p-1)^{-2},\end{aligned}}} K p p is a p 2 × × p 2 {\displaystyle K_{pp}{\text{ is a }}p^{2}\times p^{2}} commutation matrix C o v ⊗ ⊗ ( W − − 1 , W − − 1 ) = E ( W − − 1 ⊗ ⊗ W − − 1 ) − − E ( W − − 1 ) ⊗ ⊗ E ( W − − 1 ) .

{\displaystyle \mathbf {Cov} _{\otimes }\left(W^{-1},W^{-1}\right)=\mathbf {E} \left(W^{-1}\otimes W^{-1}\right)-\mathbf {E} \left(W^{-1}\right)\otimes \mathbf {E} \left(W^{-1}\right).} There appears to be a typo in the paper whereby the coefficient of K p p Ψ Ψ ⊗ ⊗ Ψ Ψ {\displaystyle K_{pp}\Psi \otimes \Psi } is given as c 1 {\displaystyle c_{1}} rather than c 2 {\displaystyle c_{2}} , and that the expression for the mean square inverse Wishart, corollary 3.1, should read E [ W − − 1 W − − 1 ] = ( c 1 + c 2 ) Σ Σ − − 1 Σ Σ − − 1 + c 2 Σ Σ − − 1 t r ( Σ Σ − − 1 ) .

{\displaystyle \mathbf {E} \left[W^{-1}W^{-1}\right]=(c_{1}+c_{2})\Sigma ^{-1}\Sigma ^{-1}+c_{2}\Sigma ^{-1}\mathbf {tr} (\Sigma ^{-1}).} To show how the interacting terms become sparse when the covariance is diagonal, let Ψ Ψ = I 3 × × 3 {\displaystyle \Psi =\mathbf {I} _{3\times 3}} and introduce some arbitrary parameters u , v , w {\displaystyle u,v,w} : E ( W − − 1 ⊗ ⊗ W − − 1 ) = u Ψ Ψ ⊗ ⊗ Ψ Ψ + v v e c ( Ψ Ψ ) v e c ( Ψ Ψ ) T + w K p p Ψ Ψ ⊗ ⊗ Ψ Ψ .

{\displaystyle \mathbf {E} \left(W^{-1}\otimes W^{-1}\right)=u\Psi \otimes \Psi +v\,\mathrm {vec} (\Psi )\,\mathrm {vec} (\Psi )^{T}+wK_{pp}\Psi \otimes \Psi .} where v e c {\displaystyle \mathrm {vec} } denotes the matrix vectorization operator. Then the second moment matrix becomes E ( W − − 1 ⊗ ⊗ W − − 1 ) = [ u + v + w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ v ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ v ⋅ ⋅ u ⋅ ⋅ w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ u ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ w ⋅ ⋅ u ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ v ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ u + v + w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ v ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ u ⋅ ⋅ w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ w ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ u ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ w ⋅ ⋅ u ⋅ ⋅ v ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ v ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ u + v + w ] {\displaystyle \mathbf {E} \left(W^{-1}\otimes W^{-1}\right)={\begin{bmatrix}u+v+w&\cdot &\cdot &\cdot &v&\cdot &\cdot &\cdot &v\\\cdot &u&\cdot &w&\cdot &\cdot &\cdot &\cdot &\cdot \\\cdot &\cdot &u&\cdot &\cdot &\cdot &w&\cdot &\cdot \\\cdot &w&\cdot &u&\cdot &\cdot &\cdot &\cdot &\cdot \\v&\cdot &\cdot &\cdot &u+v+w&\cdot &\cdot &\cdot &v\\\cdot &\cdot &\cdot &\cdot &\cdot &u&\cdot &w&\cdot \\\cdot &\cdot &w&\cdot &\cdot &\cdot &u&\cdot &\cdot \\\cdot &\cdot &\cdot &\cdot &\cdot &w&\cdot &u&\cdot \\v&\cdot &\cdot &\cdot &v&\cdot &\cdot &\cdot &u+v+w\\\end{bmatrix}}} which is non-zero only when involving the correlations of diagonal elements of W − − 1 {\displaystyle W^{-1}} , all other elements are mutually uncorrelated, though not necessarily statistically independent. The variances of the Wishart product are also obtained by Cook et al.

[ 7 ] in the singular case and, by extension, to the full rank case.

Muirhead [ 8 ] shows in Theorem 3.2.8 that if A p × × p {\displaystyle A^{p\times p}} is distributed as W p ( ν ν , Σ Σ ) {\displaystyle {\mathcal {W}}_{p}(\nu ,\Sigma )} and V {\displaystyle V} is an arbitrary vector, independent of A {\displaystyle A} then V T A V ∼ ∼ W 1 ( ν ν , A T Σ Σ A ) {\displaystyle V^{T}AV\sim {\mathcal {W}}_{1}(\nu ,A^{T}\Sigma A)} and V T A V V T Σ Σ V ∼ ∼ χ χ ν ν − − 1 2 {\displaystyle {\frac {V^{T}AV}{V^{T}\Sigma V}}\sim \chi _{\nu -1}^{2}} , one degree of freedom being relinquished by estimation of the sample mean in the latter.  Similarly, Bodnar et.al. further find that V T A − − 1 V V T Σ Σ − − 1 V ∼ ∼ Inv- χ χ ν ν − − p + 1 2 {\displaystyle {\frac {V^{T}A^{-1}V}{V^{T}\Sigma ^{-1}V}}\sim {\text{Inv-}}\chi _{\nu -p+1}^{2}} and setting V = ( 1 , 0 , ⋯ ⋯ , 0 ) T {\displaystyle V=(1,\,0,\cdots ,0)^{T}} the marginal distribution of the leading diagonal element is thus [ A − − 1 ] 1 , 1 [ Σ Σ − − 1 ] 1 , 1 ∼ ∼ 2 − − k / 2 Γ Γ ( k / 2 ) x − − k / 2 − − 1 e − − 1 / ( 2 x ) , k = ν ν − − p + 1 {\displaystyle {\frac {[A^{-1}]_{1,1}}{[\Sigma ^{-1}]_{1,1}}}\sim {\frac {2^{-k/2}}{\Gamma (k/2)}}x^{-k/2-1}e^{-1/(2x)},\;\;k=\nu -p+1} and by rotating V {\displaystyle V} end-around a similar result applies to all diagonal elements [ A − − 1 ] i , i {\displaystyle [A^{-1}]_{i,i}} .

A corresponding result in the complex Wishart case was shown by Brennan and Reed [ 9 ] and the uncorrelated inverse complex Wishart W C − − 1 ( I , ν ν , p ) {\displaystyle {\mathcal {W_{C}}}^{-1}(\mathbf {I} ,\nu ,p)} was shown by Shaman [ 10 ] to have diagonal statistical structure in which the leading diagonal elements are correlated, while all other element are uncorrelated.

Related distributions [ edit ] A univariate specialization of the inverse-Wishart distribution is the inverse-gamma distribution . With p = 1 {\displaystyle p=1} (i.e. univariate) and α α = ν ν / 2 {\displaystyle \alpha =\nu /2} , β β = Ψ Ψ / 2 {\displaystyle \beta =\mathbf {\Psi } /2} and x = X {\displaystyle x=\mathbf {X} } the probability density function of the inverse-Wishart distribution becomes matrix p ( x ∣ ∣ α α , β β ) = β β α α x − − α α − − 1 exp ⁡ ⁡ ( − − β β / x ) Γ Γ 1 ( α α ) .

{\displaystyle p(x\mid \alpha ,\beta )={\frac {\beta ^{\alpha }\,x^{-\alpha -1}\exp(-\beta /x)}{\Gamma _{1}(\alpha )}}.} i.e., the inverse-gamma distribution, where Γ Γ 1 ( ⋅ ⋅ ) {\displaystyle \Gamma _{1}(\cdot )} is the ordinary Gamma function .

The Inverse Wishart distribution is a special case of the inverse matrix gamma distribution when the shape parameter α α = ν ν 2 {\displaystyle \alpha ={\frac {\nu }{2}}} and the scale parameter β β = 2 {\displaystyle \beta =2} .

Another generalization has been termed the generalized inverse Wishart distribution, G W − − 1 {\displaystyle {\mathcal {GW}}^{-1}} . A p × × p {\displaystyle p\times p} positive definite matrix X {\displaystyle \mathbf {X} } is said to be distributed as G W − − 1 ( Ψ Ψ , ν ν , S ) {\displaystyle {\mathcal {GW}}^{-1}(\mathbf {\Psi } ,\nu ,\mathbf {S} )} if Y = X 1 / 2 S − − 1 X 1 / 2 {\displaystyle \mathbf {Y} =\mathbf {X} ^{1/2}\mathbf {S} ^{-1}\mathbf {X} ^{1/2}} is distributed as W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle {\mathcal {W}}^{-1}(\mathbf {\Psi } ,\nu )} . Here X 1 / 2 {\displaystyle \mathbf {X} ^{1/2}} denotes the symmetric matrix square root of X {\displaystyle \mathbf {X} } , the parameters Ψ Ψ , S {\displaystyle \mathbf {\Psi } ,\mathbf {S} } are p × × p {\displaystyle p\times p} positive definite matrices, and the parameter ν ν {\displaystyle \nu } is a positive scalar larger than 2 p {\displaystyle 2p} . Note that when S {\displaystyle \mathbf {S} } is equal to an identity matrix, G W − − 1 ( Ψ Ψ , ν ν , S ) = W − − 1 ( Ψ Ψ , ν ν ) {\displaystyle {\mathcal {GW}}^{-1}(\mathbf {\Psi } ,\nu ,\mathbf {S} )={\mathcal {W}}^{-1}(\mathbf {\Psi } ,\nu )} . This generalized inverse Wishart distribution has been applied to estimating the distributions of multivariate autoregressive processes.

[ 11 ] A different type of generalization is the normal-inverse-Wishart distribution , essentially the product of a multivariate normal distribution with an inverse Wishart distribution.

When the scale matrix is an identity matrix, Ψ Ψ = I {\displaystyle {\mathcal {\Psi }}=\mathbf {I} } , and Φ Φ {\displaystyle {\mathcal {\Phi }}} is an arbitrary orthogonal  matrix, replacement of X {\displaystyle \mathbf {X} } by Φ Φ X Φ Φ T {\displaystyle {\Phi }\mathbf {X} {\mathcal {\Phi }}^{T}} does not change the pdf of X {\displaystyle \mathbf {X} } so W − − 1 ( I , ν ν , p ) {\displaystyle {\mathcal {W}}^{-1}(\mathbf {I} ,\nu ,p)} belongs to the family of spherically invariant random processes (SIRPs) in some sense.

[ clarification needed ] Thus, an arbitrary p-vector V {\displaystyle V} with l 2 {\displaystyle l_{2}} length V T V = 1 {\displaystyle V^{T}V=1} can be rotated into the vector Φ Φ V = [ 1 0 0 ⋯ ⋯ ] T {\displaystyle \mathbf {\Phi } V=[1\;0\;0\cdots ]^{T}} without changing the pdf of V T X V {\displaystyle V^{T}\mathbf {X} V} , moreover Φ Φ {\displaystyle \mathbf {\Phi } } can be a permutation matrix which exchanges diagonal elements.  It follows that  the diagonal elements of X {\displaystyle \mathbf {X} } are identically inverse chi squared distributed, with pdf f x 11 {\displaystyle f_{x_{11}}} in the previous section though they are not mutually independent.  The result is known in optimal portfolio statistics, as in Theorem 2 Corollary 1 of Bodnar et al, [ 12 ] where it is expressed in the inverse form V T Ψ Ψ V V T X V ∼ ∼ χ χ ν ν − − p + 1 2 {\displaystyle {\frac {V^{T}\mathbf {\Psi } V}{V^{T}\mathbf {X} V}}\sim \chi _{\nu -p+1}^{2}} .

As is the case with the Wishart distribution linear transformations of the distribution yield a modified inverse Wishart distribution.  If X p × × p ∼ ∼ W p − − 1 ( Ψ Ψ , ν ν ) .

{\displaystyle \mathbf {X^{p\times p}} \sim {\mathcal {W}}_{p}^{-1}\left({\mathbf {\Psi } },\nu \right).} and Θ Θ p × × p {\displaystyle {\mathbf {\Theta } }^{p\times p}} are full rank matrices then [ 13 ] Θ Θ X Θ Θ T ∼ ∼ W p − − 1 ( Θ Θ Ψ Ψ Θ Θ T , ν ν ) .

{\displaystyle \mathbf {\Theta } \mathbf {X} {\mathbf {\Theta } }^{T}\sim {\mathcal {W}}_{p}^{-1}\left({\mathbf {\Theta } }{\mathbf {\Psi } }{\mathbf {\Theta } }^{T},\nu \right).} If X p × × p ∼ ∼ W p − − 1 ( Ψ Ψ , ν ν ) .

{\displaystyle \mathbf {X^{p\times p}} \sim {\mathcal {W}}_{p}^{-1}\left({\mathbf {\Psi } },\nu \right).} and Θ Θ m × × p {\displaystyle {\mathbf {\Theta } }^{m\times p}} is m × × p , m < p {\displaystyle m\times p,\;\;m<p} of  full rank m {\displaystyle m} then [ 13 ] Θ Θ X Θ Θ T ∼ ∼ W m − − 1 ( Θ Θ Ψ Ψ Θ Θ T , ν ν ) .

{\displaystyle \mathbf {\Theta } \mathbf {X} {\mathbf {\Theta } }^{T}\sim {\mathcal {W}}_{m}^{-1}\left({\mathbf {\Theta } }{\mathbf {\Psi } }{\mathbf {\Theta } }^{T},\nu \right).} See also [ edit ] Inverse matrix gamma distribution Matrix normal distribution Wishart distribution Complex inverse Wishart distribution References [ edit ] ^ A. O'Hagan, and J. J. Forster (2004).

Kendall's Advanced Theory of Statistics: Bayesian Inference . Vol. 2B (2 ed.). Arnold.

ISBN 978-0-340-80752-1 .

^ Haff, LR (1979). "An identity for the Wishart distribution with applications".

Journal of Multivariate Analysis .

9 (4): 531– 544.

doi : 10.1016/0047-259x(79)90056-3 .

^ Gelman, Andrew; Carlin, John B.; Stern, Hal S.; Dunson, David B.; Vehtari, Aki; Rubin, Donald B. (2013-11-01).

Bayesian Data Analysis, Third Edition (3rd ed.). Boca Raton: Chapman and Hall/CRC.

ISBN 9781439840955 .

^ a b Kanti V. Mardia , J. T. Kent and J. M. Bibby (1979).

Multivariate Analysis .

Academic Press .

ISBN 978-0-12-471250-8 .

^ Shahrokh Esfahani, Mohammad; Dougherty, Edward (2014). "Incorporation of Biological Pathway Knowledge in the Construction of Priors for Optimal Bayesian Classification".

IEEE/ACM Transactions on Computational Biology and Bioinformatics .

11 (1): 202– 218.

doi : 10.1109/tcbb.2013.143 .

PMID 26355519 .

S2CID 10096507 .

^ Rosen, Dietrich von (1988). "Moments for the Inverted Wishart Distribution".

Scand. J. Stat .

15 : 97– 109.

^ Cook, R D; Forzani, Liliana (August 2019). Cook, Brian (ed.).

"On the mean and variance of the generalized inverse of a singular Wishart matrix" .

Electronic Journal of Statistics .

5 .

doi : 10.4324/9780429344633 .

ISBN 9780429344633 .

S2CID 146200569 .

^ Muirhead, Robb (1982).

Aspects of Multivariate Statistical Theory . USA: Wiley. p. 93.

ISBN 0-471-76985-1 .

^ Brennan, L E; Reed, I S (January 1982). "An Adaptive Array Signal Processing Algorithm for Communications".

IEEE Transactions on Aerospace and Electronic Systems .

18 (1): 120– 130.

Bibcode : 1982ITAES..18..124B .

doi : 10.1109/TAES.1982.309212 .

S2CID 45721922 .

^ Shaman, Paul (1980).

"The Inverted Complex Wishart Distribution and Its Application to Spectral Estimation" (PDF) .

Journal of Multivariate Analysis .

10 : 51– 59.

doi : 10.1016/0047-259X(80)90081-0 .

^ Triantafyllopoulos, K. (2011). "Real-time covariance estimation for the local level model".

Journal of Time Series Analysis .

32 (2): 93– 107.

arXiv : 1311.0634 .

doi : 10.1111/j.1467-9892.2010.00686.x .

S2CID 88512953 .

^ Bodnar, T.; Mazur, S.; Podgórski, K. (January 2015).

"Singular Inverse Wishart Distribution with Application to Portfolio Theory" .

Department of Statistics, Lund University . (Working Papers in Statistics, Nr. 2): 1– 17.

^ a b Bodnar, T; Mazur, S; Podgorski, K (2015).

"Singular Inverse Wishart Distribution with Application to Portfolio Theory" .

Journal of Multivariate Analysis .

143 : 314– 326.

doi : 10.1016/j.jmva.2015.09.021 .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Inverse-Wishart_distribution&oldid=1294099906 " Categories : Continuous distributions Multivariate continuous distributions Conjugate prior distributions Exponential family distributions Hidden categories: CS1: long volume value Articles with short description Short description matches Wikidata Wikipedia articles needing clarification from March 2022 This page was last edited on 5 June 2025, at 15:44 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Inverse-Wishart distribution 4 languages Add topic

