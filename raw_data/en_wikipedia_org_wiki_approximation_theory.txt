Title: Approximation theory

URL Source: https://en.wikipedia.org/wiki/Approximation_theory

Published Time: 2004-10-22T18:53:37Z

Markdown Content:
From Wikipedia, the free encyclopedia

In [mathematics](https://en.wikipedia.org/wiki/Mathematics "Mathematics"), **approximation theory** is concerned with how [functions](https://en.wikipedia.org/wiki/Function_(mathematics) "Function (mathematics)") can best be [approximated](https://en.wikipedia.org/wiki/Approximation "Approximation") with simpler functions, and with [quantitatively](https://en.wikipedia.org/wiki/Quantitative_property "Quantitative property")[characterizing](https://en.wikipedia.org/wiki/Characterization_(mathematics) "Characterization (mathematics)") the [errors](https://en.wikipedia.org/wiki/Approximation_error "Approximation error") introduced thereby. What is meant by _best_ and _simpler_ will depend on the application.

A closely related topic is the approximation of functions by [generalized Fourier series](https://en.wikipedia.org/wiki/Generalized_Fourier_series "Generalized Fourier series"), that is, approximations based upon summation of a series of terms based upon [orthogonal polynomials](https://en.wikipedia.org/wiki/Orthogonal_polynomials "Orthogonal polynomials").

One problem of particular interest is that of approximating a function in a [computer](https://en.wikipedia.org/wiki/Computer "Computer") mathematical library, using operations that can be performed on the computer or calculator (e.g. addition and multiplication), such that the result is as close to the actual function as possible. This is typically done with [polynomial](https://en.wikipedia.org/wiki/Polynomial "Polynomial") or [rational](https://en.wikipedia.org/wiki/Rational_function "Rational function") (ratio of polynomials) approximations.

The objective is to make the approximation as close as possible to the actual function, typically with an accuracy close to that of the underlying computer's [floating point](https://en.wikipedia.org/wiki/Floating_point "Floating point") arithmetic. This is accomplished by using a polynomial of high [degree](https://en.wikipedia.org/wiki/Degree_of_a_polynomial "Degree of a polynomial"), and/or narrowing the domain over which the polynomial has to approximate the function. Narrowing the domain can often be done through the use of various addition or scaling formulas for the function being approximated. Modern mathematical libraries often reduce the domain into many tiny segments and use a low-degree polynomial for each segment.

Optimal polynomials
-------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Approximation_theory&action=edit&section=1 "Edit section: Optimal polynomials")]

Once the domain (typically an interval) and degree of the polynomial are chosen, the polynomial itself is chosen in such a way as to minimize the worst-case error. That is, the goal is to minimize the maximum value of ![Image 1: {\displaystyle \mid P(x)-f(x)\mid }](https://wikimedia.org/api/rest_v1/media/math/render/svg/48fd8ea3d5ff20f865381599ab8b93a9e06ae2f1), where _P_(_x_) is the approximating polynomial, _f_(_x_) is the actual function, and _x_ varies over the chosen interval. For well-behaved functions, there exists an _N_ th-degree polynomial that will lead to an error curve that oscillates back and forth between ![Image 2: {\displaystyle +\varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac003984fcd4c23a5db42cf7dfa2e2b5b17f68b8) and ![Image 3: {\displaystyle -\varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6a07b913c3b445bab7efbcbf683b09d53b625674) a total of _N_+2 times, giving a worst-case error of ![Image 4: {\displaystyle \varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173). It is seen that there exists an _N_ th-degree polynomial that can interpolate _N_+1 points in a curve. That such a polynomial is always optimal is asserted by the [equioscillation theorem](https://en.wikipedia.org/wiki/Equioscillation_theorem "Equioscillation theorem"). It is possible to make contrived functions _f_(_x_) for which no such polynomial exists, but these occur rarely in practice.

For example, the graphs shown to the right show the error in approximating log(x) and exp(x) for _N_=4. The red curves, for the optimal polynomial, are **level**, that is, they oscillate between ![Image 5: {\displaystyle +\varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac003984fcd4c23a5db42cf7dfa2e2b5b17f68b8) and ![Image 6: {\displaystyle -\varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6a07b913c3b445bab7efbcbf683b09d53b625674) exactly. In each case, the number of extrema is _N_+2, that is, 6. Two of the extrema are at the end points of the interval, at the left and right edges of the graphs.

[![Image 7](https://upload.wikimedia.org/wikipedia/commons/b/bb/Impossibleerror.png)](https://en.wikipedia.org/wiki/File:Impossibleerror.png)

Error _P_(_x_)−_f_(_x_) for level polynomial (red), and for purported better polynomial (blue)

To prove this is true in general, suppose _P_ is a polynomial of degree _N_ having the property described, that is, it gives rise to an error function that has _N_+2 extrema, of alternating signs and equal magnitudes. The red graph to the right shows what this error function might look like for _N_=4. Suppose _Q_(_x_) (whose error function is shown in blue to the right) is another _N_-degree polynomial that is a better approximation to _f_ than _P_. In particular, _Q_ is closer to _f_ than _P_ for each value _x i_ where an extreme of _P_−_f_ occurs, so

![Image 8: {\displaystyle |Q(x_{i})-f(x_{i})|<|P(x_{i})-f(x_{i})|.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/41067741380918bc5045487b1cc95835cc5a197a)
When a maximum of _P_−_f_ occurs at _x i_, then

![Image 9: {\displaystyle Q(x_{i})-f(x_{i})\leq |Q(x_{i})-f(x_{i})|<|P(x_{i})-f(x_{i})|=P(x_{i})-f(x_{i}),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ebbc3d97e67f5afcfdcdfac14fa402eafdd858f8)
And when a minimum of _P_−_f_ occurs at _x i_, then

![Image 10: {\displaystyle f(x_{i})-Q(x_{i})\leq |Q(x_{i})-f(x_{i})|<|P(x_{i})-f(x_{i})|=f(x_{i})-P(x_{i}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e851fda47a4f99207bc5d4feffbf2ccc9051bd24)
So, as can be seen in the graph, [_P_(_x_)−_f_(_x_)]−[_Q_(_x_)−_f_(_x_)] must alternate in sign for the _N_+2 values of _x i_. But [_P_(_x_)−_f_(_x_)]−[_Q_(_x_)−_f_(_x_)] reduces to _P_(_x_)−_Q_(_x_) which is a polynomial of degree _N_. This function changes sign at least _N_+1 times so, by the [Intermediate value theorem](https://en.wikipedia.org/wiki/Intermediate_value_theorem "Intermediate value theorem"), it has _N_+1 zeroes, which is impossible for a polynomial of degree _N_.

Chebyshev approximation
-----------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Approximation_theory&action=edit&section=2 "Edit section: Chebyshev approximation")]

One can obtain polynomials very close to the optimal one by expanding the given function in terms of [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials "Chebyshev polynomials") and then cutting off the expansion at the desired degree. This is similar to the [Fourier analysis](https://en.wikipedia.org/wiki/Harmonic_analysis "Harmonic analysis") of the function, using the Chebyshev polynomials instead of the usual trigonometric functions.

If one calculates the coefficients in the Chebyshev expansion for a function:

![Image 11: {\displaystyle f(x)\sim \sum _{i=0}^{\infty }c_{i}T_{i}(x)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7add2efbe038b20f546ed4f6e9950bd1d357694)
and then cuts off the series after the ![Image 12: {\displaystyle T_{N}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e92de0cbe77750cce632b1c75de66e558ad297) term, one gets an _N_ th-degree polynomial approximating _f_(_x_).

The reason this polynomial is nearly optimal is that, for functions with rapidly converging power series, if the series is cut off after some term, the total error arising from the cutoff is close to the first term after the cutoff. That is, the first term after the cutoff dominates all later terms. The same is true if the expansion is in terms of bucking polynomials. If a Chebyshev expansion is cut off after ![Image 13: {\displaystyle T_{N}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e92de0cbe77750cce632b1c75de66e558ad297), the error will take a form close to a multiple of ![Image 14: {\displaystyle T_{N+1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0c5fa9ceff3aa3ec6c7acb24b70839982e208ac6). The Chebyshev polynomials have the property that they are level – they oscillate between +1 and −1 in the interval [−1, 1]. ![Image 15: {\displaystyle T_{N+1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0c5fa9ceff3aa3ec6c7acb24b70839982e208ac6) has _N_+2 level extrema. This means that the error between _f_(_x_) and its Chebyshev expansion out to ![Image 16: {\displaystyle T_{N}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e92de0cbe77750cce632b1c75de66e558ad297) is close to a level function with _N_+2 extrema, so it is close to the optimal _N_ th-degree polynomial.

In the graphs above, the blue error function is sometimes better than (inside of) the red function, but sometimes worse, meaning that it is not quite the optimal polynomial. The discrepancy is less serious for the exp function, which has an extremely rapidly converging power series, than for the log function.

Chebyshev approximation is the basis for [Clenshaw–Curtis quadrature](https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature "Clenshaw–Curtis quadrature"), a [numerical integration](https://en.wikipedia.org/wiki/Numerical_integration "Numerical integration") technique.

The [Remez algorithm](https://en.wikipedia.org/wiki/Remez_algorithm "Remez algorithm") (sometimes spelled Remes) is used to produce an optimal polynomial _P_(_x_) approximating a given function _f_(_x_) over a given interval. It is an iterative algorithm that converges to a polynomial that has an error function with _N_+2 level extrema. By the theorem above, that polynomial is optimal.

Remez's algorithm uses the fact that one can construct an _N_ th-degree polynomial that leads to level and alternating error values, given _N_+2 test points.

Given _N_+2 test points ![Image 17: {\displaystyle x_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308), ![Image 18: {\displaystyle x_{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7af1b928f06e4c7e3e8ebfd60704656719bd766), ... ![Image 19: {\displaystyle x_{N+2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0d33764f1db64d3a846747b8f8c7226acd83c46) (where ![Image 20: {\displaystyle x_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308) and ![Image 21: {\displaystyle x_{N+2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0d33764f1db64d3a846747b8f8c7226acd83c46) are presumably the end points of the interval of approximation), these equations need to be solved:

![Image 22: {\displaystyle {\begin{aligned}P(x_{1})-f(x_{1})&=+\varepsilon \\P(x_{2})-f(x_{2})&=-\varepsilon \\P(x_{3})-f(x_{3})&=+\varepsilon \\&\ \ \vdots \\P(x_{N+2})-f(x_{N+2})&=\pm \varepsilon .\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c4b093be04d09d95bd4f8889f3e0b127bd3d145)
The right-hand sides alternate in sign.

That is,

![Image 23: {\displaystyle {\begin{aligned}P_{0}+P_{1}x_{1}+P_{2}x_{1}^{2}+P_{3}x_{1}^{3}+\dots +P_{N}x_{1}^{N}-f(x_{1})&=+\varepsilon \\P_{0}+P_{1}x_{2}+P_{2}x_{2}^{2}+P_{3}x_{2}^{3}+\dots +P_{N}x_{2}^{N}-f(x_{2})&=-\varepsilon \\&\ \ \vdots \end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fb1777a2434d0b882e9d692ef1259ddc945de379)
Since ![Image 24: {\displaystyle x_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308), ..., ![Image 25: {\displaystyle x_{N+2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0d33764f1db64d3a846747b8f8c7226acd83c46) were given, all of their powers are known, and ![Image 26: {\displaystyle f(x_{1})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/af71fd012905bda20f1c736716748676d67c93f3), ..., ![Image 27: {\displaystyle f(x_{N+2})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1329f4fa3df5a7617a13af2ec0be568aa602bec0) are also known. That means that the above equations are just _N_+2 linear equations in the _N_+2 variables ![Image 28: {\displaystyle P_{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/671bd891701e0d6cfa6da0114a5dd64233b58709), ![Image 29: {\displaystyle P_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/398f438d75434e6fbf48dc232c1ad7228a738568), ..., ![Image 30: {\displaystyle P_{N}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/76e19264aa7768253e3d4f07901c01f1a1a2b073), and ![Image 31: {\displaystyle \varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173). Given the test points ![Image 32: {\displaystyle x_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308), ..., ![Image 33: {\displaystyle x_{N+2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0d33764f1db64d3a846747b8f8c7226acd83c46), one can solve this system to get the polynomial _P_ and the number ![Image 34: {\displaystyle \varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173).

The graph below shows an example of this, producing a fourth-degree polynomial approximating ![Image 35: {\displaystyle e^{x}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/841c0d168e64191c45a45e54c7e447defd17ec6a) over [−1, 1]. The test points were set at −1, −0.7, −0.1, +0.4, +0.9, and 1. Those values are shown in green. The resultant value of ![Image 36: {\displaystyle \varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173) is 4.43 × 10−4

[![Image 37](https://upload.wikimedia.org/wikipedia/commons/5/57/Remesdemo.png)](https://en.wikipedia.org/wiki/File:Remesdemo.png)

Error of the polynomial produced by the first step of Remez's algorithm, approximating e x over the interval [−1, 1]. Vertical divisions are 10−4.

The error graph does indeed take on the values ![Image 38: {\displaystyle \pm \varepsilon }](https://wikimedia.org/api/rest_v1/media/math/render/svg/13cde2f5a5d5a1d223841b9e2d01fc176b75ffa3) at the six test points, including the end points, but that those points are not extrema. If the four interior test points had been extrema (that is, the function _P_(_x_)_f_(_x_) had maxima or minima there), the polynomial would be optimal.

The second step of Remez's algorithm consists of moving the test points to the approximate locations where the error function had its actual local maxima or minima. For example, one can tell from looking at the graph that the point at −0.1 should have been at about −0.28. The way to do this in the algorithm is to use a single round of [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method "Newton's method"). Since one knows the first and second derivatives of _P_(_x_) − _f_(_x_), one can calculate approximately how far a test point has to be moved so that the derivative will be zero.

Calculating the derivatives of a polynomial is straightforward. One must also be able to calculate the first and second derivatives of _f_(_x_). Remez's algorithm requires an ability to calculate ![Image 39: {\displaystyle f(x)\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/78b2b66021c2cac2b5654495678c63ff142952e5), ![Image 40: {\displaystyle f'(x)\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/89c5a552db46b5ad878e9c550c9deca30166fc9b), and ![Image 41: {\displaystyle f''(x)\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f83cab3ad61be79ceb552857801762059418864b) to extremely high precision. The entire algorithm must be carried out to higher precision than the desired precision of the result.

After moving the test points, the linear equation part is repeated, getting a new polynomial, and Newton's method is used again to move the test points again. This sequence is continued until the result converges to the desired accuracy. The algorithm converges very rapidly. Convergence is quadratic for well-behaved functions—if the test points are within ![Image 42: {\displaystyle 10^{-15}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6242c988bd86e530d477b2e94166ad159e86fdbf) of the correct result, they will be approximately within ![Image 43: {\displaystyle 10^{-30}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/98d106edd647d04d3af491f75bc5d0b50cd9ba90) of the correct result after the next round.

Remez's algorithm is typically started by choosing the extrema of the Chebyshev polynomial ![Image 44: {\displaystyle T_{N+1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0c5fa9ceff3aa3ec6c7acb24b70839982e208ac6) as the initial points, since the final error function will be similar to that polynomial.

*   _[Journal of Approximation Theory](https://en.wikipedia.org/wiki/Journal\_of\_Approximation\_Theory "Journal of Approximation Theory")_
*   _[Constructive Approximation](https://en.wikipedia.org/wiki/Constructive\_Approximation "Constructive Approximation")_
*   _[East Journal on Approximations](https://en.wikipedia.org/wiki/East\_Journal\_on\_Approximations "East Journal on Approximations")_

*   [Estimation theory](https://en.wikipedia.org/wiki/Estimation_theory "Estimation theory")
*   [Fourier series](https://en.wikipedia.org/wiki/Fourier_series "Fourier series")
*   [Function approximation](https://en.wikipedia.org/wiki/Function_approximation "Function approximation")
*   [Numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis "Numerical analysis")
*   [Orthonormal basis](https://en.wikipedia.org/wiki/Orthonormal_basis "Orthonormal basis")
*   [Padé approximant](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant "Padé approximant")
*   [Schauder basis](https://en.wikipedia.org/wiki/Schauder_basis "Schauder basis")
*   [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter "Kalman filter")

*   [Achiezer (Akhiezer), N.I.](https://en.wikipedia.org/wiki/Naum_Akhiezer "Naum Akhiezer") (2013) [1956]. [_Theory of approximation_](https://books.google.com/books?id=J9wiuIJhM6cC&pg=PR7). Translated by Hyman, C.J. Dover. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-15313-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-15313-1 "Special:BookSources/978-0-486-15313-1"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[1067500225](https://search.worldcat.org/oclc/1067500225).
*   Timan, A.F. (2014) [1963]. [_Theory of approximation of functions of a real variable_](https://books.google.com/books?id=D4TiBQAAQBAJ&pg=PR5). International Series in Pure and Applied Mathematics. Vol.34. Elsevier. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4831-8481-4](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4831-8481-4 "Special:BookSources/978-1-4831-8481-4").
*   Hastings, Jr., C. (2015) [1955]. [_Approximations for Digital Computers_](https://books.google.com/books?id=IRTWCgAAQBAJ&pg=PR7). Princeton University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4008-7559-7](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4008-7559-7 "Special:BookSources/978-1-4008-7559-7").
*   Hart, J.F.; [Cheney, E.W.](https://en.wikipedia.org/wiki/Elliott_Ward_Cheney_Jr. "Elliott Ward Cheney Jr."); Lawson, C.L.; Maehly, H.J.; Mesztenyi, C.K.; [Rice, Jr., J.R.](https://en.wikipedia.org/wiki/John_R._Rice_(professor) "John R. Rice (professor)"); Thacher, H.C.; Witzgall, C. (1968). _Computer Approximations_. Wiley. [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[0471356301](https://search.worldcat.org/oclc/0471356301).
*   Fox, L.; Parker, I.B. (1968). _Chebyshev Polynomials in Numerical Analysis_. Oxford mathematical handbooks. Oxford University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-19-859614-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-859614-1 "Special:BookSources/978-0-19-859614-1"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[9036207](https://search.worldcat.org/oclc/9036207).
*   [Press, WH](https://en.wikipedia.org/wiki/William_H._Press "William H. Press"); [Teukolsky, S.A.](https://en.wikipedia.org/wiki/Saul_Teukolsky "Saul Teukolsky"); Vetterling, W.T.; [Flannery, B.P.](https://en.wikipedia.org/wiki/Brian_P._Flannery "Brian P. Flannery") (2007). "§5.8 Chebyshev Approximation". _Numerical Recipes: The Art of Scientific Computing_ (3rd ed.). Cambridge University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-521-88068-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-88068-8 "Special:BookSources/978-0-521-88068-8").
*   Cody, Jr., W.J.; Waite, W. (1980). _Software Manual for the Elementary Functions_. Prentice-Hall. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-13-822064-6](https://en.wikipedia.org/wiki/Special:BookSources/0-13-822064-6 "Special:BookSources/0-13-822064-6"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[654695035](https://search.worldcat.org/oclc/654695035).
*   [Remes (Remez), E.](https://en.wikipedia.org/wiki/Evgeny_Yakovlevich_Remez "Evgeny Yakovlevich Remez") (1934). ["Sur le calcul effectif des polynomes d'approximation de Tschebyschef"](https://gallica.bnf.fr/ark:/12148/bpt6k3151h/f337.item). _C. R. Acad. Sci._ (in French). **199**: 337–340.
*   Steffens, K.-G. (2006). Anastassiou, George A. (ed.). _The History of Approximation Theory: From Euler to Bernstein_. Birkhauser. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/0-8176-4475-X](https://doi.org/10.1007%2F0-8176-4475-X). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-8176-4353-2](https://en.wikipedia.org/wiki/Special:BookSources/0-8176-4353-2 "Special:BookSources/0-8176-4353-2").
*   [Erdélyi, T.](https://en.wikipedia.org/wiki/Tamas_Erdelyi_(mathematician) "Tamas Erdelyi (mathematician)") (2008). ["Extensions of the Bloch-Pólya theorem on the number of distinct real zeros of polynomials"](https://doi.org/10.5802%2Fjtnb.627). _Journal de théorie des nombres de Bordeaux_. **20**: 281–7. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.5802/jtnb.627](https://doi.org/10.5802%2Fjtnb.627).
*   [Erdélyi, T.](https://en.wikipedia.org/wiki/Tamas_Erdelyi_(mathematician) "Tamas Erdelyi (mathematician)") (2009). "The Remez inequality for linear combinations of shifted Gaussians". _[Mathematical Proceedings of the Cambridge Philosophical Society](https://en.wikipedia.org/wiki/Mathematical\_Proceedings\_of\_the\_Cambridge\_Philosophical\_Society "Mathematical Proceedings of the Cambridge Philosophical Society")_. **146** (3): 523–530. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1017/S0305004108001849](https://doi.org/10.1017%2FS0305004108001849).
*   [Trefethen, L.N.](https://en.wikipedia.org/wiki/Nick_Trefethen "Nick Trefethen") (2020). [_Approximation theory and approximation practice_](https://books.google.com/books?id=xsnBDwAAQBAJ&pg=PR7). SIAM. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-61197-594-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-61197-594-9 "Special:BookSources/978-1-61197-594-9").[Ch. 1–6 of 2013 edition](http://people.maths.ox.ac.uk/trefethen/ATAP/ATAPfirst6chapters.pdf)

*   [History of Approximation Theory (HAT)](https://history-of-approximation-theory.com/)
*   [Surveys in Approximation Theory (SAT)](http://www.emis.de/journals/SAT/)
