Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Theory 2 Expression in terms of characteristic functions 3 Moments 4 Generation of random variates 5 Entropy 6 See also 7 References 8 External links Toggle the table of contents Wrapped distribution 2 languages Català Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution on a hypersphere In probability theory and directional statistics , a wrapped probability distribution is a continuous probability distribution that describes data points that lie on a unit n -sphere . In one dimension, a wrapped distribution consists of points on the unit circle . If ϕ ϕ {\displaystyle \phi } is a random variate in the interval ( − − ∞ ∞ , ∞ ∞ ) {\displaystyle (-\infty ,\infty )} with probability density function (PDF) p ( ϕ ϕ ) {\displaystyle p(\phi )} , then z = e i ϕ ϕ {\displaystyle z=e^{i\phi }} is a circular variable distributed according to the wrapped distribution p w z ( θ θ ) {\displaystyle p_{wz}(\theta )} and θ θ = arg ⁡ ⁡ ( z ) {\displaystyle \theta =\arg(z)} is an angular variable in the interval ( − − π π , π π ] {\displaystyle (-\pi ,\pi ]} distributed according to the wrapped distribution p w ( θ θ ) {\displaystyle p_{w}(\theta )} .

Any probability density function p ( ϕ ϕ ) {\displaystyle p(\phi )} on the line can be "wrapped" around the circumference of a circle of unit radius.

[ 1 ] That is, the PDF of the wrapped variable θ θ = ϕ ϕ mod 2 π π {\displaystyle \theta =\phi \mod 2\pi } in some interval of length 2 π π {\displaystyle 2\pi } is p w ( θ θ ) = ∑ ∑ k = − − ∞ ∞ ∞ ∞ p ( θ θ + 2 π π k ) {\displaystyle p_{w}(\theta )=\sum _{k=-\infty }^{\infty }{p(\theta +2\pi k)}} which is a periodic sum of period 2 π π {\displaystyle 2\pi } . The preferred interval is generally ( − − π π < θ θ ≤ ≤ π π ) {\displaystyle (-\pi <\theta \leq \pi )} for which ln ⁡ ⁡ ( e i θ θ ) = arg ⁡ ⁡ ( e i θ θ ) = θ θ {\displaystyle \ln(e^{i\theta })=\arg(e^{i\theta })=\theta } .

Theory [ edit ] In most situations, a process involving circular statistics produces angles ( ϕ ϕ {\displaystyle \phi } ) which lie in the interval ( − − ∞ ∞ , ∞ ∞ ) {\displaystyle (-\infty ,\infty )} , and are described by an "unwrapped" probability density function p ( ϕ ϕ ) {\displaystyle p(\phi )} . However, a measurement will yield an angle θ θ {\displaystyle \theta } which lies in some interval of length 2 π π {\displaystyle 2\pi } (for example, 0 to 2 π π {\displaystyle 2\pi } ). In other words, a measurement cannot tell whether the true angle ϕ ϕ {\displaystyle \phi } or a wrapped angle θ θ = ϕ ϕ + 2 π π a {\displaystyle \theta =\phi +2\pi a} , where a {\displaystyle a} is some unknown integer, has been measured.

If we wish to calculate the expected value of some function of the measured angle it will be: ⟨ ⟨ f ( θ θ ) ⟩ ⟩ = ∫ ∫ − − ∞ ∞ ∞ ∞ p ( ϕ ϕ ) f ( ϕ ϕ + 2 π π a ) d ϕ ϕ {\displaystyle \langle f(\theta )\rangle =\int _{-\infty }^{\infty }p(\phi )f(\phi +2\pi a)d\phi } .

We can express the integral as a sum of integrals over periods of 2 π π {\displaystyle 2\pi } : ⟨ ⟨ f ( θ θ ) ⟩ ⟩ = ∑ ∑ k = − − ∞ ∞ ∞ ∞ ∫ ∫ 2 π π k 2 π π ( k + 1 ) p ( ϕ ϕ ) f ( ϕ ϕ + 2 π π a ) d ϕ ϕ {\displaystyle \langle f(\theta )\rangle =\sum _{k=-\infty }^{\infty }\int _{2\pi k}^{2\pi (k+1)}p(\phi )f(\phi +2\pi a)d\phi } .

Changing the variable of integration to θ θ ′ = ϕ ϕ − − 2 π π k {\displaystyle \theta '=\phi -2\pi k} and exchanging the order of integration and summation, we have ⟨ ⟨ f ( θ θ ) ⟩ ⟩ = ∫ ∫ 0 2 π π p w ( θ θ ′ ) f ( θ θ ′ + 2 π π a ′ ) d θ θ ′ {\displaystyle \langle f(\theta )\rangle =\int _{0}^{2\pi }p_{w}(\theta ')f(\theta '+2\pi a')d\theta '} where p w ( θ θ ′ ) {\displaystyle p_{w}(\theta ')} is the PDF of the wrapped distribution and a ′ {\displaystyle a'} is another unknown integer ( a ′ = a + k ) {\displaystyle (a'=a+k)} . The unknown integer a ′ {\displaystyle a'} introduces an ambiguity into the expected value of f ( θ θ ) {\displaystyle f(\theta )} , similar to the problem of calculating angular mean . This can be resolved by introducing the parameter z = e i θ θ {\displaystyle z=e^{i\theta }} , since z {\displaystyle z} has an unambiguous relationship to the true angle ϕ ϕ {\displaystyle \phi } : z = e i θ θ = e i ϕ ϕ {\displaystyle z=e^{i\theta }=e^{i\phi }} .

Calculating the expected value of a function of z {\displaystyle z} will yield unambiguous answers: ⟨ ⟨ f ( z ) ⟩ ⟩ = ∫ ∫ 0 2 π π p w ( θ θ ′ ) f ( e i θ θ ′ ) d θ θ ′ {\displaystyle \langle f(z)\rangle =\int _{0}^{2\pi }p_{w}(\theta ')f(e^{i\theta '})d\theta '} .

For this reason, the z {\displaystyle z} parameter is preferred over measured angles θ θ {\displaystyle \theta } in circular statistical analysis. This suggests that the wrapped distribution function may itself be expressed as a function of z {\displaystyle z} such that: ⟨ ⟨ f ( z ) ⟩ ⟩ = ∮ ∮ p w z ( z ) f ( z ) d z {\displaystyle \langle f(z)\rangle =\oint p_{wz}(z)f(z)\,dz} where p w ( z ) {\displaystyle p_{w}(z)} is defined such that p w ( θ θ ) | d θ θ | = p w z ( z ) | d z | {\displaystyle p_{w}(\theta )\,|d\theta |=p_{wz}(z)\,|dz|} . This concept can be extended to the multivariate context by an extension of the simple sum to a number of F {\displaystyle F} sums that cover all dimensions in the feature space: p w ( θ θ → → ) = ∑ ∑ k 1 , .

.

.

, k F = − − ∞ ∞ ∞ ∞ p ( θ θ → → + 2 π π k 1 e 1 + ⋯ ⋯ + 2 π π k F e F ) {\displaystyle p_{w}({\vec {\theta }})=\sum _{k_{1},...,k_{F}=-\infty }^{\infty }{p({\vec {\theta }}+2\pi k_{1}\mathbf {e} _{1}+\dots +2\pi k_{F}\mathbf {e} _{F})}} where e k = ( 0 , … … , 0 , 1 , 0 , … … , 0 ) T {\displaystyle \mathbf {e} _{k}=(0,\dots ,0,1,0,\dots ,0)^{\mathsf {T}}} is the k {\displaystyle k} th Euclidean basis vector.

Expression in terms of characteristic functions [ edit ] A fundamental wrapped distribution is the Dirac comb , which is a wrapped Dirac delta function : Δ Δ 2 π π ( θ θ ) = ∑ ∑ k = − − ∞ ∞ ∞ ∞ δ δ ( θ θ + 2 π π k ) {\displaystyle \Delta _{2\pi }(\theta )=\sum _{k=-\infty }^{\infty }{\delta (\theta +2\pi k)}} .

Using the delta function, a general wrapped distribution can be written p w ( θ θ ) = ∑ ∑ k = − − ∞ ∞ ∞ ∞ ∫ ∫ − − ∞ ∞ ∞ ∞ p ( θ θ ′ ) δ δ ( θ θ − − θ θ ′ + 2 π π k ) d θ θ ′ {\displaystyle p_{w}(\theta )=\sum _{k=-\infty }^{\infty }\int _{-\infty }^{\infty }p(\theta ')\delta (\theta -\theta '+2\pi k)\,d\theta '} .

Exchanging the order of summation and integration, any wrapped distribution can be written as the convolution of the unwrapped distribution and a Dirac comb: p w ( θ θ ) = ∫ ∫ − − ∞ ∞ ∞ ∞ p ( θ θ ′ ) Δ Δ 2 π π ( θ θ − − θ θ ′ ) d θ θ ′ {\displaystyle p_{w}(\theta )=\int _{-\infty }^{\infty }p(\theta ')\Delta _{2\pi }(\theta -\theta ')\,d\theta '} .

The Dirac comb may also be expressed as a sum of exponentials, so we may write: p w ( θ θ ) = 1 2 π π ∫ ∫ − − ∞ ∞ ∞ ∞ p ( θ θ ′ ) ∑ ∑ n = − − ∞ ∞ ∞ ∞ e i n ( θ θ − − θ θ ′ ) d θ θ ′ {\displaystyle p_{w}(\theta )={\frac {1}{2\pi }}\,\int _{-\infty }^{\infty }p(\theta ')\sum _{n=-\infty }^{\infty }e^{in(\theta -\theta ')}\,d\theta '} .

Again exchanging the order of summation and integration: p w ( θ θ ) = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ ∫ ∫ − − ∞ ∞ ∞ ∞ p ( θ θ ′ ) e i n ( θ θ − − θ θ ′ ) d θ θ ′ {\displaystyle p_{w}(\theta )={\frac {1}{2\pi }}\,\sum _{n=-\infty }^{\infty }\int _{-\infty }^{\infty }p(\theta ')e^{in(\theta -\theta ')}\,d\theta '} .

Using the definition of ϕ ϕ ( s ) {\displaystyle \phi (s)} , the characteristic function of p ( θ θ ) {\displaystyle p(\theta )} yields a Laurent series about zero for the wrapped distribution in terms of the characteristic function of the unwrapped distribution: p w ( θ θ ) = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ ϕ ϕ ( n ) e − − i n θ θ {\displaystyle p_{w}(\theta )={\frac {1}{2\pi }}\,\sum _{n=-\infty }^{\infty }\phi (n)\,e^{-in\theta }} or p w z ( z ) = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ ϕ ϕ ( n ) z − − n {\displaystyle p_{wz}(z)={\frac {1}{2\pi }}\,\sum _{n=-\infty }^{\infty }\phi (n)\,z^{-n}} Analogous to linear distributions, ϕ ϕ ( m ) {\displaystyle \phi (m)} is referred to as the characteristic function of the wrapped distribution (or more accurately, the characteristic sequence ).

[ 2 ] This is an instance of the Poisson summation formula , and it can be seen that the coefficients of the Fourier series for the wrapped distribution are simply the coefficients of the Fourier transform of the unwrapped distribution at integer values.

Moments [ edit ] The moments of the wrapped distribution p w ( z ) {\displaystyle p_{w}(z)} are defined as: ⟨ ⟨ z m ⟩ ⟩ = ∮ ∮ p w z ( z ) z m d z {\displaystyle \langle z^{m}\rangle =\oint p_{wz}(z)z^{m}\,dz} .

Expressing p w ( z ) {\displaystyle p_{w}(z)} in terms of the characteristic function and exchanging the order of integration and summation yields: ⟨ ⟨ z m ⟩ ⟩ = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ ϕ ϕ ( n ) ∮ ∮ z m − − n d z {\displaystyle \langle z^{m}\rangle ={\frac {1}{2\pi }}\sum _{n=-\infty }^{\infty }\phi (n)\oint z^{m-n}\,dz} .

From the residue theorem we have ∮ ∮ z m − − n d z = 2 π π δ δ m − − n {\displaystyle \oint z^{m-n}\,dz=2\pi \delta _{m-n}} where δ δ k {\displaystyle \delta _{k}} is the Kronecker delta function. It follows that the moments are simply equal to the characteristic function of the unwrapped distribution for integer arguments: ⟨ ⟨ z m ⟩ ⟩ = ϕ ϕ ( m ) {\displaystyle \langle z^{m}\rangle =\phi (m)} .

Generation of random variates [ edit ] If X {\displaystyle X} is a random variate drawn from a linear probability distribution P {\displaystyle P} , then Z = e i X {\displaystyle Z=e^{iX}} is a circular variate distributed according to the wrapped P {\displaystyle P} distribution, and θ θ = arg ⁡ ⁡ ( Z ) {\displaystyle \theta =\arg(Z)} is the angular variate distributed according to the wrapped P {\displaystyle P} distribution, with − − π π < θ θ ≤ ≤ π π {\displaystyle -\pi <\theta \leq \pi } .

Entropy [ edit ] The information entropy of a circular distribution with probability density p w ( θ θ ) {\displaystyle p_{w}(\theta )} is defined as: H = − − ∫ ∫ Γ Γ p w ( θ θ ) ln ⁡ ⁡ ( p w ( θ θ ) ) d θ θ {\displaystyle H=-\int _{\Gamma }p_{w}(\theta )\,\ln(p_{w}(\theta ))\,d\theta } where Γ Γ {\displaystyle \Gamma } is any interval of length 2 π π {\displaystyle 2\pi } .

[ 1 ] If both the probability density and its logarithm can be expressed as a Fourier series (or more generally, any integral transform on the circle), the orthogonal basis of the series can be used to obtain a closed form expression for the entropy.

The moments of the distribution ϕ ϕ ( n ) {\displaystyle \phi (n)} are the Fourier coefficients for the Fourier series expansion of the probability density: p w ( θ θ ) = 1 2 π π ∑ ∑ n = − − ∞ ∞ ∞ ∞ ϕ ϕ n e − − i n θ θ {\displaystyle p_{w}(\theta )={\frac {1}{2\pi }}\sum _{n=-\infty }^{\infty }\phi _{n}e^{-in\theta }} .

If the logarithm of the probability density can also be expressed as a Fourier series: ln ⁡ ⁡ ( p w ( θ θ ) ) = ∑ ∑ m = − − ∞ ∞ ∞ ∞ c m e i m θ θ {\displaystyle \ln(p_{w}(\theta ))=\sum _{m=-\infty }^{\infty }c_{m}e^{im\theta }} where c m = 1 2 π π ∫ ∫ Γ Γ ln ⁡ ⁡ ( p w ( θ θ ) ) e − − i m θ θ d θ θ {\displaystyle c_{m}={\frac {1}{2\pi }}\int _{\Gamma }\ln(p_{w}(\theta ))e^{-im\theta }\,d\theta } .

Then, exchanging the order of integration and summation, the entropy may be written as: H = − − 1 2 π π ∑ ∑ m = − − ∞ ∞ ∞ ∞ ∑ ∑ n = − − ∞ ∞ ∞ ∞ c m ϕ ϕ n ∫ ∫ Γ Γ e i ( m − − n ) θ θ d θ θ {\displaystyle H=-{\frac {1}{2\pi }}\sum _{m=-\infty }^{\infty }\sum _{n=-\infty }^{\infty }c_{m}\phi _{n}\int _{\Gamma }e^{i(m-n)\theta }\,d\theta } .

Using the orthogonality of the Fourier basis, the integral may be reduced to: H = − − ∑ ∑ n = − − ∞ ∞ ∞ ∞ c n ϕ ϕ n {\displaystyle H=-\sum _{n=-\infty }^{\infty }c_{n}\phi _{n}} .

For the particular case when the probability density is symmetric about the mean, c − − m = c m {\displaystyle c_{-m}=c_{m}} and the logarithm may be written: ln ⁡ ⁡ ( p w ( θ θ ) ) = c 0 + 2 ∑ ∑ m = 1 ∞ ∞ c m cos ⁡ ⁡ ( m θ θ ) {\displaystyle \ln(p_{w}(\theta ))=c_{0}+2\sum _{m=1}^{\infty }c_{m}\cos(m\theta )} and c m = 1 2 π π ∫ ∫ Γ Γ ln ⁡ ⁡ ( p w ( θ θ ) ) cos ⁡ ⁡ ( m θ θ ) d θ θ {\displaystyle c_{m}={\frac {1}{2\pi }}\int _{\Gamma }\ln(p_{w}(\theta ))\cos(m\theta )\,d\theta } and, since normalization requires that ϕ ϕ 0 = 1 {\displaystyle \phi _{0}=1} , the entropy may be written: H = − − c 0 − − 2 ∑ ∑ n = 1 ∞ ∞ c n ϕ ϕ n {\displaystyle H=-c_{0}-2\sum _{n=1}^{\infty }c_{n}\phi _{n}} .

See also [ edit ] Wrapped normal distribution Wrapped Cauchy distribution Wrapped exponential distribution References [ edit ] This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( July 2011 ) ( Learn how and when to remove this message ) ^ a b Mardia, Kantilal ; Jupp, Peter E. (1999).

Directional Statistics . Wiley.

ISBN 978-0-471-95333-3 .

^ Mardia, K. (1972).

Statistics of Directional Data . New York: Academic press.

ISBN 978-1-4832-1866-3 .

Borradaile, Graham (2003).

Statistics of Earth Science Data . Springer.

ISBN 978-3-540-43603-4 .

Fisher, N. I. (1996).

Statistical Analysis of Circular Data . Cambridge University Press.

ISBN 978-0-521-56890-6 .

External links [ edit ] Circular Values Math and Statistics with C++11 , A C++11 infrastructure for circular values (angles, time-of-day, etc.) mathematics and statistics v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Wrapped_distribution&oldid=1294930380 " Categories : Types of probability distributions Directional statistics Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from July 2011 All articles lacking in-text citations Use dmy dates from August 2019 This page was last edited on 10 June 2025, at 17:23 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Wrapped distribution 2 languages Add topic

