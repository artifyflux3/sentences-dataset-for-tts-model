Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Examples Toggle Examples subsection 2.1 Sample variance 2.2 Estimating a Poisson probability 2.3 Maximum of a discrete uniform distribution 3 Median-unbiased estimators 4 Bias with respect to other loss functions 5 Effect of transformations 6 Bias, variance and mean squared error Toggle Bias, variance and mean squared error subsection 6.1 Example: Estimation of population variance 7 Bayesian view 8 See also 9 Notes 10 References 11 External links Toggle the table of contents Bias of an estimator 14 languages العربية বাংলা Català Dansk Deutsch Español فارسی Français 한국어 עברית Nederlands Português Shqip 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical property For broader coverage of this topic, see Bias (statistics) .

In statistics , the bias of an estimator (or bias function ) is the difference between this estimator 's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased . In statistics, "bias" is an objective property of an estimator. Bias is a distinct concept from consistency : consistent estimators converge in probability to the true value of the parameter, but may be biased or unbiased (see bias versus consistency for more).

All else being equal, an unbiased estimator is preferable to a biased estimator, although in practice, biased estimators (with generally small bias) are frequently used. When a biased estimator is used, bounds of the bias are calculated. A biased estimator may be used for various reasons: because an unbiased estimator does not exist without further assumptions about a population; because an estimator is difficult to compute (as in unbiased estimation of standard deviation ); because a biased estimator may be unbiased with respect to different measures of central tendency ; because a biased estimator gives a lower value of some loss function (particularly mean squared error ) compared with unbiased estimators (notably in shrinkage estimators ); or because in some cases being unbiased is too strong a condition, and the only unbiased estimators are not useful.

Bias can also be measured with respect to the median , rather than the mean (expected value), in which case one distinguishes median -unbiased from the usual mean -unbiasedness property. 
Mean-unbiasedness is not preserved under non-linear transformations , though median-unbiasedness is (see § Effect of transformations ); for example, the sample variance is a biased estimator for the population variance. These are all illustrated below.

An unbiased estimator for a parameter need not always exist. For example, there is no unbiased estimator for the reciprocal of the parameter of a binomial random variable.

[ 1 ] Definition [ edit ] Suppose we have a statistical model , parameterized by a real number θ , giving rise to a probability distribution for observed data, P θ θ ( x ) = P ( x ∣ ∣ θ θ ) {\displaystyle P_{\theta }(x)=P(x\mid \theta )} , and a statistic θ θ ^ ^ {\displaystyle {\hat {\theta }}} which serves as an estimator of θ based on any observed data x {\displaystyle x} . That is, we assume that our data follows some unknown distribution P ( x ∣ ∣ θ θ ) {\displaystyle P(x\mid \theta )} (where θ is a fixed, unknown constant that is part of this distribution), and then we construct some estimator θ θ ^ ^ {\displaystyle {\hat {\theta }}} that maps observed data to values that we hope are close to θ . The bias of θ θ ^ ^ {\displaystyle {\hat {\theta }}} relative to θ θ {\displaystyle \theta } is defined as [ 2 ] Bias ⁡ ⁡ ( θ θ ^ ^ , θ θ ) = Bias θ θ ⁡ ⁡ [ θ θ ^ ^ ] = E x ∣ ∣ θ θ ⁡ ⁡ [ θ θ ^ ^ ] − − θ θ = E x ∣ ∣ θ θ ⁡ ⁡ [ θ θ ^ ^ − − θ θ ] , {\displaystyle \operatorname {Bias} ({\hat {\theta }},\theta )=\operatorname {Bias} _{\theta }[\,{\hat {\theta }}\,]=\operatorname {E} _{x\mid \theta }[\,{\hat {\theta }}\,]-\theta =\operatorname {E} _{x\mid \theta }[\,{\hat {\theta }}-\theta \,],} where E x ∣ ∣ θ θ {\displaystyle \operatorname {E} _{x\mid \theta }} denotes expected value over the distribution P ( x ∣ ∣ θ θ ) {\displaystyle P(x\mid \theta )} (i.e., averaging over all possible observations x {\displaystyle x} ). The second equation follows since θ is measurable with respect to the conditional distribution P ( x ∣ ∣ θ θ ) {\displaystyle P(x\mid \theta )} .

An estimator is said to be unbiased if its bias is zero for all values of the parameter θ , or equivalently, if the expected value of the estimator matches that of the parameter.

[ 3 ] Unbiasedness is not guaranteed to carry over. For example, if θ θ ^ ^ {\displaystyle {\hat {\theta }}} is an unbiased estimator for parameter θ , it is not guaranteed in general that g( θ θ ^ ^ {\displaystyle {\hat {\theta }}} ) is an unbiased estimator for g(θ) , unless g is a linear function.

[ 4 ] In a simulation experiment concerning the properties of an estimator, the bias of the estimator may be assessed using the mean signed difference .

Examples [ edit ] Sample variance [ edit ] Main article: Sample variance The sample variance of a random variable demonstrates two aspects of estimator bias: firstly, the naive estimator is biased, which can be corrected by a scale factor; second, the unbiased estimator is not optimal in terms of mean squared error (MSE), which can be minimized by using a different scale factor, resulting in a biased estimator with lower MSE than the unbiased estimator. Concretely, the naive estimator sums the squared deviations and divides by n, which is biased. Dividing instead by n − 1 yields an unbiased estimator. Conversely, MSE can be minimized by dividing by a different number (depending on distribution), but this results in a biased estimator. This number is always larger than n − 1, so this is known as a shrinkage estimator , as it "shrinks" the unbiased estimator towards zero; for the normal distribution the optimal value is n + 1.

Suppose X 1 , ..., X n are independent and identically distributed (i.i.d.) random variables with expectation μ and variance σ 2 . If the sample mean and uncorrected sample variance are defined as X ¯ ¯ = 1 n ∑ ∑ i = 1 n X i S 2 = 1 n ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle {\overline {X}}\,={\frac {1}{n}}\sum _{i=1}^{n}X_{i}\qquad S^{2}={\frac {1}{n}}\sum _{i=1}^{n}{\big (}X_{i}-{\overline {X}}\,{\big )}^{2}\qquad } then S 2 is a biased estimator of σ 2 , because E ⁡ ⁡ [ S 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( ( X i − − μ μ ) − − ( X ¯ ¯ − − μ μ ) ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( ( X i − − μ μ ) 2 − − 2 ( X ¯ ¯ − − μ μ ) ( X i − − μ μ ) + ( X ¯ ¯ − − μ μ ) 2 ) ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 n ( X ¯ ¯ − − μ μ ) ∑ ∑ i = 1 n ( X i − − μ μ ) + 1 n ( X ¯ ¯ − − μ μ ) 2 ∑ ∑ i = 1 n 1 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 n ( X ¯ ¯ − − μ μ ) ∑ ∑ i = 1 n ( X i − − μ μ ) + 1 n ( X ¯ ¯ − − μ μ ) 2 ⋅ ⋅ n ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 n ( X ¯ ¯ − − μ μ ) ∑ ∑ i = 1 n ( X i − − μ μ ) + ( X ¯ ¯ − − μ μ ) 2 ] {\displaystyle {\begin{aligned}\operatorname {E} [S^{2}]&=\operatorname {E} \left[{\frac {1}{n}}\sum _{i=1}^{n}{\big (}X_{i}-{\overline {X}}{\big )}^{2}\right]=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}{\bigg (}(X_{i}-\mu )-({\overline {X}}-\mu ){\bigg )}^{2}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}{\bigg (}(X_{i}-\mu )^{2}-2({\overline {X}}-\mu )(X_{i}-\mu )+({\overline {X}}-\mu )^{2}{\bigg )}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-{\frac {2}{n}}({\overline {X}}-\mu )\sum _{i=1}^{n}(X_{i}-\mu )+{\frac {1}{n}}({\overline {X}}-\mu )^{2}\sum _{i=1}^{n}1{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-{\frac {2}{n}}({\overline {X}}-\mu )\sum _{i=1}^{n}(X_{i}-\mu )+{\frac {1}{n}}({\overline {X}}-\mu )^{2}\cdot n{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-{\frac {2}{n}}({\overline {X}}-\mu )\sum _{i=1}^{n}(X_{i}-\mu )+({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]\end{aligned}}} To continue, we note that by subtracting μ μ {\displaystyle \mu } from both sides of X ¯ ¯ = 1 n ∑ ∑ i = 1 n X i {\displaystyle {\overline {X}}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}} , we get X ¯ ¯ − − μ μ = 1 n ∑ ∑ i = 1 n X i − − μ μ = 1 n ∑ ∑ i = 1 n X i − − 1 n ∑ ∑ i = 1 n μ μ = 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) .

{\displaystyle {\begin{aligned}{\overline {X}}-\mu ={\frac {1}{n}}\sum _{i=1}^{n}X_{i}-\mu ={\frac {1}{n}}\sum _{i=1}^{n}X_{i}-{\frac {1}{n}}\sum _{i=1}^{n}\mu \ ={\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu ).\\[8pt]\end{aligned}}} Meaning, (by cross-multiplication) n ⋅ ⋅ ( X ¯ ¯ − − μ μ ) = ∑ ∑ i = 1 n ( X i − − μ μ ) {\displaystyle n\cdot ({\overline {X}}-\mu )=\sum _{i=1}^{n}(X_{i}-\mu )} . Then, the previous becomes: E ⁡ ⁡ [ S 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 n ( X ¯ ¯ − − μ μ ) ∑ ∑ i = 1 n ( X i − − μ μ ) + ( X ¯ ¯ − − μ μ ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 n ( X ¯ ¯ − − μ μ ) ⋅ ⋅ n ⋅ ⋅ ( X ¯ ¯ − − μ μ ) + ( X ¯ ¯ − − μ μ ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − 2 ( X ¯ ¯ − − μ μ ) 2 + ( X ¯ ¯ − − μ μ ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 − − ( X ¯ ¯ − − μ μ ) 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 ] − − E ⁡ ⁡ [ ( X ¯ ¯ − − μ μ ) 2 ] = σ σ 2 − − E ⁡ ⁡ [ ( X ¯ ¯ − − μ μ ) 2 ] = ( 1 − − 1 n ) σ σ 2 < σ σ 2 .

{\displaystyle {\begin{aligned}\operatorname {E} [S^{2}]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-{\frac {2}{n}}({\overline {X}}-\mu )\sum _{i=1}^{n}(X_{i}-\mu )+({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-{\frac {2}{n}}({\overline {X}}-\mu )\cdot n\cdot ({\overline {X}}-\mu )+({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-2({\overline {X}}-\mu )^{2}+({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}-({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}{\bigg ]}-\operatorname {E} {\bigg [}({\overline {X}}-\mu )^{2}{\bigg ]}\\[8pt]&=\sigma ^{2}-\operatorname {E} {\bigg [}({\overline {X}}-\mu )^{2}{\bigg ]}=\left(1-{\frac {1}{n}}\right)\sigma ^{2}<\sigma ^{2}.\end{aligned}}} This can be seen by noting the following formula, which follows from the Bienaymé formula , for the term in the inequality for the expectation of the uncorrected sample variance above: E ⁡ ⁡ [ ( X ¯ ¯ − − μ μ ) 2 ] = 1 n σ σ 2 {\displaystyle \operatorname {E} {\big [}({\overline {X}}-\mu )^{2}{\big ]}={\frac {1}{n}}\sigma ^{2}} .

In other words, the expected value of the uncorrected sample variance does not equal the population variance σ 2 , unless multiplied by a normalization factor. The sample mean, on the other hand, is an unbiased [ 5 ] estimator of the population mean μ .

[ 3 ] Note that the usual definition of sample variance is S 2 = 1 n − − 1 ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle S^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(X_{i}-{\overline {X}}\,)^{2}} , and this is an unbiased estimator of the population variance.

Algebraically speaking, E ⁡ ⁡ [ S 2 ] {\displaystyle \operatorname {E} [S^{2}]} is unbiased because: E ⁡ ⁡ [ S 2 ] = E ⁡ ⁡ [ 1 n − − 1 ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 ] = n n − − 1 E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 ] = n n − − 1 ( 1 − − 1 n ) σ σ 2 = σ σ 2 , {\displaystyle {\begin{aligned}\operatorname {E} [S^{2}]&=\operatorname {E} \left[{\frac {1}{n-1}}\sum _{i=1}^{n}{\big (}X_{i}-{\overline {X}}{\big )}^{2}\right]={\frac {n}{n-1}}\operatorname {E} \left[{\frac {1}{n}}\sum _{i=1}^{n}{\big (}X_{i}-{\overline {X}}{\big )}^{2}\right]\\[8pt]&={\frac {n}{n-1}}\left(1-{\frac {1}{n}}\right)\sigma ^{2}=\sigma ^{2},\\[8pt]\end{aligned}}} where the transition to the second line uses the result derived above for the biased estimator. Thus E ⁡ ⁡ [ S 2 ] = σ σ 2 {\displaystyle \operatorname {E} [S^{2}]=\sigma ^{2}} , and therefore S 2 = 1 n − − 1 ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle S^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(X_{i}-{\overline {X}}\,)^{2}} is an unbiased estimator of the population variance, σ 2 . The ratio between the biased (uncorrected) and unbiased estimates of the variance is known as Bessel's correction .

The reason that an uncorrected sample variance, S 2 , is biased stems from the fact that the sample mean is an ordinary least squares (OLS) estimator for μ : X ¯ ¯ {\displaystyle {\overline {X}}} is the number that makes the sum ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle \sum _{i=1}^{n}(X_{i}-{\overline {X}})^{2}} as small as possible. That is, when any other number is plugged into this sum, the sum can only increase. In particular, the choice μ μ ≠ ≠ X ¯ ¯ {\displaystyle \mu \neq {\overline {X}}} gives, 1 n ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 < 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 , {\displaystyle {\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-{\overline {X}})^{2}<{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2},} and then E ⁡ ⁡ [ S 2 ] = E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 ] < E ⁡ ⁡ [ 1 n ∑ ∑ i = 1 n ( X i − − μ μ ) 2 ] = σ σ 2 .

{\displaystyle {\begin{aligned}\operatorname {E} [S^{2}]&=\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-{\overline {X}})^{2}{\bigg ]}<\operatorname {E} {\bigg [}{\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-\mu )^{2}{\bigg ]}=\sigma ^{2}.\end{aligned}}} The above discussion can be understood in geometric terms: the vector C → → = ( X 1 − − μ μ , … … , X n − − μ μ ) {\displaystyle {\vec {C}}=(X_{1}-\mu ,\ldots ,X_{n}-\mu )} can be decomposed into the "mean part" and "variance part" by projecting to the direction of u → → = ( 1 , … … , 1 ) {\displaystyle {\vec {u}}=(1,\ldots ,1)} and to that direction's orthogonal complement hyperplane. One gets A → → = ( X ¯ ¯ − − μ μ , … … , X ¯ ¯ − − μ μ ) {\displaystyle {\vec {A}}=({\overline {X}}-\mu ,\ldots ,{\overline {X}}-\mu )} for the part along u → → {\displaystyle {\vec {u}}} and B → → = ( X 1 − − X ¯ ¯ , … … , X n − − X ¯ ¯ ) {\displaystyle {\vec {B}}=(X_{1}-{\overline {X}},\ldots ,X_{n}-{\overline {X}})} for the complementary part. Since this is an orthogonal decomposition, Pythagorean theorem says | C → → | 2 = | A → → | 2 + | B → → | 2 {\displaystyle |{\vec {C}}|^{2}=|{\vec {A}}|^{2}+|{\vec {B}}|^{2}} , and taking expectations we get n σ σ 2 = n E ⁡ ⁡ [ ( X ¯ ¯ − − μ μ ) 2 ] + n E ⁡ ⁡ [ S 2 ] {\displaystyle n\sigma ^{2}=n\operatorname {E} \left[({\overline {X}}-\mu )^{2}\right]+n\operatorname {E} [S^{2}]} , as above (but times n {\displaystyle n} ).
If the distribution of C → → {\displaystyle {\vec {C}}} is rotationally symmetric, as in the case when X i {\displaystyle X_{i}} are sampled from a Gaussian, then on average, the dimension along u → → {\displaystyle {\vec {u}}} contributes to | C → → | 2 {\displaystyle |{\vec {C}}|^{2}} equally as the n − − 1 {\displaystyle n-1} directions perpendicular to u → → {\displaystyle {\vec {u}}} , so that E ⁡ ⁡ [ ( X ¯ ¯ − − μ μ ) 2 ] = σ σ 2 n {\displaystyle \operatorname {E} \left[({\overline {X}}-\mu )^{2}\right]={\frac {\sigma ^{2}}{n}}} and E ⁡ ⁡ [ S 2 ] = ( n − − 1 ) σ σ 2 n {\displaystyle \operatorname {E} [S^{2}]={\frac {(n-1)\sigma ^{2}}{n}}} .  This is in fact true in general, as explained above.

Estimating a Poisson probability [ edit ] A far more extreme case of a biased estimator being better than any unbiased estimator arises from the Poisson distribution .

[ 6 ] [ 7 ] Suppose that X has a Poisson distribution with expectation λ . Suppose it is desired to estimate P ⁡ ⁡ ( X = 0 ) 2 = e − − 2 λ λ {\displaystyle \operatorname {P} (X=0)^{2}=e^{-2\lambda }\quad } with a sample of size 1. (For example, when incoming calls at a telephone switchboard are modeled as a Poisson process, and λ is the average number of calls per minute, then e −2 λ (the estimand) is the probability that no calls arrive in the next two minutes.) Since the expectation of an unbiased estimator δ ( X ) is equal to the estimand , i.e.

E ⁡ ⁡ ( δ δ ( X ) ) = ∑ ∑ x = 0 ∞ ∞ δ δ ( x ) λ λ x e − − λ λ x !

= e − − 2 λ λ , {\displaystyle \operatorname {E} (\delta (X))=\sum _{x=0}^{\infty }\delta (x){\frac {\lambda ^{x}e^{-\lambda }}{x!}}=e^{-2\lambda },} the only function of the data constituting an unbiased estimator is δ δ ( x ) = ( − − 1 ) x .

{\displaystyle \delta (x)=(-1)^{x}.\,} To see this, note that when decomposing e − λ from the above expression for expectation, the sum that is left is a Taylor series expansion of e − λ as well, yielding e − λ e − λ = e −2 λ (see Characterizations of the exponential function ).

If the observed value of X is 100, then the estimate is 1, although the true value of the quantity being estimated is very likely to be near 0, which is the opposite extreme. And, if X is observed to be 101, then the estimate is even more absurd: It is −1, although the quantity being estimated must be positive.

The (biased) maximum likelihood estimator e − − 2 X {\displaystyle e^{-2{X}}\quad } is far better than this unbiased estimator. Not only is its value always positive but it is also more accurate in the sense that its mean squared error e − − 4 λ λ − − 2 e λ λ ( 1 / e 2 − − 3 ) + e λ λ ( 1 / e 4 − − 1 ) {\displaystyle e^{-4\lambda }-2e^{\lambda (1/e^{2}-3)}+e^{\lambda (1/e^{4}-1)}\,} is smaller; compare the unbiased estimator's MSE of 1 − − e − − 4 λ λ .

{\displaystyle 1-e^{-4\lambda }.\,} The MSEs are functions of the true value λ . The bias of the maximum-likelihood estimator is: e λ λ ( 1 / e 2 − − 1 ) − − e − − 2 λ λ .

{\displaystyle e^{\lambda (1/e^{2}-1)}-e^{-2\lambda }.\,} Maximum of a discrete uniform distribution [ edit ] Main article: Maximum of a discrete uniform distribution The bias of maximum-likelihood estimators can be substantial. Consider a case where n tickets numbered from 1 to n are placed in a box and one is selected at random, giving a value X . If n is unknown, then the maximum-likelihood estimator of n is X , even though the expectation of X given n is only ( n + 1)/2; we can be certain only that n is at least X and is probably more. In this case, the natural unbiased estimator is 2 X − 1.

Median-unbiased estimators [ edit ] The theory of median -unbiased estimators was revived by George W. Brown in 1947: [ 8 ] An estimate of a one-dimensional parameter θ will be said to be median-unbiased, if, for fixed θ, the median of the distribution of the estimate is at the value θ; i.e., the estimate underestimates just as often as it overestimates. This requirement seems for most purposes to accomplish as much as the mean-unbiased requirement and has the additional property that it is invariant under one-to-one transformation.

Further properties of median-unbiased estimators have been noted by Lehmann, Birnbaum, van der Vaart and Pfanzagl.

[ 9 ] In particular, median-unbiased estimators exist in cases where mean-unbiased and maximum-likelihood estimators do not exist. They are invariant under one-to-one transformations .

There are methods of construction median-unbiased estimators for probability distributions that have monotone likelihood-functions , such as one-parameter exponential families, to ensure that they are optimal (in a sense analogous to minimum-variance property considered for mean-unbiased estimators).

[ 10 ] [ 11 ] One such procedure is an analogue of the Rao–Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao–Blackwell procedure for mean-unbiased estimation but for a larger class of loss-functions.

[ 11 ] Bias with respect to other loss functions [ edit ] Any minimum-variance mean -unbiased estimator minimizes the risk ( expected loss ) with respect to the squared-error loss function (among mean-unbiased estimators), as observed by Gauss .

[ 12 ] A minimum- average absolute deviation median -unbiased estimator minimizes the risk with respect to the absolute loss function (among median-unbiased estimators), as observed by Laplace .

[ 12 ] [ 13 ] Other loss functions are used in statistics, particularly in robust statistics .

[ 12 ] [ 14 ] Effect of transformations [ edit ] For univariate parameters, median-unbiased estimators remain median-unbiased under transformations that preserve order (or reverse order).
Note that, when a transformation is applied to a mean-unbiased estimator, the result need not be a mean-unbiased estimator of its corresponding population statistic. By Jensen's inequality , a convex function as transformation will introduce positive bias, while a concave function will introduce negative bias, and a function of mixed convexity may introduce bias in either direction, depending on the specific function and distribution. That is, for a non-linear function f and a mean-unbiased estimator U of a parameter p , the composite estimator f ( U ) need not be a mean-unbiased estimator of f ( p ). For example, the square root of the unbiased estimator of the population variance is not a mean-unbiased estimator of the population standard deviation : the square root of the unbiased sample variance , the corrected sample standard deviation , is biased. The bias depends both on the sampling distribution of the estimator and on the transform, and can be quite involved to calculate – see unbiased estimation of standard deviation for a discussion in this case.

Bias, variance and mean squared error [ edit ] Main article: Bias–variance tradeoff See also: Accuracy (trueness and precision) Sampling distributions of two alternative estimators for a parameter β 0 . Although β 1 ^ is unbiased, it is clearly inferior to the biased β 2 ^ .

Ridge regression is one example of a technique where allowing a little bias may lead to a considerable reduction in variance, and more reliable estimates overall.

While bias quantifies the average difference to be expected between an estimator and an underlying parameter, an estimator based on a finite sample can additionally be expected to differ from the parameter due to the randomness in the sample.
An estimator that minimises the bias will not necessarily minimise the mean square error.
One measure which is used to try to reflect both types of difference is the mean square error , [ 2 ] MSE ⁡ ⁡ ( θ θ ^ ^ ) = E ⁡ ⁡ [ ( θ θ ^ ^ − − θ θ ) 2 ] .

{\displaystyle \operatorname {MSE} ({\hat {\theta }})=\operatorname {E} {\big [}({\hat {\theta }}-\theta )^{2}{\big ]}.} This can be shown to be equal to the square of the bias, plus the variance: [ 2 ] MSE ⁡ ⁡ ( θ θ ^ ^ ) = ( E ⁡ ⁡ [ θ θ ^ ^ ] − − θ θ ) 2 + E ⁡ ⁡ [ ( θ θ ^ ^ − − E ⁡ ⁡ [ θ θ ^ ^ ] ) 2 ] = ( Bias ⁡ ⁡ ( θ θ ^ ^ , θ θ ) ) 2 + Var ⁡ ⁡ ( θ θ ^ ^ ) {\displaystyle {\begin{aligned}\operatorname {MSE} ({\hat {\theta }})=&(\operatorname {E} [{\hat {\theta }}]-\theta )^{2}+\operatorname {E} [\,({\hat {\theta }}-\operatorname {E} [\,{\hat {\theta }}\,])^{2}\,]\\=&(\operatorname {Bias} ({\hat {\theta }},\theta ))^{2}+\operatorname {Var} ({\hat {\theta }})\end{aligned}}} When the parameter is a vector, an analogous decomposition applies: [ 15 ] MSE ⁡ ⁡ ( θ θ ^ ^ ) = trace ⁡ ⁡ ( Cov ⁡ ⁡ ( θ θ ^ ^ ) ) + ‖ Bias ⁡ ⁡ ( θ θ ^ ^ , θ θ ) ‖ 2 {\displaystyle \operatorname {MSE} ({\hat {\theta }})=\operatorname {trace} (\operatorname {Cov} ({\hat {\theta }}))+\left\Vert \operatorname {Bias} ({\hat {\theta }},\theta )\right\Vert ^{2}} where trace ⁡ ⁡ ( Cov ⁡ ⁡ ( θ θ ^ ^ ) ) {\displaystyle \operatorname {trace} (\operatorname {Cov} ({\hat {\theta }}))} is the trace (diagonal sum) of the covariance matrix of the estimator and ‖ Bias ⁡ ⁡ ( θ θ ^ ^ , θ θ ) ‖ 2 {\displaystyle \left\Vert \operatorname {Bias} ({\hat {\theta }},\theta )\right\Vert ^{2}} is the square vector norm .

Example: Estimation of population variance [ edit ] For example, [ 16 ] suppose an estimator of the form T 2 = c ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 = c n S 2 {\displaystyle T^{2}=c\sum _{i=1}^{n}\left(X_{i}-{\overline {X}}\,\right)^{2}=cnS^{2}} is sought for the population variance as above, but this time to minimise the MSE: MSE = E ⁡ ⁡ [ ( T 2 − − σ σ 2 ) 2 ] = ( E ⁡ ⁡ [ T 2 − − σ σ 2 ] ) 2 + Var ⁡ ⁡ ( T 2 ) {\displaystyle {\begin{aligned}\operatorname {MSE} =&\operatorname {E} \left[(T^{2}-\sigma ^{2})^{2}\right]\\=&\left(\operatorname {E} \left[T^{2}-\sigma ^{2}\right]\right)^{2}+\operatorname {Var} (T^{2})\end{aligned}}} If the variables X 1 ...

X n follow a normal distribution, then nS 2 /σ 2 has a chi-squared distribution with n − 1 degrees of freedom, giving: E ⁡ ⁡ [ n S 2 ] = ( n − − 1 ) σ σ 2 and Var ⁡ ⁡ ( n S 2 ) = 2 ( n − − 1 ) σ σ 4 .

{\displaystyle \operatorname {E} [nS^{2}]=(n-1)\sigma ^{2}{\text{ and }}\operatorname {Var} (nS^{2})=2(n-1)\sigma ^{4}.} and so MSE = ( c ( n − − 1 ) − − 1 ) 2 σ σ 4 + 2 c 2 ( n − − 1 ) σ σ 4 {\displaystyle \operatorname {MSE} =(c(n-1)-1)^{2}\sigma ^{4}+2c^{2}(n-1)\sigma ^{4}} With a little algebra it can be confirmed that it is c = 1/( n + 1) which minimises this combined loss function, rather than c = 1/( n − 1) which minimises just the square of the bias.

More generally it is only in restricted classes of problems that there will be an estimator that minimises the MSE independently of the parameter values.

However it is very common that there may be perceived to be a bias–variance tradeoff , such that a small increase in bias can be traded for a larger decrease in variance, resulting in a more desirable estimator overall.

Bayesian view [ edit ] Most bayesians are rather unconcerned about unbiasedness (at least in the formal sampling-theory sense above) of their estimates.  For example, Gelman and coauthors (1995) write: "From a Bayesian perspective, the principle of unbiasedness is reasonable in the limit of large samples, but otherwise it is potentially misleading." [ 17 ] Fundamentally, the difference between the Bayesian approach and the sampling-theory approach above is that in the sampling-theory approach the parameter is taken as fixed, and then probability distributions of a statistic are considered, based on the predicted sampling distribution of the data.  For a Bayesian, however, it is the data which are known, and fixed, and it is the unknown parameter for which an attempt is made to construct a probability distribution, using Bayes' theorem : p ( θ θ ∣ ∣ D , I ) ∝ ∝ p ( θ θ ∣ ∣ I ) p ( D ∣ ∣ θ θ , I ) {\displaystyle p(\theta \mid D,I)\propto p(\theta \mid I)p(D\mid \theta ,I)} Here the second term, the likelihood of the data given the unknown parameter value θ, depends just on the data obtained and the modelling of the data generation process.  However a Bayesian calculation also includes the first term, the prior probability for θ, which takes account of everything the analyst may know or suspect about θ before the data comes in.  This information plays no part in the sampling-theory approach; indeed any attempt to include it would be considered "bias" away from what was pointed to purely by the data.  To the extent that Bayesian calculations include prior information, it is therefore essentially inevitable that their results will not be "unbiased" in sampling theory terms.

But the results of a Bayesian approach can differ from the sampling theory approach even if the Bayesian tries to adopt an "uninformative" prior.

For example, consider again the estimation of an unknown population variance σ 2 of a Normal distribution with unknown mean, where it is desired to optimise c in the expected loss function ExpectedLoss = E ⁡ ⁡ [ ( c n S 2 − − σ σ 2 ) 2 ] = E ⁡ ⁡ [ σ σ 4 ( c n S 2 σ σ 2 − − 1 ) 2 ] {\displaystyle \operatorname {ExpectedLoss} =\operatorname {E} \left[\left(cnS^{2}-\sigma ^{2}\right)^{2}\right]=\operatorname {E} \left[\sigma ^{4}\left(cn{\tfrac {S^{2}}{\sigma ^{2}}}-1\right)^{2}\right]} A standard choice of uninformative prior for this problem is the Jeffreys prior , p ( σ σ 2 ) ∝ ∝ 1 / σ σ 2 {\displaystyle \scriptstyle {p(\sigma ^{2})\;\propto \;1/\sigma ^{2}}} , which is equivalent to adopting a rescaling-invariant flat prior for ln(σ 2 ) .

One consequence of adopting this prior is that S 2 /σ 2 remains a pivotal quantity , i.e. the probability distribution of S 2 /σ 2 depends only on S 2 /σ 2 , independent of the value of S 2 or σ 2 : p ( S 2 σ σ 2 ∣ ∣ S 2 ) = p ( S 2 σ σ 2 ∣ ∣ σ σ 2 ) = g ( S 2 σ σ 2 ) {\displaystyle p\left({\tfrac {S^{2}}{\sigma ^{2}}}\mid S^{2}\right)=p\left({\tfrac {S^{2}}{\sigma ^{2}}}\mid \sigma ^{2}\right)=g\left({\tfrac {S^{2}}{\sigma ^{2}}}\right)} However, while E p ( S 2 ∣ ∣ σ σ 2 ) ⁡ ⁡ [ σ σ 4 ( c n S 2 σ σ 2 − − 1 ) 2 ] = σ σ 4 E p ( S 2 ∣ ∣ σ σ 2 ) ⁡ ⁡ [ ( c n S 2 σ σ 2 − − 1 ) 2 ] {\displaystyle \operatorname {E} _{p(S^{2}\mid \sigma ^{2})}\left[\sigma ^{4}\left(cn{\tfrac {S^{2}}{\sigma ^{2}}}-1\right)^{2}\right]=\sigma ^{4}\operatorname {E} _{p(S^{2}\mid \sigma ^{2})}\left[\left(cn{\tfrac {S^{2}}{\sigma ^{2}}}-1\right)^{2}\right]} in contrast E p ( σ σ 2 ∣ ∣ S 2 ) ⁡ ⁡ [ σ σ 4 ( c n S 2 σ σ 2 − − 1 ) 2 ] ≠ ≠ σ σ 4 E p ( σ σ 2 ∣ ∣ S 2 ) ⁡ ⁡ [ ( c n S 2 σ σ 2 − − 1 ) 2 ] {\displaystyle \operatorname {E} _{p(\sigma ^{2}\mid S^{2})}\left[\sigma ^{4}\left(cn{\tfrac {S^{2}}{\sigma ^{2}}}-1\right)^{2}\right]\neq \sigma ^{4}\operatorname {E} _{p(\sigma ^{2}\mid S^{2})}\left[\left(cn{\tfrac {S^{2}}{\sigma ^{2}}}-1\right)^{2}\right]} — when the expectation is taken over the probability distribution of σ 2 given S 2 , as it is in the Bayesian case, rather than S 2 given σ 2 , one can no longer take σ 4 as a constant and factor it out.  The consequence of this is that, compared to the sampling-theory calculation, the Bayesian calculation puts more weight on larger values of σ 2 , properly taking into account (as the sampling-theory calculation cannot) that under this squared-loss function the consequence of underestimating large values of σ 2 is more costly in squared-loss terms than that of overestimating small values of σ 2 .

The worked-out Bayesian calculation gives a scaled inverse chi-squared distribution with n − 1 degrees of freedom for the posterior probability distribution of σ 2 .  The expected loss is minimised when cnS 2 = <σ 2 >; this occurs when c = 1/( n − 3).

Even with an uninformative prior, therefore, a Bayesian calculation may not give the same expected-loss minimising result as the corresponding sampling-theory calculation.

See also [ edit ] Science portal Mathematics portal Consistent estimator Efficient estimator Estimation theory Expected loss Expected value Loss function Minimum-variance unbiased estimator Omitted-variable bias Optimism bias Ratio estimator Statistical decision theory Notes [ edit ] ^ "For the binomial distribution, why does no unbiased estimator exist for $1/p$?" .

Mathematics Stack Exchange . Retrieved 2023-12-27 .

^ a b c Kozdron, Michael (March 2016).

"Evaluating the Goodness of an Estimator: Bias, Mean-Square Error, Relative Efficiency (Chapter 3)" (PDF) .

stat.math.uregina.ca . Retrieved 2020-09-11 .

^ a b Taylor, Courtney (January 13, 2019).

"Unbiased and Biased Estimators" .

ThoughtCo . Retrieved 2020-09-12 .

^ Dekking, Michel, ed. (2005).

A modern introduction to probability and statistics: understanding why and how . Springer texts in statistics. London [Heidelberg]: Springer.

ISBN 978-1-85233-896-1 .

^ Richard Arnold Johnson; Dean W. Wichern (2007).

Applied Multivariate Statistical Analysis . Pearson Prentice Hall.

ISBN 978-0-13-187715-3 . Retrieved 10 August 2012 .

^ Romano, J. P.; Siegel, A. F. (1986).

Counterexamples in Probability and Statistics . Monterey, California, USA: Wadsworth & Brooks / Cole. p. 168.

^ Hardy, M. (1 March 2003). "An Illuminating Counterexample".

American Mathematical Monthly .

110 (3): 234– 238.

arXiv : math/0206006 .

doi : 10.2307/3647938 .

ISSN 0002-9890 .

JSTOR 3647938 .

^ Brown (1947), page 583 ^ Lehmann 1951 ; Birnbaum 1961 ; Van der Vaart 1961 ; Pfanzagl 1994 ^ Pfanzagl, Johann (1979).

"On optimal median unbiased estimators in the presence of nuisance parameters" .

The Annals of Statistics .

7 (1): 187– 193.

doi : 10.1214/aos/1176344563 .

^ a b Brown, L. D.; Cohen, Arthur; Strawderman, W. E. (1976).

"A Complete Class Theorem for Strict Monotone Likelihood Ratio With Applications" .

Ann. Statist .

4 (4): 712– 722.

doi : 10.1214/aos/1176343543 .

^ a b c Dodge, Yadolah, ed. (1987).

Statistical Data Analysis Based on the L 1 -Norm and Related Methods . Papers from the First International Conference held at Neuchâtel, August 31–September 4, 1987. Amsterdam: North-Holland.

ISBN 0-444-70273-3 .

^ Jaynes, E. T. (2007).

Probability Theory : The Logic of Science . Cambridge: Cambridge Univ. Press. p. 172.

ISBN 978-0-521-59271-0 .

^ Klebanov, Lev B.; Rachev, Svetlozar T.; Fabozzi, Frank J. (2009). "Loss Functions and the Theory of Unbiased Estimation".

Robust and Non-Robust Models in Statistics . New York: Nova Scientific.

ISBN 978-1-60741-768-2 .

^ Taboga, Marco (2010).

"Lectures on probability theory and mathematical statistics" .

^ DeGroot, Morris H. (1986).

Probability and Statistics (2nd ed.). Addison-Wesley. pp.

414 –5.

ISBN 0-201-11366-X .

But compare it with, for example, the discussion in Casella; Berger (2001).

Statistical Inference (2nd ed.). Duxbury. p. 332.

ISBN 0-534-24312-6 .

^ Gelman, A.; et al. (1995).

Bayesian Data Analysis . Chapman and Hall. p. 108.

ISBN 0-412-03991-5 .

References [ edit ] Brown, George W.

"On Small-Sample Estimation." The Annals of Mathematical Statistics , vol. 18, no. 4 (Dec., 1947), pp. 582–585.

JSTOR 2236236 .

Lehmann, E. L.

(December 1951). "A General Concept of Unbiasedness".

The Annals of Mathematical Statistics .

22 (4): 587– 592.

doi : 10.1214/aoms/1177729549 .

JSTOR 2236928 .

Birnbaum, Allan (March 1961). "A Unified Theory of Estimation, I".

The Annals of Mathematical Statistics .

32 (1): 112– 135.

doi : 10.1214/aoms/1177705145 .

Van der Vaart, H. R. (June 1961).

"Some Extensions of the Idea of Bias" .

The Annals of Mathematical Statistics .

32 (2): 436– 447.

doi : 10.1214/aoms/1177705051 .

Pfanzagl, Johann (1994).

Parametric Statistical Theory . Walter de Gruyter.

Stuart, Alan; Ord, Keith; Arnold, Steven [F.] (2010).

Classical Inference and the Linear Model . Kendall's Advanced Theory of Statistics. Vol. 2A. Wiley.

ISBN 978-0-4706-8924-0 .

.

Voinov, Vassily [G.]; Nikulin, Mikhail [S.] (1993).

Unbiased estimators and their applications . Vol. 1: Univariate case. Dordrect: Kluwer Academic Publishers.

ISBN 0-7923-2382-3 .

Voinov, Vassily [G.]; Nikulin, Mikhail [S.] (1996).

Unbiased estimators and their applications . Vol. 2: Multivariate case. Dordrect: Kluwer Academic Publishers.

ISBN 0-7923-3939-8 .

Klebanov, Lev [B.]; Rachev, Svetlozar [T.]; Fabozzi, Frank [J.] (2009).

Robust and Non-Robust Models in Statistics . New York: Nova Scientific Publishers.

ISBN 978-1-60741-768-2 .

External links [ edit ] "Unbiased estimator" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Biases Cognitive biases Acquiescence Ambiguity Affinity Anchoring Attentional Attribution Actor–observer Fundamental Group Ultimate Authority Automation Double standard Availability Mean world Belief Blind spot Choice-supportive Commitment Confirmation Selective perception Compassion fade Congruence Cultural Declinism Distinction Dunning–Kruger Egocentric Curse of knowledge Emotional Extrinsic incentives Fading affect Framing Frequency Frog pond effect Halo effect Hindsight Horn effect Hostile attribution Impact Implicit In-group Intentionality Illusion of transparency Mean world syndrome Mere-exposure effect Narrative Negativity Normalcy Omission Optimism Out-group homogeneity Outcome Overton window Precision Present Pro-innovation Proximity Response Restraint Self-serving Social comparison Social influence bias Spotlight Status quo Substitution Time-saving Trait ascription Turkey illusion von Restorff effect Zero-risk In animals Statistical biases Estimator Forecast Healthy user Information Psychological Lead time Length time Non-response Observer Omitted-variable Participation Recall Sampling Selection Self-selection Social desirability Spectrum Survivorship Systematic error Systemic Verification Wet Other biases Academic Basking in reflected glory Déformation professionnelle Funding FUTON Inductive Infrastructure Inherent In education Liking gap Media False balance Vietnam War South Asia United States Arab–Israeli conflict Ukraine Net Political bias Publication System justification Reporting White hat Ideological bias on Wikipedia Bias reduction Cognitive bias mitigation Debiasing Heuristics in judgment and decision-making Lists: General Memory NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐h559b
Cached time: 20250812005053
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.619 seconds
Real time usage: 1.151 seconds
Preprocessor visited node count: 3192/1000000
Revision size: 34430/2097152 bytes
Post‐expand include size: 209761/2097152 bytes
Template argument size: 2210/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 103530/5000000 bytes
Lua time usage: 0.331/10.000 seconds
Lua memory usage: 8063618/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  525.125      1 -total
 38.19%  200.557      1 Template:Reflist
 22.75%  119.492      1 Template:Statistics
 22.36%  117.410      1 Template:Navbox_with_collapsible_groups
 16.42%   86.242      4 Template:Cite_web
 14.30%   75.096     14 Template:Cite_book
 13.34%   70.038      1 Template:Short_description
  9.55%   50.127     12 Template:Navbox
  8.18%   42.941      2 Template:Pagetype
  6.08%   31.941      6 Template:Cite_journal Saved in parser cache with key enwiki:pcache:8450479:|#|:idhash:canonical and timestamp 20250812005053 and revision id 1285753618. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Bias_of_an_estimator&oldid=1285753618 " Categories : Accuracy and precision Point estimation performance Bias Hidden categories: Articles with short description Short description is different from Wikidata CS1: long volume value This page was last edited on 15 April 2025, at 15:44 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Bias of an estimator 14 languages Add topic

