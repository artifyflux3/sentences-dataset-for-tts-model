Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Difference from other similar concepts 2 Notation 3 Geometrical properties 4 Examples Toggle Examples subsection 4.1 f-divergences 4.2 Bregman divergences 5 History 6 See also 7 Notes 8 References Toggle References subsection 8.1 Bibliography Toggle the table of contents Divergence (statistics) 1 language Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Function that measures dissimilarity between two probability distributions Not to be confused with Deviance (statistics) , Deviation (statistics) , or Discrepancy (statistics) .

In information geometry , a divergence is a kind of statistical distance : a binary function which establishes the separation from one probability distribution to another on a statistical manifold .

The simplest divergence is squared Euclidean distance (SED), and divergences can be viewed as generalizations of SED. The other most important divergence is relative entropy (also called Kullback–Leibler divergence ), which is central to information theory . There are numerous other specific divergences and classes of divergences, notably f -divergences and Bregman divergences (see § Examples ).

Definition [ edit ] Given a differentiable manifold [ a ] M {\displaystyle M} of dimension n {\displaystyle n} , a divergence on M {\displaystyle M} is a C 2 {\displaystyle C^{2}} -function D : M × × M → → [ 0 , ∞ ∞ ) {\displaystyle D:M\times M\to [0,\infty )} satisfying: [ 1 ] [ 2 ] D ( p , q ) ≥ ≥ 0 {\displaystyle D(p,q)\geq 0} for all p , q ∈ ∈ M {\displaystyle p,q\in M} (non-negativity), D ( p , q ) = 0 {\displaystyle D(p,q)=0} if and only if p = q {\displaystyle p=q} (positivity), At every point p ∈ ∈ M {\displaystyle p\in M} , D ( p , p + d p ) {\displaystyle D(p,p+dp)} is a positive-definite quadratic form for infinitesimal displacements d p {\displaystyle dp} from p {\displaystyle p} .

In applications to statistics, the manifold M {\displaystyle M} is typically the space of parameters of a parametric family of probability distributions .

Condition 3 means that D {\displaystyle D} defines an inner product on the tangent space T p M {\displaystyle T_{p}M} for every p ∈ ∈ M {\displaystyle p\in M} . Since D {\displaystyle D} is C 2 {\displaystyle C^{2}} on M {\displaystyle M} , this defines a Riemannian metric g {\displaystyle g} on M {\displaystyle M} .

Locally at p ∈ ∈ M {\displaystyle p\in M} , we may construct a local coordinate chart with coordinates x {\displaystyle x} , then the divergence is D ( x ( p ) , x ( p ) + d x ) = 1 2 d x T g p ( x ) d x + O ( | d x | 3 ) {\displaystyle D(x(p),x(p)+dx)=\textstyle {\frac {1}{2}}dx^{T}g_{p}(x)dx+O(|dx|^{3})} where g p ( x ) {\displaystyle g_{p}(x)} is a matrix of size n × × n {\displaystyle n\times n} . It is the Riemannian metric at point p {\displaystyle p} expressed in coordinates x {\displaystyle x} .

Dimensional analysis of condition 3 shows that divergence has the dimension of squared distance.

[ 3 ] The dual divergence D ∗ ∗ {\displaystyle D^{*}} is defined as D ∗ ∗ ( p , q ) = D ( q , p ) .

{\displaystyle D^{*}(p,q)=D(q,p).} When we wish to contrast D {\displaystyle D} against D ∗ ∗ {\displaystyle D^{*}} , we refer to D {\displaystyle D} as primal divergence .

Given any divergence D {\displaystyle D} , its symmetrized version is obtained by averaging it with its dual divergence: [ 3 ] D S ( p , q ) = 1 2 ( D ( p , q ) + D ( q , p ) ) .

{\displaystyle D_{S}(p,q)=\textstyle {\frac {1}{2}}{\big (}D(p,q)+D(q,p){\big )}.} Difference from other similar concepts [ edit ] Unlike metrics , divergences are not required to be symmetric, and the asymmetry is important in applications.

[ 3 ] Accordingly, one often refers asymmetrically to the divergence "of q from p " or "from p to q ", rather than "between p and q ". Secondly, divergences generalize squared distance, not linear distance, and thus do not satisfy the triangle inequality , but some divergences (such as the Bregman divergence ) do satisfy generalizations of the Pythagorean theorem .

In general statistics and probability, "divergence" generally refers to any kind of function D ( p , q ) {\displaystyle D(p,q)} , where p , q {\displaystyle p,q} are probability distributions or other objects under consideration, such that conditions 1, 2 are satisfied. Condition 3 is required for "divergence" as used in information geometry.

As an example, the total variation distance , a commonly used statistical divergence, does not satisfy condition 3.

Notation [ edit ] Notation for divergences varies significantly between fields, though there are some conventions.

Divergences are generally notated with an uppercase 'D', as in D ( x , y ) {\displaystyle D(x,y)} , to distinguish them from metric distances, which are notated with a lowercase 'd'. When multiple divergences are in use, they are commonly distinguished with subscripts, as in D KL {\displaystyle D_{\text{KL}}} for Kullback–Leibler divergence (KL divergence).

Often a different separator between parameters is used, particularly to emphasize the asymmetry. In information theory , a double bar is commonly used: D ( p ∥ ∥ q ) {\displaystyle D(p\parallel q)} ; this is similar to, but distinct from, the notation for conditional probability , P ( A | B ) {\displaystyle P(A|B)} , and emphasizes interpreting the divergence as a relative measurement, as in relative entropy ; this notation is common for the KL divergence. A colon may be used instead, [ b ] as D ( p : q ) {\displaystyle D(p:q)} ; this emphasizes the relative information supporting the two distributions.

The notation for parameters varies as well. Uppercase P , Q {\displaystyle P,Q} interprets the parameters as probability distributions, while lowercase p , q {\displaystyle p,q} or x , y {\displaystyle x,y} interprets them geometrically as points in a space, and μ μ 1 , μ μ 2 {\displaystyle \mu _{1},\mu _{2}} or m 1 , m 2 {\displaystyle m_{1},m_{2}} interprets them as measures.

Geometrical properties [ edit ] Further information: Information geometry Many properties of divergences can be derived if we restrict S to be a statistical manifold, meaning that it can be  parametrized with a finite-dimensional coordinate system θ , so that for a distribution p ∈ S we can write p = p ( θ ) .

For a pair of points p , q ∈ S with coordinates θ p and θ q , denote the partial derivatives of D ( p , q ) as D ( ( ∂ ∂ i ) p , q ) = d e f ∂ ∂ ∂ ∂ θ θ p i D ( p , q ) , D ( ( ∂ ∂ i ∂ ∂ j ) p , ( ∂ ∂ k ) q ) = d e f ∂ ∂ ∂ ∂ θ θ p i ∂ ∂ ∂ ∂ θ θ p j ∂ ∂ ∂ ∂ θ θ q k D ( p , q ) , e t c .

{\displaystyle {\begin{aligned}D((\partial _{i})_{p},q)\ \ &{\stackrel {\mathrm {def} }{=}}\ \ {\tfrac {\partial }{\partial \theta _{p}^{i}}}D(p,q),\\D((\partial _{i}\partial _{j})_{p},(\partial _{k})_{q})\ \ &{\stackrel {\mathrm {def} }{=}}\ \ {\tfrac {\partial }{\partial \theta _{p}^{i}}}{\tfrac {\partial }{\partial \theta _{p}^{j}}}{\tfrac {\partial }{\partial \theta _{q}^{k}}}D(p,q),\ \ \mathrm {etc.} \end{aligned}}} Now we restrict these functions to a diagonal p = q , and denote [ 4 ] D [ ∂ ∂ i , ⋅ ⋅ ] : p ↦ ↦ D ( ( ∂ ∂ i ) p , p ) , D [ ∂ ∂ i , ∂ ∂ j ] : p ↦ ↦ D ( ( ∂ ∂ i ) p , ( ∂ ∂ j ) p ) , e t c .

{\displaystyle {\begin{aligned}D[\partial _{i},\cdot ]\ &:\ p\mapsto D((\partial _{i})_{p},p),\\D[\partial _{i},\partial _{j}]\ &:\ p\mapsto D((\partial _{i})_{p},(\partial _{j})_{p}),\ \ \mathrm {etc.} \end{aligned}}} By definition, the function D ( p , q ) is minimized at p = q , and therefore D [ ∂ ∂ i , ⋅ ⋅ ] = D [ ⋅ ⋅ , ∂ ∂ i ] = 0 , D [ ∂ ∂ i ∂ ∂ j , ⋅ ⋅ ] = D [ ⋅ ⋅ , ∂ ∂ i ∂ ∂ j ] = − − D [ ∂ ∂ i , ∂ ∂ j ] ≡ ≡ g i j ( D ) , {\displaystyle {\begin{aligned}&D[\partial _{i},\cdot ]=D[\cdot ,\partial _{i}]=0,\\&D[\partial _{i}\partial _{j},\cdot ]=D[\cdot ,\partial _{i}\partial _{j}]=-D[\partial _{i},\partial _{j}]\ \equiv \ g_{ij}^{(D)},\end{aligned}}} where matrix g ( D ) is positive semi-definite and defines a unique Riemannian metric on the manifold S .

Divergence D (·, ·) also defines a unique torsion -free affine connection ∇ ( D ) with coefficients Γ Γ i j , k ( D ) = − − D [ ∂ ∂ i ∂ ∂ j , ∂ ∂ k ] , {\displaystyle \Gamma _{ij,k}^{(D)}=-D[\partial _{i}\partial _{j},\partial _{k}],} and the dual to this connection ∇* is generated by the dual divergence D *.

Thus, a divergence D (·, ·) generates on a statistical manifold a unique dualistic structure ( g ( D ) , ∇ ( D ) , ∇ ( D *) ). The converse is also true: every torsion-free dualistic structure on a statistical manifold is induced from some globally defined divergence function (which however need not be unique).

[ 5 ] For example, when D is an f-divergence [ 6 ] for some function ƒ(·), then it generates the metric g ( D f ) = c·g and the connection ∇ ( D f ) = ∇ ( α ) , where g is the canonical Fisher information metric , ∇ ( α ) is the α-connection, c = ƒ′′(1) , and α = 3 + 2ƒ′′′(1)/ƒ′′(1) .

Examples [ edit ] The two most important divergences are the relative entropy ( Kullback–Leibler divergence , KL divergence), which is central to information theory and statistics, and the squared Euclidean distance (SED). Minimizing these two divergences is the main way that linear inverse problems are solved, via the principle of maximum entropy and least squares , notably in logistic regression and linear regression .

[ 7 ] The two most important classes of divergences are the f -divergences and Bregman divergences ; however, other types of divergence functions are also encountered in the literature. The only divergence for probabilities over a finite alphabet that is both an f -divergence and a Bregman divergence is the Kullback–Leibler divergence.

[ 8 ] The squared Euclidean divergence is a Bregman divergence (corresponding to the function ⁠ x 2 {\displaystyle x^{2}} ⁠ ) but not an f -divergence.

f-divergences [ edit ] Main article: f-divergence Given a convex function f : [ 0 , + ∞ ∞ ) → → ( − − ∞ ∞ , + ∞ ∞ ] {\displaystyle f:[0,+\infty )\to (-\infty ,+\infty ]} such that f ( 0 ) = lim t → → 0 + f ( t ) , f ( 1 ) = 0 {\displaystyle f(0)=\lim _{t\to 0^{+}}f(t),f(1)=0} , the f -divergence generated by f {\displaystyle f} is defined as D f ( p , q ) = ∫ ∫ p ( x ) f ( q ( x ) p ( x ) ) d x {\displaystyle D_{f}(p,q)=\int p(x)f{\bigg (}{\frac {q(x)}{p(x)}}{\bigg )}dx} .

Kullback–Leibler divergence : D K L ( p , q ) = ∫ ∫ p ( x ) ln ⁡ ⁡ ( p ( x ) q ( x ) ) d x {\displaystyle D_{\mathrm {KL} }(p,q)=\int p(x)\ln \left({\frac {p(x)}{q(x)}}\right)dx} squared Hellinger distance : H 2 ( p , q ) = 2 ∫ ∫ ( p ( x ) − − q ( x ) ) 2 d x {\displaystyle H^{2}(p,\,q)=2\int {\Big (}{\sqrt {p(x)}}-{\sqrt {q(x)}}\,{\Big )}^{2}dx} Jensen–Shannon divergence : D J S ( p , q ) = 1 2 ∫ ∫ p ( x ) ln ⁡ ⁡ ( p ( x ) ) + q ( x ) ln ⁡ ⁡ ( q ( x ) ) − − ( p ( x ) + q ( x ) ) ln ⁡ ⁡ ( p ( x ) + q ( x ) 2 ) d x {\displaystyle D_{JS}(p,q)={\frac {1}{2}}\int p(x)\ln \left(p(x)\right)+q(x)\ln \left(q(x)\right)-(p(x)+q(x))\ln \left({\frac {p(x)+q(x)}{2}}\right)dx} α-divergence D ( α α ) ( p , q ) = 4 1 − − α α 2 ( 1 − − ∫ ∫ p ( x ) 1 − − α α 2 q ( x ) 1 + α α 2 d x ) {\displaystyle D^{(\alpha )}(p,q)={\frac {4}{1-\alpha ^{2}}}{\bigg (}1-\int p(x)^{\frac {1-\alpha }{2}}q(x)^{\frac {1+\alpha }{2}}dx{\bigg )}} chi-squared divergence : D χ χ 2 ( p , q ) = ∫ ∫ ( p ( x ) − − q ( x ) ) 2 p ( x ) d x {\displaystyle D_{\chi ^{2}}(p,q)=\int {\frac {(p(x)-q(x))^{2}}{p(x)}}dx} ( α , β )-product divergence [ citation needed ] : D α α , β β ( p , q ) = 2 ( 1 − − α α ) ( 1 − − β β ) ∫ ∫ ( 1 − − ( q ( x ) p ( x ) ) 1 − − α α 2 ) ( 1 − − ( q ( x ) p ( x ) ) 1 − − β β 2 ) p ( x ) d x {\displaystyle D_{\alpha ,\beta }(p,q)={\frac {2}{(1-\alpha )(1-\beta )}}\int {\Big (}1-{\Big (}{\tfrac {q(x)}{p(x)}}{\Big )}^{\!\!{\frac {1-\alpha }{2}}}{\Big )}{\Big (}1-{\Big (}{\tfrac {q(x)}{p(x)}}{\Big )}^{\!\!{\frac {1-\beta }{2}}}{\Big )}p(x)dx} Bregman divergences [ edit ] Main article: Bregman divergence Bregman divergences correspond to convex functions on convex sets. Given a strictly convex , continuously differentiable function F on a convex set , known as the Bregman generator , the Bregman divergence measures the convexity of: the error of the linear approximation of F from q as an approximation of the value at p : D F ( p , q ) = F ( p ) − − F ( q ) − − ⟨ ⟨ ∇ ∇ F ( q ) , p − − q ⟩ ⟩ .

{\displaystyle D_{F}(p,q)=F(p)-F(q)-\langle \nabla F(q),p-q\rangle .} The dual divergence to a Bregman divergence is the divergence generated by the convex conjugate F * of the Bregman generator of the original divergence. For example, for the squared Euclidean distance, the generator is ⁠ x 2 {\displaystyle x^{2}} ⁠ , while for the relative entropy the generator is the negative entropy ⁠ x log ⁡ ⁡ x {\displaystyle x\log x} ⁠ .

History [ edit ] The use of the term "divergence" – both what functions it refers to, and what various statistical distances are called – has varied significantly over time, but by c. 2000 had settled on the current usage within information geometry, notably in the textbook Amari & Nagaoka (2000) .

[ 1 ] The term "divergence" for a statistical distance was used informally in various contexts from c. 1910 to c. 1940. Its formal use dates at least to Bhattacharyya (1943) , entitled "On a measure of divergence between two statistical populations defined by their probability distributions", which defined the Bhattacharyya distance , and Bhattacharyya (1946) , entitled "On a Measure of Divergence between Two Multinomial Populations", which defined the Bhattacharyya angle . The term was popularized by its use for the Kullback–Leibler divergence in Kullback & Leibler (1951) and its use in the textbook Kullback (1959) . The term "divergence" was used generally by Ali & Silvey (1966) for statistically distances. Numerous references to earlier uses of statistical distances are given in Adhikari & Joshi (1956) and Kullback (1959 , pp. 6–7, §1.3 Divergence).

Kullback & Leibler (1951) actually used "divergence" to refer to the symmetrized divergence (this function had already been defined and used by Harold Jeffreys in 1948 [ 9 ] ), referring to the asymmetric function as "the mean information for discrimination ... per observation", [ 10 ] while Kullback (1959) referred to the asymmetric function as the "directed divergence".

[ 11 ] Ali & Silvey (1966) referred generally to such a function as a "coefficient of divergence", and showed that many existing functions could be expressed as f -divergences, referring to Jeffreys' function as "Jeffreys' measure of divergence" (today "Jeffreys divergence"), and Kullback–Leibler's asymmetric function (in each direction) as "Kullback's and Leibler's measures of discriminatory information" (today "Kullback–Leibler divergence").

[ 12 ] The information geometry definition of divergence (the subject of this article) was initially referred to by alternative terms, including "quasi-distance" Amari (1982 , p. 369) and "contrast function" Eguchi (1985) , though "divergence" was used in Amari (1985) for the α -divergence, and has become standard for the general class.

[ 1 ] [ 2 ] The term "divergence" is in contrast to a distance (metric), since the symmetrized divergence does not satisfy the triangle inequality.

[ 13 ] For example, the term "Bregman distance" is still found, but "Bregman divergence" is now preferred.

Notationally, Kullback & Leibler (1951) denoted their asymmetric function as I ( 1 : 2 ) {\displaystyle I(1:2)} , while Ali & Silvey (1966) denote their functions with a lowercase 'd' as d ( P 1 , P 2 ) {\displaystyle d\left(P_{1},P_{2}\right)} .

See also [ edit ] Statistical distance Notes [ edit ] ^ Throughout, we only require differentiability class C 2 (continuous with continuous first and second derivatives), since only second derivatives are required. In practice, commonly used statistical manifolds and divergences are infinitely differentiable ("smooth").

^ A colon is used in Kullback & Leibler (1951 , p. 80), where the KL divergence between measure μ μ 1 {\displaystyle \mu _{1}} and μ μ 2 {\displaystyle \mu _{2}} is written as I ( 1 : 2 ) {\displaystyle I(1:2)} .

References [ edit ] ^ a b c Amari & Nagaoka 2000 , chapter 3.2.

^ a b Amari 2016 , p. 10, Definition 1.1.

^ a b c Amari 2016 , p. 10.

^ Eguchi (1992) ^ Matumoto (1993) ^ Nielsen, F.; Nock, R. (2013). "On the Chi square and higher-order Chi distances for approximating f-divergences".

IEEE Signal Processing Letters .

21 : 10– 13.

arXiv : 1309.3029 .

doi : 10.1109/LSP.2013.2288355 .

S2CID 4152365 .

^ Csiszar 1991 .

^ Jiao, Jiantao; Courtade, Thomas; No, Albert; Venkat, Kartik; Weissman, Tsachy (December 2014). "Information Measures: the Curious Case of the Binary Alphabet".

IEEE Transactions on Information Theory .

60 (12): 7616– 7626.

arXiv : 1404.6810 .

doi : 10.1109/TIT.2014.2360184 .

ISSN 0018-9448 .

S2CID 13108908 .

^ Jeffreys 1948 , p. 158.

^ Kullback & Leibler 1951 , p. 80.

^ Kullback 1959 , p. 7.

^ Ali & Silvey 1966 , p. 139.

^ Kullback 1959 , p. 6.

Bibliography [ edit ] Adhikari, B. P.; Joshi, D. D. (1956). "Distance, discrimination et résumé exhaustif".

Pub. Inst. Stat. Univ. Paris .

5 : 57– 74.

Amari, Shun-Ichi (1982).

"Differential Geometry of Curved Exponential Families-Curvatures and Information Loss" .

The Annals of Statistics .

10 (2): 357– 385.

doi : 10.1214/aos/1176345779 .

ISSN 0090-5364 .

JSTOR 2240672 .

Amari, Shun-Ichi (1985).

Differential-Geometrical Methods in Statistics . Lecture Notes in Statistics. Vol. 28. Springer-Verlag.

Amari, Shun-ichi ; Nagaoka, Hiroshi (2000).

Methods of information geometry . Oxford University Press.

ISBN 0-8218-0531-2 .

Amari, Shun-ichi (2016).

Information Geometry and Its Applications . Applied Mathematical Sciences. Vol. 194. Springer Japan. pp. XIII, 374.

doi : 10.1007/978-4-431-55978-8 .

ISBN 978-4-431-55977-1 .

Bhattacharyya, A. (1946). "On a Measure of Divergence between Two Multinomial Populations".

Sankhyā: The Indian Journal of Statistics (1933-1960) .

7 (4): 401– 406.

ISSN 0036-4452 .

JSTOR 25047882 .

Bhattacharyya, A. (1943). "On a measure of divergence between two statistical populations defined by their probability distributions".

Bull. Calcutta Math. Soc .

35 : 99– 109.

Csiszar, Imre (1 December 1991).

"Why Least Squares and Maximum Entropy? An Axiomatic Approach to Inference for Linear Inverse Problems" .

The Annals of Statistics .

19 (4).

doi : 10.1214/aos/1176348385 .

Eguchi, Shinto (1985).

"A differential geometric approach to statistical inference on the basis of contrast functionals" .

Hiroshima Mathematical Journal .

15 (2): 341– 391.

doi : 10.32917/hmj/1206130775 .

Eguchi, Shinto (1992).

"Geometry of minimum contrast" .

Hiroshima Mathematical Journal .

22 (3): 631– 647.

doi : 10.32917/hmj/1206128508 .

Ali, S. M.; Silvey, S. D. (1966). "A General Class of Coefficients of Divergence of One Distribution from Another".

Journal of the Royal Statistical Society. Series B (Methodological) .

28 (1): 131– 142.

doi : 10.1111/j.2517-6161.1966.tb00626.x .

ISSN 0035-9246 .

JSTOR 2984279 .

Jeffreys, Harold (1948).

Theory of Probability (Second ed.). Oxford University Press.

Kullback, S.

; Leibler, R.A.

(1951).

"On information and sufficiency" .

Annals of Mathematical Statistics .

22 (1): 79– 86.

doi : 10.1214/aoms/1177729694 .

JSTOR 2236703 .

MR 0039968 .

Kullback, S.

(1959), Information Theory and Statistics , John Wiley & Sons . Republished by Dover Publications in 1968; reprinted in 1978: ISBN 0-8446-5625-9 Matumoto, Takao (1993).

"Any statistical manifold has a contrast function — on the C³-functions taking the minimum at the diagonal of the product manifold" .

Hiroshima Mathematical Journal .

23 (2): 327– 332.

doi : 10.32917/hmj/1206128255 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐qxxz7
Cached time: 20250812020411
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.694 seconds
Real time usage: 0.949 seconds
Preprocessor visited node count: 4542/1000000
Revision size: 20687/2097152 bytes
Post‐expand include size: 190283/2097152 bytes
Template argument size: 5258/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 9/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 68541/5000000 bytes
Lua time usage: 0.390/10.000 seconds
Lua memory usage: 8475108/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  658.183      1 -total
 24.45%  160.908      1 Template:Statistics
 24.05%  158.265      1 Template:Navbox_with_collapsible_groups
 22.95%  151.066     12 Template:Cite_journal
 16.12%  106.108      2 Template:Reflist
 11.96%   78.693      1 Template:Short_description
  9.75%   64.169     11 Template:Navbox
  9.74%   64.081     14 Template:Sfn
  7.52%   49.478      2 Template:Pagetype
  6.90%   45.421      1 Template:Citation_needed Saved in parser cache with key enwiki:pcache:25896411:|#|:idhash:canonical and timestamp 20250812020411 and revision id 1296052833. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Divergence_(statistics)&oldid=1296052833 " Categories : Statistical distance F-divergences Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from May 2022 This page was last edited on 17 June 2025, at 14:02 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Divergence (statistics) 1 language Add topic

