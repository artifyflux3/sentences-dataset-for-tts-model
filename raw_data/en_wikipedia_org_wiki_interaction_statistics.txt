Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Introduction 2 In modeling Toggle In modeling subsection 2.1 In ANOVA 2.2 Qualitative and quantitative interactions 2.3 Unit treatment additivity 2.4 Categorical variables 2.5 Designed experiments 2.6 Model size 2.7 In regression 3 Interaction plots Toggle Interaction plots subsection 3.1 Example: Interaction of species and air temperature and their effect on body temperature 3.2 Example: effect of stroke severity and treatment on recovery 4 Hypothesis tests for interactions Toggle Hypothesis tests for interactions subsection 4.1 Example: Interaction of temperature and time in cookie baking 4.1.1 ANOVA model 1: no interaction term; yield ~ temperature + time 4.1.2 ANOVA model 2: include interaction term; yield ~ temperature * time 5 Examples 6 See also 7 References 8 Further reading 9 External links Toggle the table of contents Interaction (statistics) 8 languages Deutsch Euskara Français Polski Русский Sunda Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Causal or moderating relationship between statistical variables Interaction effect of education and ideology on concern about sea level rise In statistics , an interaction may arise when considering the relationship among three or more variables , and describes a situation in which the effect of one causal variable on an outcome depends on the state of a second causal variable (that is, when effects of the two causes are not additive ).

[ 1 ] [ 2 ] Although commonly thought of in terms of causal relationships , the concept of an interaction can also describe non-causal associations (then also called moderation or effect modification ). Interactions are often considered in the context of regression analyses or factorial experiments .

The presence of interactions can have important implications for the interpretation of statistical models . If two variables of interest interact, the relationship between each of the interacting variables and a third "dependent variable" depends on the value of the other interacting variable. In practice, this makes it more difficult to predict the consequences of changing the value of a variable, particularly if the variables it interacts with are hard to measure or difficult to control.

The notion of "interaction" is closely related to that of moderation that is common in social and health science research: the interaction between an explanatory variable and an environmental variable suggests that the effect of the explanatory variable has been moderated or modified by the environmental variable.

[ 1 ] Introduction [ edit ] An interaction variable or interaction feature is a variable constructed from an original set of variables to try to represent either all of the interaction present or some part of it. In exploratory statistical analyses it is common to use products of original variables as the basis of testing whether interaction is present with the possibility of substituting other more realistic interaction variables at a later stage. When there are more than two explanatory variables, several interaction variables are constructed, with pairwise-products representing pairwise-interactions and higher order products representing higher order interactions.

The binary factor A and the quantitative variable X interact (are non-additive) when analyzed with respect to the outcome variable Y .

Thus, for a response Y and two variables x 1 and x 2 an additive model would be: Y = c + a x 1 + b x 2 + error {\displaystyle Y=c+ax_{1}+bx_{2}+{\text{error}}\,} In contrast to this, Y = c + a x 1 + b x 2 + d ( x 1 × × x 2 ) + error {\displaystyle Y=c+ax_{1}+bx_{2}+d(x_{1}\times x_{2})+{\text{error}}\,} is an example of a model with an interaction between variables x 1 and x 2 ("error" refers to the random variable whose value is that by which Y differs from the expected value of Y ; see errors and residuals in statistics ). Often, models are presented without the interaction term d ( x 1 × × x 2 ) {\displaystyle d(x_{1}\times x_{2})} , but this confounds the main effect and interaction effect (i.e., without specifying the interaction term, it is possible that any main effect found is actually due to an interaction).

Moreover, the hierarchical principle rules that if a model includes interaction between variables, it is also necessary to include the main effects, regardless of their own statistical significance.

[ 3 ] In modeling [ edit ] In ANOVA [ edit ] A simple setting in which interactions can arise is a two-factor experiment analyzed using Analysis of Variance (ANOVA).  Suppose we have two binary factors A and B .  For example, these factors might indicate whether either of two treatments were administered to a patient, with the treatments applied either singly, or in combination.  We can then consider the average treatment response (e.g. the symptom levels following treatment) for each patient, as a function of the treatment combination that was administered.  The following table shows one possible situation: B = 0 B = 1 A = 0 6 7 A = 1 4 5 In this example, there is no interaction between the two treatments — their effects are additive.  The reason for this is that the difference in mean response between those subjects receiving treatment A and those not receiving treatment A is −2 regardless of whether treatment B is administered (−2 = 4 − 6) or not (−2 = 5 − 7). Note that it automatically follows that the difference in mean response between those subjects receiving treatment B and those not receiving treatment B is the same regardless of whether treatment A is administered (7 − 6 = 5 − 4).

In contrast, if the following average responses are observed B = 0 B = 1 A = 0 1 4 A = 1 7 6 then there is an interaction between the treatments — their effects are not additive.  Supposing that greater numbers correspond to a better response, in this situation treatment B is helpful on average if the subject is not also receiving treatment A , but is detrimental on average if given in combination with treatment A . Treatment A is helpful on average regardless of whether treatment B is also administered, but it is more helpful in both absolute and relative terms if given alone, rather than in combination with treatment B . Similar observations are made for this particular example in the next section.

Qualitative and quantitative interactions [ edit ] In many applications it is useful to distinguish between qualitative and quantitative interactions.

[ 4 ] A quantitative interaction between A and B is a situation where the magnitude of the effect of B depends on the value of A , but the direction of the effect of B is constant for all A .  A qualitative interaction between A and B refers to a situation where both the magnitude and direction of each variable's effect can depend on the value of the other variable.

The table of means on the left, below, shows a quantitative interaction — treatment A is beneficial both when B is given, and when B is not given, but the benefit is greater when B is not given (i.e. when A is given alone).  The table of means on the right shows a qualitative interaction.

A is harmful when B is given, but it is beneficial when B is not given.  Note that the same interpretation would hold if we consider the benefit of B based on whether A is given.

B = 0 B = 1 B = 0 B = 1 A = 0 2 1 A = 0 2 6 A = 1 5 3 A = 1 5 3 The distinction between qualitative and quantitative interactions depends on the order in which the variables are considered (in contrast, the property of additivity is invariant to the order of the variables). In the following table, if we focus on the effect of treatment A , there is a quantitative interaction — giving treatment A will improve the outcome on average regardless of whether treatment B is or is not already being given (although the benefit is greater if treatment A is given alone). However, if we focus on the effect of treatment B , there is a qualitative interaction — giving treatment B to a subject who is already receiving treatment A will (on average) make things worse, whereas giving treatment B to a subject who is not receiving treatment A will improve the outcome on average.

B = 0 B = 1 A = 0 1 4 A = 1 7 6 Unit treatment additivity [ edit ] In its simplest form, the assumption of treatment unit additivity states that the observed response y ij from experimental unit i when receiving treatment j can be written as the sum y ij = y i + t j .

[ 5 ] [ 6 ] [ 7 ] The assumption of unit treatment additivity implies that every treatment has exactly the same additive effect on each experimental unit. Since any given experimental unit can only undergo one of the treatments, the assumption of unit treatment additivity is a hypothesis that is not directly falsifiable, according to Cox [ citation needed ] and Kempthorne.

[ citation needed ] However, many consequences of treatment-unit additivity can be falsified.

[ citation needed ] For a randomized experiment, the assumption of treatment additivity implies that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit treatment additivity is that the variance is constant.

[ citation needed ] The property of unit treatment additivity is not invariant under a change of scale, [ citation needed ] so statisticians often use transformations to achieve unit treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance.

[ 8 ] In many cases, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.

[ 6 ] [ 9 ] The assumption of unit treatment additivity was enunciated in experimental design by Kempthorne [ citation needed ] and Cox [ citation needed ] . Kempthorne's use of unit treatment additivity and randomization is similar to the design-based analysis of finite population survey sampling.

In recent years, it has become common [ citation needed ] to use the terminology of Donald Rubin, which uses counterfactuals. Suppose we are comparing two groups of people with respect to some attribute y .  For example, the first group might consist of people who are given a standard treatment for a medical condition, with the second group consisting of people who receive a new treatment with unknown effect.  Taking a "counterfactual" perspective, we can consider an individual whose attribute has value y if that individual belongs to the first group, and whose attribute has value τ ( y ) if the individual belongs to the second group.  The assumption of "unit treatment additivity" is that τ ( y ) = τ , that is, the "treatment effect" does not depend on y .  Since we cannot observe both y and τ( y ) for a given individual, this is not testable at the individual level.  However, unit treatment additivity implies that the cumulative distribution functions F 1 and F 2 for the two groups satisfy F 2 ( y )  = F 1 ( y − τ ), as long as the assignment of individuals to groups 1 and 2 is independent of all other factors influencing y (i.e. there are no confounders ).  Lack of unit treatment additivity can be viewed as a form of interaction between the treatment assignment (e.g. to groups 1 or 2), and the baseline, or untreated value of y .

Categorical variables [ edit ] Sometimes the interacting variables are categorical variables rather than real numbers and the study might then be dealt with as an analysis of variance problem.  For example, members of a population may be classified by religion and by occupation.  If one wishes to predict a person's height based only on the person's religion and occupation, a simple additive model, i.e., a model without interaction, would add to an overall average height an adjustment for a particular religion and another for a particular occupation.  A model with interaction, unlike an additive model , could add a further adjustment for the "interaction" between that religion and that occupation.  This example may cause one to suspect that the word interaction is something of a misnomer.

Statistically, the presence of an interaction between categorical variables is generally tested using a form of analysis of variance (ANOVA). If one or more of the variables is continuous in nature, however, it would typically be tested using moderated multiple regression.

[ 10 ] This is so-called because a moderator is a variable that affects the strength of a relationship between two other variables.

Designed experiments [ edit ] Genichi Taguchi contended [ 11 ] that interactions could be eliminated from a system by appropriate choice of response variable and transformation. However George Box and others have argued that this is not the case in general.

[ 12 ] Model size [ edit ] Given n predictors, the number of terms in a linear model that includes a constant, every predictor, and every possible interaction is ( n 0 ) + ( n 1 ) + ( n 2 ) + ⋯ ⋯ + ( n n ) = 2 n {\displaystyle {\tbinom {n}{0}}+{\tbinom {n}{1}}+{\tbinom {n}{2}}+\cdots +{\tbinom {n}{n}}=2^{n}} . Since this quantity grows exponentially, it readily becomes impractically large. One method to limit the size of the model is to limit the order of interactions. For example, if only two-way interactions are allowed, the number of terms becomes ( n 0 ) + ( n 1 ) + ( n 2 ) = 1 + 1 2 n + 1 2 n 2 {\displaystyle {\tbinom {n}{0}}+{\tbinom {n}{1}}+{\tbinom {n}{2}}=1+{\tfrac {1}{2}}n+{\tfrac {1}{2}}n^{2}} . The below table shows the number of terms for each number of predictors and maximum order of interaction.

Number of terms Predictors Including up to m -way interactions 2 3 4 5 ∞ 1 2 2 2 2 2 2 4 4 4 4 4 3 7 8 8 8 8 4 11 15 16 16 16 5 16 26 31 32 32 6 22 42 57 63 64 7 29 64 99 120 128 8 37 93 163 219 256 9 46 130 256 382 512 10 56 176 386 638 1,024 11 67 232 562 1,024 2,048 12 79 299 794 1,586 4,096 13 92 378 1,093 2,380 8,192 14 106 470 1,471 3,473 16,384 15 121 576 1,941 4,944 32,768 20 211 1,351 6,196 21,700 1,048,576 25 326 2,626 15,276 68,406 33,554,432 50 1,276 20,876 251,176 2,369,936 10 15 100 5,051 166,751 4,087,976 79,375,496 10 30 1,000 500,501 166,667,501 10 10 10 12 10 300 In regression [ edit ] The most general approach to modeling interaction effects involves regression, starting from the elementary version given above: Y = c + a x 1 + b x 2 + d ( x 1 × × x 2 ) + error {\displaystyle Y=c+ax_{1}+bx_{2}+d(x_{1}\times x_{2})+{\text{error}}\,} where the interaction term ( x 1 × × x 2 ) {\displaystyle (x_{1}\times x_{2})} could be formed explicitly by multiplying two (or more) variables, or implicitly using factorial notation in modern statistical packages such as Stata . The components x 1 and x 2 might be measurements or {0,1} dummy variables in any combination. Interactions involving a dummy variable multiplied by a measurement variable are termed slope dummy variables , [ 13 ] because they estimate and test the difference in slopes between groups 0 and 1.

When measurement variables are employed in interactions, it is often desirable to work with centered versions, where the variable's mean (or some other reasonably central value) is set as zero. Centering can make the main effects in interaction models more interpretable, as it reduces the multicollinearity between the interaction term and the main effects.

[ 14 ] The coefficient a in the equation above, for example, represents the effect of x 1 when x 2 equals zero.

Interaction of education and political party affecting beliefs about climate change Regression approaches to interaction modeling are very general because they can accommodate additional predictors, and many alternative specifications or estimation strategies beyond ordinary least squares .

Robust , quantile , and mixed-effects ( multilevel ) models are among the possibilities, as is generalized linear modeling encompassing a wide range of categorical, ordered, counted or otherwise limited dependent variables. The graph depicts an education*politics interaction, from a probability-weighted logit regression analysis of survey data.

[ 15 ] Interaction plots [ edit ] Interaction plots, also called simple-slope plots , show possible interactions among variables.

Example: Interaction of species and air temperature and their effect on body temperature [ edit ] Consider a study of the body temperature of different species at different air temperatures, in degrees Fahrenheit. The data are shown in the table below.

The interaction plot may use either the air temperature or the species as the x axis. The second factor is represented by lines on the interaction plot.

There is an interaction between the two factors (air temperature and species) in their effect on the response (body temperature), because the effect of the air temperature depends on the species. The interaction is indicated on the plot because the lines are not parallel.

Example: effect of stroke severity and treatment on recovery [ edit ] As a second example, consider a clinical trial on the interaction between stroke severity and the efficacy of a drug on patient survival. The data are shown in the table below.

In the interaction plot, the lines for the mild and moderate stroke groups are parallel, indicating that the drug has the same effect in both groups, so there is no interaction. The line for the severe stroke group is not parallel to the other lines, indicating that there is an interaction between stroke severity and drug effect on survival. The line for the severe stroke group is flat, indicating that, among these patients, there is no difference in survival between the drug and placebo treatments. In contrast, the lines for the mild and moderate stroke groups slope down to the right, indicating that, among these patients, the placebo group has lower survival than drug-treated group.

Hypothesis tests for interactions [ edit ] Analysis of variance and regression analysis are used to test for significant interactions.

Example: Interaction of temperature and time in cookie baking [ edit ] Is the yield of good cookies affected by the baking temperature and time in the oven? The table shows data for 8 batches of cookies.

The data show that the yield of good cookies is best when either (i) temperature is high and time in the oven is short, or (ii) temperature is low and time in the oven is long. If the cookies are left in the oven for a long time at a high temperature, there are burnt cookies and the yield is low.

From the graph and the data, it is clear that the lines are not parallel, indicating that there is an interaction. This can be tested using analysis of variance (ANOVA). The first ANOVA model will not include the interaction term. That is, the first ANOVA model ignores possible interaction. The second ANOVA model will include the interaction term. That is, the second ANOVA model explicitly performs a hypothesis test for interaction.

ANOVA model 1: no interaction term; yield ~ temperature + time [ edit ] In the ANOVA model that ignores interaction, neither temperature nor time has a significant effect on yield (p=0.91), which is clearly the incorrect conclusion. The more appropriate ANOVA model should test for possible interaction.

ANOVA model 2: include interaction term; yield ~ temperature * time [ edit ] The temperature:time interaction term is significant (p=0.000180). Based on the interaction test and the interaction plot, it appears that the effect of time on yield depends on temperature and vice versa.

Examples [ edit ] Real-world examples of interaction include: Interaction between adding sugar to coffee and stirring the coffee. Neither of the two individual variables has much effect on sweetness but a combination of the two does.

Interaction between adding carbon to steel and quenching . Neither of the two individually has much effect on strength but a combination of the two has a dramatic effect.

Interaction between smoking and inhaling asbestos fibres: Both raise lung carcinoma risk, but exposure to asbestos multiplies the cancer risk in smokers and non-smokers. Here, the joint effect of inhaling asbestos and smoking is higher than the sum of both effects.

[ 16 ] Interaction between genetic risk factors for type 2 diabetes and diet (specifically, a "western" dietary pattern). The western dietary pattern was shown to increase diabetes risk for subjects with a high "genetic risk score", but not for other subjects.

[ 17 ] Interaction between education and political orientation, affecting general-public perceptions about climate change. For example, US surveys often find that acceptance of the reality of anthropogenic climate change rises with education among moderate or liberal survey respondents, but declines with education among the most conservative.

[ 18 ] [ 19 ] Similar interactions have been observed to affect some non-climate science or environmental perceptions, [ 20 ] and to operate with science literacy or other knowledge indicators in place of education.

[ 21 ] [ 22 ] See also [ edit ] Analysis of variance Factorial experiment Generalized randomized block design Intersectionality Linear model Main effect Tukey's test of additivity References [ edit ] ^ a b Dodge, Y. (2003).

The Oxford Dictionary of Statistical Terms . Oxford University Press.

ISBN 978-0-19-920613-1 .

^ Cox, D.R. (1984). "Interaction".

International Statistical Review .

52 (1): 1– 25.

doi : 10.2307/1403235 .

JSTOR 1403235 .

^ James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2021).

An introduction to statistical learning: with applications in R (Second ed.). New York, NY: Springer. p. 103.

ISBN 978-1-0716-1418-1 . Retrieved 29 October 2024 .

^ Peto, D. P. (1982). "Statistical aspects of cancer trials".

Treatment of Cancer (First ed.). London: Chapman and Hall.

ISBN 0-412-21850-X .

^ Kempthorne, Oscar (1979).

The Design and Analysis of Experiments (Corrected reprint of (1952) Wiley ed.). Robert E. Krieger.

ISBN 978-0-88275-105-4 .

^ a b Cox, David R.

(1958).

Planning of experiments . Wiley. Chapter 2.

ISBN 0-471-57429-5 .

{{ cite book }} : ISBN / Date incompatibility ( help ) ^ Hinkelmann, Klaus and Kempthorne, Oscar (2008).

Design and Analysis of Experiments, Volume I: Introduction to Experimental Design (Second ed.). Wiley. Chapters 5-6.

ISBN 978-0-471-72756-9 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Hinkelmann, Klaus and Kempthorne, Oscar (2008).

Design and Analysis of Experiments, Volume I: Introduction to Experimental Design (Second ed.). Wiley. Chapters 7-8.

ISBN 978-0-471-72756-9 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Bailey, R. A. (2008).

Design of Comparative Experiments . Cambridge University Press.

ISBN 978-0-521-68357-9 .

Pre-publication chapters are available on-line.

^ Overton, R. C. (2001). "Moderated multiple regression for interactions involving categorical variables: a statistical control for heterogeneous variance across two groups".

Psychol Methods .

6 (3): 218– 33.

doi : 10.1037/1082-989X.6.3.218 .

PMID 11570229 .

^ "Design of Experiments - Taguchi Experiments" .

www.qualitytrainingportal.com . Retrieved 2015-11-27 .

^ George E. P. Box (1990).

"Do interactions matter?" (PDF) .

Quality Engineering .

2 : 365– 369.

doi : 10.1080/08982119008962728 . Archived from the original (PDF) on 2010-06-10 . Retrieved 2009-07-28 .

^ Hamilton, L.C. 1992.

Regression with Graphics: A Second Course in Applied Statistics . Pacific Grove, CA: Brooks/Cole.

ISBN 978-0534159009 ^ Iacobucci, Dawn; Schneider, Matthew J.; Popovich, Deidre L.; Bakamitsos, Georgios A. (2016).

"Mean centering helps alleviate "micro" but not "macro" multicollinearity" .

Behavior Research Methods .

48 (4): 1308– 1317.

doi : 10.3758/s13428-015-0624-x .

ISSN 1554-3528 .

PMID 26148824 .

^ Hamilton, L.C.; Saito, K. (2015). "A four-party view of U.S. environmental concern".

Environmental Politics .

24 (2): 212– 227.

Bibcode : 2015EnvPo..24..212H .

doi : 10.1080/09644016.2014.976485 .

S2CID 154762226 .

^ Lee, P. N. (2001).

"Relation between exposure to asbestos and smoking jointly and the risk of lung cancer" .

Occupational and Environmental Medicine .

58 (3): 145– 53.

doi : 10.1136/oem.58.3.145 .

PMC 1740104 .

PMID 11171926 .

^ Lu, Q.; et al. (2009).

"Genetic predisposition, Western dietary pattern, and the risk of type 2 diabetes in men" .

Am J Clin Nutr .

89 (5): 1453– 1458.

doi : 10.3945/ajcn.2008.27249 .

PMC 2676999 .

PMID 19279076 .

^ Hamilton, L.C. (2011).

"Education, politics and opinions about climate change: Evidence for interaction effects" .

Climatic Change .

104 (2): 231– 242.

Bibcode : 2011ClCh..104..231H .

doi : 10.1007/s10584-010-9957-8 .

S2CID 16481640 .

^ McCright, A. M. (2011). "Political orientation moderates Americans' beliefs and concern about climate change".

Climatic Change .

104 (2): 243– 253.

Bibcode : 2011ClCh..104..243M .

doi : 10.1007/s10584-010-9946-y .

S2CID 152795205 .

^ Hamilton, Lawrence C.; Saito, Kei (2015). "A four-party view of US environmental concern".

Environmental Politics .

24 (2): 212– 227.

Bibcode : 2015EnvPo..24..212H .

doi : 10.1080/09644016.2014.976485 .

S2CID 154762226 .

^ Kahan, D.M.; Jenkins-Smith, H.; Braman, D. (2011).

"Cultural cognition of scientific consensus" .

Journal of Risk Research .

14 (2): 147– 174.

doi : 10.1080/13669877.2010.511246 .

hdl : 10.1080/13669877.2010.511246 .

S2CID 216092368 .

^ Hamilton, L.C.; Cutler, M.J.; Schaefer, A. (2012). "Public knowledge and concern about polar-region warming".

Polar Geography .

35 (2): 155– 168.

Bibcode : 2012PolGe..35..155H .

doi : 10.1080/1088937X.2012.684155 .

S2CID 12437794 .

Further reading [ edit ] Cox, David R.

and Reid, Nancy M. (2000) The theory of design of experiments , Chapman & Hall/CRC.

ISBN 1-58488-195-X Southwood, K.E. (1978). "Substantive Theory and Statistical Interaction: Five Models".

The American Journal of Sociology .

83 (5): 1154– 1203.

doi : 10.1086/226678 .

S2CID 143521842 .

Brambor, T.; Clark, W. R. (2006). "Understanding Interaction Models: Improving Empirical Analyses".

Political Analysis .

14 (1): 63– 82.

doi : 10.1093/pan/mpi014 .

Hayes, A. F.; Matthes, J. (2009).

"Computational procedures for probing interactions in OLS and logistic regression: SPSS and SAS implementations" .

Behavior Research Methods .

41 (3): 924– 936.

doi : 10.3758/BRM.41.3.924 .

PMID 19587209 .

Balli, H. O.; Sørensen, B. E. (2012). "Interaction effects in econometrics".

Empirical Economics .

43 (x): 1– 21.

CiteSeerX 10.1.1.691.4349 .

doi : 10.1007/s00181-012-0604-2 .

S2CID 53504187 .

External links [ edit ] "Using Indicator and Interaction Variables" (PDF) . Archived from the original (PDF) on 2016-03-03 . Retrieved 2010-02-03 .

(158 KiB ) Credibility and the Statistical Interaction Variable: Speaking Up for Multiplication as a Source of Understanding Fundamentals of Statistical Interactions: What is the difference between "main effects" and "interaction effects"?

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Design of experiments Scientific method Scientific experiment Statistical design Control Internal and external validity Experimental unit Blinding Optimal design : Bayesian Random assignment Randomization Restricted randomization Replication versus subsampling Sample size Treatment and blocking Treatment Effect size Contrast Interaction Confounding Orthogonality Blocking Covariate Nuisance variable Models and inference Linear regression Ordinary least squares Bayesian Random effect Mixed model Hierarchical model: Bayesian Analysis of variance (Anova) Cochran's theorem Manova ( multivariate ) Ancova ( covariance ) Compare means Multiple comparison Designs Completely randomized Factorial Fractional factorial Plackett–Burman Taguchi Response surface methodology Polynomial and rational modeling Box–Behnken Central composite Block Generalized randomized block design (GRBD) Latin square Graeco-Latin square Orthogonal array Latin hypercube Repeated measures design Crossover study Randomized controlled trial Sequential analysis Sequential probability ratio test Glossary Category Mathematics portal Statistical outline Statistical topics Retrieved from " https://en.wikipedia.org/w/index.php?title=Interaction_(statistics)&oldid=1292018338 " Categories : Analysis of variance Regression analysis Design of experiments Hidden categories: CS1 errors: ISBN date CS1 maint: multiple names: authors list Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from April 2010 This page was last edited on 24 May 2025, at 19:04 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Interaction (statistics) 8 languages Add topic

