Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History of coding theory 2 Source coding Toggle Source coding subsection 2.1 Definition 2.2 Properties 2.3 Principle 2.4 Example 3 Channel coding Toggle Channel coding subsection 3.1 Linear codes 3.1.1 Linear block codes 3.1.2 Convolutional codes 4 Cryptographic coding 5 Line coding 6 Other applications of coding theory Toggle Other applications of coding theory subsection 6.1 Group testing 6.2 Analog coding 7 Neural coding 8 See also 9 Notes 10 References Toggle the table of contents Coding theory 35 languages العربية Asturianu বাংলা Български Català Čeština Dansk Deutsch Ελληνικά Español Euskara فارسی Français Galego 한국어 हिन्दी Bahasa Indonesia Italiano עברית Nederlands 日本語 Norsk nynorsk Oʻzbekcha / ўзбекча Português Русский Simple English Српски / srpski Suomi Svenska Tagalog Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Study of the properties of codes and their fitness A two-dimensional visualisation of the Hamming distance , a critical measure in coding theory Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression , cryptography , error detection and correction , data transmission and data storage . Codes are studied by various scientific disciplines—such as information theory , electrical engineering , mathematics , linguistics , and computer science —for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

There are four types of coding: [ 1 ] Data compression (or source coding ) Error control (or channel coding ) Cryptographic coding Line coding Data compression attempts to remove unwanted redundancy from the data from a source in order to transmit it more efficiently. For example, DEFLATE data compression makes files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination .

Error correction adds useful redundancy to the data from a source to make the transmission more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music compact disc (CD) uses the Reed–Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes .

History of coding theory [ edit ] This section is an excerpt from History of information theory .

[ edit ] The decisive event which established the discipline of information theory , and brought it to immediate worldwide attention, was the publication of Claude E. Shannon 's classic paper " A Mathematical Theory of Communication " in the Bell System Technical Journal in July and October 1948.

In this revolutionary and groundbreaking paper, the work for which Shannon had substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that "The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point." With it came the ideas of the information entropy and redundancy of a source, and its relevance through the source coding theorem ; the mutual information , and the channel capacity of a noisy channel, including the promise of perfect loss-free communication given by the noisy-channel coding theorem ; the practical result of the Shannon–Hartley law for the channel capacity of a Gaussian channel; and of course the bit - a new way of seeing the most fundamental unit of information.

Shannon’s paper focuses on the problem of how to best encode the information a sender wants to transmit. In this fundamental work he used tools in probability theory, developed by Norbert Wiener , which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure for the uncertainty in a message while essentially inventing the field of information theory .

The binary Golay code was developed in 1949. It is an error-correcting code capable of correcting up to three errors in each 24-bit word, and detecting a fourth.

Richard Hamming won the Turing Award in 1968 for his work at Bell Labs in numerical methods, automatic coding systems, and error-detecting and error-correcting codes. He invented the concepts known as Hamming codes , Hamming windows , Hamming numbers , and Hamming distance .

In 1972, Nasir Ahmed proposed the discrete cosine transform (DCT), which he developed with T. Natarajan and K. R. Rao in 1973.

[ 2 ] The DCT is the most widely used lossy compression algorithm, the basis for multimedia formats such as JPEG , MPEG and MP3 .

Source coding [ edit ] Main article: Data compression The aim of source coding is to take the source data and make it smaller.

Definition [ edit ] Data can be seen as a random variable X : Ω Ω → → X {\displaystyle X:\Omega \to {\mathcal {X}}} , where x ∈ ∈ X {\displaystyle x\in {\mathcal {X}}} appears with probability P [ X = x ] {\displaystyle \mathbb {P} [X=x]} .

Data are encoded by strings (words) over an alphabet Σ Σ {\displaystyle \Sigma } .

A code is a function C : X → → Σ Σ ∗ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} (or Σ Σ + {\displaystyle \Sigma ^{+}} if the empty string is not part of the alphabet).

C ( x ) {\displaystyle C(x)} is the code word associated with x {\displaystyle x} .

Length of the code word is written as l ( C ( x ) ) .

{\displaystyle l(C(x)).} Expected length of a code is l ( C ) = ∑ ∑ x ∈ ∈ X l ( C ( x ) ) P [ X = x ] .

{\displaystyle l(C)=\sum _{x\in {\mathcal {X}}}l(C(x))\mathbb {P} [X=x].} The concatenation of code words C ( x 1 , … … , x k ) = C ( x 1 ) C ( x 2 ) ⋯ ⋯ C ( x k ) {\displaystyle C(x_{1},\ldots ,x_{k})=C(x_{1})C(x_{2})\cdots C(x_{k})} .

The code word of the empty string is the empty string itself: C ( ϵ ϵ ) = ϵ ϵ {\displaystyle C(\epsilon )=\epsilon } Properties [ edit ] C : X → → Σ Σ ∗ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} is non-singular if injective .

C : X ∗ ∗ → → Σ Σ ∗ ∗ {\displaystyle C:{\mathcal {X}}^{*}\to \Sigma ^{*}} is uniquely decodable if injective.

C : X → → Σ Σ ∗ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} is instantaneous if C ( x 1 ) {\displaystyle C(x_{1})} is not a proper prefix of C ( x 2 ) {\displaystyle C(x_{2})} (and vice versa).

Principle [ edit ] Entropy of a source is the measure of information. Basically, source codes try to reduce the redundancy present in the source, and represent the source with fewer bits that carry more information.

Data compression which explicitly tries to minimize the average length of messages according to a particular assumed probability model is called entropy encoding .

Various techniques used by source coding schemes try to achieve the limit of entropy of the source.

C ( x ) ≥ H ( x ), where H ( x ) is entropy of source (bitrate), and C ( x ) is the bitrate after compression. In particular, no source coding scheme can be better than the entropy of the source.

Example [ edit ] Facsimile transmission uses a simple run length code . Source coding removes all data superfluous to the need of the transmitter, decreasing the bandwidth required for transmission.

Channel coding [ edit ] Main article: Error detection and correction The purpose of channel coding theory is to find codes which transmit quickly, contain many valid code words and can correct or at least detect many errors. While not mutually exclusive, performance in these areas is a trade-off. So, different codes are optimal for different applications. The needed properties of this code mainly depend on the probability of errors happening during transmission. In a typical CD, the impairment is mainly dust or scratches.

CDs use cross-interleaved Reed–Solomon coding to spread the data out over the disk.

[ 3 ] Although not a very good code, a simple repeat code can serve as an understandable example. Suppose we take a block of data bits (representing sound) and send it three times. At the receiver we will examine the three repetitions bit by bit and take a majority vote. The twist on this is that we do not merely send the bits in order. We interleave them. The block of data bits is first divided into 4 smaller blocks. Then we cycle through the block and send one bit from the first, then the second, etc. This is done three times to spread the data out over the surface of the disk. In the context of the simple repeat code, this may not appear effective. However, there are more powerful codes known which are very effective at correcting the "burst" error of a scratch or a dust spot when this interleaving technique is used.

Other codes are more appropriate for different applications. Deep space communications are limited by the thermal noise of the receiver which is more of a continuous nature than a bursty nature. Likewise, narrowband modems are limited by the noise, present in the telephone network and also modeled better as a continuous disturbance.

[ citation needed ] Cell phones are subject to rapid fading . The high frequencies used can cause rapid fading of the signal even if the receiver is moved a few inches. Again there are a class of channel codes that are designed to combat fading.

[ citation needed ] Linear codes [ edit ] Main article: Linear code The term algebraic coding theory denotes the sub-field of coding theory where the properties of codes are expressed in algebraic terms and then further researched.

[ citation needed ] Algebraic coding theory is basically divided into two major types of codes: [ citation needed ] Linear block codes Convolutional codes It analyzes the following three properties of a code – mainly: [ citation needed ] Code word length Total number of valid code words The minimum distance between two valid code words, using mainly the Hamming distance , sometimes also other distances like the Lee distance Linear block codes [ edit ] Main article: Block code Linear block codes have the property of linearity , i.e. the sum of any two codewords is also a code word, and they are applied to the source bits in blocks, hence the name linear block codes. There are block codes that are not linear, but it is difficult to prove that a code is a good one without this property.

[ 4 ] Linear block codes are summarized by their symbol alphabets (e.g., binary or ternary) and parameters ( n , m , d min ) [ 5 ] where n is the length of the codeword, in symbols, m is the number of source symbols that will be used for encoding at once, d min is the minimum hamming distance for the code.

There are many types of linear block codes, such as Cyclic codes (e.g., Hamming codes ) Repetition codes Parity codes Polynomial codes (e.g., BCH codes ) Reed–Solomon codes Algebraic geometric codes Reed–Muller codes Perfect codes Locally recoverable code Block codes are tied to the sphere packing problem, which has received some attention over the years. In two dimensions, it is easy to visualize. Take a bunch of pennies flat on the table and push them together. The result is a hexagon pattern like a bee's nest. But block codes rely on more dimensions which cannot easily be visualized. The powerful (24,12) Golay code used in deep space communications uses 24 dimensions. If used as a binary code (which it usually is) the dimensions refer to the length of the codeword as defined above.

The theory of coding uses the N -dimensional sphere model. For example, how many pennies can be packed into a circle on a tabletop, or in 3 dimensions, how many marbles can be packed into a globe. Other considerations enter the choice of a code. For example, hexagon packing into the constraint of a rectangular box will leave empty space at the corners. As the dimensions get larger, the percentage of empty space grows smaller. But at certain dimensions, the packing uses all the space and these codes are the so-called "perfect" codes. The only nontrivial and useful perfect codes are the distance-3 Hamming codes with parameters satisfying (2 r – 1, 2 r – 1 – r , 3), and the [23,12,7] binary and [11,6,5] ternary Golay codes.

[ 4 ] [ 5 ] Another code property is the number of neighbors that a single codeword may have.

[ 6 ] Again, consider pennies as an example. First we pack the pennies in a rectangular grid. Each penny will have 4 near neighbors (and 4 at the corners which are farther away). In a hexagon, each penny will have 6 near neighbors. When we increase the dimensions, the number of near neighbors increases very rapidly.  The result is the number of ways for noise to make the receiver choose a neighbor (hence an error) grows as well. This is a fundamental limitation of block codes, and indeed all codes. It may be harder to cause an error to a single neighbor, but the number of neighbors can be large enough so the total error probability actually suffers.

[ 6 ] Properties of linear block codes are used in many applications.  For example, the syndrome-coset uniqueness property of linear block codes is used in trellis shaping, [ 7 ] one of the best-known shaping codes .

Convolutional codes [ edit ] Main article: Convolutional code The idea behind a convolutional code is to make every codeword symbol be the weighted sum of the various input message symbols. This is like convolution used in LTI systems to find the output of a system, when you know the input and impulse response.

So we generally find the output of the system convolutional encoder, which is the convolution of the input bit, against the states of the convolution encoder, registers.

Fundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. The encoder is usually a simple circuit which has state memory and some feedback logic, normally XOR gates . The decoder can be implemented in software or firmware.

The Viterbi algorithm is the optimum algorithm used to decode convolutional codes. There are simplifications to reduce the computational load. They rely on searching only the most likely paths. Although not optimum, they have generally been found to give good results in low noise environments.

Convolutional codes are used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices.

Cryptographic coding [ edit ] Main article: Cryptography Cryptography or cryptographic coding is the practice and study of techniques for secure communication in the presence of third parties (called adversaries ).

[ 8 ] More generally, it is about constructing and analyzing protocols that block adversaries; [ 9 ] various aspects in information security such as data confidentiality , data integrity , authentication , and non-repudiation [ 10 ] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics , computer science , and electrical engineering . Applications of cryptography include ATM cards , computer passwords , and electronic commerce .

Cryptography prior to the modern age was effectively synonymous with encryption , the conversion of information from a readable state to apparent nonsense . The originator of an encrypted message shared the decoding technique needed to recover the original information only with intended recipients, thereby precluding unwanted persons from doing the same. Since World War I and the advent of the computer , the methods used to carry out cryptology have become increasingly complex and its application more widespread.

Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions , making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad —but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.

Line coding [ edit ] Main article: Line code A line code (also called digital baseband modulation or digital baseband transmission method) is a code chosen for use within a communications system for baseband transmission purposes.

Line coding is often used for digital data transport. It consists of representing the digital signal to be transported by an amplitude- and time-discrete signal that is optimally tuned for the specific properties of the physical channel (and of the receiving equipment). The waveform pattern of voltage or current used to represent the 1s and 0s of a digital data on a transmission link is called line encoding . The common types of line encoding are unipolar , polar , bipolar , and Manchester encoding .

Other applications of coding theory [ edit ] This article contains content that may be misleading to readers .

Please help improve it by clarifying such content. Relevant discussion may be found on the talk page .

( August 2012 ) Another concern of coding theory is designing codes that help synchronization . A code may be designed so that a phase shift can be easily detected and corrected and that multiple signals can be sent on the same channel.

[ citation needed ] Another application of codes, used in some mobile phone systems, is code-division multiple access (CDMA). Each phone is assigned a code sequence that is approximately uncorrelated with the codes of other phones.

[ citation needed ] When transmitting, the code word is used to modulate the data bits representing the voice message. At the receiver, a demodulation process is performed to recover the data. The properties of this class of codes allow many users (with different codes) to use the same radio channel at the same time. To the receiver, the signals of other users will appear to the demodulator only as a low-level noise.

[ citation needed ] Another general class of codes are the automatic repeat-request (ARQ) codes. In these codes the sender adds redundancy to each message for error checking, usually by adding check bits. If the check bits are not consistent with the rest of the message when it arrives, the receiver will ask the sender to retransmit the message. All but the simplest wide area network protocols use ARQ. Common protocols include SDLC (IBM), TCP (Internet), X.25 (International) and many others. There is an extensive field of research on this topic because of the problem of matching a rejected packet against a new packet. Is it a new one or is it a retransmission? Typically numbering schemes are used, as in TCP.

"RFC793" .

RFCS .

Internet Engineering Task Force (IETF). September 1981.

Group testing [ edit ] Group testing uses codes in a different way. Consider a large group of items in which a very few are different in a particular way (e.g., defective products or infected test subjects). The idea of group testing is to determine which items are "different" by using as few tests as possible. The origin of the problem has its roots in the Second World War when the United States Army Air Forces needed to test its soldiers for syphilis .

[ 11 ] Analog coding [ edit ] Information is encoded analogously in the neural networks of brains , in analog signal processing , and analog electronics . Aspects of analog coding include analog error correction, [ 12 ] analog data compression [ 13 ] and analog encryption.

[ 14 ] Neural coding [ edit ] Neural coding is a neuroscience -related field concerned with how sensory and other information is represented in the brain by networks of neurons . The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble.

[ 15 ] It is thought that neurons can encode both digital and analog information, [ 16 ] and that neurons follow the principles of information theory and compress information, [ 17 ] and detect and correct [ 18 ] errors in the signals that are sent throughout the brain and wider nervous system.

See also [ edit ] Telecommunication portal Coding gain Covering code Error correction code Folded Reed–Solomon code Group testing Hamming distance , Hamming weight Lee distance List of algebraic coding theory topics Spatial coding and MIMO in multiple antenna research Spatial diversity coding is spatial coding that transmits replicas of the information signal along different spatial paths, so as to increase the reliability of the data transmission.

Spatial interference cancellation coding Spatial multiplex coding Timeline of information theory, data compression, and error correcting codes Notes [ edit ] ^ James Irvine; David Harle (2002). "2.4.4 Types of Coding".

Data Communications and Networks . John Wiley & Sons. p. 18.

ISBN 9780471808725 .

There are four types of coding ^ Nasir Ahmed .

"How I Came Up With the Discrete Cosine Transform" . Digital Signal Processing, Vol. 1, Iss. 1, 1991, pp. 4-5.

^ Todd Campbell.

"Answer Geek: Error Correction Rule CDs" .

^ a b Terras, Audrey (1999).

Fourier Analysis on Finite Groups and Applications .

Cambridge University Press . p.

195 .

ISBN 978-0-521-45718-7 .

^ a b Blahut, Richard E.

(2003).

Algebraic Codes for Data Transmission . Cambridge University Press.

ISBN 978-0-521-55374-2 .

^ a b Christian Schlegel; Lance Pérez (2004).

Trellis and turbo coding . Wiley-IEEE. p. 73.

ISBN 978-0-471-22755-7 .

^ Forney, G.D. Jr.

(March 1992). "Trellis shaping".

IEEE Transactions on Information Theory .

38 (2 Pt 2): 281– 300.

doi : 10.1109/18.119687 .

S2CID 37984132 .

^ Rivest, Ronald L.

(1990). "Cryptology". In J. Van Leeuwen (ed.).

Handbook of Theoretical Computer Science . Vol. 1. Elsevier.

^ Bellare, Mihir; Rogaway, Phillip (21 September 2005). "Introduction".

Introduction to Modern Cryptography . p. 10.

^ Menezes, A. J.; van Oorschot, P. C.; Vanstone, S. A. (1997).

Handbook of Applied Cryptography . Taylor & Francis.

ISBN 978-0-8493-8523-0 .

^ Dorfman, Robert (1943).

"The detection of defective members of large populations" .

Annals of Mathematical Statistics .

14 (4): 436– 440.

doi : 10.1214/aoms/1177731363 .

^ Chen, Brian; Wornell, Gregory W. (July 1998).

"Analog Error-Correcting Codes Based on Chaotic Dynamical Systems" (PDF) .

IEEE Transactions on Communications .

46 (7): 881– 890.

CiteSeerX 10.1.1.30.4093 .

doi : 10.1109/26.701312 . Archived from the original (PDF) on 2001-09-27 . Retrieved 2013-06-30 .

^ Novak, Franc; Hvala, Bojan; Klavžar, Sandi (1999). "On Analog Signature Analysis".

Proceedings of the conference on Design, automation and test in Europe .

CiteSeerX 10.1.1.142.5853 .

ISBN 1-58113-121-6 .

^ Shujun Li; Chengqing Li; Kwok-Tung Lo; Guanrong Chen (April 2008).

"Cryptanalyzing an Encryption Scheme Based on Blind Source Separation" (PDF) .

IEEE Transactions on Circuits and Systems I .

55 (4): 1055– 63.

arXiv : cs/0608024 .

doi : 10.1109/TCSI.2008.916540 .

S2CID 2224947 .

^ Brown EN, Kass RE, Mitra PP (May 2004).

"Multiple neural spike train data analysis: state-of-the-art and future challenges" (PDF) .

Nature Neuroscience .

7 (5): 456– 461.

doi : 10.1038/nn1228 .

PMID 15114358 .

S2CID 562815 .

^ Thorpe, S.J. (1990).

"Spike arrival times: A highly efficient coding scheme for neural networks" (PDF) . In Eckmiller, R.; Hartmann, G.; Hauske, G.

(eds.).

Parallel processing in neural systems and computers (PDF) . North-Holland. pp.

91– 94.

ISBN 978-0-444-88390-2 . Retrieved 30 June 2013 .

^ Gedeon, T.; Parker, A.E.; Dimitrov, A.G. (Spring 2002).

"Information Distortion and Neural Coding" .

Canadian Applied Mathematics Quarterly .

10 (1): 10.

CiteSeerX 10.1.1.5.6365 . Archived from the original on 2016-11-17 . Retrieved 2013-06-30 .

^ Stiber, M. (July 2005). "Spike timing precision and neural error correction: local behavior".

Neural Computation .

17 (7): 1577– 1601.

arXiv : q-bio/0501021 .

doi : 10.1162/0899766053723069 .

PMID 15901408 .

S2CID 2064645 .

References [ edit ] Elwyn R. Berlekamp (2014), Algebraic Coding Theory , World Scientific Publishing (revised edition), ISBN 978-9-81463-589-9 .

MacKay, David J. C.

Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003.

ISBN 0-521-64298-1 Vera Pless (1982), Introduction to the Theory of Error-Correcting Codes , John Wiley & Sons, Inc., ISBN 0-471-08684-3 .

Randy Yates, A Coding Theory Tutorial .

v t e Industrial and applied mathematics Computational Algorithms design analysis Automata theory Automated theorem proving Coding theory Computational geometry Constraint satisfaction Constraint programming Computational logic Cryptography Information theory Statistics Mathematical software Arbitrary-precision arithmetic Finite element analysis Tensor software Interactive geometry software Optimization software Statistical software Numerical-analysis software Numerical libraries Solvers Discrete Computer algebra Computational number theory Combinatorics Graph theory Discrete geometry Analysis Approximation theory Clifford analysis Clifford algebra Differential equations Ordinary differential equations Partial differential equations Stochastic differential equations Differential geometry Differential forms Gauge theory Geometric analysis Dynamical systems Chaos theory Control theory Functional analysis Operator algebra Operator theory Harmonic analysis Fourier analysis Multilinear algebra Exterior Geometric Tensor Vector Multivariable calculus Exterior Geometric Tensor Vector Numerical analysis Numerical linear algebra Numerical methods for ordinary differential equations Numerical methods for partial differential equations Validated numerics Variational calculus Probability theory Distributions ( random variables ) Stochastic processes / analysis Path integral Stochastic variational calculus Mathematical physics Analytical mechanics Lagrangian Hamiltonian Field theory Classical Conformal Effective Gauge Quantum Statistical Topological Perturbation theory in quantum mechanics Potential theory String theory Bosonic Topological Supersymmetry Supersymmetric quantum mechanics Supersymmetric theory of stochastic dynamics Algebraic structures Algebra of physical space Feynman integral Poisson algebra Quantum group Renormalization group Representation theory Spacetime algebra Superalgebra Supersymmetry algebra Decision sciences Game theory Operations research Optimization Social choice theory Statistics Mathematical economics Mathematical finance Other applications Biology Chemistry Psychology Sociology " The Unreasonable Effectiveness of Mathematics in the Natural Sciences " Related Mathematics Organizations Society for Industrial and Applied Mathematics Japan Society for Industrial and Applied Mathematics Société de Mathématiques Appliquées et Industrielles International Council for Industrial and Applied Mathematics European Community on Computational Methods in Applied Sciences Category Mathematics portal / outline / topics list Authority control databases National Germany United States France BnF data Japan Israel Other Yale LUX NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐77cw5
Cached time: 20250812014818
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.680 seconds
Real time usage: 0.886 seconds
Preprocessor visited node count: 3326/1000000
Revision size: 27857/2097152 bytes
Post‐expand include size: 100195/2097152 bytes
Template argument size: 5207/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 13/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 93414/5000000 bytes
Lua time usage: 0.431/10.000 seconds
Lua memory usage: 7221725/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  659.262      1 -total
 21.00%  138.418      1 Template:Reflist
 19.25%  126.905      8 Template:Cite_journal
 14.00%   92.265      8 Template:Citation_needed
 12.56%   82.785      3 Template:Navbox
 12.44%   81.981      1 Template:Industrial_and_applied_mathematics
 12.38%   81.600      1 Template:Short_description
 11.71%   77.228      8 Template:Fix
 10.97%   72.328      1 Template:Excerpt
  7.72%   50.872      8 Template:Cite_book Saved in parser cache with key enwiki:pcache:321869:|#|:idhash:canonical and timestamp 20250812014818 and revision id 1296324839. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Coding_theory&oldid=1296324839 " Categories : Coding theory Error detection and correction Hidden categories: Articles with short description Short description is different from Wikidata Articles with excerpts All articles with unsourced statements Articles with unsourced statements from July 2008 Misleading articles from August 2012 All misleading articles This page was last edited on 19 June 2025, at 08:44 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Coding theory 35 languages Add topic

