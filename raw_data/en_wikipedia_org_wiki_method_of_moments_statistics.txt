Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Method 2 Advantages and disadvantages 3 Alternative method of moments 4 Examples Toggle Examples subsection 4.1 Proving the central limit theorem 4.2 Uniform distribution 5 See also 6 References Toggle the table of contents Method of moments (statistics) 17 languages العربية Català Deutsch Español Euskara فارسی Français Italiano עברית Lietuvių Magyar Nederlands Русский Simple English Svenska Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Method of moments" statistics – news · newspapers · books · scholar · JSTOR ( June 2020 ) ( Learn how and when to remove this message ) Parameter estimation technique in statistics For the technique used to prove convergence in distribution , see Method of moments (probability theory) .

In statistics , the method of moments is a method of estimation of population parameters . The same principle is used to derive higher moments like skewness and kurtosis .

It starts by expressing the population moments (i.e., the expected values of powers of the random variable under consideration) as functions of the parameters of interest. Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.

The method of moments was introduced by Pafnuty Chebyshev in 1887 in the proof of the central limit theorem . The idea of matching empirical moments of a distribution to the population moments dates back at least to Karl Pearson .

[1] Method [ edit ] Suppose that the parameter θ θ {\displaystyle \theta } = ( θ θ 1 , θ θ 2 , … … , θ θ k {\displaystyle \theta _{1},\theta _{2},\dots ,\theta _{k}} ) characterizes the distribution f W ( w ; θ θ ) {\displaystyle f_{W}(w;\theta )} of the random variable W {\displaystyle W} .

[ 1 ] Suppose the first k {\displaystyle k} moments of the true distribution (the "population moments") can be expressed as functions of the θ θ {\displaystyle \theta } s: μ μ 1 ≡ ≡ E ⁡ ⁡ [ W ] = g 1 ( θ θ 1 , θ θ 2 , … … , θ θ k ) , μ μ 2 ≡ ≡ E ⁡ ⁡ [ W 2 ] = g 2 ( θ θ 1 , θ θ 2 , … … , θ θ k ) , ⋮ ⋮ μ μ k ≡ ≡ E ⁡ ⁡ [ W k ] = g k ( θ θ 1 , θ θ 2 , … … , θ θ k ) .

{\displaystyle {\begin{aligned}\mu _{1}&\equiv \operatorname {E} [W]=g_{1}(\theta _{1},\theta _{2},\ldots ,\theta _{k}),\\[4pt]\mu _{2}&\equiv \operatorname {E} [W^{2}]=g_{2}(\theta _{1},\theta _{2},\ldots ,\theta _{k}),\\&\,\,\,\vdots \\\mu _{k}&\equiv \operatorname {E} [W^{k}]=g_{k}(\theta _{1},\theta _{2},\ldots ,\theta _{k}).\end{aligned}}} Suppose a sample of size n {\displaystyle n} is drawn, resulting in the values w 1 , … … , w n {\displaystyle w_{1},\dots ,w_{n}} . For j = 1 , … … , k {\displaystyle j=1,\dots ,k} , let μ μ ^ ^ j = 1 n ∑ ∑ i = 1 n w i j {\displaystyle {\hat {\mu }}_{j}={\frac {1}{n}}\sum _{i=1}^{n}w_{i}^{j}} be the j -th sample moment, an estimate of μ μ j {\displaystyle \mu _{j}} . The method of moments estimator for θ θ 1 , θ θ 2 , … … , θ θ k {\displaystyle \theta _{1},\theta _{2},\ldots ,\theta _{k}} denoted by θ θ ^ ^ 1 , θ θ ^ ^ 2 , … … , θ θ ^ ^ k {\displaystyle {\hat {\theta }}_{1},{\hat {\theta }}_{2},\dots ,{\hat {\theta }}_{k}} is defined to be the solution (if one exists) to the equations: [2] μ μ ^ ^ 1 = g 1 ( θ θ ^ ^ 1 , θ θ ^ ^ 2 , … … , θ θ ^ ^ k ) , μ μ ^ ^ 2 = g 2 ( θ θ ^ ^ 1 , θ θ ^ ^ 2 , … … , θ θ ^ ^ k ) , ⋮ ⋮ μ μ ^ ^ k = g k ( θ θ ^ ^ 1 , θ θ ^ ^ 2 , … … , θ θ ^ ^ k ) .

{\displaystyle {\begin{aligned}{\hat {\mu }}_{1}&=g_{1}({\hat {\theta }}_{1},{\hat {\theta }}_{2},\ldots ,{\hat {\theta }}_{k}),\\[4pt]{\hat {\mu }}_{2}&=g_{2}({\hat {\theta }}_{1},{\hat {\theta }}_{2},\ldots ,{\hat {\theta }}_{k}),\\&\,\,\,\vdots \\{\hat {\mu }}_{k}&=g_{k}({\hat {\theta }}_{1},{\hat {\theta }}_{2},\ldots ,{\hat {\theta }}_{k}).\end{aligned}}} The method described here for single random variables generalizes in an obvious manner to multiple random variables leading to multiple choices for moments to be used. Different choices generally lead to different solutions.

[ 2 ] [ 3 ] Advantages and disadvantages [ edit ] The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased .

It is an alternative to the method of maximum likelihood .

However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be computed much more quickly and easily. Due to easy computability, method-of-moments estimates may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton–Raphson method .  In this way the method of moments can assist in finding maximum likelihood estimates.

In some cases, infrequent with large samples but less infrequent with small samples, the estimates given by the method of moments are outside of the parameter space (as shown in the example below); it does not make sense to rely on them then.  That problem never arises in the method of maximum likelihood [3] Also, estimates by the method of moments are not necessarily sufficient statistics , i.e., they sometimes fail to take into account all relevant information in the sample.

When estimating other structural parameters (e.g., parameters of a utility function , instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.

Alternative method of moments [ edit ] The equations to be solved in the method of moments (MoM) are in general nonlinear and there are no generally applicable guarantees that tractable solutions exist [ citation needed ] . But there is an alternative approach to using sample moments to estimate data model parameters in terms of known dependence of model moments on these parameters, and this alternative requires the solution of only linear equations or, more generally, tensor equations. This alternative is referred to as the Bayesian-Like MoM (BL-MoM), and it differs from the classical MoM in that it uses optimally weighted sample moments. Considering that the MoM is typically motivated by a lack of sufficient knowledge about the data model to determine likelihood functions and associated a posteriori probabilities of unknown or random parameters, it is odd that there exists a type of MoM that is Bayesian-Like . But the particular meaning of Bayesian-Like leads to a problem formulation in which required knowledge of a posteriori probabilities is replaced with required knowledge of only the dependence of model moments on unknown model parameters, which is exactly the knowledge required by the traditional MoM [1],[2].

[ 3 ] [ 2 ] [ 4 ] [ 5 ] [ 6 ] The BL-MoM also uses knowledge of a priori probabilities of the parameters to be estimated, when available, but otherwise uses uniform priors.

[ citation needed ] The BL-MoM has been reported on in only the applied statistics literature in connection with parameter estimation and hypothesis testing using observations of stochastic processes for problems in Information and Communications Theory and, in particular, communications receiver design in the absence of knowledge of likelihood functions or associated a posteriori probabilities [ 7 ] and references therein. In addition, the restatement of this receiver design approach for stochastic process models as an alternative to the classical MoM for any type of multivariate data is available in tutorial form at the university website.

[ 8 ] The applications in [ 7 ] and references demonstrate some important characteristics of this alternative to the classical MoM, and a detailed list of relative advantages and disadvantages is given in, [ 8 ] but the literature is missing direct comparisons in specific applications of the classical MoM and the BL-MoM.

[ citation needed ] Examples [ edit ] An example application of the method of moments is to estimate polynomial probability density distributions. In this case, an approximating polynomial of order N {\displaystyle N} is defined on an interval [ a , b ] {\displaystyle [a,b]} . The  method of moments then yields a system of equations, whose solution involves the inversion of a Hankel matrix .

[ 9 ] Proving the central limit theorem [ edit ] This section's factual accuracy is disputed .

Relevant discussion may be found on the talk page . Please help to ensure that disputed statements are reliably sourced .

( October 2024 ) ( Learn how and when to remove this message ) Let X 1 , X 2 , ⋯ ⋯ {\displaystyle X_{1},X_{2},\cdots } be independent random variables with mean 0 and variance 1, then let S n := 1 n ∑ ∑ i = 1 n X i {\textstyle S_{n}:={\frac {1}{\sqrt {n}}}\sum _{i=1}^{n}X_{i}} . We can compute the moments of S n {\displaystyle S_{n}} as E ⁡ ⁡ [ S n 0 ] = 1 , E ⁡ ⁡ [ S n 1 ] = 0 , E ⁡ ⁡ [ S n 2 ] = 1 , E ⁡ ⁡ [ S n 3 ] = 0 , … … {\displaystyle {\begin{aligned}\operatorname {E} \left[S_{n}^{0}\right]&=1,&\operatorname {E} \left[S_{n}^{1}\right]&=0,\\[0.5ex]\operatorname {E} \left[S_{n}^{2}\right]&=1,&\operatorname {E} \left[S_{n}^{3}\right]&=0,\dots \end{aligned}}} Explicit expansion shows that E ⁡ ⁡ [ S n 2 k + 1 ] = 0 ; E ⁡ ⁡ [ S n 2 k ] = ( n k ) ( 2 k ) !

2 k n k = n ( n − − 1 ) ⋯ ⋯ ( n − − k + 1 ) n k ( 2 k − − 1 ) !

!

{\displaystyle {\begin{aligned}\operatorname {E} \left[S_{n}^{2k+1}\right]&=0;\\[1ex]\operatorname {E} \left[S_{n}^{2k}\right]&={\frac {{\binom {n}{k}}{\frac {(2k)!}{2^{k}}}}{n^{k}}}\\[0.6ex]&={\frac {n(n-1)\cdots (n-k+1)}{n^{k}}}(2k-1)!!\end{aligned}}} where the numerator is the number of ways to select k {\displaystyle k} distinct pairs of balls by picking one each from 2 k {\displaystyle 2k} buckets, each containing balls numbered from 1 {\displaystyle 1} to n {\displaystyle n} . At the n → → ∞ ∞ {\displaystyle n\to \infty } limit, all moments converge to that of a standard normal distribution. More analysis then show that this convergence in moments imply a convergence in distribution.

Essentially this argument was published by Chebyshev in 1887.

[ 10 ] Uniform distribution [ edit ] Consider the uniform distribution on the interval [ a , b ] {\displaystyle [a,b]} , U ( a , b ) {\displaystyle U(a,b)} . If W ∼ ∼ U ( a , b ) {\displaystyle W\sim U(a,b)} then we have μ μ 1 = E ⁡ ⁡ [ W ] = 1 2 ( a + b ) μ μ 2 = E ⁡ ⁡ [ W 2 ] = 1 3 ( a 2 + a b + b 2 ) {\displaystyle {\begin{aligned}\mu _{1}&=\operatorname {E} \left[W\right]&=&{\tfrac {1}{2}}(a+b)\\[1ex]\mu _{2}&=\operatorname {E} \left[W^{2}\right]&=&{\tfrac {1}{3}}\left(a^{2}+ab+b^{2}\right)\end{aligned}}} Solving these equations gives a ^ ^ = μ μ 1 − − 3 ( μ μ 2 − − μ μ 1 2 ) b ^ ^ = μ μ 1 + 3 ( μ μ 2 − − μ μ 1 2 ) {\displaystyle {\begin{aligned}{\hat {a}}&=\mu _{1}-{\sqrt {3\left(\mu _{2}-\mu _{1}^{2}\right)}}\\{\hat {b}}&=\mu _{1}+{\sqrt {3\left(\mu _{2}-\mu _{1}^{2}\right)}}\end{aligned}}} Given a set of samples { w i } {\displaystyle \{w_{i}\}} we can use the sample moments μ μ ^ ^ 1 {\displaystyle {\hat {\mu }}_{1}} and μ μ ^ ^ 2 {\displaystyle {\hat {\mu }}_{2}} in these formulae in order to estimate a {\displaystyle a} and b {\displaystyle b} .

Note, however, that this method can produce inconsistent results in some cases. For example, the set of samples { 0 , 0 , 0 , 0 , 1 } {\displaystyle \{0,0,0,0,1\}} results in the estimate a ^ ^ = 1 5 ( 1 − − 2 3 ) = − − 0.4928 {\textstyle {\hat {a}}={\frac {1}{5}}\left(1-2{\sqrt {3}}\right)=-0.4928} , b ^ ^ = 1 5 ( 1 + 2 3 ) = 0.8928 {\textstyle {\hat {b}}={\frac {1}{5}}\left(1+2{\sqrt {3}}\right)=0.8928} . Since b ^ ^ < 1 {\displaystyle {\hat {b}}<1} it is impossible for the set { 0 , 0 , 0 , 0 , 1 } {\displaystyle \{0,0,0,0,1\}} to have been drawn from U ( a ^ ^ , b ^ ^ ) {\displaystyle U({\hat {a}},{\hat {b}})} in this case.

See also [ edit ] Generalized method of moments Decoding methods References [ edit ] ^ Kimiko O. Bowman and L. R. Shenton, "Estimator: Method of Moments", pp 2092–2098, Encyclopedia of statistical sciences , Wiley (1998).

^ a b Quandt, R.E. & Ramsey, J.B. (1978). “Estimating mixtures of normal distributions and switching regressions”, Journal of the American Statistical Association 73 , 730–752.

^ a b Lindsay, B.G. & Basak P. (1993). “Multivariate normal mixtures: a fast consistent method of moments”, Journal of the American Statistical Association 88 , 468–476.

^ https://real-statistics.com/distribution-fitting/method-of-moments/ ^ Hansen, L. (1982). “Large sample properties of generalized method of moments estimators”, Econometrica 50 , 1029–1054.

^ Lindsay, B.G. (1982). “Conditional score functions: some optimality results”, Biometrika 69 , 503–512.

^ a b Gardner, W.A., “Design of nearest prototype signal classifiers”, IEEE Transactions on Information Theory 27 (3), 368–372,1981 ^ a b Cyclostationarity , page 11.4 ^ J. Munkhammar, L. Mattsson, J. Rydén (2017) "Polynomial probability distribution estimation using the method of moments". PLoS ONE 12(4): e0174573.

https://doi.org/10.1371/journal.pone.0174573 ^ Fischer, Hans (2011). "4. Chebyshev's and Markov's Contributions".

History of the central limit theorem : from classical to modern probability theory . New York: Springer.

ISBN 978-0-387-87857-7 .

OCLC 682910965 .

NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐q8bzf
Cached time: 20250812021431
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.274 seconds
Real time usage: 0.468 seconds
Preprocessor visited node count: 1434/1000000
Revision size: 12322/2097152 bytes
Post‐expand include size: 28759/2097152 bytes
Template argument size: 3098/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 15433/5000000 bytes
Lua time usage: 0.148/10.000 seconds
Lua memory usage: 4076644/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  288.874      1 -total
 31.58%   91.233      1 Template:Reflist
 26.64%   76.961      1 Template:Cite_book
 25.39%   73.357      1 Template:More_citations
 21.71%   62.712      2 Template:Ambox
 19.90%   57.485      1 Template:Short_description
 11.12%   32.109      3 Template:Citation_needed
 11.10%   32.071      2 Template:Pagetype
  8.94%   25.830      3 Template:Fix
  5.84%   16.859      1 Template:For Saved in parser cache with key enwiki:pcache:2175504:|#|:idhash:canonical and timestamp 20250812021431 and revision id 1301134644. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Method_of_moments_(statistics)&oldid=1301134644 " Categories : Probability distribution fitting Moments (mathematics) Hidden categories: Articles needing additional references from June 2020 All articles needing additional references Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from August 2023 Accuracy disputes from October 2024 All accuracy disputes This page was last edited on 18 July 2025, at 07:30 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Method of moments (statistics) 17 languages Add topic

