Title: Degrees of freedom (statistics)

URL Source: https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)

Published Time: 2005-07-27T19:23:40Z

Markdown Content:
In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the number of **degrees of freedom** is the number of values in the final calculation of a [statistic](https://en.wikipedia.org/wiki/Statistic "Statistic") that are free to vary.[[1]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-1)

Estimates of [statistical parameters](https://en.wikipedia.org/wiki/Statistical_parameter "Statistical parameter") can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent [scores](https://en.wikipedia.org/wiki/Realization_(probability) "Realization (probability)") that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself. For example, if the [variance](https://en.wikipedia.org/wiki/Variance "Variance") is to be estimated from a random sample of ![Image 1: {\textstyle N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0d21d55fc102ec49600d3d5522a59ae4561acc22) independent scores, then the degrees of freedom is equal to the number of independent scores (_N_) minus the number of parameters estimated as intermediate steps (one, namely, the sample mean) and is therefore equal to ![Image 2: {\textstyle N-1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8ef96563e7dd934ca5dbf2bff24fa1e17c987a4f).[[2]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-2)

Mathematically, degrees of freedom is the number of [dimensions](https://en.wikipedia.org/wiki/Dimension "Dimension") of the domain of a [random vector](https://en.wikipedia.org/wiki/Random_vector "Random vector"), or essentially the number of "free" components (how many components need to be known before the vector is fully determined).

The term is most often used in the context of [linear models](https://en.wikipedia.org/wiki/Linear_models "Linear models") ([linear regression](https://en.wikipedia.org/wiki/Linear_regression "Linear regression"), [analysis of variance](https://en.wikipedia.org/wiki/Analysis_of_variance "Analysis of variance")), where certain random vectors are constrained to lie in [linear subspaces](https://en.wikipedia.org/wiki/Linear_subspace "Linear subspace"), and the number of degrees of freedom is the dimension of the [subspace](https://en.wikipedia.org/wiki/Linear_subspace "Linear subspace"). The degrees of freedom are also commonly associated with the squared lengths (or "sum of squares" of the coordinates) of such vectors, and the parameters of [chi-squared](https://en.wikipedia.org/wiki/Chi-squared_distribution "Chi-squared distribution") and other distributions that arise in associated statistical testing problems.

While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept.

Although the basic concept of degrees of freedom was recognized as early as 1821 in the work of German astronomer and mathematician [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss "Carl Friedrich Gauss"),[[3]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-3) its modern definition and usage was first elaborated by English statistician [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset "William Sealy Gosset") in his 1908 _[Biometrika](https://en.wikipedia.org/wiki/Biometrika "Biometrika")_ article "The Probable Error of a Mean", published under the pen name "Student".[[4]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-4) While Gosset did not actually use the term 'degrees of freedom', he explained the concept in the course of developing what became known as [Student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution "Student's t-distribution"). The term itself was popularized by English statistician and biologist [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher "Ronald Fisher"), beginning with his 1922 work on chi squares.[[5]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-5)

In equations, the typical symbol for degrees of freedom is _ν_ (lowercase [Greek letter nu](https://en.wikipedia.org/wiki/Nu_(letter) "Nu (letter)")). In text and tables, the abbreviation "d.f." is commonly used. [R. A. Fisher](https://en.wikipedia.org/wiki/Ronald_A._Fisher "Ronald A. Fisher") used _n_ to symbolize degrees of freedom but modern usage typically reserves _n_ for sample size. When reporting the results of [statistical tests](https://en.wikipedia.org/wiki/Statistical_hypothesis_test "Statistical hypothesis test"), the degrees of freedom are typically noted beside the [test statistic](https://en.wikipedia.org/wiki/Test_statistic "Test statistic") as either subscript or in parentheses.[[6]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-6)

Geometrically, the degrees of freedom can be interpreted as the dimension of certain vector subspaces. As a starting point, suppose that we have a sample of independent normally distributed observations,

![Image 3: {\displaystyle X_{1},\dots ,X_{n}.\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2935ee8cb387ec8229625597827aa98a1b7e5a78)
This can be represented as an _n_-dimensional [random vector](https://en.wikipedia.org/wiki/Random_vector "Random vector"):

![Image 4: {\displaystyle {\begin{pmatrix}X_{1}\\\vdots \\X_{n}\end{pmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fa97eac6efd0f31c9aaa6449faa3d724b762566c)
Since this random vector can lie anywhere in _n_-dimensional space, it has _n_ degrees of freedom.

Now, let ![Image 5: {\displaystyle {\bar {X}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/90b968141b314f4de17f5e63f18dcdc126352bac) be the [sample mean](https://en.wikipedia.org/wiki/Sample_mean "Sample mean"). The random vector can be decomposed as the sum of the sample mean plus a vector of residuals:

![Image 6: {\displaystyle {\begin{pmatrix}X_{1}\\\vdots \\X_{n}\end{pmatrix}}={\bar {X}}{\begin{pmatrix}1\\\vdots \\1\end{pmatrix}}+{\begin{pmatrix}X_{1}-{\bar {X}}\\\vdots \\X_{n}-{\bar {X}}\end{pmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/31b5a7372cded5d3da1f2c3304f5e21c0068b609)
The first vector on the right-hand side is constrained to be a multiple of the vector of 1's, and the only free quantity is ![Image 7: {\displaystyle {\bar {X}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/90b968141b314f4de17f5e63f18dcdc126352bac). It therefore has 1 degree of freedom.

The second vector is constrained by the relation ![Image 8: {\textstyle \sum _{i=1}^{n}(X_{i}-{\bar {X}})=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2d191749cbfc98e84744d2a406f2b5c934afa126). The first _n_−1 components of this vector can be anything. However, once you know the first _n_−1 components, the constraint tells you the value of the _n_ th component. Therefore, this vector has _n_−1 degrees of freedom.

Mathematically, the first vector is the [oblique projection](https://en.wikipedia.org/wiki/Oblique_projection "Oblique projection") of the data vector onto the [subspace](https://en.wikipedia.org/wiki/Euclidean_subspace "Euclidean subspace")[spanned](https://en.wikipedia.org/wiki/Linear_span "Linear span") by the vector of 1's. The 1 degree of freedom is the dimension of this subspace. The second residual vector is the least-squares projection onto the (_n_−1)-dimensional [orthogonal complement](https://en.wikipedia.org/wiki/Orthogonal_complement "Orthogonal complement") of this subspace, and has _n_−1 degrees of freedom.

In statistical testing applications, often one is not directly interested in the component vectors, but rather in their squared lengths. In the example above, the [residual sum-of-squares](https://en.wikipedia.org/wiki/Residual_sum-of-squares "Residual sum-of-squares") is

![Image 9: {\displaystyle \sum _{i=1}^{n}(X_{i}-{\bar {X}})^{2}={\begin{Vmatrix}X_{1}-{\bar {X}}\\\vdots \\X_{n}-{\bar {X}}\end{Vmatrix}}^{2}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c044de90a2079fc16ab8d32bc5c761f9b5f1bfc8)
If the data points ![Image 10: {\displaystyle X_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/af4a0955af42beb5f85aa05fb8c07abedc13990d) are normally distributed with mean 0 and variance ![Image 11: {\displaystyle \sigma ^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5c55e536acf250c1d3e0f754be5692b843ef5), then the residual sum of squares has a scaled [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution "Chi-squared distribution") (scaled by the factor ![Image 12: {\displaystyle \sigma ^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5c55e536acf250c1d3e0f754be5692b843ef5)), with _n_−1 degrees of freedom. The degrees-of-freedom, here a parameter of the distribution, can still be interpreted as the dimension of an underlying vector subspace.

Likewise, the one-sample [_t_-test](https://en.wikipedia.org/wiki/T-test "T-test") statistic,

![Image 13: {\displaystyle {\frac {{\sqrt {n}}({\bar {X}}-\mu _{0})}{\sqrt {\sum \limits _{i=1}^{n}(X_{i}-{\bar {X}})^{2}/(n-1)}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c05fac41ce6d62cf37224a9d312b7463cd29219)
follows a [Student's t](https://en.wikipedia.org/wiki/Student%27s_t "Student's t") distribution with _n_−1 degrees of freedom when the hypothesized mean ![Image 14: {\displaystyle \mu _{0}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fe2fd9b8decb38a3cd158e7b6c0c6e2d987fefcc) is correct. Again, the degrees-of-freedom arises from the residual vector in the denominator.

In structural equation models
-----------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=4 "Edit section: In structural equation models")]

When the results of structural equation models (SEM) are presented, they generally include one or more indices of overall model fit, the most common of which is a _χ_ 2 statistic. This forms the basis for other indices that are commonly reported. Although it is these other statistics that are most commonly interpreted, the _degrees of freedom_ of the _χ_ 2 are essential to understanding model fit as well as the nature of the model itself.

Degrees of freedom in SEM are computed as a difference between the number of unique pieces of information that are used as input into the analysis, sometimes called knowns, and the number of parameters that are uniquely estimated, sometimes called unknowns. For example, in a one-factor confirmatory factor analysis with 4 items, there are 10 knowns (the six unique covariances among the four items and the four item variances) and 8 unknowns (4 factor loadings and 4 error variances) for 2 degrees of freedom. Degrees of freedom are important to the understanding of model fit if for no other reason than that, all else being equal, the fewer degrees of freedom, the better indices such as _χ_ 2 will be.

It has been shown that degrees of freedom can be used by readers of papers that contain SEMs to determine if the authors of those papers are in fact reporting the correct model fit statistics. In the organizational sciences, for example, nearly half of papers published in top journals report degrees of freedom that are inconsistent with the models described in those papers, leaving the reader to wonder which models were actually tested.[[7]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-7)

A common way to think of degrees of freedom is as the number of independent pieces of information available to estimate another piece of information. More concretely, the number of degrees of freedom is the number of independent observations in a sample of data that are available to estimate a parameter of the population from which that sample is drawn. For example, if we have two observations, when calculating the mean we have two independent observations; however, when calculating the variance, we have only one independent observation, since the two observations are equally distant from the sample mean.

In fitting statistical models to data, the vectors of residuals are constrained to lie in a space of smaller dimension than the number of components in the vector. That smaller dimension is the number of _degrees of freedom for error_, also called _residual degrees of freedom_.

Perhaps the simplest example is this. Suppose

![Image 15: {\displaystyle X_{1},\dots ,X_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/38ed92ce88f900210607bbb8f4d66e14d52d7a17)
are [random variables](https://en.wikipedia.org/wiki/Random_variable "Random variable") each with [expected value](https://en.wikipedia.org/wiki/Expected_value "Expected value")_μ_, and let

![Image 16: {\displaystyle {\overline {X}}_{n}={\frac {X_{1}+\cdots +X_{n}}{n}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4daaa75fa616da2130e0be0ea62e6310f4884ed)
be the "sample mean." Then the quantities

![Image 17: {\displaystyle X_{i}-{\overline {X}}_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/00590f6d4dc67a1248ca5cea85aa08c8207eced0)
are residuals that may be considered [estimates](https://en.wikipedia.org/wiki/Estimation_theory "Estimation theory") of the [errors](https://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics "Errors and residuals in statistics")_X_ _i_−_μ_. The sum of the residuals (unlike the sum of the errors) is necessarily 0. If one knows the values of any _n_−1 of the residuals, one can thus find the last one. That means they are constrained to lie in a space of dimension _n_−1. One says that there are _n_−1 degrees of freedom for errors.

An example which is only slightly less simple is that of [least squares](https://en.wikipedia.org/wiki/Least_squares "Least squares") estimation of _a_ and _b_ in the model

![Image 18: {\displaystyle Y_{i}=a+bx_{i}+e_{i}{\text{ for }}i=1,\dots ,n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ccf2550085550debe5deae986d75a54e867168cb)
where _x_ _i_ is given, but e _i_ and hence _Y_ _i_ are random. Let ![Image 19: {\displaystyle {\widehat {a}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f313e02d8b4119b2bc9e6dda9e50a4c223dba5ed) and ![Image 20: {\displaystyle {\widehat {b}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/23f59992ebb96e304d692e0b6399d82c73945ddd) be the least-squares estimates of _a_ and _b_. Then the residuals

![Image 21: {\displaystyle {\widehat {e}}_{i}=y_{i}-({\widehat {a}}+{\widehat {b}}x_{i})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fd2826cee765a02b49e0959226a92d2111d19440)
are constrained to lie within the space defined by the two equations

![Image 22: {\displaystyle {\widehat {e}}_{1}+\cdots +{\widehat {e}}_{n}=0,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/914585e07679e615a62af3ce3e5ade6604784492)![Image 23: {\displaystyle x_{1}{\widehat {e}}_{1}+\cdots +x_{n}{\widehat {e}}_{n}=0.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1d70f9b661f6b350bccdefbd624fb312fe2b1d5d)
One says that there are _n_−2 degrees of freedom for error.

Notationally, the capital letter _Y_ is used in specifying the model, while lower-case _y_ in the definition of the residuals; that is because the former are hypothesized random variables and the latter are actual data.

We can generalise this to multiple regression involving _p_ parameters and covariates (e.g. _p_−1 predictors and one mean (=intercept in the regression)), in which case the cost in _degrees of freedom of the fit_ is _p_, leaving _n - p_ degrees of freedom for errors

The demonstration of the _t_ and chi-squared distributions for one-sample problems above is the simplest example where degrees-of-freedom arise. However, similar geometry and vector decompositions underlie much of the theory of [linear models](https://en.wikipedia.org/wiki/Linear_models "Linear models"), including [linear regression](https://en.wikipedia.org/wiki/Linear_regression "Linear regression") and [analysis of variance](https://en.wikipedia.org/wiki/Analysis_of_variance "Analysis of variance"). An explicit example based on comparison of three means is presented here; the geometry of linear models is discussed in more complete detail by Christensen (2002).[[8]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-8)

Suppose independent observations are made for three populations, ![Image 24: {\displaystyle X_{1},\ldots ,X_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac794f5521dcce89913085a6d566e7cdb615dbb0), ![Image 25: {\displaystyle Y_{1},\ldots ,Y_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1df095625cb0644ba7ed0c6a0cb2812fa210bd61) and ![Image 26: {\displaystyle Z_{1},\ldots ,Z_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e581860b7dadf563e3b50e7fbc851c2b25e032d8). The restriction to three groups and equal sample sizes simplifies notation, but the ideas are easily generalized.

The observations can be decomposed as

![Image 27: {\displaystyle {\begin{aligned}X_{i}&={\bar {M}}+({\bar {X}}-{\bar {M}})+(X_{i}-{\bar {X}})\\Y_{i}&={\bar {M}}+({\bar {Y}}-{\bar {M}})+(Y_{i}-{\bar {Y}})\\Z_{i}&={\bar {M}}+({\bar {Z}}-{\bar {M}})+(Z_{i}-{\bar {Z}})\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/05921f9e1feb6525d081e2a6318340502ba772c9)
where ![Image 28: {\displaystyle {\bar {X}},{\bar {Y}},{\bar {Z}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/49f23f3c27ee32519499c4df8ed73b0ae2446d24) are the means of the individual samples, and ![Image 29: {\displaystyle {\bar {M}}=({\bar {X}}+{\bar {Y}}+{\bar {Z}})/3}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7764ca5edf006cb1293309a523af44649c08adfa) is the mean of all 3 _n_ observations. In [vector notation](https://en.wikipedia.org/wiki/Vector_notation "Vector notation") this decomposition can be written as

![Image 30: {\displaystyle {\begin{pmatrix}X_{1}\\\vdots \\X_{n}\\Y_{1}\\\vdots \\Y_{n}\\Z_{1}\\\vdots \\Z_{n}\end{pmatrix}}={\bar {M}}{\begin{pmatrix}1\\\vdots \\1\\1\\\vdots \\1\\1\\\vdots \\1\end{pmatrix}}+{\begin{pmatrix}{\bar {X}}-{\bar {M}}\\\vdots \\{\bar {X}}-{\bar {M}}\\{\bar {Y}}-{\bar {M}}\\\vdots \\{\bar {Y}}-{\bar {M}}\\{\bar {Z}}-{\bar {M}}\\\vdots \\{\bar {Z}}-{\bar {M}}\end{pmatrix}}+{\begin{pmatrix}X_{1}-{\bar {X}}\\\vdots \\X_{n}-{\bar {X}}\\Y_{1}-{\bar {Y}}\\\vdots \\Y_{n}-{\bar {Y}}\\Z_{1}-{\bar {Z}}\\\vdots \\Z_{n}-{\bar {Z}}\end{pmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/66c2d1ee5a6357d35d5430b42c239a102ca140b7)
The observation vector, on the left-hand side, has 3 _n_ degrees of freedom. On the right-hand side, the first vector has one degree of freedom (or dimension) for the overall mean. The second vector depends on three random variables, ![Image 31: {\displaystyle {\bar {X}}-{\bar {M}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/62d3180dafb6a62ffc2af2e4861c3afbe9266f74), ![Image 32: {\displaystyle {\bar {Y}}-{\bar {M}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e9b369f411c49fbc11406b2f51a3923643426d6b) and ![Image 33: {\displaystyle {\overline {Z}}-{\overline {M}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e44e83990aa3c96611dd3fe39662f791c8260bc9). However, these must sum to 0 and so are constrained; the vector therefore must lie in a 2-dimensional subspace, and has 2 degrees of freedom. The remaining 3 _n_−3 degrees of freedom are in the residual vector (made up of _n_−1 degrees of freedom within each of the populations).

In analysis of variance (ANOVA)
-------------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=8 "Edit section: In analysis of variance (ANOVA)")]

In statistical testing problems, one usually is not interested in the component vectors themselves, but rather in their squared lengths, or Sum of Squares. The degrees of freedom associated with a sum-of-squares is the degrees-of-freedom of the corresponding component vectors.

The three-population example above is an example of [one-way Analysis of Variance](https://en.wikipedia.org/wiki/One-way_ANOVA "One-way ANOVA"). The model, or treatment, sum-of-squares is the squared length of the second vector,

![Image 34: {\displaystyle {\text{SST}}=n({\bar {X}}-{\bar {M}})^{2}+n({\bar {Y}}-{\bar {M}})^{2}+n({\bar {Z}}-{\bar {M}})^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/dbe6a9524348395dcdac14fa655f705baf2adc87)
with 2 degrees of freedom. The residual, or error, sum-of-squares is

![Image 35: {\displaystyle {\text{SSE}}=\sum _{i=1}^{n}\left[(X_{i}-{\bar {X}})^{2}+(Y_{i}-{\bar {Y}})^{2}+(Z_{i}-{\bar {Z}})^{2}\right]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0442b8c8b9f4961b809a604bb5f5dc03a429fbf3)
with 3(_n_−1) degrees of freedom. Of course, introductory books on ANOVA usually state formulae without showing the vectors, but it is this underlying geometry that gives rise to SS formulae, and shows how to unambiguously determine the degrees of freedom in any given situation.

Under the null hypothesis of no difference between population means (and assuming that standard ANOVA regularity assumptions are satisfied) the sums of squares have scaled chi-squared distributions, with the corresponding degrees of freedom. The F-test statistic is the ratio, after scaling by the degrees of freedom. If there is no difference between population means this ratio follows an [_F_-distribution](https://en.wikipedia.org/wiki/F-distribution "F-distribution") with 2 and 3 _n_−3 degrees of freedom.

In some complicated settings, such as unbalanced [split-plot](https://en.wikipedia.org/wiki/Split-plot "Split-plot") designs, the sums-of-squares no longer have scaled chi-squared distributions. Comparison of sum-of-squares with degrees-of-freedom is no longer meaningful, and software may report certain fractional 'degrees of freedom' in these cases. Such numbers have no genuine degrees-of-freedom interpretation, but are simply providing an _approximate_ chi-squared distribution for the corresponding sum-of-squares. The details of such approximations are beyond the scope of this page.

In probability distributions
----------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=9 "Edit section: In probability distributions")]

Several commonly encountered statistical distributions ([Student's _t_](https://en.wikipedia.org/wiki/Student%27s_t_distribution "Student's t distribution"), [chi-squared](https://en.wikipedia.org/wiki/Chi-squared_distribution "Chi-squared distribution"), [_F_](https://en.wikipedia.org/wiki/F-distribution "F-distribution")) have parameters that are commonly referred to as _degrees of freedom_. This terminology simply reflects that in many applications where these distributions occur, the parameter corresponds to the degrees of freedom of an underlying random vector, as in the preceding ANOVA example. Another simple example is: if ![Image 36: {\displaystyle X_{i};i=1,\ldots ,n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a649c46649de9f0706df7e8a1614c6c36c9db4ea) are independent normal ![Image 37: {\displaystyle (\mu ,\sigma ^{2})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/47e17934b7de386b1a76ae4064223cc6177ce794) random variables, the statistic

![Image 38: {\displaystyle {\frac {\sum _{i=1}^{n}(X_{i}-{\bar {X}})^{2}}{\sigma ^{2}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/84414d88610065271212d774fcf91efc232368dd)
follows a chi-squared distribution with _n_−1 degrees of freedom. Here, the degrees of freedom arises from the residual sum-of-squares in the numerator, and in turn the _n_−1 degrees of freedom of the underlying residual vector ![Image 39: {\displaystyle \{X_{i}-{\bar {X}}\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/811bf9a5af0d7f03edf6babd9c671cdd9b374e28).

In the application of these distributions to linear models, the degrees of freedom parameters can take only [integer](https://en.wikipedia.org/wiki/Integer "Integer") values. The underlying families of distributions allow fractional values for the degrees-of-freedom parameters, which can arise in more sophisticated uses. One set of examples is problems where chi-squared approximations based on [effective degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#Effective_degrees_of_freedom) are used. In other applications, such as modelling [heavy-tailed](https://en.wikipedia.org/wiki/Heavy_tailed_distribution "Heavy tailed distribution") data, a t or _F_-distribution may be used as an empirical model. In these cases, there is no particular _degrees of freedom_ interpretation to the distribution parameters, even though the terminology may continue to be used.

In non-standard regression
--------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=10 "Edit section: In non-standard regression")]

Many non-standard regression methods, including [regularized least squares](https://en.wikipedia.org/wiki/Regularized_least_squares "Regularized least squares") (e.g., [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression "Ridge regression")), [linear smoothers](https://en.wikipedia.org/wiki/Smoothing#Linear_smoothers "Smoothing"), [smoothing splines](https://en.wikipedia.org/wiki/Smoothing_splines "Smoothing splines"), and [semiparametric regression](https://en.wikipedia.org/wiki/Semiparametric_regression "Semiparametric regression"), are not based on [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares "Ordinary least squares") projections, but rather on [regularized](https://en.wikipedia.org/wiki/Regularization_(mathematics) "Regularization (mathematics)") ([generalized](https://en.wikipedia.org/wiki/Generalized_least_squares "Generalized least squares") and/or penalized) least-squares, and so degrees of freedom defined in terms of dimensionality is generally not useful for these procedures. However, these procedures are still linear in the observations, and the fitted values of the regression can be expressed in the form

![Image 40: {\displaystyle {\hat {y}}=Hy,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c4ff1888333018019a6bad84bdf575061c63f7d1)
where ![Image 41: {\displaystyle {\hat {y}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3dc8de3d8ea01304329ef9518fad7a6d196c4c01) is the vector of fitted values at each of the original covariate values from the fitted model, _y_ is the original vector of responses, and _H_ is the [hat matrix](https://en.wikipedia.org/wiki/Hat_matrix "Hat matrix") or, more generally, smoother matrix.

For statistical inference, sums-of-squares can still be formed: the model sum-of-squares is ![Image 42: {\displaystyle \|Hy\|^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a42ee41fa288732ef75dfcba26d44e220e98529a); the residual sum-of-squares is ![Image 43: {\displaystyle \|y-Hy\|^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b92dd7b690d13c8a8de6d7698850567eb1d17658). However, because _H_ does not correspond to an ordinary least-squares fit (i.e. is not an orthogonal projection), these sums-of-squares no longer have (scaled, non-central) chi-squared distributions, and dimensionally defined degrees-of-freedom are not useful.

The _effective degrees of freedom_ of the fit can be defined in various ways to implement [goodness-of-fit tests](https://en.wikipedia.org/wiki/Goodness-of-fit_test "Goodness-of-fit test"), [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) "Cross-validation (statistics)"), and other [statistical inference](https://en.wikipedia.org/wiki/Statistical_inference "Statistical inference") procedures. Here one can distinguish between _regression effective degrees of freedom_ and _residual effective degrees of freedom_.

### Regression effective degrees of freedom

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=11 "Edit section: Regression effective degrees of freedom")]

For the regression effective degrees of freedom, appropriate definitions can include the [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra) "Trace (linear algebra)") of the hat matrix,[[9]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-9) tr(_H_), the trace of the [quadratic form](https://en.wikipedia.org/wiki/Quadratic_form "Quadratic form") of the hat matrix, tr(_H'H_), the form tr(2 _H_ – _H_ _H'_), or the [Satterthwaite approximation](https://en.wikipedia.org/wiki/Welch-Satterthwaite_equation "Welch-Satterthwaite equation"), tr(_H'H_)2/tr(_H'HH'H_).[[10]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-Fox_Sage_Publications_SAGE._2000_p._58-10) In the case of linear regression, the hat matrix _H_ is _X_(_X_'_X_)−1 _X'_, and all these definitions reduce to the usual degrees of freedom. Notice that

![Image 44: {\displaystyle \operatorname {tr} (H)=\sum _{i}h_{ii}=\sum _{i}{\frac {\partial {\hat {y}}_{i}}{\partial y_{i}}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/23ea8c41c5b543ec7b8f94babc559ab26e52fd02)
the regression (not residual) degrees of freedom in linear models are "the sum of the sensitivities of the fitted values with respect to the observed response values",[[11]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-11) i.e. the sum of [leverage scores](https://en.wikipedia.org/wiki/Leverage_score "Leverage score").

One way to help to conceptualize this is to consider a simple smoothing matrix like a [Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur "Gaussian blur"), used to mitigate data noise. In contrast to a simple linear or polynomial fit, computing the effective degrees of freedom of the smoothing function is not straightforward. In these cases, it is important to estimate the Degrees of Freedom permitted by the ![Image 45: {\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) matrix so that the residual degrees of freedom can then be used to estimate statistical tests such as ![Image 46: {\displaystyle \chi ^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8c0cc9237ec72a1da6d18bc8e7fb24cdda43a49a).

### Residual effective degrees of freedom

[[edit](https://en.wikipedia.org/w/index.php?title=Degrees_of_freedom_(statistics)&action=edit&section=12 "Edit section: Residual effective degrees of freedom")]

There are corresponding definitions of residual effective degrees-of-freedom (redf), with _H_ replaced by _I_−_H_. For example, if the goal is to estimate error variance, the redf would be defined as tr((_I_−_H_)'(_I_−_H_)), and the unbiased estimate is (with ![Image 47: {\displaystyle {\hat {r}}=y-Hy}](https://wikimedia.org/api/rest_v1/media/math/render/svg/077b26c44e31a317277b2ffb675eaf29b3598d10)),

![Image 48: {\displaystyle {\hat {\sigma }}^{2}={\frac {\|{\hat {r}}\|^{2}}{\operatorname {tr} \left((I-H)'(I-H)\right)}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b5d87168801cce551c8d777b18298351911359e4)
or:[[12]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-12)[[13]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-Hastie1990-13)[[14]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-Wood2006-14)[[15]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-15)

![Image 49: {\displaystyle {\hat {\sigma }}^{2}={\frac {\|{\hat {r}}\|^{2}}{n-\operatorname {tr} (2H-HH')}}={\frac {\|{\hat {r}}\|^{2}}{n-2\operatorname {tr} (H)+\operatorname {tr} (HH')}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/093c3b519d87e0b16539f1d12aa3210430fa721b)![Image 50: {\displaystyle {\hat {\sigma }}^{2}\approx {\frac {\|{\hat {r}}\|^{2}}{n-1.25\operatorname {tr} (H)+0.5}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fbe37d537d6284bf986ff58e6dab5390a2212589)
The last approximation above[[13]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-Hastie1990-13) reduces the computational cost from _O_(_n_ 2) to only _O_(_n_). In general the numerator would be the objective function being minimized; e.g., if the hat matrix includes an observation covariance matrix, Σ, then ![Image 51: {\displaystyle \|{\hat {r}}\|^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6460be42964cf72752ef0343a5d6797eea6add31) becomes ![Image 52: {\displaystyle {\hat {r}}'\Sigma ^{-1}{\hat {r}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/440f10fbf05e605993f61e8074cb92fb4a088802).

Note that unlike in the original case, non-integer degrees of freedom are allowed, though the value must usually still be constrained between 0 and _n_.[[16]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-16)

Consider, as an example, the _k_-[nearest neighbour](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm "K-nearest neighbor algorithm") smoother, which is the average of the _k_ nearest measured values to the given point. Then, at each of the _n_ measured points, the weight of the original value on the linear combination that makes up the predicted value is just 1/_k_. Thus, the trace of the hat matrix is _n/k_. Thus the smooth costs _n/k_ effective degrees of freedom.

As another example, consider the existence of nearly duplicated observations. Naive application of classical formula, _n_ − _p_, would lead to over-estimation of the residuals degree of freedom, as if each observation were independent. More realistically, though, the hat matrix _H_ = _X_(_X_' Σ−1 _X_)−1 _X'_ Σ−1 would involve an observation covariance matrix Σ indicating the non-zero correlation among observations.

The more general formulation of effective degree of freedom would result in a more realistic estimate for, e.g., the error variance σ 2, which in its turn scales the unknown parameters' _a posteriori_ standard deviation; the degree of freedom will also affect the expansion factor necessary to produce an [error ellipse](https://en.wikipedia.org/wiki/Error_ellipse "Error ellipse") for a given [confidence level](https://en.wikipedia.org/wiki/Confidence_level "Confidence level").

Similar concepts are the _equivalent degrees of freedom_ in [non-parametric regression](https://en.wikipedia.org/wiki/Non-parametric_regression "Non-parametric regression"),[[17]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-17) the _degree of freedom of signal_ in atmospheric studies,[[18]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-18)[[19]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-19) and the _non-integer degree of freedom_ in geodesy.[[20]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-20)[[21]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-21)

The residual sum-of-squares ![Image 53: {\displaystyle \|y-Hy\|^{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b92dd7b690d13c8a8de6d7698850567eb1d17658) has a [generalized chi-squared distribution](https://en.wikipedia.org/wiki/Generalized_chi-squared_distribution "Generalized chi-squared distribution"), and the theory associated with this distribution[[22]](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_note-Jones1-22) provides an alternative route to the answers provided above.[_[further explanation needed](https://en.wikipedia.org/wiki/Wikipedia:Please\_clarify "Wikipedia:Please clarify")_]

*   [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction "Bessel's correction")
*   [Chi-squared per degree of freedom](https://en.wikipedia.org/wiki/Chi-squared_per_degree_of_freedom "Chi-squared per degree of freedom")
*   [Pooled degrees of freedom](https://en.wikipedia.org/wiki/Pooled_degrees_of_freedom "Pooled degrees of freedom")
*   [Replication (statistics)](https://en.wikipedia.org/wiki/Replication_(statistics) "Replication (statistics)")
*   [Sample size](https://en.wikipedia.org/wiki/Sample_size "Sample size")
*   [Statistical model](https://en.wikipedia.org/wiki/Statistical_model "Statistical model")
*   [Variance](https://en.wikipedia.org/wiki/Variance "Variance")

1.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-1)**["Degrees of Freedom"](http://www.animatedsoftware.com/statglos/sgdegree.htm). _Glossary of Statistical Terms_. Animated Software. Retrieved 2008-08-21.
2.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-2)**Lane, David M. ["Degrees of Freedom"](https://davidmlane.com/hyperstat/A42408.html). _HyperStat Online_. Statistics Solutions. Retrieved 2008-08-21.
3.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-3)**Walker, H. M. (April 1940). ["Degrees of Freedom"](http://www.nohsteachers.info/pcaso/ap_statistics/PDFs/DegreesOfFreedom.pdf)(PDF). _Journal of Educational Psychology_. **31** (4): 253–269. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1037/h0054588](https://doi.org/10.1037%2Fh0054588).
4.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-4)**Student (March 1908). ["The Probable Error of a Mean"](https://zenodo.org/record/1449458). _Biometrika_. **6** (1): 1–25. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.2307/2331554](https://doi.org/10.2307%2F2331554). [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)")[2331554](https://www.jstor.org/stable/2331554).
5.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-5)**Fisher, R. A. (January 1922). ["On the Interpretation of χ2 from Contingency Tables, and the Calculation of P"](https://zenodo.org/record/1449484). _Journal of the Royal Statistical Society_. **85** (1): 87–94. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.2307/2340521](https://doi.org/10.2307%2F2340521). [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)")[2340521](https://www.jstor.org/stable/2340521).
6.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-6)**Cichoń, Mariusz (2020-06-01). ["Reporting statistical methods and outcome of statistical analyses in research articles"](https://link.springer.com/article/10.1007/s43440-020-00110-5). _Pharmacological Reports_. **72** (3): 481–485. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/s43440-020-00110-5](https://doi.org/10.1007%2Fs43440-020-00110-5). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[2299-5684](https://search.worldcat.org/issn/2299-5684). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")[32542585](https://pubmed.ncbi.nlm.nih.gov/32542585).
7.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-7)**Cortina, J. M., Green, J. P., Keeler, K. R., & Vandenberg, R. J. (2017). Degrees of freedom in SEM: Are we testing the models that we claim to test?. Organizational Research Methods, 20(3), 350-378.
8.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-8)**Christensen, Ronald (2002). _Plane Answers to Complex Questions: The Theory of Linear Models_ (Third ed.). New York: Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-387-95361-2](https://en.wikipedia.org/wiki/Special:BookSources/0-387-95361-2 "Special:BookSources/0-387-95361-2").
9.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-9)**[Trevor Hastie](https://en.wikipedia.org/wiki/Trevor_Hastie "Trevor Hastie"), [Robert Tibshirani](https://en.wikipedia.org/wiki/Robert_Tibshirani "Robert Tibshirani"), Jerome H. Friedman (2009), _The elements of statistical learning: data mining, inference, and prediction_, 2nd ed., 746 p. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-84857-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-84857-0 "Special:BookSources/978-0-387-84857-0"), [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/978-0-387-84858-7](https://doi.org/10.1007%2F978-0-387-84858-7), [[1]](https://books.google.com/books?id=tVIjmNS3Ob8C&dq=degrees+of+freedom+of+a+smoother&pg=PA154) (eq.(5.16))
10.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-Fox_Sage_Publications_SAGE._2000_p._58_10-0)**Fox, J. (2000). [_Nonparametric Simple Regression: Smoothing Scatterplots_](https://books.google.com/books?id=cLL3TKeEa9QC&pg=PA58). Quantitative Applications in the Social Sciences. Vol.130. SAGE Publications. p.58. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-7619-1585-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-7619-1585-0 "Special:BookSources/978-0-7619-1585-0"). Retrieved 2020-08-28.
11.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-11)**Ye, J. (1998), "On Measuring and Correcting the Effects of Data Mining and Model Selection", _[Journal of the American Statistical Association](https://en.wikipedia.org/wiki/Journal\_of\_the\_American\_Statistical\_Association "Journal of the American Statistical Association")_, 93 (441), 120–131. [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)")[2669609](https://www.jstor.org/stable/2669609) (eq.(7))
12.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-12)**Catherine Loader (1999), [_Local regression and likelihood_](https://books.google.com/books?id=D7GgBAfL4ngC&q=degree+of+freedom&pg=PA28), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-98775-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-98775-0 "Special:BookSources/978-0-387-98775-0"), [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/b98858](https://doi.org/10.1007%2Fb98858), (eq.(2.18), p. 30)
13.   ^ [_**a**_](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-Hastie1990_13-0)[_**b**_](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-Hastie1990_13-1)Trevor Hastie, Robert Tibshirani (1990), [_Generalized additive models_](https://books.google.com/books?id=qa29r1Ze1coC&q=degrees+of+freedom&pg=PA54), CRC Press, (p. 54) and (eq.(B.1), p. 305))
14.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-Wood2006_14-0)**Simon N. Wood (2006), [_Generalized additive models: an introduction with R_](https://books.google.com/books?id=hr17lZC-3jQC&dq=Effective%20degrees%20of%20freedom&pg=PA172), CRC Press, (eq.(4,14), p. 172)
15.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-15)**David Ruppert, M. P. Wand, R. J. Carroll (2003), _Semiparametric Regression_, Cambridge University Press (eq.(3.28), p. 82)
16.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-16)**James S. Hodges (2014), _Richly Parameterized Linear Models_, CRC Press. [[2]](https://books.google.com/books?id=eDcNTwEACAAJ&pg=PA56)
17.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-17)**Peter J. Green, B. W. Silverman (1994), [_Nonparametric regression and generalized linear models: a roughness penalty approach_](https://books.google.com/books?id=-AIVXozvpLUC&dq=generalized%20effective%20degrees%20of%20freedom&pg=PA37), CRC Press (eq.(3.15), p. 37)
18.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-18)**Clive D. Rodgers (2000), _Inverse methods for atmospheric sounding: theory and practice_, World Scientific (eq.(2.56), p. 31)
19.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-19)**Adrian Doicu, Thomas Trautmann, Franz Schreier (2010), _Numerical Regularization for Atmospheric Inverse Problems_, Springer (eq.(4.26), p. 114)
20.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-20)**D. Dong, T. A. Herring and R. W. King (1997), Estimating regional deformation from a combination of space and terrestrial geodetic data, _J. Geodesy_, 72 (4), 200–214, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/s001900050161](https://doi.org/10.1007%2Fs001900050161) (eq.(27), p. 205)
21.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-21)**H. Theil (1963), "On the Use of Incomplete Prior Information in Regression Analysis", _[Journal of the American Statistical Association](https://en.wikipedia.org/wiki/Journal\_of\_the\_American\_Statistical\_Association "Journal of the American Statistical Association")_, 58 (302), 401–414 [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)")[2283275](https://www.jstor.org/stable/2283275) (eq.(5.19)–(5.20))
22.   **[^](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)#cite_ref-Jones1_22-0)**Jones, D.A. (1983) "Statistical analysis of empirical models fitted by optimisation", [Biometrika](https://en.wikipedia.org/wiki/Biometrika "Biometrika"), 70 (1), 67–88

*   Bowers, David (1982). _Statistics for Economists_. London: Macmillan. pp.175–178. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-333-30110-2](https://en.wikipedia.org/wiki/Special:BookSources/0-333-30110-2 "Special:BookSources/0-333-30110-2").
*   Eisenhauer, J. G. (2008). ["Degrees of Freedom"](https://doi.org/10.1111%2Fj.1467-9639.2008.00324.x). _Teaching Statistics_. **30** (3): 75–78. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1111/j.1467-9639.2008.00324.x](https://doi.org/10.1111%2Fj.1467-9639.2008.00324.x). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[121982952](https://api.semanticscholar.org/CorpusID:121982952).
*   Good, I. J. (1973). "What Are Degrees of Freedom?". _[The American Statistician](https://en.wikipedia.org/wiki/The\_American\_Statistician "The American Statistician")_. **27** (5): 227–228. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1080/00031305.1973.10479042](https://doi.org/10.1080%2F00031305.1973.10479042). [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)")[3087407](https://www.jstor.org/stable/3087407).
*   Walker, H. W. (1940). "Degrees of Freedom". _Journal of Educational Psychology_. **31** (4): 253–269. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1037/h0054588](https://doi.org/10.1037%2Fh0054588).[Transcription by C Olsen with errata](https://web.archive.org/web/20110927081634/http://courses.ncssm.edu/math/Stat_Inst/PDFS/DFWalker.pdf)

*   Yu, Chong-ho (1997) [Illustrating degrees of freedom in terms of sample size and dimensionality](https://www.creative-wisdom.com/computer/sas/df.html)
*   Dallal, GE. (2003) [Degrees of Freedom](https://web.archive.org/web/20121017075636/http://www.tufts.edu/~gdallal/dof.htm)
