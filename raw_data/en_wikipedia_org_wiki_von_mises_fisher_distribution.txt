Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Support 1.2 Note on the normalization constant 2 Relation to normal distribution 3 Estimation of parameters Toggle Estimation of parameters subsection 3.1 Mean direction 3.2 Concentration parameter 3.3 Standard error 4 Expected value 5 Entropy and KL divergence 6 Transformation 7 Pseudo-random number generation Toggle Pseudo-random number generation subsection 7.1 General case 7.2 3-D sphere 8 Distribution of polar angle 9 The uniform hypersphere distribution Toggle The uniform hypersphere distribution subsection 9.1 Component marginal of uniform distribution 9.2 Distribution of dot-products 10 Generalizations Toggle Generalizations subsection 10.1 Matrix Von Mises-Fisher 10.2 Saw distributions 10.3 Weighted Rademacher Distribution 11 See also 12 References 13 Further reading Toggle the table of contents von Mises–Fisher distribution 2 languages Català Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution on a hyper-sphere of arbitrary dimension In directional statistics , the von Mises–Fisher distribution (named after Richard von Mises and Ronald Fisher ), is a probability distribution on the ( p − − 1 ) {\displaystyle (p-1)} - sphere in R p {\displaystyle \mathbb {R} ^{p}} . If p = 2 {\displaystyle p=2} the distribution reduces to the von Mises distribution on the circle .

Definition [ edit ] The probability density function of the von Mises–Fisher distribution for the random p -dimensional unit vector x {\displaystyle \mathbf {x} } is given by: f p ( x ; μ μ , κ κ ) = C p ( κ κ ) exp ⁡ ⁡ ( κ κ μ μ T x ) , {\displaystyle f_{p}(\mathbf {x} ;{\boldsymbol {\mu }},\kappa )=C_{p}(\kappa )\exp \left({\kappa {\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} }\right),} where κ κ ≥ ≥ 0 , ‖ μ μ ‖ = 1 {\displaystyle \kappa \geq 0,\left\Vert {\boldsymbol {\mu }}\right\Vert =1} and  
the normalization constant C p ( κ κ ) {\displaystyle C_{p}(\kappa )} is equal to C p ( κ κ ) = κ κ p / 2 − − 1 ( 2 π π ) p / 2 I p / 2 − − 1 ( κ κ ) , {\displaystyle C_{p}(\kappa )={\frac {\kappa ^{p/2-1}}{(2\pi )^{p/2}I_{p/2-1}(\kappa )}},} where I v {\displaystyle I_{v}} denotes the modified Bessel function of the first kind at order v {\displaystyle v} . If p = 3 {\displaystyle p=3} , the normalization constant reduces to C 3 ( κ κ ) = κ κ 4 π π sinh ⁡ ⁡ κ κ = κ κ 2 π π ( e κ κ − − e − − κ κ ) .

{\displaystyle C_{3}(\kappa )={\frac {\kappa }{4\pi \sinh \kappa }}={\frac {\kappa }{2\pi (e^{\kappa }-e^{-\kappa })}}.} The parameters μ μ {\displaystyle {\boldsymbol {\mu }}} and κ κ {\displaystyle \kappa } are called the mean direction and concentration parameter , respectively. The greater the value of κ κ {\displaystyle \kappa } , the higher the concentration of the distribution around the mean direction μ μ {\displaystyle {\boldsymbol {\mu }}} . The distribution is unimodal for κ κ > 0 {\displaystyle \kappa >0} , and is uniform on the sphere for κ κ = 0 {\displaystyle \kappa =0} .

The von Mises–Fisher distribution for p = 3 {\displaystyle p=3} is also called the Fisher distribution .

[ 1 ] [ 2 ] It was first used to model the interaction of electric dipoles in an electric field .

[ 3 ] Other applications are found in geology , bioinformatics , and text mining .

Support [ edit ] The support of the Von Mises–Fisher distribution is the hypersphere , or more specifically, the ( p − − 1 ) {\displaystyle (p-1)} -sphere , denoted as S p − − 1 = { x ∈ ∈ R p : ‖ x ‖ = 1 } {\displaystyle \mathbb {S} ^{p-1}=\left\{\mathbf {x} \in \mathbb {R} ^{p}:\left\|\mathbf {x} \right\|=1\right\}} This is a ( p − − 1 ) {\displaystyle (p-1)} -dimensional manifold embedded in p {\displaystyle p} -dimensional Euclidean space, R p {\displaystyle \mathbb {R} ^{p}} .

Note on the normalization constant [ edit ] In the textbook, Directional Statistics [ 3 ] by Mardia and Jupp, the normalization constant given for the Von Mises Fisher (VMF) probability density is apparently different from the one given here: C p ( κ κ ) {\displaystyle C_{p}(\kappa )} . In that book, for VMF ( μ μ , κ κ ) {\displaystyle {\text{VMF}}({\boldsymbol {\mu }},\kappa )} the normalization constant is specified as: C p ∗ ∗ ( κ κ ) = ( κ κ 2 ) p / 2 − − 1 Γ Γ ( p / 2 ) I p / 2 − − 1 ( κ κ ) {\displaystyle C_{p}^{*}(\kappa )={\frac {({\frac {\kappa }{2}})^{p/2-1}}{\Gamma (p/2)I_{p/2-1}(\kappa )}}} where Γ Γ {\displaystyle \Gamma } is the gamma function . This is resolved by noting that Mardia and Jupp give the density "with respect to the uniform distribution", while the density here is specified with respect to scaled Hausdorff measure , H ¯ ¯ p − − 1 {\displaystyle {\bar {H}}^{p-1}} , which gives the surface area of the whole (p-1) -sphere as: H λ λ p − − 1 ( S p − − 1 ) = 2 π π p / 2 Γ Γ ( p / 2 ) , {\displaystyle H_{\lambda }^{p-1}(\mathbb {S} ^{p-1})={\frac {2\pi ^{p/2}}{\Gamma (p/2)}},} the reciprocal of which gives the (constant) density of the uniform distribution, VMF ( μ μ , κ κ = 0 ) , {\displaystyle {\text{VMF}}({\boldsymbol {\mu }},\kappa =0),} as: C p ( 0 ) = Γ Γ ( p / 2 ) 2 π π p / 2 {\displaystyle C_{p}(0)={\frac {\Gamma (p/2)}{2\pi ^{p/2}}}} It then follows that: C p ∗ ∗ ( κ κ ) = C p ( κ κ ) C p ( 0 ) {\displaystyle C_{p}^{*}(\kappa )={\frac {C_{p}(\kappa )}{C_{p}(0)}}} While the value for C p ( 0 ) {\displaystyle C_{p}(0)} was derived above via the surface area, the same result may be obtained by setting κ κ = 0 {\displaystyle \kappa =0} in the above formula for C p ( κ κ ) {\displaystyle C_{p}(\kappa )} . This can be done by noting that the series expansion for I p / 2 − − 1 ( κ κ ) {\displaystyle I_{p/2-1}(\kappa )} divided by κ κ p / 2 − − 1 {\displaystyle \kappa ^{p/2-1}} has but one non-zero term at κ κ = 0 {\displaystyle \kappa =0} . (To evaluate that term, one needs to use the definition 0 0 = 1 {\displaystyle 0^{0}=1} .) For further understanding of density functions on the hypersphere, see: projected normal distribution § note on density definition .

Relation to normal distribution [ edit ] Starting from a multivariate normal distribution with isotropic covariance κ κ − − 1 I {\displaystyle \kappa ^{-1}\mathbf {I} } and mean μ μ {\displaystyle {\boldsymbol {\mu }}} of length r > 0 {\displaystyle r>0} , whose density function is: N p ( x ; μ μ , κ κ ) = ( κ κ 2 π π ) p exp ⁡ ⁡ ( − − κ κ ( x − − μ μ ) T ( x − − μ μ ) 2 ) , {\displaystyle {\mathcal {N}}_{p}(\mathbf {x} ;{\boldsymbol {\mu }},\kappa )=\left({\sqrt {\frac {\kappa }{2\pi }}}\right)^{p}\exp \left(-\kappa {\frac {(\mathbf {x} -{\boldsymbol {\mu }})^{\mathsf {T}}(\mathbf {x} -{\boldsymbol {\mu }})}{2}}\right),} the Von Mises–Fisher distribution is obtained by conditioning on ‖ x ‖ = 1 {\displaystyle \left\|\mathbf {x} \right\|=1} . By expanding ( x − − μ μ ) T ( x − − μ μ ) = x T x + μ μ T μ μ − − 2 μ μ T x = 1 + r 2 − − 2 μ μ T x , {\displaystyle (\mathbf {x} -{\boldsymbol {\mu }})^{\mathsf {T}}(\mathbf {x} -{\boldsymbol {\mu }})=\mathbf {x} ^{\mathsf {T}}\mathbf {x} +{\boldsymbol {\mu }}^{\mathsf {T}}{\boldsymbol {\mu }}-2{\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} =1+r^{2}-2{\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} ,} the Von Mises-Fisher density, f p ( x ; r − − 1 μ μ , r κ κ ) ∝ ∝ e κ κ μ μ T x {\displaystyle f_{p}(\mathbf {x} ;r^{-1}{\boldsymbol {\mu }},r\kappa )\propto e^{\kappa {\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} }} is recovered by recomputing the normalization constant by integrating x {\displaystyle \mathbf {x} } over the unit sphere. If μ μ = 0 {\displaystyle {\boldsymbol {\mu }}={\boldsymbol {0}}} , we get the uniform distribution, with (constant) density f p ( x ; μ μ ~ ~ , 0 ) {\displaystyle f_{p}(\mathbf {x} ;{\tilde {\boldsymbol {\mu }}},0)} , where μ μ ~ ~ ∈ ∈ S p − − 1 {\displaystyle {\tilde {\boldsymbol {\mu }}}\in \mathbb {S} ^{p-1}} is arbitrary.

More succinctly, the restriction of any isotropic multivariate normal density to the unit hypersphere gives a Von Mises-Fisher density, up to normalization.

See also: This construction can be generalized by starting with a normal distribution with a general covariance matrix, in which case restricting to ‖ x ‖ = 1 {\displaystyle \left\|\mathbf {x} \right\|=1} gives the Fisher-Bingham distribution .

Restriction is not to be confused with projection . If z ∼ ∼ N p ( μ μ , Σ Σ ) {\displaystyle \mathbf {z} \sim {\mathcal {N}}_{p}({\boldsymbol {\mu }},{\boldsymbol {\Sigma }})} , which we project onto the unitsphere: x = ‖ ‖ z ‖ ‖ − − 1 z {\displaystyle \mathbf {x} =\lVert \mathbf {z} \rVert ^{-1}\mathbf {z} } , we get the projected normal distribution . (Informally, restriction can be thought of as rejection sampling with an infinite sampling budget, where we keep only those z {\displaystyle \mathbf {z} } that land on the unitsphere, while with projection we use all samples.) Estimation of parameters [ edit ] Mean direction [ edit ] A series of N independent unit vectors x i {\displaystyle x_{i}} are drawn from a von Mises–Fisher distribution. 
The maximum likelihood estimates of the mean direction μ μ {\displaystyle \mu } is simply the normalized arithmetic mean , a sufficient statistic : [ 3 ] μ μ = x ¯ ¯ / R ¯ ¯ , where x ¯ ¯ = 1 N ∑ ∑ i N x i , and R ¯ ¯ = ‖ ‖ x ¯ ¯ ‖ ‖ , {\displaystyle \mu ={\bar {x}}/{\bar {R}},{\text{where }}{\bar {x}}={\frac {1}{N}}\sum _{i}^{N}x_{i},{\text{and }}{\bar {R}}=\|{\bar {x}}\|,} Concentration parameter [ edit ] Use the modified Bessel function of the first kind to define A p ( κ κ ) = I p / 2 ( κ κ ) I p / 2 − − 1 ( κ κ ) .

{\displaystyle A_{p}(\kappa )={\frac {I_{p/2}(\kappa )}{I_{p/2-1}(\kappa )}}.} Then: κ κ = A p − − 1 ( R ¯ ¯ ) .

{\displaystyle \kappa =A_{p}^{-1}({\bar {R}}).} Thus κ κ {\displaystyle \kappa } is the solution to A p ( κ κ ) = ‖ ∑ ∑ i N x i ‖ N = R ¯ ¯ .

{\displaystyle A_{p}(\kappa )={\frac {\left\|\sum _{i}^{N}x_{i}\right\|}{N}}={\bar {R}}.} A simple approximation to κ κ {\displaystyle \kappa } is (Sra, 2011) κ κ ^ ^ = R ¯ ¯ ( p − − R ¯ ¯ 2 ) 1 − − R ¯ ¯ 2 , {\displaystyle {\hat {\kappa }}={\frac {{\bar {R}}(p-{\bar {R}}^{2})}{1-{\bar {R}}^{2}}},} A more accurate inversion can be obtained by iterating the Newton method a few times κ κ ^ ^ 1 = κ κ ^ ^ − − A p ( κ κ ^ ^ ) − − R ¯ ¯ 1 − − A p ( κ κ ^ ^ ) 2 − − p − − 1 κ κ ^ ^ A p ( κ κ ^ ^ ) , {\displaystyle {\hat {\kappa }}_{1}={\hat {\kappa }}-{\frac {A_{p}({\hat {\kappa }})-{\bar {R}}}{1-A_{p}({\hat {\kappa }})^{2}-{\frac {p-1}{\hat {\kappa }}}A_{p}({\hat {\kappa }})}},} κ κ ^ ^ 2 = κ κ ^ ^ 1 − − A p ( κ κ ^ ^ 1 ) − − R ¯ ¯ 1 − − A p ( κ κ ^ ^ 1 ) 2 − − p − − 1 κ κ ^ ^ 1 A p ( κ κ ^ ^ 1 ) .

{\displaystyle {\hat {\kappa }}_{2}={\hat {\kappa }}_{1}-{\frac {A_{p}({\hat {\kappa }}_{1})-{\bar {R}}}{1-A_{p}({\hat {\kappa }}_{1})^{2}-{\frac {p-1}{{\hat {\kappa }}_{1}}}A_{p}({\hat {\kappa }}_{1})}}.} Standard error [ edit ] For N ≥ 25, the estimated spherical standard error of the sample mean direction can be computed as: [ 4 ] σ σ ^ ^ = ( d N R ¯ ¯ 2 ) 1 / 2 {\displaystyle {\hat {\sigma }}=\left({\frac {d}{N{\bar {R}}^{2}}}\right)^{1/2}} where d = 1 − − 1 N ∑ ∑ i N ( μ μ T x i ) 2 {\displaystyle d=1-{\frac {1}{N}}\sum _{i}^{N}\left(\mu ^{T}x_{i}\right)^{2}} It is then possible to approximate a 100 ( 1 − − α α ) % % {\displaystyle 100(1-\alpha )\%} a spherical confidence interval (a confidence cone ) about μ μ {\displaystyle \mu } with semi-vertical angle: q = arcsin ⁡ ⁡ ( e α α 1 / 2 σ σ ^ ^ ) , {\displaystyle q=\arcsin \left(e_{\alpha }^{1/2}{\hat {\sigma }}\right),} where e α α = − − ln ⁡ ⁡ ( α α ) .

{\displaystyle e_{\alpha }=-\ln(\alpha ).} For example, for a 95% confidence cone, α α = 0.05 , e α α = − − ln ⁡ ⁡ ( 0.05 ) = 2.996 , {\displaystyle \alpha =0.05,e_{\alpha }=-\ln(0.05)=2.996,} and thus q = arcsin ⁡ ⁡ ( 1.731 σ σ ^ ^ ) .

{\displaystyle q=\arcsin(1.731{\hat {\sigma }}).} Expected value [ edit ] The expected value of the Von Mises–Fisher distribution is not on the unit hypersphere, but instead has a length of less than one. This length is given by A p ( κ κ ) {\displaystyle A_{p}(\kappa )} as defined above. For a Von Mises–Fisher distribution with mean direction μ μ {\displaystyle {\boldsymbol {\mu }}} and concentration κ κ > 0 {\displaystyle \kappa >0} , the expected value is: A p ( κ κ ) μ μ {\displaystyle A_{p}(\kappa ){\boldsymbol {\mu }}} .

For κ κ = 0 {\displaystyle \kappa =0} , the expected value is at the origin. For finite κ κ > 0 {\displaystyle \kappa >0} , the length of the expected value is strictly between zero and one and is a monotonic rising function of κ κ {\displaystyle \kappa } .

The empirical mean ( arithmetic average ) of a collection of points on the unit hypersphere behaves in a similar manner, being close to the origin for widely  spread data and close to the sphere for concentrated data. Indeed, for the Von Mises–Fisher distribution, the expected value of the maximum-likelihood estimate based on a collection of points is equal to the empirical mean of those points.

Entropy and KL divergence [ edit ] The expected value can be used to compute differential entropy and KL divergence .

The differential entropy of VMF ( μ μ , κ κ ) {\displaystyle {\text{VMF}}({\boldsymbol {\mu }},\kappa )} is: ⟨ − − log ⁡ ⁡ f p ( x ; μ μ , κ κ ) ⟩ x ∼ ∼ VMF ( μ μ , κ κ ) = − − log ⁡ ⁡ f p ( A p ( κ κ ) μ μ ; μ μ , κ κ ) = − − log ⁡ ⁡ C p ( κ κ ) − − κ κ A p ( κ κ ) {\displaystyle {\bigl \langle }-\log f_{p}(\mathbf {x} ;{\boldsymbol {\mu }},\kappa ){\bigr \rangle }_{\mathbf {x} \sim {\text{VMF}}({\boldsymbol {\mu }},\kappa )}=-\log f_{p}(A_{p}(\kappa ){\boldsymbol {\mu }};{\boldsymbol {\mu }},\kappa )=-\log C_{p}(\kappa )-\kappa A_{p}(\kappa )} where the angle brackets denote expectation. Notice that the entropy is a function of κ κ {\displaystyle \kappa } only.

The KL divergence between VMF ( μ μ 0 , κ κ 0 ) {\displaystyle {\text{VMF}}({\boldsymbol {\mu _{0}}},\kappa _{0})} and VMF ( μ μ 1 , κ κ 1 ) {\displaystyle {\text{VMF}}({\boldsymbol {\mu _{1}}},\kappa _{1})} is: ⟨ log ⁡ ⁡ f p ( x ; μ μ 0 , κ κ 0 ) f p ( x ; μ μ 1 , κ κ 1 ) ⟩ x ∼ ∼ VMF ( μ μ 0 , κ κ 0 ) = log ⁡ ⁡ f p ( A p ( κ κ 0 ) μ μ 0 ; μ μ 0 , κ κ 0 ) f p ( A p ( κ κ 0 ) μ μ 0 ; μ μ 1 , κ κ 1 ) {\displaystyle {\Bigl \langle }\log {\frac {f_{p}(\mathbf {x} ;{\boldsymbol {\mu _{0}}},\kappa _{0})}{f_{p}(\mathbf {x} ;{\boldsymbol {\mu _{1}}},\kappa _{1})}}{\Bigr \rangle }_{\mathbf {x} \sim {\text{VMF}}({\boldsymbol {\mu _{0}}},\kappa _{0})}=\log {\frac {f_{p}(A_{p}(\kappa _{0}){\boldsymbol {\mu _{0}}};{\boldsymbol {\mu _{0}}},\kappa _{0})}{f_{p}(A_{p}(\kappa _{0}){\boldsymbol {\mu _{0}}};{\boldsymbol {\mu _{1}}},\kappa _{1})}}} Transformation [ edit ] Von Mises-Fisher (VMF) distributions are closed under orthogonal linear transforms. Let U {\displaystyle \mathbf {U} } be a p {\displaystyle p} -by- p {\displaystyle p} orthogonal matrix . Let x ∼ ∼ VMF ( μ μ , κ κ ) {\displaystyle \mathbf {x} \sim {\text{VMF}}({\boldsymbol {\mu }},\kappa )} and apply the invertible linear transform: y = U x {\displaystyle \mathbf {y} =\mathbf {Ux} } . The inverse transform is x = U ′ y {\displaystyle \mathbf {x} =\mathbf {U'y} } , because the inverse of an orthogonal matrix is its transpose : U − − 1 = U ′ {\displaystyle \mathbf {U} ^{-1}=\mathbf {U} '} . The Jacobian of the transform is U {\displaystyle \mathbf {U} } , for which the absolute value of its determinant is 1, also because of the orthogonality. Using these facts and the form of the VMF density, it follows that: y ∼ ∼ VMF ( U μ μ , κ κ ) .

{\displaystyle \mathbf {y} \sim {\text{VMF}}(\mathbf {U} {\boldsymbol {\mu }},\kappa ).} One may verify that since μ μ {\displaystyle {\boldsymbol {\mu }}} and x {\displaystyle \mathbf {x} } are unit vectors, then by the orthogonality, so are U μ μ {\displaystyle \mathbf {U} {\boldsymbol {\mu }}} and y {\displaystyle \mathbf {y} } .

Pseudo-random number generation [ edit ] General case [ edit ] An algorithm for drawing pseudo-random samples from the Von Mises Fisher (VMF) distribution was given by Ulrich [ 5 ] and later corrected by Wood.

[ 6 ] An implementation in R is given by Hornik and Grün; [ 7 ] and a fast Python implementation is described by Pinzón and Jung.

[ 8 ] To simulate from a VMF distribution on the ( p − − 1 ) {\displaystyle (p-1)} -dimensional unitsphere , S p − − 1 {\displaystyle S^{p-1}} , with mean direction μ μ ∈ ∈ S p − − 1 {\displaystyle {\boldsymbol {\mu }}\in S^{p-1}} , these algorithms use the following radial-tangential decomposition for a point x ∈ ∈ S p − − 1 ⊂ ⊂ R p {\displaystyle \mathbf {x} \in S^{p-1}\subset \mathbb {R} ^{p}} : x = t μ μ + 1 − − t 2 v {\displaystyle \mathbf {x} =t{\boldsymbol {\mu }}+{\sqrt {1-t^{2}}}\mathbf {v} } where v ∈ ∈ R p {\displaystyle \mathbf {v} \in \mathbb {R} ^{p}} lives in the tangential ( p − − 2 ) {\displaystyle (p-2)} -dimensional unit-subsphere that is centered at and perpendicular to μ μ {\displaystyle {\boldsymbol {\mu }}} ; while t ∈ ∈ [ − − 1 , 1 ] {\displaystyle t\in [-1,1]} . To draw a sample x {\displaystyle \mathbf {x} } from a VMF with parameters μ μ {\displaystyle {\boldsymbol {\mu }}} and κ κ {\displaystyle \kappa } , v {\displaystyle \mathbf {v} } must be drawn from the uniform distribution on the tangential subsphere; and the radial component, t {\displaystyle t} , must be drawn independently from the distribution with density: f radial ( t ; κ κ , p ) = ( κ κ / 2 ) ν ν Γ Γ ( 1 2 ) Γ Γ ( ν ν + 1 2 ) I ν ν ( κ κ ) e t κ κ ( 1 − − t 2 ) ν ν − − 1 2 {\displaystyle f_{\text{radial}}(t;\kappa ,p)={\frac {(\kappa /2)^{\nu }}{\Gamma ({\frac {1}{2}})\Gamma (\nu +{\frac {1}{2}})I_{\nu }(\kappa )}}e^{t\kappa }(1-t^{2})^{\nu -{\frac {1}{2}}}} where ν ν = p 2 − − 1 {\displaystyle \nu ={\frac {p}{2}}-1} . The normalization constant for this density may be verified by using: I ν ν ( κ κ ) = ( κ κ / 2 ) ν ν Γ Γ ( 1 2 ) Γ Γ ( ν ν + 1 2 ) ∫ ∫ − − 1 1 e t κ κ ( 1 − − t 2 ) ν ν − − 1 2 d t {\displaystyle I_{\nu }(\kappa )={\frac {(\kappa /2)^{\nu }}{\Gamma ({\frac {1}{2}})\Gamma (\nu +{\frac {1}{2}})}}\int _{-1}^{1}e^{t\kappa }(1-t^{2})^{\nu -{\frac {1}{2}}}\,dt} as given in Appendix 1 (A.3) in Directional Statistics .

[ 3 ] Drawing the t {\displaystyle t} samples from this density by using a rejection sampling algorithm is explained in the above references. To draw the uniform v {\displaystyle \mathbf {v} } samples perpendicular to μ μ {\displaystyle {\boldsymbol {\mu }}} , see the algorithm in, [ 8 ] or otherwise a Householder transform can be used as explained in Algorithm 1 in.

[ 9 ] 3-D sphere [ edit ] To generate a Von Mises–Fisher distributed pseudo-random spherical 3-D unit vector [ 10 ] [ 11 ] X s {\textstyle \mathbf {X} _{s}} on the S 2 {\textstyle S^{2}} sphere for a given μ μ {\textstyle \mu } and κ κ {\textstyle \kappa } , define X s = [ r , θ θ , ϕ ϕ ] {\displaystyle \mathbf {X} _{s}=[r,\theta ,\phi ]} where θ θ {\textstyle \theta } is the polar angle, ϕ ϕ {\textstyle \phi } the azimuthal angle, and r = 1 {\textstyle r=1} the distance to the center of the sphere for μ μ = [ 0 , ( .

) , 1 ] {\textstyle \mathbf {\mu } =[0,(.),1]} the pseudo-random triplet is then given by X s = [ 1 , arccos ⁡ ⁡ W , V ] {\displaystyle \mathbf {X} _{s}=[1,\arccos W,V]} where V {\textstyle V} is sampled from the continuous uniform distribution U ( a , b ) {\textstyle U(a,b)} with lower bound a {\textstyle a} and upper bound b {\textstyle b} V ∼ ∼ U ( 0 , 2 π π ) {\displaystyle V\sim U(0,2\pi )} and W = cos ⁡ ⁡ θ θ = 1 + 1 κ κ ( ln ⁡ ⁡ ξ ξ + ln ⁡ ⁡ ( 1 − − ξ ξ − − 1 ξ ξ e − − 2 κ κ ) ) {\displaystyle W=\cos \theta =1+{\frac {1}{\kappa }}(\ln \xi +\ln(1-{\frac {\xi -1}{\xi }}e^{-2\kappa }))} where ξ ξ {\textstyle \xi } is sampled from the standard continuous uniform distribution U ( 0 , 1 ) {\textstyle U(0,1)} ξ ξ ∼ ∼ U ( 0 , 1 ) {\displaystyle \xi \sim U(0,1)} here, W {\textstyle W} should be set to W = 1 {\textstyle W=1} when ξ ξ = 0 {\textstyle \mathbf {\xi } =0} and X s {\textstyle \mathbf {X} _{s}} rotated to match any other desired μ μ {\textstyle \mu } .

Distribution of polar angle [ edit ] For p = 3 {\displaystyle p=3} , the angle θ between x {\displaystyle \mathbf {x} } and μ μ {\displaystyle {\boldsymbol {\mu }}} satisfies cos ⁡ ⁡ θ θ = μ μ T x {\displaystyle \cos \theta ={\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} } . It has the distribution p ( θ θ ) = ∫ ∫ d 2 x f ( x ; μ μ , κ κ ) δ δ ( θ θ − − arc cos ( μ μ T x ) ) {\displaystyle p(\theta )=\int d^{2}xf(x;{\boldsymbol {\mu }},\kappa )\,\delta \left(\theta -{\text{arc cos}}({\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} )\right)} , which can be easily evaluated as p ( θ θ ) = 2 π π C 3 ( κ κ ) sin ⁡ ⁡ θ θ e κ κ cos ⁡ ⁡ θ θ {\displaystyle p(\theta )=2\pi C_{3}(\kappa )\,\sin \theta \,e^{\kappa \cos \theta }} .

For the general case, p ≥ ≥ 2 {\displaystyle p\geq 2} , the distribution for the cosine of this angle: cos ⁡ ⁡ θ θ = t = μ μ T x {\displaystyle \cos \theta =t={\boldsymbol {\mu }}^{\mathsf {T}}\mathbf {x} } is given by f radial ( t ; κ κ , p ) {\displaystyle f_{\text{radial}}(t;\kappa ,p)} , as explained above .

The uniform hypersphere distribution [ edit ] See also: N-sphere § Uniformly at random on the (n − 1)-sphere .

When κ κ = 0 {\displaystyle \kappa =0} , the Von Mises–Fisher distribution, VMF ( μ μ , κ κ ) {\displaystyle {\text{VMF}}({\boldsymbol {\mu }},\kappa )} simplifies to the uniform distribution on S p − − 1 ⊂ ⊂ R p {\displaystyle \mathbb {S} ^{p-1}\subset \mathbb {R} ^{p}} . The density is constant with value C p ( 0 ) {\displaystyle C_{p}(0)} . Pseudo-random samples can be generated by generating samples in R p {\displaystyle \mathbb {R} ^{p}} from the standard multivariate normal distribution, followed by normalization to unit norm.

Component marginal of uniform distribution [ edit ] For 1 ≤ ≤ i ≤ ≤ p {\displaystyle 1\leq i\leq p} , let x i {\displaystyle x_{i}} be any component of x ∈ ∈ S p − − 1 {\displaystyle \mathbf {x} \in \mathbb {S} ^{p-1}} . The marginal distribution for x i {\displaystyle x_{i}} has the density: [ 12 ] [ 13 ] f i ( x i ; p ) = f radial ( x i ; κ κ = 0 , p ) = ( 1 − − x i 2 ) p − − 1 2 − − 1 B ( 1 2 , p − − 1 2 ) {\displaystyle f_{i}(x_{i};p)=f_{\text{radial}}(x_{i};\kappa =0,p)={\frac {(1-x_{i}^{2})^{{\frac {p-1}{2}}-1}}{B{\bigl (}{\frac {1}{2}},{\frac {p-1}{2}}{\bigr )}}}} where B ( α α , β β ) {\displaystyle B(\alpha ,\beta )} is the beta function . This distribution may be better understood by highlighting its relation to the beta distribution : x i 2 ∼ ∼ Beta ( 1 2 , p − − 1 2 ) and x i + 1 2 ∼ ∼ Beta ( p − − 1 2 , p − − 1 2 ) {\displaystyle {\begin{aligned}x_{i}^{2}&\sim {\text{Beta}}{\bigl (}{\frac {1}{2}},{\frac {p-1}{2}}{\bigr )}&&{\text{and}}&{\frac {x_{i}+1}{2}}&\sim {\text{Beta}}{\bigl (}{\frac {p-1}{2}},{\frac {p-1}{2}}{\bigr )}\end{aligned}}} where the Legendre duplication formula is useful to understand the relationships between the normalization constants of the various densities above.

Note that the components of x ∈ ∈ S p − − 1 {\displaystyle \mathbf {x} \in \mathbb {S} ^{p-1}} are not independent, so that the uniform density is not the product of the marginal densities; and x {\displaystyle \mathbf {x} } cannot be assembled by independent sampling of the components.

Distribution of dot-products [ edit ] In machine learning , especially in image classification , to-be-classified inputs (e.g. images) are often compared using cosine similarity , which is the dot product between intermediate representations in the form of unitvectors (termed embeddings ). The dimensionality is typically high, with p {\displaystyle p} at least several hundreds. The deep neural networks that extract embeddings for classification should learn to spread the classes as far apart as possible and ideally this should give classes that are uniformly distributed on S p − − 1 {\displaystyle \mathbb {S} ^{p-1}} .

[ 14 ] For a better statistical understanding of across-class cosine similarity , the distribution of dot-products between unitvectors independently sampled from the uniform distribution may be helpful.

Let x , y ∈ ∈ S p − − 1 {\displaystyle \mathbf {x} ,\mathbf {y} \in \mathbb {S} ^{p-1}} be unitvectors in R p {\displaystyle \mathbb {R} ^{p}} , independently sampled from the uniform distribution. Define: t = x ′ y ∈ ∈ [ − − 1 , 1 ] , r = t + 1 2 ∈ ∈ [ 0 , 1 ] , s = logit ( r ) = log ⁡ ⁡ 1 + t 1 − − t ∈ ∈ R {\displaystyle {\begin{aligned}t&=\mathbf {x} '\mathbf {y} \in [-1,1],&r&={\frac {t+1}{2}}\in [0,1],&s&={\text{logit}}(r)=\log {\frac {1+t}{1-t}}\in \mathbb {R} \end{aligned}}} where t {\displaystyle t} is the dot-product and r , s {\displaystyle r,s} are transformed versions of it. Then the distribution for t {\displaystyle t} is the same as the marginal component distribution given above ; [ 13 ] the distribution for r {\displaystyle r} is symmetric beta and the distribution for s {\displaystyle s} is  symmetric logistic-beta : r ∼ ∼ Beta ( p − − 1 2 , p − − 1 2 ) , s ∼ ∼ B σ σ ( p − − 1 2 , p − − 1 2 ) {\displaystyle {\begin{aligned}r&\sim {\text{Beta}}{\bigl (}{\frac {p-1}{2}},{\frac {p-1}{2}}{\bigr )},&s&\sim B_{\sigma }{\bigl (}{\frac {p-1}{2}},{\frac {p-1}{2}}{\bigr )}\end{aligned}}} The means and variances are: E [ t ] = 0 , E [ r ] = 1 2 , E [ s ] = 0 , {\displaystyle {\begin{aligned}E[t]&=0,&E[r]&={\frac {1}{2}},&E[s]&=0,\end{aligned}}} and var [ t ] = 1 p , var [ r ] = 1 4 p , var [ s ] = 2 ψ ψ ′ ( p − − 1 2 ) ≈ ≈ 4 p − − 1 {\displaystyle {\begin{aligned}{\text{var}}[t]&={\frac {1}{p}},&{\text{var}}[r]&={\frac {1}{4p}},&{\text{var}}[s]&=2\psi '{\bigl (}{\frac {p-1}{2}}{\bigr )}\approx {\frac {4}{p-1}}\end{aligned}}} where ψ ψ ′ = ψ ψ ( 1 ) {\displaystyle \psi '=\psi ^{(1)}} is the first polygamma function . The variances decrease, the distributions of all three variables become more Gaussian, and the final approximation gets better as the dimensionality, p {\displaystyle p} , is increased.

Generalizations [ edit ] Matrix Von Mises-Fisher [ edit ] Further information: Random matrix The matrix von Mises-Fisher distribution (also known as matrix Langevin distribution [ 15 ] [ 16 ] ) has the density f n , p ( X ; F ) ∝ ∝ exp ⁡ ⁡ ( tr ⁡ ⁡ ( F T X ) ) {\displaystyle f_{n,p}(\mathbf {X} ;\mathbf {F} )\propto \exp(\operatorname {tr} (\mathbf {F} ^{\mathsf {T}}\mathbf {X} ))} supported on the Stiefel manifold of n × × p {\displaystyle n\times p} orthonormal p-frames X {\displaystyle \mathbf {X} } , where F {\displaystyle \mathbf {F} } is an arbitrary n × × p {\displaystyle n\times p} real matrix.

[ 17 ] [ 18 ] Saw distributions [ edit ] Ulrich, [ 5 ] in designing an algorithm for sampling from the VMF distribution, makes use of a family of distributions named after and explored by John G. Saw.

[ 19 ] A Saw distribution is a distribution on the ( p − − 1 ) {\displaystyle (p-1)} -sphere, S p − − 1 {\displaystyle S^{p-1}} , with modal vector μ μ ∈ ∈ S p − − 1 {\displaystyle {\boldsymbol {\mu }}\in S^{p-1}} and concentration κ κ ≥ ≥ 0 {\displaystyle \kappa \geq 0} , and of which the density function has the form: f Saw ( x ; μ μ , κ κ ) = g ( κ κ x ′ μ μ ) K p ( κ κ ) {\displaystyle f_{\text{Saw}}(\mathbf {x} ;{\boldsymbol {\mu }},\kappa )={\frac {g(\kappa \mathbf {x} '{\boldsymbol {\mu }})}{K_{p}(\kappa )}}} where g {\displaystyle g} is a non-negative, increasing function; and where K P ( κ κ ) {\displaystyle K_{P}(\kappa )} is the normalization constant. The above-mentioned radial-tangential decomposition generalizes to the Saw family and the radial component, t = x ′ μ μ {\displaystyle t=\mathbf {x} '{\boldsymbol {\mu }}} has the density: f Saw-radial ( t ; κ κ ) = 2 π π p / 2 Γ Γ ( p / 2 ) g ( κ κ t ) ( 1 − − t 2 ) ( p − − 3 ) / 2 B ( 1 2 , p − − 1 2 ) K p ( κ κ ) .

{\displaystyle f_{\text{Saw-radial}}(t;\kappa )={\frac {2\pi ^{p/2}}{\Gamma (p/2)}}{\frac {g(\kappa t)(1-t^{2})^{(p-3)/2}}{B{\bigl (}{\frac {1}{2}},{\frac {p-1}{2}}{\bigr )}K_{p}(\kappa )}}.} where B {\displaystyle B} is the beta function. Also notice that the left-hand factor of the radial density is the surface area of S p − − 1 {\displaystyle S^{p-1}} .

By setting g ( κ κ x ′ μ μ ) = e κ κ x ′ μ μ {\displaystyle g(\kappa \mathbf {x} '{\boldsymbol {\mu }})=e^{\kappa \mathbf {x} '{\boldsymbol {\mu }}}} , one recovers the VMF distribution.

Weighted Rademacher Distribution [ edit ] The definition of the Von Mises–Fisher distribution can be extended to include also the case where p = 1 {\displaystyle p=1} , so that the support is the 0-dimensional hypersphere, which when embedded into 1-dimensional Euclidean space is the discrete set, { − − 1 , 1 } {\displaystyle \{-1,1\}} . The mean direction is μ μ ∈ ∈ { − − 1 , 1 } {\displaystyle \mu \in \{-1,1\}} and the concentration is κ κ ≥ ≥ 0 {\displaystyle \kappa \geq 0} . The probability mass function, for x ∈ ∈ { − − 1 , 1 } {\displaystyle x\in \{-1,1\}} is: f 1 ( x ∣ ∣ μ μ , κ κ ) = e κ κ μ μ x e − − κ κ + e κ κ = σ σ ( 2 κ κ μ μ x ) {\displaystyle f_{1}(x\mid \mu ,\kappa )={\frac {e^{\kappa \mu x}}{e^{-\kappa }+e^{\kappa }}}=\sigma (2\kappa \mu x)} where σ σ ( z ) = 1 / ( 1 + e − − z ) {\displaystyle \sigma (z)=1/(1+e^{-z})} is the logistic sigmoid . The expected value is μ μ tanh ( κ κ ) {\displaystyle \mu \,{\text{tanh}}(\kappa )} . In the uniform case, at κ κ = 0 {\displaystyle \kappa =0} , this distribution degenerates to the Rademacher distribution .

See also [ edit ] Kent distribution , a related distribution on the two-dimensional unit sphere von Mises distribution , von Mises–Fisher distribution where p = 2, the one-dimensional unit circle Bivariate von Mises distribution Directional statistics References [ edit ] ^ Fisher, R. A. (1953). "Dispersion on a sphere".

Proc. R. Soc. Lond. A .

217 (1130): 295– 305.

Bibcode : 1953RSPSA.217..295F .

doi : 10.1098/rspa.1953.0064 .

S2CID 123166853 .

^ Watson, G. S. (1980). "Distributions on the Circle and on the Sphere".

J. Appl. Probab .

19 : 265– 280.

doi : 10.2307/3213566 .

JSTOR 3213566 .

S2CID 222325569 .

^ a b c d Mardia, Kanti ; Jupp, P. E. (1999).

Directional Statistics . John Wiley & Sons Ltd.

ISBN 978-0-471-95333-3 .

^ Embleton, N. I. Fisher, T. Lewis, B. J. J. (1993).

Statistical analysis of spherical data (1st pbk. ed.). Cambridge: Cambridge University Press. pp.

115–116 .

ISBN 0-521-45699-1 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ a b Ulrich, Gary (1984).

"Computer generation of distributions on the m-sphere" .

Applied Statistics .

33 (2): 158– 163.

doi : 10.2307/2347441 .

JSTOR 2347441 .

^ Wood, Andrew T (1994).

"Simulation of the Von Mises Fisher distribution" .

Communications in Statistics - Simulation and Computation .

23 (1): 157– 164.

doi : 10.1080/03610919408813161 .

^ Hornik, Kurt; Grün, Bettina (2014).

"movMF: An R Package for Fitting Mixtures of Von Mises-Fisher Distributions" .

Journal of Statistical Software .

58 (10).

doi : 10.18637/jss.v058.i10 .

S2CID 13171102 .

^ a b Pinzón, Carlos; Jung, Kangsoo (2023-03-03), Fast Python sampler for the von Mises Fisher distribution , retrieved 2023-03-30 ^ De Cao, Nicola; Aziz, Wilker (13 Feb 2023). "The Power Spherical distribution".

arXiv : 2006.04437 [ stat.ML ].

^ Pakyuz-Charrier, Evren; Lindsay, Mark; Ogarko, Vitaliy; Giraud, Jeremie; Jessell, Mark (2018-04-06).

"Monte Carlo simulation for uncertainty estimation on structural data in implicit 3-D geological modeling, a guide for disturbance distribution selection and parameterization" .

Solid Earth .

9 (2): 385– 402.

Bibcode : 2018SolE....9..385P .

doi : 10.5194/se-9-385-2018 .

ISSN 1869-9510 .

^ A., Wood, Andrew T. (1992).

Simulation of the Von Mises Fisher distribution . Centre for Mathematics & its Applications, Australian National University.

OCLC 221030477 .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Gosmann, J; Eliasmith, C (2016).

"Optimizing Semantic Pointer Representations for Symbol-Like Processing in Spiking Neural Networks" .

PLOS ONE .

11 (2): e0149928.

Bibcode : 2016PLoSO..1149928G .

doi : 10.1371/journal.pone.0149928 .

PMC 4762696 .

PMID 26900931 .

^ a b Voelker, Aaron R.; Gosmann, Jan; Stewart, Terrence C.

"Efficiently sampling vectors and coordinates from the n-sphere and n-ball" (PDF) . Centre for Theoretical Neuroscience – Technical Report, 2017 . Retrieved 22 April 2023 .

^ Wang, Tongzhou; Isola, Phillip (2020). "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere".

International Conference on Machine Learning .

arXiv : 2005.10242 .

^ Pal, Subhadip; Sengupta, Subhajit; Mitra, Riten; Banerjee, Arunava (2020).

"Conjugate Priors and Posterior Inference for the Matrix Langevin Distribution on the Stiefel Manifold" .

Bayesian Analysis .

15 (3): 871– 908.

doi : 10.1214/19-BA1176 .

ISSN 1936-0975 .

^ Chikuse, Yasuko (1 May 2003).

"Concentrated matrix Langevin distributions" .

Journal of Multivariate Analysis .

85 (2): 375– 394.

doi : 10.1016/S0047-259X(02)00065-9 .

ISSN 0047-259X .

^ Jupp (1979).

"Maximum likelihood estimators for the matrix von Mises-Fisher and Bingham distributions" .

The Annals of Statistics .

7 (3): 599– 606.

doi : 10.1214/aos/1176344681 .

^ Downs (1972). "Orientational statistics".

Biometrika .

59 (3): 665– 676.

doi : 10.1093/biomet/59.3.665 .

^ Saw, John G (1978).

"A family of distributions on the m-sphere and some hypothesis tests" .

Biometrika .

65 (`): 69– 73.

doi : 10.2307/2335278 .

JSTOR 2335278 .

Further reading [ edit ] Dhillon, I., Sra, S. (2003) "Modeling Data using Directional Distributions". Tech. rep., University of Texas, Austin.

Banerjee, A., Dhillon, I. S., Ghosh, J., & Sra, S. (2005). "Clustering on the unit hypersphere using von Mises-Fisher distributions". Journal of Machine Learning Research, 6(Sep), 1345-1382.

Sra, S. (2011). "A short note on parameter approximation for von Mises-Fisher distributions: And a fast implementation of I_s(x)".

Computational Statistics .

27 : 177– 190.

CiteSeerX 10.1.1.186.1887 .

doi : 10.1007/s00180-011-0232-x .

S2CID 3654195 .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Von_Mises–Fisher_distribution&oldid=1301702913 " Categories : Directional statistics Multivariate continuous distributions Exponential family distributions Continuous distributions Hidden categories: CS1 maint: multiple names: authors list Articles with short description Short description matches Wikidata Pages that use a deprecated format of the math tags This page was last edited on 21 July 2025, at 07:14 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents von Mises–Fisher distribution 2 languages Add topic

