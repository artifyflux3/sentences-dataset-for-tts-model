Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Pseudocount Toggle Pseudocount subsection 2.1 Choice of pseudocount 2.1.1 Weakly-informative prior 2.1.2 Frequentist interval 2.1.3 Known incidence rates 3 Applications Toggle Applications subsection 3.1 Classification 3.2 Statistical language modelling 4 See also 5 References 6 Sources 7 External links Toggle the table of contents Additive smoothing 3 languages العربية Català Deutsch Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical technique for smoothing categorical data For the image processing technique, see Laplacian smoothing .

In statistics , additive smoothing , also called Laplace smoothing [ 1 ] or Lidstone smoothing , is a technique used to smooth count data, eliminating issues caused by certain values having 0 occurrences. Given a set of observation counts x = ⟨ ⟨ x 1 , x 2 , … … , x d ⟩ ⟩ {\displaystyle \mathbf {x} =\langle x_{1},x_{2},\ldots ,x_{d}\rangle } from a d {\displaystyle d} -dimensional multinomial distribution with N {\displaystyle N} trials, a "smoothed" version of the counts gives the estimator θ θ ^ ^ i = x i + α α N + α α d ( i = 1 , … … , d ) , {\displaystyle {\hat {\theta }}_{i}={\frac {x_{i}+\alpha }{N+\alpha d}}\qquad (i=1,\ldots ,d),} where the smoothed count x ^ ^ i = N θ θ ^ ^ i {\displaystyle {\hat {x}}_{i}=N{\hat {\theta }}_{i}} , and the "pseudocount" α > 0 is a smoothing parameter , with α = 0 corresponding to no smoothing (this parameter is explained in § Pseudocount below). Additive smoothing is a type of shrinkage estimator , as the resulting estimate will be between the empirical probability ( relative frequency ) x i / N {\displaystyle x_{i}/N} and the uniform probability 1 / d .

{\displaystyle 1/d.} Common choices for α are 0 (no smoothing), + 1 ⁄ 2 (the Jeffreys prior ), or 1 (Laplace's rule of succession ), [ 2 ] [ 3 ] but the parameter may also be set empirically based on the observed data.

From a Bayesian point of view, this corresponds to the expected value of the posterior distribution , using a symmetric Dirichlet distribution with parameter α as a prior distribution . In the special case where the number of categories is 2, this is equivalent to using a beta distribution as the conjugate prior for the parameters of the binomial distribution .

History [ edit ] Laplace came up with this smoothing technique when he tried to estimate the chance that the sun will rise tomorrow. His rationale was that even given a large sample of days with the rising sun, we still can not be completely sure that the sun will still rise tomorrow (known as the sunrise problem ).

[ 4 ] Pseudocount [ edit ] A pseudocount is an amount (not generally an integer, despite its name) added to the number of observed cases in order to change the expected probability in a model of those data, when not known to be zero. It is so named because, roughly speaking, a pseudo-count of value α α {\displaystyle \alpha } weighs into the posterior distribution similarly to each category having an additional count of α α {\displaystyle \alpha } . If the frequency of each item i {\displaystyle i} is x i {\displaystyle x_{i}} out of N {\displaystyle N} samples, the empirical probability of event i {\displaystyle i} is p i , empirical = x i N , {\displaystyle p_{i,{\text{empirical}}}={\frac {x_{i}}{N}},} but the posterior probability when additively smoothed is p i , α α -smoothed = x i + α α N + α α d , {\displaystyle p_{i,\alpha {\text{-smoothed}}}={\frac {x_{i}+\alpha }{N+\alpha d}},} as if to increase each count x i {\displaystyle x_{i}} by α α {\displaystyle \alpha } a priori.

Depending on the prior knowledge, which is sometimes a subjective value, a pseudocount may have any non-negative finite value. It may only be zero (or the possibility ignored) if impossible by definition, such as the possibility of a decimal digit of π being a letter, or a physical possibility that would be rejected and so not counted, such as a computer printing a letter when a valid program for π is run, or excluded and not counted because of no interest, such as if only interested in the zeros and ones. Generally, there is also a possibility that no value may be computable or observable in a finite time (see the halting problem ). But at least one possibility must have a non-zero pseudocount, otherwise no prediction could be computed before the first observation. The relative values of pseudocounts represent the relative prior expected probabilities of their possibilities. The sum of the pseudocounts, which may be very large, represents the estimated weight of the prior knowledge compared with all the actual observations (one for each) when determining the expected probability.

In any observed data set or sample there is the possibility, especially with low-probability events and with small data sets, of a possible event not occurring. Its observed frequency is therefore zero, apparently implying a probability of zero. This oversimplification is inaccurate and often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and hidden Markov models . By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly zero, zero-frequency problems are avoided. Also see Cromwell's rule .

Choice of pseudocount [ edit ] Weakly-informative prior [ edit ] See also: Uninformative prior One common approach is to add 1 to each observed number of events, including the zero-count possibilities. This is sometimes called Laplace's rule of succession . This approach is equivalent to assuming a uniform prior distribution over the probabilities for each possible event (spanning the simplex where each probability is between 0 and 1, and they all sum to 1).

Using the Jeffreys prior approach, a pseudocount of one half should be added to each possible outcome.

Pseudocounts should be set to one or one-half only when there is no prior knowledge at all – see the principle of indifference . However, given appropriate prior knowledge, the sum should be adjusted in proportion to the expectation that the prior probabilities should be considered correct, despite evidence to the contrary – see further analysis . Higher values are appropriate inasmuch as there is prior knowledge of the true values (for a mint-condition coin, say); lower values inasmuch as there is prior knowledge that there is probable bias, but of unknown degree (for a bent coin, say).

Frequentist interval [ edit ] One way to motivate pseudocounts, particularly for binomial data, is via a formula for the midpoint of an interval estimate , particularly a binomial proportion confidence interval . The best-known is due to Edwin Bidwell Wilson , in Wilson (1927) : the midpoint of the Wilson score interval corresponding to ⁠ z {\displaystyle z} ⁠ standard deviations on either side is n S + z n + 2 z {\displaystyle {\frac {n_{S}+z}{n+2z}}} Taking z = 2 {\displaystyle z=2} standard deviations to approximate a 95% confidence interval ( ⁠ z ≈ ≈ 1.96 {\displaystyle z\approx 1.96} ⁠ ) yields pseudocount of 2 for each outcome, so 4 in total, colloquially known as the "plus four rule": n S + 2 n + 4 {\displaystyle {\frac {n_{S}+2}{n+4}}} This is also the midpoint of the Agresti–Coull interval ( Agresti & Coull 1998 ).

Known incidence rates [ edit ] See also: Empirical Bayes method Often the bias of an unknown trial population is tested against a control population with known parameters (incidence rates) μ μ = ⟨ ⟨ μ μ 1 , μ μ 2 , … … , μ μ d ⟩ ⟩ .

{\displaystyle {\boldsymbol {\mu }}=\langle \mu _{1},\mu _{2},\ldots ,\mu _{d}\rangle .} In this case the uniform probability 1 / d {\displaystyle 1/d} should be replaced by the known incidence rate of the control population μ μ i {\displaystyle \mu _{i}} to calculate the smoothed estimator: θ θ ^ ^ i = x i + μ μ i α α d N + α α d ( i = 1 , … … , d ) .

{\displaystyle {\hat {\theta }}_{i}={\frac {x_{i}+\mu _{i}\alpha d}{N+\alpha d}}\qquad (i=1,\ldots ,d).} As a consistency check, if the empirical estimator happens to equal the incidence rate, i.e.

μ μ i = x i / N , {\displaystyle \mu _{i}=x_{i}/N,} the smoothed estimator is independent of α α {\displaystyle \alpha } and also equals the incidence rate.

Applications [ edit ] Classification [ edit ] Additive smoothing is commonly a component of naive Bayes classifiers .

Statistical language modelling [ edit ] In a bag of words model of natural language processing and information retrieval, the data consists of the number of occurrences of each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Studies have shown that additive smoothing is more effective than other probability smoothing methods in several retrieval tasks such as language-model-based pseudo-relevance feedback and recommender systems .

[ 5 ] [ 6 ] See also [ edit ] Bayesian average Prediction by partial matching Categorical distribution References [ edit ] ^ C. D. Manning, P. Raghavan and H. Schütze (2008).

Introduction to Information Retrieval . Cambridge University Press, p. 260.

^ Jurafsky, Daniel; Martin, James H. (June 2008).

Speech and Language Processing (2nd ed.). Prentice Hall. p. 132.

ISBN 978-0-13-187321-6 .

^ Russell, Stuart; Norvig, Peter (2010).

Artificial Intelligence: A Modern Approach (2nd ed.). Pearson Education, Inc. p. 863.

^ Lecture 5 | Machine Learning (Stanford) at 1h10m into the lecture ^ Hazimeh, Hussein; Zhai, ChengXiang (2015).

"Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback" .

Proceedings of the 2015 International Conference on the Theory of Information Retrieval . pp.

141– 150.

doi : 10.1145/2808194.2809471 .

hdl : 2142/92709 .

ISBN 978-1-4503-3833-2 .

^ Valcarce, Daniel; Parapar, Javier; Barreiro, Álvaro (2016).

"Additive Smoothing for Relevance-Based Language Modelling of Recommender Systems" .

Proceedings of the 4th Spanish Conference on Information Retrieval . pp.

1– 8.

doi : 10.1145/2934732.2934737 .

ISBN 978-1-4503-4141-7 .

Sources [ edit ] Wilson, E. B.

(1927). "Probable inference, the law of succession, and statistical inference".

Journal of the American Statistical Association .

22 (158): 209– 212.

doi : 10.1080/01621459.1927.10502953 .

JSTOR 2276774 .

Agresti, Alan; Coull, Brent A. (1998). "Approximate is better than 'exact' for interval estimation of binomial proportions".

The American Statistician .

52 (2): 119– 126.

doi : 10.2307/2685469 .

JSTOR 2685469 .

MR 1628435 .

External links [ edit ] SF Chen, J Goodman (1996). " An empirical study of smoothing techniques for language modeling ".

Proceedings of the 34th annual meeting on Association for Computational Linguistics .

Pseudocounts Bayesian interpretation of pseudocount regularizers A video explaining the use of Additive smoothing in a Naïve Bayes classifier NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐cwjfw
Cached time: 20250812013040
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.273 seconds
Real time usage: 0.457 seconds
Preprocessor visited node count: 891/1000000
Revision size: 11792/2097152 bytes
Post‐expand include size: 15820/2097152 bytes
Template argument size: 658/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 24341/5000000 bytes
Lua time usage: 0.156/10.000 seconds
Lua memory usage: 6472145/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  294.925      1 -total
 38.86%  114.597      1 Template:Reflist
 33.98%  100.212      4 Template:Cite_book
 23.43%   69.102      1 Template:Short_description
 13.11%   38.660      2 Template:Pagetype
  9.16%   27.005      1 Template:Harvtxt
  8.11%   23.923      1 Template:For
  6.86%   20.241      3 Template:Main_other
  6.21%   18.314      1 Template:SDcat
  4.78%   14.090      2 Template:Cite_journal Saved in parser cache with key enwiki:pcache:17110513:|#|:idhash:canonical and timestamp 20250812013040 and revision id 1285902791. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Additive_smoothing&oldid=1285902791 " Categories : Statistical natural language processing Categorical data Probability theory Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 16 April 2025, at 13:29 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Additive smoothing 3 languages Add topic

