Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Overview Toggle Overview subsection 1.1 Example 2 Column space Toggle Column space subsection 2.1 Definition 2.1.1 Example 2.2 Basis 2.3 Dimension 2.4 Relation to the left null space 2.5 For matrices over a ring 3 Row space Toggle Row space subsection 3.1 Definition 3.2 Basis 3.3 Dimension 3.4 Relation to the null space 3.5 Relation to coimage 4 See also 5 References & Notes 6 Further reading 7 External links Toggle the table of contents Row and column spaces 12 languages Чӑвашла Ελληνικά Français Italiano Nederlands 日本語 Português Русский Українська اردو Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Vector spaces associated to a matrix The row vectors of a matrix . The row space of this matrix is the vector space spanned by the row vectors.

The column vectors of a matrix . The column space of this matrix is the vector space spanned by the column vectors.

In linear algebra , the column space (also called the range or image ) of a matrix A is the span (set of all possible linear combinations ) of its column vectors . The column space of a matrix is the image or range of the corresponding matrix transformation .

Let F {\displaystyle F} be a field . The column space of an m × n matrix with components from F {\displaystyle F} is a linear subspace of the m -space F m {\displaystyle F^{m}} .  The dimension of the column space is called the rank of the matrix and is at most min( m , n ) .

[ 1 ] A definition for matrices over a ring R {\displaystyle R} is also possible .

The row space is defined similarly.

The row space and the column space of a matrix A are sometimes denoted as C ( A T ) and C ( A ) respectively.

[ 2 ] This article considers matrices of real numbers .  The row and column spaces are subspaces of the real spaces R n {\displaystyle \mathbb {R} ^{n}} and R m {\displaystyle \mathbb {R} ^{m}} respectively.

[ 3 ] Overview [ edit ] Let A be an m -by- n matrix. Then rank( A ) = dim(rowsp( A )) = dim(colsp( A )) , [ 4 ] rank( A ) = number of pivots in any echelon form of A , rank( A ) = the maximum number of linearly independent rows or columns of A .

[ 5 ] If the matrix represents a linear transformation , the column space of the matrix equals the image of this linear transformation.

The column space of a matrix A is the set of all linear combinations of the columns in A .  If A = [ a 1 ⋯ a n ] , then colsp( A ) = span({ a 1 , ..., a n }) .

Given a matrix A , the action of the matrix A on a vector x returns a linear combination of the columns of A with the coordinates of x as coefficients; that is, the columns of the matrix generate the column space.

Example [ edit ] Given a matrix J : J = [ 2 4 1 3 2 − − 1 − − 2 1 0 5 1 6 2 2 2 3 6 2 5 1 ] {\displaystyle J={\begin{bmatrix}2&4&1&3&2\\-1&-2&1&0&5\\1&6&2&2&2\\3&6&2&5&1\end{bmatrix}}} the rows are r 1 = [ 2 4 1 3 2 ] {\displaystyle \mathbf {r} _{1}={\begin{bmatrix}2&4&1&3&2\end{bmatrix}}} , r 2 = [ − − 1 − − 2 1 0 5 ] {\displaystyle \mathbf {r} _{2}={\begin{bmatrix}-1&-2&1&0&5\end{bmatrix}}} , r 3 = [ 1 6 2 2 2 ] {\displaystyle \mathbf {r} _{3}={\begin{bmatrix}1&6&2&2&2\end{bmatrix}}} , r 4 = [ 3 6 2 5 1 ] {\displaystyle \mathbf {r} _{4}={\begin{bmatrix}3&6&2&5&1\end{bmatrix}}} .
Consequently, the row space of J is the subspace of R 5 {\displaystyle \mathbb {R} ^{5}} spanned by { r 1 , r 2 , r 3 , r 4 } .   
Since these four row vectors are linearly independent , the row space is 4-dimensional.  Moreover, in this case it can be seen that they are all orthogonal to the vector n = [6, −1, 4, −4, 0] ( n is an element of the kernel of J ), so it can be deduced that the row space consists of all vectors in R 5 {\displaystyle \mathbb {R} ^{5}} that are orthogonal to n .

Column space [ edit ] Definition [ edit ] Let K be a field of scalars . Let A be an m × n matrix, with column vectors v 1 , v 2 , ..., v n .  A linear combination of these vectors is any vector of the form c 1 v 1 + c 2 v 2 + ⋯ ⋯ + c n v n , {\displaystyle c_{1}\mathbf {v} _{1}+c_{2}\mathbf {v} _{2}+\cdots +c_{n}\mathbf {v} _{n},} where c 1 , c 2 , ..., c n are scalars.  The set of all possible linear combinations of v 1 , ..., v n is called the column space of A .  That is, the column space of A is the span of the vectors v 1 , ..., v n .

Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector: A [ c 1 ⋮ ⋮ c n ] = [ a 11 ⋯ ⋯ a 1 n ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ a m 1 ⋯ ⋯ a m n ] [ c 1 ⋮ ⋮ c n ] = [ c 1 a 11 + ⋯ ⋯ + c n a 1 n ⋮ ⋮ c 1 a m 1 + ⋯ ⋯ + c n a m n ] = c 1 [ a 11 ⋮ ⋮ a m 1 ] + ⋯ ⋯ + c n [ a 1 n ⋮ ⋮ a m n ] = c 1 v 1 + ⋯ ⋯ + c n v n {\displaystyle {\begin{array}{rcl}A{\begin{bmatrix}c_{1}\\\vdots \\c_{n}\end{bmatrix}}&=&{\begin{bmatrix}a_{11}&\cdots &a_{1n}\\\vdots &\ddots &\vdots \\a_{m1}&\cdots &a_{mn}\end{bmatrix}}{\begin{bmatrix}c_{1}\\\vdots \\c_{n}\end{bmatrix}}={\begin{bmatrix}c_{1}a_{11}+\cdots +c_{n}a_{1n}\\\vdots \\c_{1}a_{m1}+\cdots +c_{n}a_{mn}\end{bmatrix}}=c_{1}{\begin{bmatrix}a_{11}\\\vdots \\a_{m1}\end{bmatrix}}+\cdots +c_{n}{\begin{bmatrix}a_{1n}\\\vdots \\a_{mn}\end{bmatrix}}\\&=&c_{1}\mathbf {v} _{1}+\cdots +c_{n}\mathbf {v} _{n}\end{array}}} Therefore, the column space of A consists of all possible products A x , for x ∈ K n .  This is the same as the image (or range ) of the corresponding matrix transformation .

Example [ edit ] If A = [ 1 0 0 1 2 0 ] {\displaystyle A={\begin{bmatrix}1&0\\0&1\\2&0\end{bmatrix}}} , then the column vectors are v 1 = [1, 0, 2] T and v 2 = [0, 1, 0] T .
A linear combination of v 1 and v 2 is any vector of the form c 1 [ 1 0 2 ] + c 2 [ 0 1 0 ] = [ c 1 c 2 2 c 1 ] {\displaystyle c_{1}{\begin{bmatrix}1\\0\\2\end{bmatrix}}+c_{2}{\begin{bmatrix}0\\1\\0\end{bmatrix}}={\begin{bmatrix}c_{1}\\c_{2}\\2c_{1}\end{bmatrix}}} The set of all such vectors is the column space of A . In this case, the column space is precisely the set of vectors ( x , y , z ) ∈ R 3 satisfying the equation z = 2 x (using Cartesian coordinates , this set is a plane through the origin in three-dimensional space ).

Basis [ edit ] The columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent .  Fortunately, elementary row operations do not affect the dependence relations between the column vectors.  This makes it possible to use row reduction to find a basis for the column space.

For example, consider the matrix A = [ 1 3 1 4 2 7 3 9 1 5 3 1 1 2 0 8 ] .

{\displaystyle A={\begin{bmatrix}1&3&1&4\\2&7&3&9\\1&5&3&1\\1&2&0&8\end{bmatrix}}.} The columns of this matrix span the column space, but they may not be linearly independent , in which case some subset of them will form a basis.  To find this basis, we reduce A to reduced row echelon form : [ 1 3 1 4 2 7 3 9 1 5 3 1 1 2 0 8 ] ∼ ∼ [ 1 3 1 4 0 1 1 1 0 2 2 − − 3 0 − − 1 − − 1 4 ] ∼ ∼ [ 1 0 − − 2 1 0 1 1 1 0 0 0 − − 5 0 0 0 5 ] ∼ ∼ [ 1 0 − − 2 0 0 1 1 0 0 0 0 1 0 0 0 0 ] .

{\displaystyle {\begin{bmatrix}1&3&1&4\\2&7&3&9\\1&5&3&1\\1&2&0&8\end{bmatrix}}\sim {\begin{bmatrix}1&3&1&4\\0&1&1&1\\0&2&2&-3\\0&-1&-1&4\end{bmatrix}}\sim {\begin{bmatrix}1&0&-2&1\\0&1&1&1\\0&0&0&-5\\0&0&0&5\end{bmatrix}}\sim {\begin{bmatrix}1&0&-2&0\\0&1&1&0\\0&0&0&1\\0&0&0&0\end{bmatrix}}.} [ 6 ] At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two.  (Specifically, v 3 = −2 v 1 + v 2 .)  Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space: [ 1 2 1 1 ] , [ 3 7 5 2 ] , [ 4 9 1 8 ] .

{\displaystyle {\begin{bmatrix}1\\2\\1\\1\end{bmatrix}},\;\;{\begin{bmatrix}3\\7\\5\\2\end{bmatrix}},\;\;{\begin{bmatrix}4\\9\\1\\8\end{bmatrix}}.} Note that the independent columns of the reduced row echelon form are precisely the columns with pivots .  This makes it possible to determine which columns are linearly independent by reducing only to echelon form .

The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set.  Also finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix A T .

To find the basis in a practical setting (e.g., for large matrices), the singular-value decomposition is typically used.

Dimension [ edit ] Main article: Rank (linear algebra) The dimension of the column space is called the rank of the matrix.  The rank is equal to the number of pivots in the reduced row echelon form , and is the maximum number of linearly independent columns that can be chosen from the matrix.  For example, the 4 × 4 matrix in the example above has rank three.

Because the column space is the image of the corresponding matrix transformation , the rank of a matrix is the same as the dimension of the image.  For example, the transformation R 4 → → R 4 {\displaystyle \mathbb {R} ^{4}\to \mathbb {R} ^{4}} described by the matrix above maps all of R 4 {\displaystyle \mathbb {R} ^{4}} to some three-dimensional subspace .

The nullity of a matrix is the dimension of the null space , and is equal to the number of columns in the reduced row echelon form that do not have pivots.

[ 7 ] The rank and nullity of a matrix A with n columns are related by the equation: rank ⁡ ⁡ ( A ) + nullity ⁡ ⁡ ( A ) = n .

{\displaystyle \operatorname {rank} (A)+\operatorname {nullity} (A)=n.\,} This is known as the rank–nullity theorem .

Relation to the left null space [ edit ] The left null space of A is the set of all vectors x such that x T A = 0 T .  It is the same as the null space of the transpose of A . The product of the matrix A T and the vector x can be written in terms of the dot product of vectors: A T x = [ v 1 ⋅ ⋅ x v 2 ⋅ ⋅ x ⋮ ⋮ v n ⋅ ⋅ x ] , {\displaystyle A^{\mathsf {T}}\mathbf {x} ={\begin{bmatrix}\mathbf {v} _{1}\cdot \mathbf {x} \\\mathbf {v} _{2}\cdot \mathbf {x} \\\vdots \\\mathbf {v} _{n}\cdot \mathbf {x} \end{bmatrix}},} because row vectors of A T are transposes of column vectors v k of A .  Thus A T x = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A .

It follows that the left null space (the null space of A T ) is the orthogonal complement to the column space of A .

For a matrix A , the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces .

For matrices over a ring [ edit ] Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K as ∑ ∑ k = 1 n v k c k {\displaystyle \sum \limits _{k=1}^{n}\mathbf {v} _{k}c_{k}} for any c 1 , ..., c n , with replacement of the vector m -space with " right free module ", which changes the order of scalar multiplication of the vector v k to the scalar c k such that it is written in an unusual order vector – scalar .

[ 8 ] Row space [ edit ] Definition [ edit ] Let K be a field of scalars . Let A be an m × n matrix, with row vectors r 1 , r 2 , ..., r m .  A linear combination of these vectors is any vector of the form c 1 r 1 + c 2 r 2 + ⋯ ⋯ + c m r m , {\displaystyle c_{1}\mathbf {r} _{1}+c_{2}\mathbf {r} _{2}+\cdots +c_{m}\mathbf {r} _{m},} where c 1 , c 2 , ..., c m are scalars.  The set of all possible linear combinations of r 1 , ..., r m is called the row space of A .  That is, the row space of A is the span of the vectors r 1 , ..., r m .

For example, if A = [ 1 0 2 0 1 0 ] , {\displaystyle A={\begin{bmatrix}1&0&2\\0&1&0\end{bmatrix}},} then the row vectors are r 1 = [1, 0, 2] and r 2 = [0, 1, 0] .  A linear combination of r 1 and r 2 is any vector of the form c 1 [ 1 0 2 ] + c 2 [ 0 1 0 ] = [ c 1 c 2 2 c 1 ] .

{\displaystyle c_{1}{\begin{bmatrix}1&0&2\end{bmatrix}}+c_{2}{\begin{bmatrix}0&1&0\end{bmatrix}}={\begin{bmatrix}c_{1}&c_{2}&2c_{1}\end{bmatrix}}.} The set of all such vectors is the row space of A .  In this case, the row space is precisely the set of vectors ( x , y , z ) ∈ K 3 satisfying the equation z = 2 x (using Cartesian coordinates , this set is a plane through the origin in three-dimensional space ).

For a matrix that represents a homogeneous system of linear equations , the row space consists of all linear equations that follow from those in the system.

The column space of A is equal to the row space of A T .

Basis [ edit ] The row space is not affected by elementary row operations .  This makes it possible to use row reduction to find a basis for the row space.

For example, consider the matrix A = [ 1 3 2 2 7 4 1 5 2 ] .

{\displaystyle A={\begin{bmatrix}1&3&2\\2&7&4\\1&5&2\end{bmatrix}}.} The rows of this matrix span the row space, but they may not be linearly independent , in which case the rows will not be a basis.  To find a basis, we reduce A to row echelon form : r 1 , r 2 , r 3 represents the rows.

[ 1 3 2 2 7 4 1 5 2 ] → r 2 − − 2 r 1 → → r 2 [ 1 3 2 0 1 0 1 5 2 ] → r 3 − − r 1 → → r 3 [ 1 3 2 0 1 0 0 2 0 ] → r 3 − − 2 r 2 → → r 3 [ 1 3 2 0 1 0 0 0 0 ] → r 1 − − 3 r 2 → → r 1 [ 1 0 2 0 1 0 0 0 0 ] .

{\displaystyle {\begin{aligned}{\begin{bmatrix}1&3&2\\2&7&4\\1&5&2\end{bmatrix}}&\xrightarrow {\mathbf {r} _{2}-2\mathbf {r} _{1}\to \mathbf {r} _{2}} {\begin{bmatrix}1&3&2\\0&1&0\\1&5&2\end{bmatrix}}\xrightarrow {\mathbf {r} _{3}-\,\,\mathbf {r} _{1}\to \mathbf {r} _{3}} {\begin{bmatrix}1&3&2\\0&1&0\\0&2&0\end{bmatrix}}\\&\xrightarrow {\mathbf {r} _{3}-2\mathbf {r} _{2}\to \mathbf {r} _{3}} {\begin{bmatrix}1&3&2\\0&1&0\\0&0&0\end{bmatrix}}\xrightarrow {\mathbf {r} _{1}-3\mathbf {r} _{2}\to \mathbf {r} _{1}} {\begin{bmatrix}1&0&2\\0&1&0\\0&0&0\end{bmatrix}}.\end{aligned}}} Once the matrix is in echelon form, the nonzero rows are a basis for the row space.  In this case, the basis is { [1, 3, 2], [2, 7, 4] } . Another possible basis { [1, 0, 2], [0, 1, 0] } comes from a further reduction.

[ 9 ] This algorithm can be used in general to find a basis for the span of a set of vectors.  If the matrix is further simplified to reduced row echelon form , then the resulting basis is uniquely determined by the row space.

It is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the determinantal rank of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of A T is equal to the row space of A . Using the example matrix A above, find A T and reduce it to row echelon form: A T = [ 1 2 1 3 7 5 2 4 2 ] ∼ ∼ [ 1 2 1 0 1 2 0 0 0 ] .

{\displaystyle A^{\mathrm {T} }={\begin{bmatrix}1&2&1\\3&7&5\\2&4&2\end{bmatrix}}\sim {\begin{bmatrix}1&2&1\\0&1&2\\0&0&0\end{bmatrix}}.} The pivots indicate that the first two columns of A T form a basis of the column space of A T . Therefore, the first two rows of A (before any row reductions) also form a basis of the row space of A .

Dimension [ edit ] Main article: Rank (linear algebra) The dimension of the row space is called the rank of the matrix.  This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots.  For example, the 3 × 3 matrix in the example above has rank two.

[ 9 ] The rank of a matrix is also equal to the dimension of the column space .  The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation: rank ⁡ ⁡ ( A ) + nullity ⁡ ⁡ ( A ) = n , {\displaystyle \operatorname {rank} (A)+\operatorname {nullity} (A)=n,} where n is the number of columns of the matrix A .  The equation above is known as the rank–nullity theorem .

Relation to the null space [ edit ] The null space of matrix A is the set of all vectors x for which A x = 0 .  The product of the matrix A and the vector x can be written in terms of the dot product of vectors: A x = [ r 1 ⋅ ⋅ x r 2 ⋅ ⋅ x ⋮ ⋮ r m ⋅ ⋅ x ] , {\displaystyle A\mathbf {x} ={\begin{bmatrix}\mathbf {r} _{1}\cdot \mathbf {x} \\\mathbf {r} _{2}\cdot \mathbf {x} \\\vdots \\\mathbf {r} _{m}\cdot \mathbf {x} \end{bmatrix}},} where r 1 , ..., r m are the row vectors of A .  Thus A x = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A .

It follows that the null space of A is the orthogonal complement to the row space.  For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin.  This provides a proof of the rank–nullity theorem (see dimension above).

The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space ).

Relation to coimage [ edit ] If V and W are vector spaces , then the kernel of a linear transformation T : V → W is the set of vectors v ∈ V for which T ( v ) = 0 .  The kernel of a linear transformation is analogous to the null space of a matrix.

If V is an inner product space , then the orthogonal complement to the kernel can be thought of as a generalization of the row space.  This is sometimes called the coimage of T .  The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T .

When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker( T ) .

See also [ edit ] Euclidean subspace References & Notes [ edit ] ^ Linear algebra, as discussed in this article, is a very well established mathematical discipline for which there are many sources. Almost all of the material in this article can be found in Lay 2005, Meyer 2001, and Strang 2005.

^ Strang, Gilbert (2016).

Introduction to linear algebra (Fifth ed.). Wellesley, MA: Wellesley-Cambridge Press. pp. 128, 168.

ISBN 978-0-9802327-7-6 .

OCLC 956503593 .

^ Anton (1987 , p. 179) ^ Anton (1987 , p. 183) ^ Beauregard & Fraleigh (1973 , p. 254) ^ This computation uses the Gauss–Jordan row-reduction algorithm.  Each of the shown steps involves multiple elementary row operations.

^ Columns without pivots represent free variables in the associated homogeneous system of linear equations .

^ Important only if K is not commutative . Actually, this form is merely a product A c of the matrix A to the column vector c from K n where the order of factors is preserved , unlike the formula above .

^ a b The example is valid over the real numbers , the rational numbers , and other number fields . It is not necessarily correct over fields and rings with non-zero characteristic .

See also: Linear algebra § Further reading Further reading [ edit ] Anton, Howard (1987), Elementary Linear Algebra (5th ed.), New York: Wiley , ISBN 0-471-84819-0 Axler, Sheldon Jay (1997), Linear Algebra Done Right (2nd ed.), Springer-Verlag, ISBN 0-387-98259-0 Banerjee, Sudipto; Roy, Anindya (June 6, 2014), Linear Algebra and Matrix Analysis for Statistics (1st ed.), CRC Press, ISBN 978-1-42-009538-8 Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields , Boston: Houghton Mifflin Company , ISBN 0-395-14017-X Lay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall Meyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra , Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8 , archived from the original on March 1, 2001 Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Brooks/Cole, ISBN 0-534-99845-3 Strang, Gilbert (July 19, 2005), Linear Algebra and Its Applications (4th ed.), Brooks Cole, ISBN 978-0-03-010567-8 External links [ edit ] Wikibooks has a book on the topic of: Linear Algebra/Column and Row Spaces Weisstein, Eric W.

"Row Space" .

MathWorld .

Weisstein, Eric W.

"Column Space" .

MathWorld .

Gilbert Strang , MIT Linear Algebra Lecture on the Four Fundamental Subspaces at Google Video, from MIT OpenCourseWare Khan Academy video tutorial Lecture on column space and nullspace by Gilbert Strang of MIT Row Space and Column Space v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐brvfm
Cached time: 20250812013601
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.590 seconds
Real time usage: 0.741 seconds
Preprocessor visited node count: 5419/1000000
Revision size: 24551/2097152 bytes
Post‐expand include size: 55978/2097152 bytes
Template argument size: 7936/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 45331/5000000 bytes
Lua time usage: 0.335/10.000 seconds
Lua memory usage: 6870555/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  559.124      1 -total
 28.67%  160.320      1 Template:Reflist
 17.33%   96.921      1 Template:Cite_book
 15.64%   87.463      1 Template:Linear_algebra
 15.38%   85.993      1 Template:Navbox
 15.29%   85.472      1 Template:Short_description
 14.28%   79.832     75 Template:Math
 10.51%   58.782      9 Template:Citation
 10.10%   56.491      2 Template:Pagetype
  7.11%   39.748      3 Template:Harvtxt Saved in parser cache with key enwiki:pcache:97848:|#|:idhash:canonical and timestamp 20250812013601 and revision id 1303691529. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Row_and_column_spaces&oldid=1303691529 " Categories : Abstract algebra Linear algebra Matrices (mathematics) Hidden categories: Articles with short description Short description is different from Wikidata This page was last edited on 1 August 2025, at 14:01 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Row and column spaces 12 languages Add topic

