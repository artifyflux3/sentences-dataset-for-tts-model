Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Statistical physics 2 Markov property 3 Formal definition on lattices Toggle Formal definition on lattices subsection 3.1 An example 4 See also 5 References 6 Further reading Toggle the table of contents Gibbs measure 1 language Português Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Mathematical concept In physics and mathematics , the Gibbs measure , named after Josiah Willard Gibbs , is a probability measure frequently seen in many problems of probability theory and statistical mechanics . It is a generalization of the canonical ensemble to infinite systems. 
The canonical ensemble gives the probability of the system X being in state x (equivalently, of the random variable X having value x ) as P ( X = x ) = 1 Z ( β β ) exp ⁡ ⁡ ( − − β β E ( x ) ) .

{\displaystyle P(X=x)={\frac {1}{Z(\beta )}}\exp(-\beta E(x)).} Here, E is a function from the space of states to the real numbers; in physics applications, E ( x ) is interpreted as the energy of the configuration x . The parameter β is a free parameter; in physics, it is the inverse temperature . The normalizing constant Z ( β ) is the partition function . However, in infinite systems, the total energy is no longer a finite number and cannot be used in the traditional construction of the probability distribution of a canonical ensemble. Traditional approaches in statistical physics studied the limit of intensive properties as the size of a finite system approaches infinity (the thermodynamic limit ). When the energy function can be written as a sum of terms that each involve only variables from a finite subsystem, the notion of a Gibbs measure provides an alternative approach. Gibbs measures were proposed by probability theorists such as Dobrushin , Lanford , and Ruelle and provided a framework to directly study infinite systems, instead of taking the limit of finite systems.

A measure is a Gibbs measure if the conditional probabilities it induces on each finite subsystem satisfy a consistency condition: if all degrees of freedom outside the finite subsystem are frozen, the canonical ensemble for the subsystem subject to these boundary conditions matches the probabilities in the Gibbs measure conditional on the frozen degrees of freedom.

The Hammersley–Clifford theorem implies that any probability measure that satisfies a Markov property is a Gibbs measure for an appropriate choice of (locally defined) energy function. Therefore, the Gibbs measure applies to widespread problems outside of physics , such as Hopfield networks , Markov networks , Markov logic networks , and boundedly rational potential games in game theory and economics. 
A Gibbs measure in a system with local (finite-range) interactions maximizes the entropy density for a given expected energy density ; or, equivalently, it minimizes the free energy density.

The Gibbs measure of an infinite system is not necessarily unique, in contrast to the canonical ensemble of a finite system, which is unique. The existence of more than one Gibbs measure is associated with statistical phenomena such as symmetry breaking and phase coexistence .

Statistical physics [ edit ] The set of Gibbs measures on a system is always convex, [ 1 ] so there is either a unique Gibbs measure (in which case the system is said to be " ergodic "), or there are infinitely many (and the system is called "nonergodic"). In the nonergodic case, the Gibbs measures can be expressed as the set of convex combinations of a much smaller number of special Gibbs measures known as "pure states" (not to be confused with the related but distinct notion of pure states in quantum mechanics ). In physical applications, the Hamiltonian (the energy function) usually has some sense of locality , and the pure states have the cluster decomposition property that "far-separated subsystems" are independent. In practice, physically realistic systems are found in one of these pure states.

If the Hamiltonian possesses a symmetry, then a unique (i.e. ergodic) Gibbs measure will necessarily be invariant under the symmetry. But in the case of multiple (i.e. nonergodic) Gibbs measures, the pure states are typically not invariant under the Hamiltonian's symmetry. For example, in the infinite ferromagnetic Ising model below the critical temperature, there are two pure states, the "mostly-up" and "mostly-down" states, which are interchanged under the model's Z 2 {\displaystyle \mathbb {Z} _{2}} symmetry.

Markov property [ edit ] An example of the Markov property can be seen in the Gibbs measure of the Ising model . The probability for a given spin σ k to be in state s could, in principle, depend on the states of all other spins in the system. Thus, we may write the probability as P ( σ σ k = s ∣ ∣ σ σ j , j ≠ ≠ k ) {\displaystyle P(\sigma _{k}=s\mid \sigma _{j},\,j\neq k)} .

However, in an Ising model with only finite-range interactions (for example, nearest-neighbor interactions), we actually have P ( σ σ k = s ∣ ∣ σ σ j , j ≠ ≠ k ) = P ( σ σ k = s ∣ ∣ σ σ j , j ∈ ∈ N k ) {\displaystyle P(\sigma _{k}=s\mid \sigma _{j},\,j\neq k)=P(\sigma _{k}=s\mid \sigma _{j},\,j\in N_{k})} , where N k is a neighborhood of the site k . That is, the probability at site k depends only on the spins in a finite neighborhood. This last equation is in the form of a local Markov property . Measures with this property are sometimes called Markov random fields .  More strongly, the converse is also true: any positive probability distribution (nonzero density everywhere) having the Markov property can be represented as a Gibbs measure for an appropriate energy function.

[ 2 ] This is the Hammersley–Clifford theorem .

Formal definition on lattices [ edit ] What follows is a formal definition for the special case of a random field on a lattice. The idea of a Gibbs measure is, however, much more general than this.

The definition of a Gibbs random field on a lattice requires some terminology: The lattice : A countable set L {\displaystyle \mathbb {L} } .

The single-spin space : A probability space ( S , S , λ λ ) {\displaystyle (S,{\mathcal {S}},\lambda )} .

The configuration space : ( Ω Ω , F ) {\displaystyle (\Omega ,{\mathcal {F}})} , where Ω Ω = S L {\displaystyle \Omega =S^{\mathbb {L} }} and F = S L {\displaystyle {\mathcal {F}}={\mathcal {S}}^{\mathbb {L} }} .

Given a configuration ω ∈ Ω and a subset Λ Λ ⊂ ⊂ L {\displaystyle \Lambda \subset \mathbb {L} } , the restriction of ω to Λ is ω ω Λ Λ = ( ω ω ( t ) ) t ∈ ∈ Λ Λ {\displaystyle \omega _{\Lambda }=(\omega (t))_{t\in \Lambda }} . If Λ Λ 1 ∩ ∩ Λ Λ 2 = ∅ ∅ {\displaystyle \Lambda _{1}\cap \Lambda _{2}=\emptyset } and Λ Λ 1 ∪ ∪ Λ Λ 2 = L {\displaystyle \Lambda _{1}\cup \Lambda _{2}=\mathbb {L} } , then the configuration ω ω Λ Λ 1 ω ω Λ Λ 2 {\displaystyle \omega _{\Lambda _{1}}\omega _{\Lambda _{2}}} is the configuration whose restrictions to Λ 1 and Λ 2 are ω ω Λ Λ 1 {\displaystyle \omega _{\Lambda _{1}}} and ω ω Λ Λ 2 {\displaystyle \omega _{\Lambda _{2}}} , respectively.

The set L {\displaystyle {\mathcal {L}}} of all finite subsets of L {\displaystyle \mathbb {L} } .

For each subset Λ Λ ⊂ ⊂ L {\displaystyle \Lambda \subset \mathbb {L} } , F Λ Λ {\displaystyle {\mathcal {F}}_{\Lambda }} is the σ -algebra generated by the family of functions ( σ σ ( t ) ) t ∈ ∈ Λ Λ {\displaystyle (\sigma (t))_{t\in \Lambda }} , where σ σ ( t ) ( ω ω ) = ω ω ( t ) {\displaystyle \sigma (t)(\omega )=\omega (t)} . The union of these σ -algebras as Λ Λ {\displaystyle \Lambda } varies over L {\displaystyle {\mathcal {L}}} is the algebra of cylinder sets on the lattice.

The potential : A family Φ Φ = ( Φ Φ A ) A ∈ ∈ L {\displaystyle \Phi =(\Phi _{A})_{A\in {\mathcal {L}}}} of functions Φ A : Ω → R such that For each A ∈ ∈ L , Φ Φ A {\displaystyle A\in {\mathcal {L}},\Phi _{A}} is F A {\displaystyle {\mathcal {F}}_{A}} - measurable , meaning it depends only on the restriction ω ω A {\displaystyle \omega _{A}} (and does so measurably).

For all Λ Λ ∈ ∈ L {\displaystyle \Lambda \in {\mathcal {L}}} and ω ∈ Ω , the following series exists: [ when defined as?

] H Λ Λ Φ Φ ( ω ω ) = ∑ ∑ A ∈ ∈ L , A ∩ ∩ Λ Λ ≠ ≠ ∅ ∅ Φ Φ A ( ω ω ) .

{\displaystyle H_{\Lambda }^{\Phi }(\omega )=\sum _{A\in {\mathcal {L}},A\cap \Lambda \neq \emptyset }\Phi _{A}(\omega ).} We interpret Φ A as the contribution to the total energy (the Hamiltonian) associated to the interaction among all the points of finite set A . 
Then H Λ Λ Φ Φ ( ω ω ) {\displaystyle H_{\Lambda }^{\Phi }(\omega )} as the contribution to the total energy of all the finite sets A that meet Λ Λ {\displaystyle \Lambda } . Note that the total energy is typically infinite, but when we "localize" to each Λ Λ {\displaystyle \Lambda } it may be finite, we hope.

The Hamiltonian in Λ Λ ∈ ∈ L {\displaystyle \Lambda \in {\mathcal {L}}} with boundary conditions ω ω ¯ ¯ {\displaystyle {\bar {\omega }}} , for the potential Φ , is defined by H Λ Λ Φ Φ ( ω ω ∣ ∣ ω ω ¯ ¯ ) = H Λ Λ Φ Φ ( ω ω Λ Λ ω ω ¯ ¯ Λ Λ c ) {\displaystyle H_{\Lambda }^{\Phi }(\omega \mid {\bar {\omega }})=H_{\Lambda }^{\Phi }\left(\omega _{\Lambda }{\bar {\omega }}_{\Lambda ^{c}}\right)} where ω ω Λ Λ ω ω ¯ ¯ Λ Λ c {\displaystyle \omega _{\Lambda }{\bar {\omega }}_{\Lambda ^{c}}} denotes the configuration taking the values of ω ω {\displaystyle \omega } in Λ Λ {\displaystyle \Lambda } , and those of ω ω ¯ ¯ {\displaystyle {\bar {\omega }}} in Λ Λ c := L ∖ ∖ Λ Λ {\displaystyle \Lambda ^{c}:=\mathbb {L} \setminus \Lambda } .

The partition function in Λ Λ ∈ ∈ L {\displaystyle \Lambda \in {\mathcal {L}}} with boundary conditions ω ω ¯ ¯ {\displaystyle {\bar {\omega }}} and inverse temperature β > 0 (for the potential Φ and λ ) is defined by Z Λ Λ Φ Φ ( ω ω ¯ ¯ ) = ∫ ∫ λ λ Λ Λ ( d ω ω ) exp ⁡ ⁡ ( − − β β H Λ Λ Φ Φ ( ω ω ∣ ∣ ω ω ¯ ¯ ) ) , {\displaystyle Z_{\Lambda }^{\Phi }({\bar {\omega }})=\int \lambda ^{\Lambda }(\mathrm {d} \omega )\exp(-\beta H_{\Lambda }^{\Phi }(\omega \mid {\bar {\omega }})),} where λ λ Λ Λ ( d ω ω ) = ∏ ∏ t ∈ ∈ Λ Λ λ λ ( d ω ω ( t ) ) , {\displaystyle \lambda ^{\Lambda }(\mathrm {d} \omega )=\prod _{t\in \Lambda }\lambda (\mathrm {d} \omega (t)),} is the product measure A potential Φ is λ -admissible if Z Λ Λ Φ Φ ( ω ω ¯ ¯ ) {\displaystyle Z_{\Lambda }^{\Phi }({\bar {\omega }})} is finite for all Λ Λ ∈ ∈ L , ω ω ¯ ¯ ∈ ∈ Ω Ω {\displaystyle \Lambda \in {\mathcal {L}},{\bar {\omega }}\in \Omega } and β > 0 .

A probability measure μ on ( Ω Ω , F ) {\displaystyle (\Omega ,{\mathcal {F}})} is a Gibbs measure for a λ -admissible potential Φ if it satisfies the Dobrushin–Lanford–Ruelle (DLR) equation ∫ ∫ μ μ ( d ω ω ¯ ¯ ) Z Λ Λ Φ Φ ( ω ω ¯ ¯ ) − − 1 ∫ ∫ λ λ Λ Λ ( d ω ω ) exp ⁡ ⁡ ( − − β β H Λ Λ Φ Φ ( ω ω ∣ ∣ ω ω ¯ ¯ ) ) 1 A ( ω ω Λ Λ ω ω ¯ ¯ Λ Λ c ) = μ μ ( A ) , {\displaystyle \int \mu (\mathrm {d} {\bar {\omega }})Z_{\Lambda }^{\Phi }({\bar {\omega }})^{-1}\int \lambda ^{\Lambda }(\mathrm {d} \omega )\exp(-\beta H_{\Lambda }^{\Phi }(\omega \mid {\bar {\omega }}))\,1_{A}(\omega _{\Lambda }{\bar {\omega }}_{\Lambda ^{c}})=\mu (A),} for all A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} and Λ Λ ∈ ∈ L {\displaystyle \Lambda \in {\mathcal {L}}} .

An example [ edit ] To help understand the above definitions, here are the corresponding quantities in the important example of the Ising model with nearest-neighbor interactions (coupling constant J ) and a magnetic field ( h ), on Z d : The lattice is simply L = Z d {\displaystyle \mathbb {L} =\mathbf {Z} ^{d}} .

The single-spin space is S = {−1, 1}.

The potential is given by Φ Φ A ( ω ω ) = { − − J ω ω ( t 1 ) ω ω ( t 2 ) if A = { t 1 , t 2 } with ‖ ‖ t 2 − − t 1 ‖ ‖ 1 = 1 − − h ω ω ( t ) if A = { t } 0 otherwise {\displaystyle \Phi _{A}(\omega )={\begin{cases}-J\,\omega (t_{1})\omega (t_{2})&{\text{if }}A=\{t_{1},t_{2}\}{\text{ with }}\|t_{2}-t_{1}\|_{1}=1\\-h\,\omega (t)&{\text{if }}A=\{t\}\\0&{\text{otherwise}}\end{cases}}} See also [ edit ] Boltzmann distribution Exponential family Gibbs algorithm Gibbs sampling Interacting particle system Potential game Softmax Stochastic cellular automata References [ edit ] ^ "Gibbs measures" (PDF) .

^ Ross Kindermann and J. Laurie Snell, Markov Random Fields and Their Applications (1980) American Mathematical Society, ISBN 0-8218-5001-6 Further reading [ edit ] Georgii, H.-O. (2011) [1988].

Gibbs Measures and Phase Transitions (2nd ed.). Berlin: de Gruyter.

ISBN 978-3-11-025029-9 .

Friedli, S.; Velenik, Y. (2017).

Statistical Mechanics of Lattice Systems: a Concrete Mathematical Introduction . Cambridge: Cambridge University Press.

ISBN 9781107184824 .

v t e Stochastic processes Discrete time Bernoulli process Branching process Chinese restaurant process Galton–Watson process Independent and identically distributed random variables Markov chain Moran process Random walk Loop-erased Self-avoiding Biased Maximal entropy Continuous time Additive process Airy process Bessel process Birth–death process pure birth Brownian motion Bridge Dyson Excursion Fractional Geometric Meander Cauchy process Contact process Continuous-time random walk Cox process Diffusion process Empirical process Feller process Fleming–Viot process Gamma process Geometric process Hawkes process Hunt process Interacting particle systems Itô diffusion Itô process Jump diffusion Jump process Lévy process Local time Markov additive process McKean–Vlasov process Ornstein–Uhlenbeck process Poisson process Compound Non-homogeneous Quasimartingale Schramm–Loewner evolution Semimartingale Sigma-martingale Stable process Superprocess Telegraph process Variance gamma process Wiener process Wiener sausage Both Branching process Gaussian process Hidden Markov model (HMM) Markov process Martingale Differences Local Sub- Super- Random dynamical system Regenerative process Renewal process Stochastic chains with memory of variable length White noise Fields and other Dirichlet process Gaussian random field Gibbs measure Hopfield model Ising model Potts model Boolean network Markov random field Percolation Pitman–Yor process Point process Cox Determinantal Poisson Random field Random graph Time series models Autoregressive conditional heteroskedasticity (ARCH) model Autoregressive integrated moving average (ARIMA) model Autoregressive (AR) model Autoregressive–moving-average (ARMA) model Generalized autoregressive conditional heteroskedasticity (GARCH) model Moving-average (MA) model Financial models Binomial options pricing model Black–Derman–Toy Black–Karasinski Black–Scholes Chan–Karolyi–Longstaff–Sanders (CKLS) Chen Constant elasticity of variance (CEV) Cox–Ingersoll–Ross (CIR) Garman–Kohlhagen Heath–Jarrow–Morton (HJM) Heston Ho–Lee Hull–White Korn-Kreer-Lenssen LIBOR market Rendleman–Bartter SABR volatility Vašíček Wilkie Actuarial models Bühlmann Cramér–Lundberg Risk process Sparre–Anderson Queueing models Bulk Fluid Generalized queueing network M/G/1 M/M/1 M/M/c Properties Càdlàg paths Continuous Continuous paths Ergodic Exchangeable Feller-continuous Gauss–Markov Markov Mixing Piecewise-deterministic Predictable Progressively measurable Self-similar Stationary Time-reversible Limit theorems Central limit theorem Donsker's theorem Doob's martingale convergence theorems Ergodic theorem Fisher–Tippett–Gnedenko theorem Large deviation principle Law of large numbers (weak/strong) Law of the iterated logarithm Maximal ergodic theorem Sanov's theorem Zero–one laws ( Blumenthal , Borel–Cantelli , Engelbert–Schmidt , Hewitt–Savage , Kolmogorov , Lévy ) Inequalities Burkholder–Davis–Gundy Doob's martingale Doob's upcrossing Kunita–Watanabe Marcinkiewicz–Zygmund Tools Cameron–Martin formula Convergence of random variables Doléans-Dade exponential Doob decomposition theorem Doob–Meyer decomposition theorem Doob's optional stopping theorem Dynkin's formula Feynman–Kac formula Filtration Girsanov theorem Infinitesimal generator Itô integral Itô's lemma Karhunen–Loève theorem Kolmogorov continuity theorem Kolmogorov extension theorem Lévy–Prokhorov metric Malliavin calculus Martingale representation theorem Optional stopping theorem Prokhorov's theorem Quadratic variation Reflection principle Skorokhod integral Skorokhod's representation theorem Skorokhod space Snell envelope Stochastic differential equation Tanaka Stopping time Stratonovich integral Uniform integrability Usual hypotheses Wiener space Classical Abstract Disciplines Actuarial mathematics Control theory Econometrics Ergodic theory Extreme value theory (EVT) Large deviations theory Mathematical finance Mathematical statistics Probability theory Queueing theory Renewal theory Ruin theory Signal processing Statistics Stochastic analysis Time series analysis Machine learning List of topics Category Retrieved from " https://en.wikipedia.org/w/index.php?title=Gibbs_measure&oldid=1226846861 " Categories : Measures (measure theory) Statistical mechanics Game theory equilibrium concepts Hidden categories: Articles with short description Short description matches Wikidata Wikipedia articles needing clarification from December 2017 This page was last edited on 2 June 2024, at 05:42 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Gibbs measure 1 language Add topic

