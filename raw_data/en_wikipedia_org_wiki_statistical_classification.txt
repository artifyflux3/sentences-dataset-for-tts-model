Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Relation to other problems 2 Frequentist procedures 3 Bayesian procedures 4 Binary and multiclass classification 5 Feature vectors 6 Linear classifiers 7 Algorithms 8 Application domains 9 See also 10 References Toggle the table of contents Statistical classification 31 languages العربية Azərbaycanca Беларуская Български Català Čeština Deutsch Ελληνικά Español فارسی Français 한국어 Հայերեն Bahasa Indonesia Italiano עברית Lietuvių Bahasa Melayu 日本語 ଓଡ଼ିଆ Polski Português Русский کوردی Српски / srpski ไทย Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Categorization of data using statistics When classification is performed by a computer, statistical methods are normally used to develop the algorithm.

Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features .  These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type ), ordinal (e.g. "large", "medium" or "small"), integer-valued (e.g. the number of occurrences of a particular word in an email ) or real-valued (e.g. a measurement of blood pressure ). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.

An algorithm that implements classification, especially in a concrete implementation, is known as a classifier .  The term "classifier" sometimes also refers to the mathematical function , implemented by a classification algorithm, that maps input data to a category.

Terminology across fields is quite varied. In statistics , where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables , regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable .  In machine learning , the observations are often known as instances , the explanatory variables are termed features (grouped into a feature vector ), and the possible categories to be predicted are classes .  Other fields may use different terminology: e.g. in community ecology , the term "classification" normally refers to cluster analysis .

Relation to other problems [ edit ] Classification and clustering are examples of the more general problem of pattern recognition , which is the assignment of some sort of output value to a given input value.  Other examples are regression , which assigns a real-valued output to each input; sequence labeling , which assigns a class to each member of a sequence of values (for example, part of speech tagging , which assigns a part of speech to each word in an input sentence); parsing , which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.

A common subclass of classification is probabilistic classification .  Algorithms of this nature use statistical inference to find the best class for a given instance.  Unlike other algorithms, which simply output a "best" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers: It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier ).

Correspondingly, it can abstain when its confidence of choosing any particular output is too low.

Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation .

Frequentist procedures [ edit ] Early work on statistical classification was undertaken by Fisher , [ 1 ] [ 2 ] in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation.

[ 3 ] This early work assumed that data-values within each of the two groups had a multivariate normal distribution . The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule should be linear .

[ 3 ] [ 4 ] Later work for the multivariate normal distribution allowed the classifier to be nonlinear : [ 5 ] several classification rules can be derived based on different adjustments of the Mahalanobis distance , with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.

Bayesian procedures [ edit ] Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population.

[ 6 ] Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.

[ 7 ] Some Bayesian procedures involve the calculation of group-membership probabilities : these provide a more informative outcome than a simple attribution of a single group-label to each new observation.

Binary and multiclass classification [ edit ] Classification can be thought of as two separate problems – binary classification and multiclass classification . In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.

[ 8 ] Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.

Feature vectors [ edit ] Main article: Feature vector Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance.  Each property is termed a feature , also known in statistics as an explanatory variable (or independent variable , although features may or may not be statistically independent ).  Features may variously be binary (e.g. "on" or "off"); categorical (e.g. "A", "B", "AB" or "O", for blood type ); ordinal (e.g. "large", "medium" or "small"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10).

Linear classifiers [ edit ] Main article: Linear classifier A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product .  The predicted category is the one with the highest score.  This type of score function is known as a linear predictor function and has the following general form: score ⁡ ⁡ ( X i , k ) = β β k ⋅ ⋅ X i , {\displaystyle \operatorname {score} (\mathbf {X} _{i},k)={\boldsymbol {\beta }}_{k}\cdot \mathbf {X} _{i},} where X i is the feature vector for instance i , β k is the vector of weights corresponding to category k , and score( X i , k ) is the score associated with assigning instance i to category k .  In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k .

Algorithms with this basic setup are known as linear classifiers .  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.

Examples of such algorithms include Logistic regression – Statistical model for a binary dependent variable Multinomial logistic regression – Regression for more than two discrete outcomes Probit regression – Statistical regression where the dependent variable can take only two values Pages displaying short descriptions of redirect targets The perceptron algorithm Support vector machine – Set of methods for supervised statistical learning Linear discriminant analysis – Method used in statistics, pattern recognition, and other fields Algorithms [ edit ] Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms has been developed. The most commonly used include: [ 9 ] Artificial neural networks – Computational model used in machine learning, based on connected, hierarchical functions Pages displaying short descriptions of redirect targets Boosting (machine learning) – Ensemble learning method Random forest – Tree-based ensemble machine learning method Genetic programming – Evolving computer programs with techniques analogous to natural genetic processes Gene expression programming – Evolutionary algorithm Multi expression programming Linear genetic programming Kernel estimation – Window function Pages displaying short descriptions of redirect targets k-nearest neighbor – Non-parametric classification method Pages displaying short descriptions of redirect targets Learning vector quantization Linear classifier – Statistical classification in machine learning Fisher's linear discriminant – Method used in statistics, pattern recognition, and other fields Pages displaying short descriptions of redirect targets Logistic regression – Statistical model for a binary dependent variable Naive Bayes classifier – Probabilistic classification algorithm Perceptron – Algorithm for supervised learning of binary classifiers Quadratic classifier Support vector machine – Set of methods for supervised statistical learning Least squares support vector machine Choices between different possible algorithms are frequently made on the basis of quantitative evaluation of accuracy .

Application domains [ edit ] See also: Cluster analysis § Applications Classification has many applications. In some of these, it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.

Biological classification – The science of identifying, describing, defining and naming groups of biological organisms Biometric – Metrics related to human characteristics Pages displaying short descriptions of redirect targets identification Computer vision – Computerized information extraction from images Medical image analysis and medical imaging – Technique and process of creating visual representations of the interior of a body Optical character recognition – Computer recognition of visual text Video tracking – Locating a moving object by analyzing frames of a video Credit scoring – Numerical expression representing a person's creditworthiness Pages displaying short descriptions of redirect targets Document classification – Process of categorizing documents Drug discovery and development – Process of bringing a new pharmaceutical drug to the market Toxicogenomics Quantitative structure-activity relationship – Predictive chemical model Pages displaying short descriptions of redirect targets Geostatistics – Branch of statistics focusing on spatial data sets Handwriting recognition – Ability of a computer to receive and interpret intelligible handwritten input Internet search engines Micro-array classification Pattern recognition – Automated recognition of patterns and regularities in data Recommender system – System to predict users' preferences Speech recognition – Automatic conversion of spoken language into text Statistical natural language processing – Processing of natural language by a computer Pages displaying short descriptions of redirect targets This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( January 2010 ) ( Learn how and when to remove this message ) See also [ edit ] Mathematics portal Artificial intelligence – Intelligence of machines Binary classification – Dividing things between two categories Multiclass classification – Problem in machine learning and statistical classification Class membership probabilities – Machine learning problem Pages displaying short descriptions of redirect targets Classification rule Compound term processing Confusion matrix – Table layout for visualizing performance; also called an error matrix Data mining – Process of extracting and discovering patterns in large data sets Data warehouse – Centralized storage of knowledge Fuzzy logic – System for reasoning about vagueness Information retrieval – Obtaining information resources relevant to an information need List of datasets for machine learning research Machine learning – Study of algorithms that improve automatically through experience Recommender system – System to predict users' preferences References [ edit ] Wikimedia Commons has media related to Statistical classification .

^ Fisher, R. A. (1936). "The Use of Multiple Measurements in Taxonomic Problems".

Annals of Eugenics .

7 (2): 179– 188.

doi : 10.1111/j.1469-1809.1936.tb02137.x .

hdl : 2440/15227 .

^ Fisher, R. A. (1938). "The Statistical Utilization of Multiple Measurements".

Annals of Eugenics .

8 (4): 376– 386.

doi : 10.1111/j.1469-1809.1938.tb02189.x .

hdl : 2440/15232 .

^ a b Gnanadesikan, R. (1977) Methods for Statistical Data Analysis of Multivariate Observations , Wiley.

ISBN 0-471-30845-5 (p. 83–86) ^ Rao, C.R.

(1952) Advanced Statistical Methods in Multivariate Analysis , Wiley. (Section 9c) ^ Anderson, T.W.

(1958) An Introduction to Multivariate Statistical Analysis , Wiley.

^ Binder, D. A. (1978). "Bayesian cluster analysis".

Biometrika .

65 : 31– 38.

doi : 10.1093/biomet/65.1.31 .

^ Binder, David A. (1981). "Approximations to Bayesian clustering rules".

Biometrika .

68 : 275– 285.

doi : 10.1093/biomet/68.1.275 .

^ Har-Peled, S.

, Roth, D., Zimak, D. (2003) "Constraint Classification for Multiclass Classification and Ranking." In: Becker, B., Thrun, S.

, Obermayer, K. (Eds) Advances in Neural Information Processing Systems 15: Proceedings of the 2002 Conference , MIT Press.

ISBN 0-262-02550-7 ^ "A Tour of The Top 10 Algorithms for Machine Learning Newbies" .

Built In . 2018-01-20 . Retrieved 2019-06-10 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Authority control databases : National Czech Republic NewPP limit report
Parsed by mw‐web.codfw.main‐5667c77bf8‐rf2tv
Cached time: 20250818143307
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.734 seconds
Real time usage: 1.082 seconds
Preprocessor visited node count: 10047/1000000
Revision size: 13657/2097152 bytes
Post‐expand include size: 180918/2097152 bytes
Template argument size: 3600/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 46442/5000000 bytes
Lua time usage: 0.499/10.000 seconds
Lua memory usage: 25139202/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  933.948      1 -total
 50.88%  475.170     55 Template:Annotated_link
 13.64%  127.365      1 Template:Statistics
 13.43%  125.462      1 Template:Navbox_with_collapsible_groups
 12.87%  120.208      1 Template:Reflist
 10.48%   97.913      4 Template:Cite_journal
  5.67%   52.916      1 Template:Short_description
  4.63%   43.222      1 Template:Commons_category
  4.57%   42.641     11 Template:Navbox
  4.51%   42.100      1 Template:Sister_project Saved in parser cache with key enwiki:pcache:1579244:|#|:idhash:canonical and timestamp 20250818143307 and revision id 1234696920. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Statistical_classification&oldid=1234696920 " Categories : Statistical classification Classification algorithms Hidden categories: Articles with short description Short description is different from Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link Articles lacking in-text citations from January 2010 All articles lacking in-text citations Commons category link from Wikidata This page was last edited on 15 July 2024, at 17:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Statistical classification 31 languages Add topic

