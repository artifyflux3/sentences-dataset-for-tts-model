Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Feature types 2 Classification 3 Examples 4 Feature vectors 5 Selection and extraction 6 See also 7 References Toggle the table of contents Feature (machine learning) 15 languages العربية Català Deutsch Español فارسی 한국어 Italiano 日本語 Русский Suomi ไทย Türkçe Українська Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Measurable property or characteristic Not to be confused with Feature (computer vision) .

This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Feature" machine learning – news · newspapers · books · scholar · JSTOR ( December 2014 ) ( Learn how and when to remove this message ) Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning Journals and conferences AAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning v t e In machine learning and pattern recognition , a feature is an individual measurable property or characteristic of a data set.

[ 1 ] Choosing informative, discriminating, and independent features is crucial to produce effective algorithms for pattern recognition , classification , and regression tasks. Features are usually numeric, but other types such as strings and graphs are used in syntactic pattern recognition , after some pre-processing step such as one-hot encoding . The concept of "features" is related to that of explanatory variables used in statistical techniques such as linear regression .

Feature types [ edit ] In feature engineering, two types of features are commonly used: numerical and categorical.

Numerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly.

[ citation needed ] Categorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding.

The type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees, can handle both numerical and categorical features. Other machine learning algorithms, such as linear regression, can only handle numerical features.

Classification [ edit ] A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron ) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold.

Algorithms for classification from a feature vector include nearest neighbor classification , neural networks , and statistical techniques such as Bayesian approaches .

Examples [ edit ] See also: Feature (computer vision) In character recognition , features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.

In speech recognition , features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others.

In spam detection algorithms, features may include the presence or absence of certain email headers, 
the email structure, the language, the frequency of specific terms, the grammatical correctness of the text.

In computer vision , there are a large number of possible features , such as edges and objects.

Feature vectors [ edit ] See also: Word embedding "Feature space" redirects here. For feature spaces in kernel machines, see Kernel method .

In pattern recognition and machine learning , a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression .  Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.

The vector space associated with these vectors is often called the feature space . In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.

Higher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' minus 'Year of birth' . This process is referred to as feature construction .

[ 2 ] [ 3 ] Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C) [ 4 ] that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems.

[ 5 ] Applications include studies of disease and emotion recognition from speech.

[ 6 ] Selection and extraction [ edit ] Main articles: Feature selection and Feature extraction This section's tone or style may not reflect the encyclopedic tone used on Wikipedia .

See Wikipedia's guide to writing better articles for suggestions.

( August 2025 ) ( Learn how and when to remove this message ) The initial set of raw features can be redundant and large enough that estimation and optimization is made difficult or ineffective. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability.

[ 7 ] Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering . It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert . Automating this process is feature learning , where a machine not only uses features for learning, but learns the features itself.

See also [ edit ] Covariate Dimensionality reduction Feature engineering Hashing trick Statistical classification Explainable artificial intelligence References [ edit ] ^ Bishop, Christopher (2006).

Pattern recognition and machine learning . Berlin: Springer.

ISBN 0-387-31073-8 .

^ Liu, H., Motoda H. (1998) Feature Selection for Knowledge Discovery and Data Mining .

, Kluwer Academic Publishers. Norwell, MA, USA. 1998.

^ Piramuthu, S., Sikora R. T.

Iterative feature construction for improving inductive learning algorithms . In Journal of Expert Systems with Applications. Vol. 36 , Iss. 2 (March 2009), pp. 3401-3406, 2009 ^ Bloedorn, E., Michalski, R. Data-driven constructive induction: a methodology and its applications. IEEE Intelligent Systems, Special issue on Feature Transformation and Subset Selection, pp. 30-37, March/April, 1998 ^ Breiman, L. Friedman, T., Olshen, R., Stone, C. (1984) Classification and regression trees , Wadsworth ^ Sidorova, J., Badia T.

Syntactic learning for ESEDA.1, tool for enhanced speech emotion detection and analysis . Internet Technology and Secured Transactions Conference 2009 (ICITST-2009), London, November 9–12. IEEE ^ Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome H. (2009).

The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer.

ISBN 978-0-387-84884-6 .

NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐5pgzd
Cached time: 20250812014148
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.346 seconds
Real time usage: 0.452 seconds
Preprocessor visited node count: 1156/1000000
Revision size: 8896/2097152 bytes
Post‐expand include size: 55797/2097152 bytes
Template argument size: 3080/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 25703/5000000 bytes
Lua time usage: 0.222/10.000 seconds
Lua memory usage: 5023421/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  390.677      1 -total
 24.30%   94.939      1 Template:Machine_learning_bar
 24.03%   93.884      1 Template:Reflist
 23.47%   91.673      1 Template:Sidebar_with_collapsible_lists
 21.09%   82.405      2 Template:Cite_book
 16.96%   66.275      1 Template:Short_description
 14.42%   56.355      1 Template:Refimprove
 13.40%   52.370      2 Template:Ambox
 10.18%   39.782      2 Template:Pagetype
  5.65%   22.058      1 Template:Distinguish Saved in parser cache with key enwiki:pcache:1299404:|#|:idhash:canonical and timestamp 20250812014148 and revision id 1304168512. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Feature_(machine_learning)&oldid=1304168512 " Categories : Data mining Machine learning Pattern recognition Hidden categories: Articles with short description Short description is different from Wikidata Articles needing additional references from December 2014 All articles needing additional references All articles with unsourced statements Articles with unsourced statements from June 2024 Wikipedia articles with style issues from August 2025 All articles with style issues This page was last edited on 4 August 2025, at 11:36 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Feature (machine learning) 15 languages Add topic

