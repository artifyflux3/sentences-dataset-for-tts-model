Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Statement and derivation 2 Detailed proofs/derivations from definition Toggle Detailed proofs/derivations from definition subsection 2.1 Linearity (directly) 2.2 Sum 2.3 Difference 2.4 Constant coefficient 3 See also 4 References Toggle the table of contents Linearity of differentiation 4 languages Català Čeština Esperanto 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Calculus property In calculus , the derivative of any linear combination of functions equals the same linear combination of the derivatives of the functions; [ 1 ] this property is known as linearity of differentiation , the rule of linearity , [ 2 ] or the superposition rule for differentiation.

[ 3 ] It is a fundamental property of the derivative that encapsulates in a single rule two simpler rules of differentiation, the sum rule (the derivative of the sum of two functions is the sum of the derivatives) and the constant factor rule (the derivative of a constant multiple of a function is the same constant multiple of the derivative).

[ 4 ] [ 5 ] Thus it can be said that differentiation is linear , or the differential operator is a linear operator.

[ 6 ] Statement and derivation [ edit ] Let f and g be functions, with α and β constants. Now consider d d x ( α α ⋅ ⋅ f ( x ) + β β ⋅ ⋅ g ( x ) ) .

{\displaystyle {\frac {\mbox{d}}{{\mbox{d}}x}}(\alpha \cdot f(x)+\beta \cdot g(x)).} By the sum rule in differentiation , this is d d x ( α α ⋅ ⋅ f ( x ) ) + d d x ( β β ⋅ ⋅ g ( x ) ) , {\displaystyle {\frac {\mbox{d}}{{\mbox{d}}x}}(\alpha \cdot f(x))+{\frac {\mbox{d}}{{\mbox{d}}x}}(\beta \cdot g(x)),} and by the constant factor rule in differentiation , this reduces to α α ⋅ ⋅ f ′ ( x ) + β β ⋅ ⋅ g ′ ( x ) .

{\displaystyle \alpha \cdot f'(x)+\beta \cdot g'(x).} Therefore, d d x ( α α ⋅ ⋅ f ( x ) + β β ⋅ ⋅ g ( x ) ) = α α ⋅ ⋅ f ′ ( x ) + β β ⋅ ⋅ g ′ ( x ) .

{\displaystyle {\frac {\mbox{d}}{{\mbox{d}}x}}(\alpha \cdot f(x)+\beta \cdot g(x))=\alpha \cdot f'(x)+\beta \cdot g'(x).} Omitting the brackets , this is often written as: ( α α ⋅ ⋅ f + β β ⋅ ⋅ g ) ′ = α α ⋅ ⋅ f ′ + β β ⋅ ⋅ g ′ .

{\displaystyle (\alpha \cdot f+\beta \cdot g)'=\alpha \cdot f'+\beta \cdot g'.} Detailed proofs/derivations from definition [ edit ] We can prove the entire linearity principle at once, or, we can prove the individual steps (of constant factor and adding) individually. Here, both will be shown.

Proving linearity directly also proves the constant factor rule, the sum rule, and the difference rule as special cases. The sum rule is obtained by setting both constant coefficients to 1 {\displaystyle 1} . The difference rule is obtained by setting the first constant coefficient to 1 {\displaystyle 1} and the second constant coefficient to − − 1 {\displaystyle -1} . The constant factor rule is obtained by setting either the second constant coefficient or the second function to 0 {\displaystyle 0} . (From a technical standpoint, the domain of the second function must also be considered - one way to avoid issues is setting the second function equal to the first function and the second constant coefficient equal to 0 {\displaystyle 0} . One could also define both the second constant coefficient and the second function to be 0, where the domain of the second function is a superset of the first function, among other possibilities.) On the contrary, if we first prove the constant factor rule and the sum rule, we can prove linearity and the difference rule. Proving linearity is done by defining the first and second functions as being two other functions being multiplied by constant coefficients. Then, as shown in the derivation from the previous section, we can first use the sum law while differentiation, and then use the constant factor rule, which will reach our conclusion for linearity. In order to prove the difference rule, the second function can be redefined as another function multiplied by the constant coefficient of − − 1 {\displaystyle -1} . This would, when simplified, give us the difference rule for differentiation.

In the proofs/derivations below, [ 7 ] [ 8 ] the coefficients a , b {\displaystyle a,b} are used; they correspond to the coefficients α α , β β {\displaystyle \alpha ,\beta } above.

Linearity (directly) [ edit ] Let a , b ∈ ∈ R {\displaystyle a,b\in \mathbb {R} } . Let f , g {\displaystyle f,g} be functions. Let j {\displaystyle j} be a function, where j {\displaystyle j} is defined only where f {\displaystyle f} and g {\displaystyle g} are both defined. (In other words, the domain of j {\displaystyle j} is the intersection of the domains of f {\displaystyle f} and g {\displaystyle g} .) Let x {\displaystyle x} be in the domain of j {\displaystyle j} . Let j ( x ) = a f ( x ) + b g ( x ) {\displaystyle j(x)=af(x)+bg(x)} .

We want to prove that j ′ ′ ( x ) = a f ′ ′ ( x ) + b g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=af^{\prime }(x)+bg^{\prime }(x)} .

By definition, we can see that j ′ ′ ( x ) = lim h → → 0 j ( x + h ) − − j ( x ) h = lim h → → 0 ( a f ( x + h ) + b g ( x + h ) ) − − ( a f ( x ) + b g ( x ) ) h = lim h → → 0 ( a f ( x + h ) − − f ( x ) h + b g ( x + h ) − − g ( x ) h ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}{\frac {j(x+h)-j(x)}{h}}\\&=\lim _{h\rightarrow 0}{\frac {\left(af(x+h)+bg(x+h)\right)-\left(af(x)+bg(x)\right)}{h}}\\&=\lim _{h\rightarrow 0}\left(a{\frac {f(x+h)-f(x)}{h}}+b{\frac {g(x+h)-g(x)}{h}}\right)\\\end{aligned}}} In order to use the limits law for the sum of limits, we need to know that lim h → → 0 a f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\to 0}a{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 b g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\to 0}b{\frac {g(x+h)-g(x)}{h}}} both individually exist. For these smaller limits, we need to know that lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\to 0}{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\to 0}{\frac {g(x+h)-g(x)}{h}}} both individually exist to use the coefficient law for limits. By definition, f ′ ′ ( x ) = lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle f^{\prime }(x)=\lim _{h\to 0}{\frac {f(x+h)-f(x)}{h}}} and g ′ ′ ( x ) = lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle g^{\prime }(x)=\lim _{h\to 0}{\frac {g(x+h)-g(x)}{h}}} . So, if we know that f ′ ′ ( x ) {\displaystyle f^{\prime }(x)} and g ′ ′ ( x ) {\displaystyle g^{\prime }(x)} both exist, we will know that lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\to 0}{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\to 0}{\frac {g(x+h)-g(x)}{h}}} both individually exist. This allows us to use the coefficient law for limits to write lim h → → 0 a f ( x + h ) − − f ( x ) h = a lim h → → 0 f ( x + h ) − − f ( x ) h {\displaystyle \lim _{h\to 0}a{\frac {f(x+h)-f(x)}{h}}=a\lim _{h\to 0}{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 b g ( x + h ) − − g ( x ) h = b lim h → → 0 g ( x + h ) − − g ( x ) h .

{\displaystyle \lim _{h\to 0}b{\frac {g(x+h)-g(x)}{h}}=b\lim _{h\to 0}{\frac {g(x+h)-g(x)}{h}}.} With this, we can go back to apply the limit law for the sum of limits, since we know that lim h → → 0 a f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\rightarrow 0}a{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 b g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\rightarrow 0}b{\frac {g(x+h)-g(x)}{h}}} both individually exist. From here, we can directly go back to the derivative we were working on.

j ′ ′ ( x ) = lim h → → 0 ( a f ( x + h ) − − f ( x ) h + b g ( x + h ) − − g ( x ) h ) = lim h → → 0 ( a f ( x + h ) − − f ( x ) h ) + lim h → → 0 ( b g ( x + h ) − − g ( x ) h ) = a lim h → → 0 ( f ( x + h ) − − f ( x ) h ) + b lim h → → 0 ( g ( x + h ) − − g ( x ) h ) = a f ′ ′ ( x ) + b g ′ ′ ( x ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}\left(a{\frac {f(x+h)-f(x)}{h}}+b{\frac {g(x+h)-g(x)}{h}}\right)\\&=\lim _{h\rightarrow 0}\left(a{\frac {f(x+h)-f(x)}{h}}\right)+\lim _{h\rightarrow 0}\left(b{\frac {g(x+h)-g(x)}{h}}\right)\\&=a\lim _{h\rightarrow 0}\left({\frac {f(x+h)-f(x)}{h}}\right)+b\lim _{h\rightarrow 0}\left({\frac {g(x+h)-g(x)}{h}}\right)\\&=af^{\prime }(x)+bg^{\prime }(x)\end{aligned}}} Finally, we have shown what we claimed in the beginning: j ′ ′ ( x ) = a f ′ ′ ( x ) + b g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=af^{\prime }(x)+bg^{\prime }(x)} .

Sum [ edit ] Let f , g {\displaystyle f,g} be functions. Let j {\displaystyle j} be a function, where j {\displaystyle j} is defined only where f {\displaystyle f} and g {\displaystyle g} are both defined.
(In other words, the domain of j {\displaystyle j} is the intersection of the domains of f {\displaystyle f} and g {\displaystyle g} .) Let x {\displaystyle x} be in the domain of j {\displaystyle j} . Let j ( x ) = f ( x ) + g ( x ) {\displaystyle j(x)=f(x)+g(x)} .

We want to prove that j ′ ′ ( x ) = f ′ ′ ( x ) + g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=f^{\prime }(x)+g^{\prime }(x)} .

By definition, we can see that j ′ ′ ( x ) = lim h → → 0 j ( x + h ) − − j ( x ) h = lim h → → 0 ( f ( x + h ) + g ( x + h ) ) − − ( f ( x ) + g ( x ) ) h = lim h → → 0 ( f ( x + h ) − − f ( x ) h + g ( x + h ) − − g ( x ) h ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}{\frac {j(x+h)-j(x)}{h}}\\&=\lim _{h\rightarrow 0}{\frac {\left(f(x+h)+g(x+h)\right)-\left(f(x)+g(x)\right)}{h}}\\&=\lim _{h\rightarrow 0}\left({\frac {f(x+h)-f(x)}{h}}+{\frac {g(x+h)-g(x)}{h}}\right)\\\end{aligned}}} In order to use the law for the sum of limits here, we need to show that the individual limits, lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}} both exist. By definition, f ′ ′ ( x ) = lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle f^{\prime }(x)=\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} and g ′ ′ ( x ) = lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle g^{\prime }(x)=\lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}} , so the limits exist whenever the derivatives f ′ ′ ( x ) {\displaystyle f^{\prime }(x)} and g ′ ′ ( x ) {\displaystyle g^{\prime }(x)} exist. So, assuming that the derivatives exist, we can continue the above derivation j ′ ′ ( x ) = lim h → → 0 ( f ( x + h ) − − f ( x ) h + g ( x + h ) − − g ( x ) h ) = lim h → → 0 f ( x + h ) − − f ( x ) h + lim h → → 0 g ( x + h ) − − g ( x ) h = f ′ ′ ( x ) + g ′ ′ ( x ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}\left({\frac {f(x+h)-f(x)}{h}}+{\frac {g(x+h)-g(x)}{h}}\right)\\&=\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}+\lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}\\&=f^{\prime }(x)+g^{\prime }(x)\end{aligned}}} Thus, we have shown what we wanted to show, that: j ′ ′ ( x ) = f ′ ′ ( x ) + g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=f^{\prime }(x)+g^{\prime }(x)} .

Difference [ edit ] Let f , g {\displaystyle f,g} be functions. Let j {\displaystyle j} be a function, where j {\displaystyle j} is defined only where f {\displaystyle f} and g {\displaystyle g} are both defined. (In other words, the domain of j {\displaystyle j} is the intersection of the domains of f {\displaystyle f} and g {\displaystyle g} .) Let x {\displaystyle x} be in the domain of j {\displaystyle j} . Let j ( x ) = f ( x ) − − g ( x ) {\displaystyle j(x)=f(x)-g(x)} .

We want to prove that j ′ ′ ( x ) = f ′ ′ ( x ) − − g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=f^{\prime }(x)-g^{\prime }(x)} .

By definition, we can see that: j ′ ′ ( x ) = lim h → → 0 j ( x + h ) − − j ( x ) h = lim h → → 0 ( f ( x + h ) − − ( g ( x + h ) ) − − ( f ( x ) − − g ( x ) ) h = lim h → → 0 ( f ( x + h ) − − f ( x ) h − − g ( x + h ) − − g ( x ) h ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}{\frac {j(x+h)-j(x)}{h}}\\&=\lim _{h\rightarrow 0}{\frac {\left(f(x+h)-(g(x+h)\right)-\left(f(x)-g(x)\right)}{h}}\\&=\lim _{h\rightarrow 0}\left({\frac {f(x+h)-f(x)}{h}}-{\frac {g(x+h)-g(x)}{h}}\right)\\\end{aligned}}} In order to use the law for the difference of limits here, we need to show that the individual limits, lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} and lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}} both exist. By definition, f ′ ′ ( x ) = lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle f^{\prime }(x)=\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} and that g ′ ′ ( x ) = lim h → → 0 g ( x + h ) − − g ( x ) h {\textstyle g^{\prime }(x)=\lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}} , so these limits exist whenever the derivatives f ′ ′ ( x ) {\displaystyle f^{\prime }(x)} and g ′ ′ ( x ) {\displaystyle g^{\prime }(x)} exist. So, assuming that the derivatives exist, we can continue the above derivation j ′ ′ ( x ) = lim h → → 0 ( f ( x + h ) − − f ( x ) h − − g ( x + h ) − − g ( x ) h ) = lim h → → 0 f ( x + h ) − − f ( x ) h − − lim h → → 0 g ( x + h ) − − g ( x ) h = f ′ ′ ( x ) − − g ′ ′ ( x ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}\left({\frac {f(x+h)-f(x)}{h}}-{\frac {g(x+h)-g(x)}{h}}\right)\\&=\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}-\lim _{h\rightarrow 0}{\frac {g(x+h)-g(x)}{h}}\\&=f^{\prime }(x)-g^{\prime }(x)\end{aligned}}} Thus, we have shown what we wanted to show, that: j ′ ′ ( x ) = f ′ ′ ( x ) − − g ′ ′ ( x ) {\displaystyle j^{\prime }(x)=f^{\prime }(x)-g^{\prime }(x)} .

Constant coefficient [ edit ] Let f {\displaystyle f} be a function. Let a ∈ ∈ R {\displaystyle a\in \mathbb {R} } ; a {\displaystyle a} will be the constant coefficient. Let j {\displaystyle j} be a function, where j is defined only where f {\displaystyle f} is defined. (In other words, the domain of j {\displaystyle j} is equal to the domain of f {\displaystyle f} .) Let x {\displaystyle x} be in the domain of j {\displaystyle j} . Let j ( x ) = a f ( x ) {\displaystyle j(x)=af(x)} .

We want to prove that j ′ ′ ( x ) = a f ′ ′ ( x ) {\displaystyle j^{\prime }(x)=af^{\prime }(x)} .

By definition, we can see that: j ′ ′ ( x ) = lim h → → 0 j ( x + h ) − − j ( x ) h = lim h → → 0 a f ( x + h ) − − a f ( x ) h = lim h → → 0 a f ( x + h ) − − f ( x ) h {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}{\frac {j(x+h)-j(x)}{h}}\\&=\lim _{h\rightarrow 0}{\frac {af(x+h)-af(x)}{h}}\\&=\lim _{h\rightarrow 0}a{\frac {f(x+h)-f(x)}{h}}\\\end{aligned}}} Now, in order to use a limit law for constant coefficients to show that lim h → → 0 a f ( x + h ) − − f ( x ) h = a lim h → → 0 f ( x + h ) − − f ( x ) h {\displaystyle \lim _{h\rightarrow 0}a{\frac {f(x+h)-f(x)}{h}}=a\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} we need to show that lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} exists.
However, f ′ ′ ( x ) = lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle f^{\prime }(x)=\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} , by the definition of the derivative. So, if f ′ ′ ( x ) {\displaystyle f^{\prime }(x)} exists, then lim h → → 0 f ( x + h ) − − f ( x ) h {\textstyle \lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}} exists.

Thus, if we assume that f ′ ′ ( x ) {\displaystyle f^{\prime }(x)} exists, we can use the limit law and continue our proof.

j ′ ′ ( x ) = lim h → → 0 a f ( x + h ) − − f ( x ) h = a lim h → → 0 f ( x + h ) − − f ( x ) h = a f ′ ′ ( x ) {\displaystyle {\begin{aligned}j^{\prime }(x)&=\lim _{h\rightarrow 0}a{\frac {f(x+h)-f(x)}{h}}\\&=a\lim _{h\rightarrow 0}{\frac {f(x+h)-f(x)}{h}}\\&=af^{\prime }(x)\\\end{aligned}}} Thus, we have proven that when j ( x ) = a f ( x ) {\displaystyle j(x)=af(x)} , we have j ′ ′ ( x ) = a f ′ ′ ( x ) {\displaystyle j^{\prime }(x)=af^{\prime }(x)} .

See also [ edit ] Differentiation of integrals – Problem in mathematics Differentiation of trigonometric functions – Mathematical process of finding the derivative of a trigonometric function Differentiation rules – Rules for computing derivatives of functions Distribution (mathematics) – Mathematical term generalizing the concept of function General Leibniz rule – Generalization of the product rule in calculus Integration by parts – Mathematical method in calculus Inverse functions and differentiation – Formula for the derivative of an inverse function Pages displaying short descriptions of redirect targets Product rule – Formula for the derivative of a product Quotient rule – Formula for the derivative of a ratio of functions Table of derivatives – Rules for computing derivatives of functions Pages displaying short descriptions of redirect targets Vector calculus identities – Mathematical identities References [ edit ] ^ Blank, Brian E.; Krantz, Steven George (2006), Calculus: Single Variable, Volume 1 , Springer, p. 177, ISBN 9781931914598 .

^ Strang, Gilbert (1991), Calculus, Volume 1 , SIAM, pp.

71– 72, ISBN 9780961408824 .

^ Stroyan, K. D. (2014), Calculus Using Mathematica , Academic Press, p. 89, ISBN 9781483267975 .

^ Estep, Donald (2002), "20.1 Linear Combinations of Functions", Practical Analysis in One Variable , Undergraduate Texts in Mathematics , Springer, pp.

259– 260, ISBN 9780387954844 .

^ Zorn, Paul (2010), Understanding Real Analysis , CRC Press, p. 184, ISBN 9781439894323 .

^ Gockenbach, Mark S. (2011), Finite-Dimensional Linear Algebra , Discrete Mathematics and Its Applications, CRC Press, p. 103, ISBN 9781439815649 .

^ "Differentiation Rules" .

CEMC's Open Courseware . Retrieved 3 May 2022 .

^ Dawkins, Paul.

"Proof Of Various Derivative Properties" .

Paul's Online Notes . Retrieved 3 May 2022 .

v t e Calculus Precalculus Binomial theorem Concave function Continuous function Factorial Finite difference Free variables and bound variables Graph of a function Linear function Radian Rolle's theorem Secant Slope Tangent Limits Indeterminate form Limit of a function One-sided limit Limit of a sequence Order of approximation (ε, δ)-definition of limit Differential calculus Derivative Second derivative Partial derivative Differential Differential operator Mean value theorem Notation Leibniz's notation Newton's notation Rules of differentiation linearity Power Sum Chain L'Hôpital's Product General Leibniz's rule Quotient Other techniques Implicit differentiation Inverse functions and differentiation Logarithmic derivative Related rates Stationary points First derivative test Second derivative test Extreme value theorem Maximum and minimum Further applications Newton's method Taylor's theorem Differential equation Ordinary differential equation Partial differential equation Stochastic differential equation Integral calculus Antiderivative Arc length Riemann integral Basic properties Constant of integration Fundamental theorem of calculus Differentiating under the integral sign Integration by parts Integration by substitution trigonometric Euler Tangent half-angle substitution Partial fractions in integration Quadratic integral Trapezoidal rule Volumes Washer method Shell method Integral equation Integro-differential equation Vector calculus Derivatives Curl Directional derivative Divergence Gradient Laplacian Basic theorems Line integrals Green's Stokes' Gauss' Multivariable calculus Divergence theorem Geometric Hessian matrix Jacobian matrix and determinant Lagrange multiplier Line integral Matrix Multiple integral Partial derivative Surface integral Volume integral Advanced topics Differential forms Exterior derivative Generalized Stokes' theorem Tensor calculus Sequences and series Arithmetico-geometric sequence Types of series Alternating Binomial Fourier Geometric Harmonic Infinite Power Maclaurin Taylor Telescoping Tests of convergence Abel's Alternating series Cauchy condensation Direct comparison Dirichlet's Integral Limit comparison Ratio Root Term Special functions and numbers Bernoulli numbers e (mathematical constant) Exponential function Natural logarithm Stirling's approximation History of calculus Adequality Brook Taylor Colin Maclaurin Generality of algebra Gottfried Wilhelm Leibniz Infinitesimal Infinitesimal calculus Isaac Newton Fluxion Law of Continuity Leonhard Euler Method of Fluxions The Method of Mechanical Theorems Lists Integrals rational functions irrational algebraic functions exponential functions logarithmic functions hyperbolic functions inverse trigonometric functions inverse Secant Secant cubed List of limits List of derivatives Miscellaneous topics Complex calculus Contour integral Differential geometry Manifold Curvature of curves of surfaces Tensor Euler–Maclaurin formula Gabriel's horn Integration Bee Proof that 22/7 exceeds π Regiomontanus' angle maximization problem Steinmetz solid Retrieved from " https://en.wikipedia.org/w/index.php?title=Linearity_of_differentiation&oldid=1286472589 " Categories : Differential calculus Differentiation rules Theorems in mathematical analysis Theorems in calculus Hidden categories: Articles with short description Short description matches Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link Articles containing proofs This page was last edited on 20 April 2025, at 03:09 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Linearity of differentiation 4 languages Add topic

