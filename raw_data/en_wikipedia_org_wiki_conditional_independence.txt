Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Conditional independence of events Toggle Conditional independence of events subsection 1.1 Proof of the equivalent definition 1.2 Examples 1.2.1 Coloured boxes 1.2.2 Proximity and delays 1.2.3 Dice rolling 1.2.4 Height and vocabulary 2 Conditional independence of random variables 3 Conditional independence of random vectors 4 Uses in Bayesian inference 5 Rules of conditional independence Toggle Rules of conditional independence subsection 5.1 Symmetry 5.2 Decomposition 5.3 Weak union 5.4 Contraction 5.5 Intersection 6 See also 7 References 8 External links Toggle the table of contents Conditional independence 14 languages Català Чӑвашла Deutsch Español فارسی 한국어 Italiano Magyar 日本語 Português Русский Српски / srpski 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability theory concept See also: Conditional dependence Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e In probability theory , conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability , as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If A {\displaystyle A} is the hypothesis, and B {\displaystyle B} and C {\displaystyle C} are observations, conditional independence can be stated as an equality: P ( A ∣ ∣ B , C ) = P ( A ∣ ∣ C ) {\displaystyle P(A\mid B,C)=P(A\mid C)} where P ( A ∣ ∣ B , C ) {\displaystyle P(A\mid B,C)} is the probability of A {\displaystyle A} given both B {\displaystyle B} and C {\displaystyle C} . Since the probability of A {\displaystyle A} given C {\displaystyle C} is the same as the probability of A {\displaystyle A} given both B {\displaystyle B} and C {\displaystyle C} , this equality expresses that B {\displaystyle B} contributes nothing to the certainty of A {\displaystyle A} . In this case, A {\displaystyle A} and B {\displaystyle B} are said to be conditionally independent given C {\displaystyle C} , written symbolically as: ( A ⊥ ⊥ ⊥ ⊥ B ∣ ∣ C ) {\displaystyle (A\perp \!\!\!\perp B\mid C)} .

The concept of conditional independence is essential to graph-based theories of statistical inference, as it establishes a mathematical relation between a collection of conditional statements and a graphoid .

Conditional independence of events [ edit ] Let A {\displaystyle A} , B {\displaystyle B} , and C {\displaystyle C} be events .

A {\displaystyle A} and B {\displaystyle B} are said to be conditionally independent given C {\displaystyle C} if and only if P ( C ) > 0 {\displaystyle P(C)>0} and: P ( A ∣ ∣ B , C ) = P ( A ∣ ∣ C ) {\displaystyle P(A\mid B,C)=P(A\mid C)} This property is often written: ( A ⊥ ⊥ ⊥ ⊥ B ∣ ∣ C ) {\displaystyle (A\perp \!\!\!\perp B\mid C)} , which should be read ( ( A ⊥ ⊥ ⊥ ⊥ B ) | C ) {\displaystyle ((A\perp \!\!\!\perp B)\vert C)} .

Equivalently, conditional independence may be stated as: P ( A , B | C ) = P ( A | C ) P ( B | C ) {\displaystyle P(A,B|C)=P(A|C)P(B|C)} where P ( A , B | C ) {\displaystyle P(A,B|C)} is the joint probability of A {\displaystyle A} and B {\displaystyle B} given C {\displaystyle C} . This alternate formulation states that A {\displaystyle A} and B {\displaystyle B} are independent events , given C {\displaystyle C} .

It demonstrates that ( A ⊥ ⊥ ⊥ ⊥ B ∣ ∣ C ) {\displaystyle (A\perp \!\!\!\perp B\mid C)} is equivalent to ( B ⊥ ⊥ ⊥ ⊥ A ∣ ∣ C ) {\displaystyle (B\perp \!\!\!\perp A\mid C)} .

Proof of the equivalent definition [ edit ] P ( A , B ∣ ∣ C ) = P ( A ∣ ∣ C ) P ( B ∣ ∣ C ) {\displaystyle P(A,B\mid C)=P(A\mid C)P(B\mid C)} iff P ( A , B , C ) P ( C ) = ( P ( A , C ) P ( C ) ) ( P ( B , C ) P ( C ) ) {\displaystyle {\frac {P(A,B,C)}{P(C)}}=\left({\frac {P(A,C)}{P(C)}}\right)\left({\frac {P(B,C)}{P(C)}}\right)} (definition of conditional probability ) iff P ( A , B , C ) = P ( A , C ) P ( B , C ) P ( C ) {\displaystyle P(A,B,C)={\frac {P(A,C)P(B,C)}{P(C)}}} (multiply both sides by P ( C ) {\displaystyle P(C)} ) iff P ( A , B , C ) P ( B , C ) = P ( A , C ) P ( C ) {\displaystyle {\frac {P(A,B,C)}{P(B,C)}}={\frac {P(A,C)}{P(C)}}} (divide both sides by P ( B , C ) {\displaystyle P(B,C)} ) iff P ( A ∣ ∣ B , C ) = P ( A ∣ ∣ C ) {\displaystyle P(A\mid B,C)=P(A\mid C)} (definition of conditional probability) ∴ ∴ {\displaystyle \therefore } Examples [ edit ] Coloured boxes [ edit ] Each cell represents a possible outcome. The events R {\displaystyle \color {red}R} , B {\displaystyle \color {blue}B} and Y {\displaystyle \color {gold}Y} are represented by the areas shaded red , blue and yellow respectively. The overlap between the events R {\displaystyle \color {red}R} and B {\displaystyle \color {blue}B} is shaded purple .

The probabilities of these events are shaded areas with respect to the total area. In both examples R {\displaystyle \color {red}R} and B {\displaystyle \color {blue}B} are conditionally independent given Y {\displaystyle \color {gold}Y} because: Pr ( R , B ∣ ∣ Y ) = Pr ( R ∣ ∣ Y ) Pr ( B ∣ ∣ Y ) {\displaystyle \Pr({\color {red}R},{\color {blue}B}\mid {\color {gold}Y})=\Pr({\color {red}R}\mid {\color {gold}Y})\Pr({\color {blue}B}\mid {\color {gold}Y})} [ 1 ] but not conditionally independent given [ not Y ] {\displaystyle \left[{\text{not }}{\color {gold}Y}\right]} because: Pr ( R , B ∣ ∣ not Y ) ≠ Pr ( R ∣ ∣ not Y ) Pr ( B ∣ ∣ not Y ) {\displaystyle \Pr({\color {red}R},{\color {blue}B}\mid {\text{not }}{\color {gold}Y})\not =\Pr({\color {red}R}\mid {\text{not }}{\color {gold}Y})\Pr({\color {blue}B}\mid {\text{not }}{\color {gold}Y})} Proximity and delays [ edit ] Let events A and B be defined as the probability that person A and person B will be home in time for dinner where both people are randomly sampled from the entire world. Events A and B can be assumed to be independent i.e. knowledge that A is late has minimal to no change on the probability that B will be late. However, if a third event is introduced, person A and person B live in the same neighborhood, the two events are now considered not conditionally independent. Traffic conditions and weather-related events that might delay person A, might delay person B as well. Given the third event and knowledge that person A was late, the probability that person B will be late does meaningfully change.

[ 2 ] Dice rolling [ edit ] Conditional independence depends on the nature of the third event. If you roll two dice, one may assume that the two dice behave independently of each other. Looking at the results of one die will not tell you about the result of the second die. (That is, the two dice are independent.) If, however, the 1st die's result is a 3, and someone tells you about a third event - that the sum of the two results is even - then this extra unit of information restricts the options for the 2nd result to an odd number. In other words, two events can be independent, but NOT conditionally independent.

[ 2 ] Height and vocabulary [ edit ] Height and vocabulary are dependent since very small people tend to be children, known for their more basic vocabularies. But knowing that two people are 19 years old (i.e., conditional on age) there is no reason to think that one person's vocabulary is larger if we are told that they are taller.

Conditional independence of random variables [ edit ] Two discrete random variables X {\displaystyle X} and Y {\displaystyle Y} are conditionally independent given a third discrete random variable Z {\displaystyle Z} if and only if they are independent in their conditional probability distribution given Z {\displaystyle Z} . That is, X {\displaystyle X} and Y {\displaystyle Y} are conditionally independent given Z {\displaystyle Z} if and only if, given any value of Z {\displaystyle Z} , the probability distribution of X {\displaystyle X} is the same for all values of Y {\displaystyle Y} and the probability distribution of Y {\displaystyle Y} is the same for all values of X {\displaystyle X} . Formally: ( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ Z ⟺ ⟺ F X , Y ∣ ∣ Z = z ( x , y ) = F X ∣ ∣ Z = z ( x ) ⋅ ⋅ F Y ∣ ∣ Z = z ( y ) for all x , y , z {\displaystyle (X\perp \!\!\!\perp Y)\mid Z\quad \iff \quad F_{X,Y\,\mid \,Z\,=\,z}(x,y)=F_{X\,\mid \,Z\,=\,z}(x)\cdot F_{Y\,\mid \,Z\,=\,z}(y)\quad {\text{for all }}x,y,z} Eq.2 where F X , Y ∣ ∣ Z = z ( x , y ) = Pr ( X ≤ ≤ x , Y ≤ ≤ y ∣ ∣ Z = z ) {\displaystyle F_{X,Y\,\mid \,Z\,=\,z}(x,y)=\Pr(X\leq x,Y\leq y\mid Z=z)} is the conditional cumulative distribution function of X {\displaystyle X} and Y {\displaystyle Y} given Z {\displaystyle Z} .

Two events R {\displaystyle R} and B {\displaystyle B} are conditionally independent given a σ-algebra Σ Σ {\displaystyle \Sigma } if Pr ( R , B ∣ ∣ Σ Σ ) = Pr ( R ∣ ∣ Σ Σ ) Pr ( B ∣ ∣ Σ Σ ) a.s.

{\displaystyle \Pr(R,B\mid \Sigma )=\Pr(R\mid \Sigma )\Pr(B\mid \Sigma ){\text{ a.s.}}} where Pr ( A ∣ ∣ Σ Σ ) {\displaystyle \Pr(A\mid \Sigma )} denotes the conditional expectation of the indicator function of the event A {\displaystyle A} , χ χ A {\displaystyle \chi _{A}} , given the sigma algebra Σ Σ {\displaystyle \Sigma } . That is, Pr ( A ∣ ∣ Σ Σ ) := E ⁡ ⁡ [ χ χ A ∣ ∣ Σ Σ ] .

{\displaystyle \Pr(A\mid \Sigma ):=\operatorname {E} [\chi _{A}\mid \Sigma ].} Two random variables X {\displaystyle X} and Y {\displaystyle Y} are conditionally independent given a σ-algebra Σ Σ {\displaystyle \Sigma } if the above equation holds for all R {\displaystyle R} in σ σ ( X ) {\displaystyle \sigma (X)} and B {\displaystyle B} in σ σ ( Y ) {\displaystyle \sigma (Y)} .

Two random variables X {\displaystyle X} and Y {\displaystyle Y} are conditionally independent given a random variable W {\displaystyle W} if they are independent given σ ( W ): the σ-algebra generated by W {\displaystyle W} . This is commonly written: X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ W {\displaystyle X\perp \!\!\!\perp Y\mid W} or X ⊥ ⊥ Y ∣ ∣ W {\displaystyle X\perp Y\mid W} This is read " X {\displaystyle X} is independent of Y {\displaystyle Y} , given W {\displaystyle W} "; the conditioning applies to the whole statement: "( X {\displaystyle X} is independent of Y {\displaystyle Y} ) given W {\displaystyle W} ".

( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ W {\displaystyle (X\perp \!\!\!\perp Y)\mid W} This notation extends X ⊥ ⊥ ⊥ ⊥ Y {\displaystyle X\perp \!\!\!\perp Y} for " X {\displaystyle X} is independent of Y {\displaystyle Y} ." If W {\displaystyle W} assumes a countable set of values, this is equivalent to the conditional independence of X and Y for the events of the form [ W = w ] {\displaystyle [W=w]} .
Conditional independence of more than two events, or of more than two random variables, is defined analogously.

The following two examples show that X ⊥ ⊥ ⊥ ⊥ Y {\displaystyle X\perp \!\!\!\perp Y} neither implies nor is implied by ( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ W {\displaystyle (X\perp \!\!\!\perp Y)\mid W} .

First, suppose W {\displaystyle W} is 0 with probability 0.5 and 1 otherwise.  When W = 0 take X {\displaystyle X} and Y {\displaystyle Y} to be independent, each having the value 0 with probability 0.99 and the value 1 otherwise.  When W = 1 {\displaystyle W=1} , X {\displaystyle X} and Y {\displaystyle Y} are again independent, but this time they take the value 1 with probability 0.99.  Then ( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ W {\displaystyle (X\perp \!\!\!\perp Y)\mid W} . But X {\displaystyle X} and Y {\displaystyle Y} are dependent, because Pr( X = 0) < Pr( X = 0| Y = 0).  This is because Pr( X = 0) = 0.5, but if Y = 0 then it's very likely that W = 0 and thus that X = 0 as well, so Pr( X = 0| Y = 0) > 0.5.

For the second example, suppose X ⊥ ⊥ ⊥ ⊥ Y {\displaystyle X\perp \!\!\!\perp Y} , each taking the values 0 and 1 with probability 0.5. Let W {\displaystyle W} be the product X ⋅ ⋅ Y {\displaystyle X\cdot Y} .  Then when W = 0 {\displaystyle W=0} , Pr( X = 0) = 2/3, but Pr( X = 0| Y = 0) = 1/2, so ( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ W {\displaystyle (X\perp \!\!\!\perp Y)\mid W} is false.
This is also an example of Explaining Away. See Kevin Murphy's tutorial [ 3 ] where X {\displaystyle X} and Y {\displaystyle Y} take the values "brainy" and "sporty".

Conditional independence of random vectors [ edit ] Two random vectors X = ( X 1 , … … , X l ) T {\displaystyle \mathbf {X} =(X_{1},\ldots ,X_{l})^{\mathrm {T} }} and Y = ( Y 1 , … … , Y m ) T {\displaystyle \mathbf {Y} =(Y_{1},\ldots ,Y_{m})^{\mathrm {T} }} are conditionally independent given a third random vector Z = ( Z 1 , … … , Z n ) T {\displaystyle \mathbf {Z} =(Z_{1},\ldots ,Z_{n})^{\mathrm {T} }} if and only if they are independent in their conditional cumulative distribution given Z {\displaystyle \mathbf {Z} } . Formally: ( X ⊥ ⊥ ⊥ ⊥ Y ) ∣ ∣ Z ⟺ ⟺ F X , Y | Z = z ( x , y ) = F X ∣ ∣ Z = z ( x ) ⋅ ⋅ F Y ∣ ∣ Z = z ( y ) for all x , y , z {\displaystyle (\mathbf {X} \perp \!\!\!\perp \mathbf {Y} )\mid \mathbf {Z} \quad \iff \quad F_{\mathbf {X} ,\mathbf {Y} |\mathbf {Z} =\mathbf {z} }(\mathbf {x} ,\mathbf {y} )=F_{\mathbf {X} \,\mid \,\mathbf {Z} \,=\,\mathbf {z} }(\mathbf {x} )\cdot F_{\mathbf {Y} \,\mid \,\mathbf {Z} \,=\,\mathbf {z} }(\mathbf {y} )\quad {\text{for all }}\mathbf {x} ,\mathbf {y} ,\mathbf {z} } Eq.3 where x = ( x 1 , … … , x l ) T {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{l})^{\mathrm {T} }} , y = ( y 1 , … … , y m ) T {\displaystyle \mathbf {y} =(y_{1},\ldots ,y_{m})^{\mathrm {T} }} and z = ( z 1 , … … , z n ) T {\displaystyle \mathbf {z} =(z_{1},\ldots ,z_{n})^{\mathrm {T} }} and the conditional cumulative distributions are defined as follows.

F X , Y ∣ ∣ Z = z ( x , y ) = Pr ( X 1 ≤ ≤ x 1 , … … , X l ≤ ≤ x l , Y 1 ≤ ≤ y 1 , … … , Y m ≤ ≤ y m ∣ ∣ Z 1 = z 1 , … … , Z n = z n ) F X ∣ ∣ Z = z ( x ) = Pr ( X 1 ≤ ≤ x 1 , … … , X l ≤ ≤ x l ∣ ∣ Z 1 = z 1 , … … , Z n = z n ) F Y ∣ ∣ Z = z ( y ) = Pr ( Y 1 ≤ ≤ y 1 , … … , Y m ≤ ≤ y m ∣ ∣ Z 1 = z 1 , … … , Z n = z n ) {\displaystyle {\begin{aligned}F_{\mathbf {X} ,\mathbf {Y} \,\mid \,\mathbf {Z} \,=\,\mathbf {z} }(\mathbf {x} ,\mathbf {y} )&=\Pr(X_{1}\leq x_{1},\ldots ,X_{l}\leq x_{l},Y_{1}\leq y_{1},\ldots ,Y_{m}\leq y_{m}\mid Z_{1}=z_{1},\ldots ,Z_{n}=z_{n})\\[6pt]F_{\mathbf {X} \,\mid \,\mathbf {Z} \,=\,\mathbf {z} }(\mathbf {x} )&=\Pr(X_{1}\leq x_{1},\ldots ,X_{l}\leq x_{l}\mid Z_{1}=z_{1},\ldots ,Z_{n}=z_{n})\\[6pt]F_{\mathbf {Y} \,\mid \,\mathbf {Z} \,=\,\mathbf {z} }(\mathbf {y} )&=\Pr(Y_{1}\leq y_{1},\ldots ,Y_{m}\leq y_{m}\mid Z_{1}=z_{1},\ldots ,Z_{n}=z_{n})\end{aligned}}} Uses in Bayesian inference [ edit ] Let p be the proportion of voters who will vote "yes" in an upcoming referendum . In taking an opinion poll , one chooses n voters randomly from the population. For i = 1, ..., n , let X i = 1 or 0 corresponding, respectively, to whether or not the i th chosen voter will or will not vote "yes".

In a frequentist approach to statistical inference one would not attribute any probability distribution to p (unless the probabilities could be somehow interpreted as relative frequencies of occurrence of some event or as proportions of some population) and one would say that X 1 , ..., X n are independent random variables.

By contrast, in a Bayesian approach to statistical inference, one would assign a probability distribution to p regardless of the non-existence of any such "frequency" interpretation, and one would construe the probabilities as degrees of belief that p is in any interval to which a probability is assigned. In that model, the random variables X 1 , ..., X n are not independent, but they are conditionally independent given the value of p . In particular, if a large number of the X s are observed to be equal to 1, that would imply a high conditional probability , given that observation, that p is near 1, and thus a high conditional probability , given that observation, that the next X to be observed will be equal to 1.

Rules of conditional independence [ edit ] A set of rules governing statements of conditional independence have been derived from the basic definition.

[ 4 ] [ 5 ] These rules were termed " Graphoid Axioms"
by Pearl and Paz, [ 6 ] because they hold in graphs, where X ⊥ ⊥ ⊥ ⊥ A ∣ ∣ B {\displaystyle X\perp \!\!\!\perp A\mid B} is interpreted to mean: "All paths from X to A are intercepted by the set B ".

[ 7 ] Symmetry [ edit ] X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇔ ⇔ Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z {\displaystyle X\perp \!\!\!\perp Y\mid Z\quad \Leftrightarrow \quad Y\perp \!\!\!\perp X\mid Z} Proof: From the definition of conditional independence, X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇔ ⇔ P ( X , Y ∣ ∣ Z ) = P ( X ∣ ∣ Z ) P ( Y ∣ ∣ Z ) ⇔ ⇔ Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z {\displaystyle X\perp \!\!\!\perp Y\mid Z\quad \Leftrightarrow \quad P(X,Y\mid Z)=P(X\mid Z)P(Y\mid Z)\quad \Leftrightarrow \quad Y\perp \!\!\!\perp X\mid Z} Decomposition [ edit ] X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ h ( X ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z {\displaystyle X\perp \!\!\!\perp Y\mid Z\quad \Rightarrow \quad h(X)\perp \!\!\!\perp Y\mid Z} Proof From the definition of conditional independence, we seek to show that: X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ P ( h ( X ) , Y ∣ ∣ Z ) = P ( h ( X ) ∣ ∣ Z ) P ( Y ∣ ∣ Z ) {\displaystyle X\perp \!\!\!\perp Y\mid Z\quad \Rightarrow \quad P(h(X),Y\mid Z)=P(h(X)\mid Z)P(Y\mid Z)} . The left side of this equality is: P ( h ( X ) = a , Y = y ∣ ∣ Z = z ) = ∑ ∑ X : : h ( X ) = a P ( X = x , Y = y ∣ ∣ Z = z ) {\displaystyle P(h(X)=a,Y=y\mid Z=z)=\sum _{X\colon h(X)=a}P(X=x,Y=y\mid Z=z)} , where the expression on the right side of this equality is the summation over X {\displaystyle X} such that h ( X ) = a {\displaystyle h(X)=a} of the conditional probability of X , Y {\displaystyle X,Y} on Z {\displaystyle Z} .
Further decomposing, ∑ ∑ X : : h ( X ) = a P ( X = x , Y = y ∣ ∣ Z = z ) = ∑ ∑ X : : h ( X ) = a P ( X = x ∣ ∣ Z = z ) P ( Y = y ∣ ∣ Z = z ) = P ( Y = y ∣ ∣ Z = z ) ∑ ∑ X : : h ( X ) = a P ( X = x ∣ ∣ Z = z ) = P ( Y ∣ ∣ Z ) P ( h ( X ) ∣ ∣ Z ) {\displaystyle {\begin{aligned}\sum _{X\colon h(X)=a}P(X=x,Y=y\mid Z=z)=&\sum _{X\colon h(X)=a}P(X=x\mid Z=z)P(Y=y\mid Z=z)\\=&P(Y=y\mid Z=z)\sum _{X\colon h(X)=a}P(X=x\mid Z=z)\\=&P(Y\mid Z)P(h(X)\mid Z)\end{aligned}}} . Special cases of this property include ( X , W ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z {\displaystyle (X,W)\perp \!\!\!\perp Y\mid Z\quad \Rightarrow \quad X\perp \!\!\!\perp Y\mid Z} Proof: Let us define A = ( X , W ) {\displaystyle A=(X,W)} and h ( ⋅ ⋅ ) {\displaystyle h(\cdot )} be an 'extraction' function h ( X , W ) = X {\displaystyle h(X,W)=X} . Then: ( X , W ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇔ ⇔ A ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ h ( A ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z Decomposition ⇔ ⇔ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z {\displaystyle {\begin{aligned}(X,W)\perp \!\!\!\perp Y\mid Z\quad &\Leftrightarrow \quad A\perp \!\!\!\perp Y\mid Z\\&\Rightarrow \quad h(A)\perp \!\!\!\perp Y\mid Z\quad &{\text{Decomposition}}\\&\Leftrightarrow \quad X\perp \!\!\!\perp Y\mid Z\end{aligned}}} X ⊥ ⊥ ⊥ ⊥ ( Y , W ) ∣ ∣ Z ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z {\displaystyle X\perp \!\!\!\perp (Y,W)\mid Z\quad \Rightarrow \quad X\perp \!\!\!\perp Y\mid Z} Proof: Let us define V = ( Y , W ) {\displaystyle V=(Y,W)} and h ( ⋅ ⋅ ) {\displaystyle h(\cdot )} be again an 'extraction' function h ( Y , W ) = Y {\displaystyle h(Y,W)=Y} . Then: X ⊥ ⊥ ⊥ ⊥ ( Y , W ) ∣ ∣ Z ⇔ ⇔ X ⊥ ⊥ ⊥ ⊥ V ∣ ∣ Z ⇔ ⇔ V ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z Symmetry ⇒ ⇒ h ( V ) ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z Decomposition ⇔ ⇔ Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z ⇔ ⇔ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z Symmetry {\displaystyle {\begin{aligned}X\perp \!\!\!\perp (Y,W)\mid Z\quad &\Leftrightarrow \quad X\perp \!\!\!\perp V\mid Z\\&\Leftrightarrow \quad V\perp \!\!\!\perp X\mid Z\quad &{\text{Symmetry}}\\&\Rightarrow \quad h(V)\perp \!\!\!\perp X\mid Z\quad &{\text{Decomposition}}\\&\Leftrightarrow \quad Y\perp \!\!\!\perp X\mid Z\\&\Leftrightarrow \quad X\perp \!\!\!\perp Y\mid Z\quad &{\text{Symmetry}}\end{aligned}}} Weak union [ edit ] X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ ( Z , h ( X ) ) {\displaystyle X\perp \!\!\!\perp Y\mid Z\quad \Rightarrow \quad X\perp \!\!\!\perp Y\mid (Z,h(X))} Proof: Given X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z {\displaystyle X\perp \!\!\!\perp Y\mid Z} , we aim to show X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ ( Z , h ( X ) ) ⇔ ⇔ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ U where U = ( Z , h ( X ) ) ⇔ ⇔ Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ U Symmetry ⇔ ⇔ P ( Y ∣ ∣ X , U ) = P ( Y ∣ ∣ U ) ⇔ ⇔ P ( Y ∣ ∣ X , Z , h ( X ) ) = P ( Y ∣ ∣ Z , h ( X ) ) {\displaystyle {\begin{aligned}X\perp \!\!\!\perp Y\mid (Z,h(X))\quad &\Leftrightarrow \quad X\perp \!\!\!\perp Y\mid U\quad &{\text{where}}\quad U=(Z,h(X))\\&\Leftrightarrow \quad Y\perp \!\!\!\perp X\mid U\quad &{\text{Symmetry}}\\&\Leftrightarrow \quad P(Y\mid X,U)=P(Y\mid U)\\&\Leftrightarrow \quad P(Y\mid X,Z,h(X))=P(Y\mid Z,h(X))\end{aligned}}} . We begin with the left side of the equation P ( Y ∣ ∣ X , Z , h ( X ) ) = P ( Y ∣ ∣ X , Z ) = P ( Y ∣ ∣ Z ) Since by symmetry Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ Z {\displaystyle {\begin{aligned}P(Y\mid X,Z,h(X))&=P(Y\mid X,Z)\\&=P(Y\mid Z)&{\text{Since by symmetry }}Y\perp \!\!\!\perp X\mid Z\end{aligned}}} . From the given condition X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ h ( X ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z Decomposition ⇔ ⇔ Y ⊥ ⊥ ⊥ ⊥ h ( X ) ∣ ∣ Z Symmetry ⇒ ⇒ P ( Y ∣ ∣ Z , h ( X ) ) = P ( Y ∣ ∣ Z ) {\displaystyle {\begin{aligned}X\perp \!\!\!\perp Y\mid Z\quad &\Rightarrow \quad h(X)\perp \!\!\!\perp Y\mid Z\quad &{\text{Decomposition}}\\&\Leftrightarrow \quad Y\perp \!\!\!\perp h(X)\mid Z\quad &{\text{Symmetry}}\\&\Rightarrow \quad P(Y\mid Z,h(X))=P(Y\mid Z)\end{aligned}}} . Thus P ( Y ∣ ∣ X , Z , h ( X ) ) = P ( Y ∣ ∣ Z , h ( X ) ) {\displaystyle P(Y\mid X,Z,h(X))=P(Y\mid Z,h(X))} , so we have shown that X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ ( Z , h ( X ) ) {\displaystyle X\perp \!\!\!\perp Y\mid (Z,h(X))} .

Special Cases: Some textbooks present the property as X ⊥ ⊥ ⊥ ⊥ ( Y , W ) ∣ ∣ Z ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ ( Z , W ) {\displaystyle X\perp \!\!\!\perp (Y,W)\mid Z\quad \Rightarrow \quad X\perp \!\!\!\perp Y\mid (Z,W)} [ 8 ] .

( X , W ) ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ ( Z , W ) {\displaystyle (X,W)\perp \!\!\!\perp Y\mid Z\quad \Rightarrow \quad X\perp \!\!\!\perp Y\mid (Z,W)} .

Both versions can be shown to follow from the weak union property given initially via the same method as in the decomposition section above.

Contraction [ edit ] X ⊥ ⊥ ⊥ ⊥ A ∣ ∣ B X ⊥ ⊥ ⊥ ⊥ B } and ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ A , B {\displaystyle \left.{\begin{aligned}X\perp \!\!\!\perp A\mid B\\X\perp \!\!\!\perp B\end{aligned}}\right\}{\text{ and }}\quad \Rightarrow \quad X\perp \!\!\!\perp A,B} Proof This property can be proved by noticing Pr ( X ∣ ∣ A , B ) = Pr ( X ∣ ∣ B ) = Pr ( X ) {\displaystyle \Pr(X\mid A,B)=\Pr(X\mid B)=\Pr(X)} , each equality of which is asserted by X ⊥ ⊥ ⊥ ⊥ A ∣ ∣ B {\displaystyle X\perp \!\!\!\perp A\mid B} and X ⊥ ⊥ ⊥ ⊥ B {\displaystyle X\perp \!\!\!\perp B} , respectively.

Intersection [ edit ] For strictly positive probability distributions, [ 5 ] the following also holds: X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ Z , W X ⊥ ⊥ ⊥ ⊥ W ∣ ∣ Z , Y } and ⇒ ⇒ X ⊥ ⊥ ⊥ ⊥ W , Y ∣ ∣ Z {\displaystyle \left.{\begin{aligned}X\perp \!\!\!\perp Y\mid Z,W\\X\perp \!\!\!\perp W\mid Z,Y\end{aligned}}\right\}{\text{ and }}\quad \Rightarrow \quad X\perp \!\!\!\perp W,Y\mid Z} Proof By assumption: P ( X | Z , W , Y ) = P ( X | Z , W ) ∧ ∧ P ( X | Z , W , Y ) = P ( X | Z , Y ) ⟹ ⟹ P ( X | Z , Y ) = P ( X | Z , W ) {\displaystyle P(X|Z,W,Y)=P(X|Z,W)\land P(X|Z,W,Y)=P(X|Z,Y)\implies P(X|Z,Y)=P(X|Z,W)} Using this equality, together with the Law of total probability applied to P ( X | Z ) {\displaystyle P(X|Z)} : P ( X | Z ) = ∑ ∑ w ∈ ∈ W P ( X | Z , W = w ) P ( W = w | Z ) = ∑ ∑ w ∈ ∈ W P ( X | Y , Z ) P ( W = w | Z ) = P ( X | Z , Y ) ∑ ∑ w ∈ ∈ W P ( W = w | Z ) = P ( X | Z , Y ) {\displaystyle {\begin{aligned}P(X|Z)&=\sum _{w\in W}P(X|Z,W=w)P(W=w|Z)\\[4pt]&=\sum _{w\in W}P(X|Y,Z)P(W=w|Z)\\[4pt]&=P(X|Z,Y)\sum _{w\in W}P(W=w|Z)\\[4pt]&=P(X|Z,Y)\end{aligned}}} Since P ( X | Z , W , Y ) = P ( X | Z , Y ) {\displaystyle P(X|Z,W,Y)=P(X|Z,Y)} and P ( X | Z , Y ) = P ( X | Z ) {\displaystyle P(X|Z,Y)=P(X|Z)} , it follows that P ( X | Z , W , Y ) = P ( X | Z ) ⟺ ⟺ X ⊥ ⊥ ⊥ ⊥ Y , W | Z {\displaystyle P(X|Z,W,Y)=P(X|Z)\iff X\perp \!\!\!\perp Y,W|Z} .

Technical note: since these implications hold for any probability space , they will still hold if one considers a sub-universe by conditioning everything on another variable, say K . For example, X ⊥ ⊥ ⊥ ⊥ Y ⇒ ⇒ Y ⊥ ⊥ ⊥ ⊥ X {\displaystyle X\perp \!\!\!\perp Y\Rightarrow Y\perp \!\!\!\perp X} would also mean that X ⊥ ⊥ ⊥ ⊥ Y ∣ ∣ K ⇒ ⇒ Y ⊥ ⊥ ⊥ ⊥ X ∣ ∣ K {\displaystyle X\perp \!\!\!\perp Y\mid K\Rightarrow Y\perp \!\!\!\perp X\mid K} .

See also [ edit ] Graphoid Conditional dependence de Finetti's theorem Conditional expectation References [ edit ] ^ To see that this is the case, one needs to realise that Pr( R ∩ B | Y ) is the probability of an overlap of R and B (the purple shaded area) in the Y area. Since, in the picture on the left, there are two squares where R and B overlap within the Y area, and the Y area has twelve squares, Pr( R ∩ B | Y ) = ⁠ 2 / 12 ⁠ = ⁠ 1 / 6 ⁠ . Similarly, Pr( R | Y ) = ⁠ 4 / 12 ⁠ = ⁠ 1 / 3 ⁠ and Pr( B | Y ) = ⁠ 6 / 12 ⁠ = ⁠ 1 / 2 ⁠ .

^ a b Could someone explain conditional independence?

^ "Graphical Models" .

^ Dawid, A. P.

(1979). "Conditional Independence in Statistical Theory".

Journal of the Royal Statistical Society, Series B .

41 (1): 1– 31.

JSTOR 2984718 .

MR 0535541 .

^ a b J Pearl, Causality: Models, Reasoning, and Inference, 2000, Cambridge University Press ^ Pearl, Judea ; Paz, Azaria (1986). "Graphoids: Graph-Based Logic for Reasoning about Relevance Relations or When would x tell you more about y if you already know z?". In du Boulay, Benedict; Hogg, David C.; Steels, Luc (eds.).

Advances in Artificial Intelligence II, Seventh European Conference on Artificial Intelligence, ECAI 1986, Brighton, UK, July 20–25, 1986, Proceedings (PDF) . North-Holland. pp.

357– 363.

^ Pearl, Judea (1988).

Probabilistic reasoning in intelligent systems: networks of plausible inference . Morgan Kaufmann.

ISBN 9780934613736 .

^ Koller, Daphne; Friedman, Nir (2009).

Probabilistic Graphical Models . Cambridge, MA: The MIT Press.

ISBN 9780262013192 .

External links [ edit ] Media related to Conditional independence at Wikimedia Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Conditional_independence&oldid=1304762352 " Category : Independence (probability theory) Hidden categories: Articles with short description Short description is different from Wikidata Commons category link from Wikidata This page was last edited on 8 August 2025, at 01:13 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Conditional independence 14 languages Add topic

