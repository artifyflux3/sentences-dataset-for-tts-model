Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Conflicting nomenclatures and notations 2 Properties Toggle Properties subsection 2.1 Relation to the autocorrelation matrix 2.2 Relation to the correlation matrix 2.3 Inverse of the covariance matrix 2.4 Basic properties 2.5 Block matrices 3 Partial covariance matrix 4 Standard deviation matrix 5 Covariance matrix as a parameter of a distribution 6 Covariance matrix as a linear operator 7 Admissibility 8 Complex random vectors Toggle Complex random vectors subsection 8.1 Pseudo-covariance matrix 9 Estimation 10 Applications Toggle Applications subsection 10.1 Use in optimization 10.2 Covariance mapping 10.3 Two-dimensional infrared spectroscopy 11 See also 12 References 13 Further reading Toggle the table of contents Covariance matrix 23 languages العربية Català Čeština Deutsch Español فارسی Français 한국어 Italiano Magyar Nederlands 日本語 Polski Português Русский Slovenčina Slovenščina Sunda Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Measure of covariance of components of a random vector Not to be confused with Cross-covariance matrix .

A bivariate Gaussian probability density function centered at (0, 0), with covariance matrix given by [ 1 0.5 0.5 1 ] {\displaystyle {\begin{bmatrix}1&0.5\\0.5&1\end{bmatrix}}} Sample points from a bivariate Gaussian distribution with a standard deviation of 3 in roughly the lower left–upper right direction and of 1 in the orthogonal direction. Because the x and y components co-vary, the variances of x {\displaystyle x} and y {\displaystyle y} do not fully describe the distribution. A 2 × × 2 {\displaystyle 2\times 2} covariance matrix is needed; the directions of the arrows correspond to the eigenvectors of this covariance matrix and their lengths to the square roots of the eigenvalues .

Part of a series on Statistics Correlation and covariance For random vectors Autocorrelation matrix Cross-correlation matrix Auto-covariance matrix Cross-covariance matrix For stochastic processes Autocorrelation function Cross-correlation function Autocovariance function Cross-covariance function For deterministic signals Autocorrelation function Cross-correlation function Autocovariance function Cross-covariance function v t e In probability theory and statistics , a covariance matrix (also known as auto-covariance matrix , dispersion matrix , variance matrix , or variance–covariance matrix ) is a square matrix giving the covariance between each pair of elements of a given random vector .

Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the x {\displaystyle x} and y {\displaystyle y} directions contain all of the necessary information; a 2 × × 2 {\displaystyle 2\times 2} matrix would be necessary to fully characterize the two-dimensional variation.

Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself).

The covariance matrix of a random vector X {\displaystyle \mathbf {X} } is typically denoted by K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} , Σ Σ {\displaystyle \Sigma } or S {\displaystyle S} .

Definition [ edit ] Throughout this article, boldfaced unsubscripted X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are used to refer to random vectors, and Roman subscripted X i {\displaystyle X_{i}} and Y i {\displaystyle Y_{i}} are used to refer to scalar random variables.

If the entries in the column vector X = ( X 1 , X 2 , … … , X n ) T {\displaystyle \mathbf {X} =(X_{1},X_{2},\dots ,X_{n})^{\mathsf {T}}} are random variables , each with finite variance and expected value , then the covariance matrix K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} is the matrix whose ( i , j ) {\displaystyle (i,j)} entry is the covariance [ 1 ] : 177 K X i X j = cov ⁡ ⁡ [ X i , X j ] = E ⁡ ⁡ [ ( X i − − E ⁡ ⁡ [ X i ] ) ( X j − − E ⁡ ⁡ [ X j ] ) ] {\displaystyle \operatorname {K} _{X_{i}X_{j}}=\operatorname {cov} [X_{i},X_{j}]=\operatorname {E} [(X_{i}-\operatorname {E} [X_{i}])(X_{j}-\operatorname {E} [X_{j}])]} where the operator E {\displaystyle \operatorname {E} } denotes the expected value (mean) of its argument.

Conflicting nomenclatures and notations [ edit ] Nomenclatures differ. Some statisticians, following the probabilist William Feller in his two-volume book An Introduction to Probability Theory and Its Applications , [ 2 ] call the matrix K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} the variance of the random vector X {\displaystyle \mathbf {X} } , because it is the natural generalization to higher dimensions of the 1-dimensional variance. Others call it the covariance matrix , because it is the matrix of covariances between the scalar components of the vector X {\displaystyle \mathbf {X} } .

var ⁡ ⁡ ( X ) = cov ⁡ ⁡ ( X , X ) = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T ] .

{\displaystyle \operatorname {var} (\mathbf {X} )=\operatorname {cov} (\mathbf {X} ,\mathbf {X} )=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {X} -\operatorname {E} [\mathbf {X} ])^{\mathsf {T}}\right].} Both forms are quite standard, and there is no ambiguity between them. The matrix K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} is also often called the variance-covariance matrix , since the diagonal terms are in fact variances.

By comparison, the notation for the cross-covariance matrix between two vectors is cov ⁡ ⁡ ( X , Y ) = K X Y = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( Y − − E ⁡ ⁡ [ Y ] ) T ] .

{\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )=\operatorname {K} _{\mathbf {X} \mathbf {Y} }=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{\mathsf {T}}\right].} Properties [ edit ] Relation to the autocorrelation matrix [ edit ] The auto-covariance matrix K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} is related to the autocorrelation matrix R X X {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {X} }} by K X X = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T ] = R X X − − E ⁡ ⁡ [ X ] E ⁡ ⁡ [ X ] T {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }=\operatorname {E} [(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {X} -\operatorname {E} [\mathbf {X} ])^{\mathsf {T}}]=\operatorname {R} _{\mathbf {X} \mathbf {X} }-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {X} ]^{\mathsf {T}}} where the autocorrelation matrix is defined as R X X = E ⁡ ⁡ [ X X T ] {\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {X} }=\operatorname {E} [\mathbf {X} \mathbf {X} ^{\mathsf {T}}]} .

Relation to the correlation matrix [ edit ] Further information: Correlation matrix An entity closely related to the covariance matrix is the matrix of Pearson product-moment correlation coefficients between each of the random variables in the random vector X {\displaystyle \mathbf {X} } , which can be written as corr ⁡ ⁡ ( X ) = ( diag ⁡ ⁡ ( K X X ) ) − − 1 2 K X X ( diag ⁡ ⁡ ( K X X ) ) − − 1 2 , {\displaystyle \operatorname {corr} (\mathbf {X} )={\big (}\operatorname {diag} (\operatorname {K} _{\mathbf {X} \mathbf {X} }){\big )}^{-{\frac {1}{2}}}\,\operatorname {K} _{\mathbf {X} \mathbf {X} }\,{\big (}\operatorname {diag} (\operatorname {K} _{\mathbf {X} \mathbf {X} }){\big )}^{-{\frac {1}{2}}},} where diag ⁡ ⁡ ( K X X ) {\displaystyle \operatorname {diag} (\operatorname {K} _{\mathbf {X} \mathbf {X} })} is the matrix of the diagonal elements of K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }} (i.e., a diagonal matrix of the variances of X i {\displaystyle X_{i}} for i = 1 , … … , n {\displaystyle i=1,\dots ,n} ).

Equivalently, the correlation matrix can be seen as the covariance matrix of the standardized random variables X i / σ σ ( X i ) {\displaystyle X_{i}/\sigma (X_{i})} for i = 1 , … … , n {\displaystyle i=1,\dots ,n} .

corr ⁡ ⁡ ( X ) = [ 1 E ⁡ ⁡ [ ( X 1 − − μ μ 1 ) ( X 2 − − μ μ 2 ) ] σ σ ( X 1 ) σ σ ( X 2 ) ⋯ ⋯ E ⁡ ⁡ [ ( X 1 − − μ μ 1 ) ( X n − − μ μ n ) ] σ σ ( X 1 ) σ σ ( X n ) E ⁡ ⁡ [ ( X 2 − − μ μ 2 ) ( X 1 − − μ μ 1 ) ] σ σ ( X 2 ) σ σ ( X 1 ) 1 ⋯ ⋯ E ⁡ ⁡ [ ( X 2 − − μ μ 2 ) ( X n − − μ μ n ) ] σ σ ( X 2 ) σ σ ( X n ) ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ E ⁡ ⁡ [ ( X n − − μ μ n ) ( X 1 − − μ μ 1 ) ] σ σ ( X n ) σ σ ( X 1 ) E ⁡ ⁡ [ ( X n − − μ μ n ) ( X 2 − − μ μ 2 ) ] σ σ ( X n ) σ σ ( X 2 ) ⋯ ⋯ 1 ] .

{\displaystyle \operatorname {corr} (\mathbf {X} )={\begin{bmatrix}1&{\frac {\operatorname {E} [(X_{1}-\mu _{1})(X_{2}-\mu _{2})]}{\sigma (X_{1})\sigma (X_{2})}}&\cdots &{\frac {\operatorname {E} [(X_{1}-\mu _{1})(X_{n}-\mu _{n})]}{\sigma (X_{1})\sigma (X_{n})}}\\\\{\frac {\operatorname {E} [(X_{2}-\mu _{2})(X_{1}-\mu _{1})]}{\sigma (X_{2})\sigma (X_{1})}}&1&\cdots &{\frac {\operatorname {E} [(X_{2}-\mu _{2})(X_{n}-\mu _{n})]}{\sigma (X_{2})\sigma (X_{n})}}\\\\\vdots &\vdots &\ddots &\vdots \\\\{\frac {\operatorname {E} [(X_{n}-\mu _{n})(X_{1}-\mu _{1})]}{\sigma (X_{n})\sigma (X_{1})}}&{\frac {\operatorname {E} [(X_{n}-\mu _{n})(X_{2}-\mu _{2})]}{\sigma (X_{n})\sigma (X_{2})}}&\cdots &1\end{bmatrix}}.} Each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1. Each off-diagonal element is between −1 and +1 inclusive.

Inverse of the covariance matrix [ edit ] The inverse of this matrix , K X X − − 1 {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }^{-1}} , if it exists, is the inverse covariance matrix (or inverse concentration matrix [ dubious – discuss ] ), also known as the precision matrix (or concentration matrix ).

[ 3 ] Just as the covariance matrix can be written as the rescaling of a correlation matrix by the marginal variances: cov ⁡ ⁡ ( X ) = [ σ σ x 1 0 σ σ x 2 ⋱ ⋱ 0 σ σ x n ] [ 1 ρ ρ x 1 , x 2 ⋯ ⋯ ρ ρ x 1 , x n ρ ρ x 2 , x 1 1 ⋯ ⋯ ρ ρ x 2 , x n ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ ρ ρ x n , x 1 ρ ρ x n , x 2 ⋯ ⋯ 1 ] [ σ σ x 1 0 σ σ x 2 ⋱ ⋱ 0 σ σ x n ] {\displaystyle \operatorname {cov} (\mathbf {X} )={\begin{bmatrix}\sigma _{x_{1}}&&&0\\&\sigma _{x_{2}}\\&&\ddots \\0&&&\sigma _{x_{n}}\end{bmatrix}}{\begin{bmatrix}1&\rho _{x_{1},x_{2}}&\cdots &\rho _{x_{1},x_{n}}\\\rho _{x_{2},x_{1}}&1&\cdots &\rho _{x_{2},x_{n}}\\\vdots &\vdots &\ddots &\vdots \\\rho _{x_{n},x_{1}}&\rho _{x_{n},x_{2}}&\cdots &1\\\end{bmatrix}}{\begin{bmatrix}\sigma _{x_{1}}&&&0\\&\sigma _{x_{2}}\\&&\ddots \\0&&&\sigma _{x_{n}}\end{bmatrix}}} So, using the idea of partial correlation , and partial variance, the inverse covariance matrix can be expressed analogously: cov ⁡ ⁡ ( X ) − − 1 = [ 1 σ σ x 1 | x 2 .

.

.

0 1 σ σ x 2 | x 1 , x 3 .

.

.

⋱ ⋱ 0 1 σ σ x n | x 1 .

.

.

x n − − 1 ] [ 1 − − ρ ρ x 1 , x 2 ∣ ∣ x 3 .

.

.

⋯ ⋯ − − ρ ρ x 1 , x n ∣ ∣ x 2 .

.

.

x n − − 1 − − ρ ρ x 2 , x 1 ∣ ∣ x 3 .

.

.

1 ⋯ ⋯ − − ρ ρ x 2 , x n ∣ ∣ x 1 , x 3 .

.

.

x n − − 1 ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ − − ρ ρ x n , x 1 ∣ ∣ x 2 .

.

.

x n − − 1 − − ρ ρ x n , x 2 ∣ ∣ x 1 , x 3 .

.

.

x n − − 1 ⋯ ⋯ 1 ] [ 1 σ σ x 1 | x 2 .

.

.

0 1 σ σ x 2 | x 1 , x 3 .

.

.

⋱ ⋱ 0 1 σ σ x n | x 1 .

.

.

x n − − 1 ] {\displaystyle \operatorname {cov} (\mathbf {X} )^{-1}={\begin{bmatrix}{\frac {1}{\sigma _{x_{1}|x_{2}...}}}&&&0\\&{\frac {1}{\sigma _{x_{2}|x_{1},x_{3}...}}}\\&&\ddots \\0&&&{\frac {1}{\sigma _{x_{n}|x_{1}...x_{n-1}}}}\end{bmatrix}}{\begin{bmatrix}1&-\rho _{x_{1},x_{2}\mid x_{3}...}&\cdots &-\rho _{x_{1},x_{n}\mid x_{2}...x_{n-1}}\\-\rho _{x_{2},x_{1}\mid x_{3}...}&1&\cdots &-\rho _{x_{2},x_{n}\mid x_{1},x_{3}...x_{n-1}}\\\vdots &\vdots &\ddots &\vdots \\-\rho _{x_{n},x_{1}\mid x_{2}...x_{n-1}}&-\rho _{x_{n},x_{2}\mid x_{1},x_{3}...x_{n-1}}&\cdots &1\\\end{bmatrix}}{\begin{bmatrix}{\frac {1}{\sigma _{x_{1}|x_{2}...}}}&&&0\\&{\frac {1}{\sigma _{x_{2}|x_{1},x_{3}...}}}\\&&\ddots \\0&&&{\frac {1}{\sigma _{x_{n}|x_{1}...x_{n-1}}}}\end{bmatrix}}} This duality motivates a number of other dualities between marginalizing and conditioning for Gaussian random variables.

Basic properties [ edit ] For K X X = var ⁡ ⁡ ( X ) = E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T ] {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }=\operatorname {var} (\mathbf {X} )=\operatorname {E} \left[\left(\mathbf {X} -\operatorname {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\operatorname {E} [\mathbf {X} ]\right)^{\mathsf {T}}\right]} and μ μ X = E ⁡ ⁡ [ X ] {\displaystyle {\boldsymbol {\mu }}_{\mathbf {X} }=\operatorname {E} [{\textbf {X}}]} , where X = ( X 1 , … … , X n ) T {\displaystyle \mathbf {X} =(X_{1},\ldots ,X_{n})^{\mathsf {T}}} is an n {\displaystyle n} -dimensional random variable, the following basic properties apply: [ 4 ] K X X = E ⁡ ⁡ ( X X T ) − − μ μ X μ μ X T {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }=\operatorname {E} (\mathbf {XX^{\mathsf {T}}} )-{\boldsymbol {\mu }}_{\mathbf {X} }{\boldsymbol {\mu }}_{\mathbf {X} }^{\mathsf {T}}} K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }\,} is positive-semidefinite , i.e.

a T K X X ⁡ ⁡ a ≥ ≥ 0 for all a ∈ ∈ R n {\displaystyle \mathbf {a} ^{T}\operatorname {K} _{\mathbf {X} \mathbf {X} }\mathbf {a} \geq 0\quad {\text{for all }}\mathbf {a} \in \mathbb {R} ^{n}} Proof Indeed, from the property 4 it follows that under linear transformation of random variable X {\displaystyle \mathbf {X} } with covariation matrix Σ Σ X = c o v ( X ) {\displaystyle \mathbf {\Sigma _{X}} =\mathrm {cov} (\mathbf {X} )} by linear operator A {\displaystyle \mathbf {A} } s.a.

Y = A X {\displaystyle \mathbf {Y} =\mathbf {A} \mathbf {X} } , the covariation matrix is tranformed as Σ Σ Y = c o v ( Y ) = A Σ Σ X A ⊤ ⊤ {\displaystyle \mathbf {\Sigma _{Y}} =\mathrm {cov} \left(\mathbf {Y} \right)=\mathbf {A\,\Sigma _{X}\,A} ^{\top }} .

As according to the property 3 matrix Σ Σ X {\displaystyle \mathbf {\Sigma _{X}} } is symmetric, it can be diagonalized by a linear orthogonal transformation, i.e. there exists such orthogonal matrix A {\displaystyle \mathbf {A} } (meanwhile A ⊤ ⊤ = A − − 1 {\displaystyle \mathbf {A} ^{\top }=\mathbf {A} ^{-1}} ), that A Σ Σ X A ⊤ ⊤ = A Σ Σ X A − − 1 = diag ( σ σ 1 , … … , σ σ n ) , {\displaystyle \mathbf {A\,\Sigma _{X}\,A} ^{\top }=\mathbf {A\,\Sigma _{X}\,A} ^{-1}={\mbox{diag}}(\sigma _{1},\ldots ,\sigma _{n}),} and σ σ 1 , … … , σ σ n {\displaystyle \sigma _{1},\ldots ,\sigma _{n}} are eigenvalues of Σ Σ X {\displaystyle \mathbf {\Sigma _{X}} } . But this means that this matrix is a covariation matrix for a random variable Y = A X {\displaystyle \mathbf {Y} =\mathbf {A} \mathbf {X} } , and the main diagonal of Σ Σ Y = c o v ( Y ) {\displaystyle \mathbf {\Sigma _{Y}} =\mathrm {cov} \left(\mathbf {Y} \right)} consists of variances of elements of Y {\displaystyle \mathbf {Y} } vector. As variance is always non-negative, we conclude that σ σ i ≥ ≥ 0 {\displaystyle \sigma _{i}\geq 0} for any i {\displaystyle i} . But this means that matrix Σ Σ X {\displaystyle \mathbf {\Sigma _{X}} } is positive-semidefinite.

K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }\,} is symmetric , i.e.

K X X T = K X X {\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }^{\mathsf {T}}=\operatorname {K} _{\mathbf {X} \mathbf {X} }} For any constant (i.e. non-random) m × × n {\displaystyle m\times n} matrix A {\displaystyle \mathbf {A} } and constant m × × 1 {\displaystyle m\times 1} vector a {\displaystyle \mathbf {a} } , one has var ⁡ ⁡ ( A X + a ) = A var ⁡ ⁡ ( X ) A T {\displaystyle \operatorname {var} (\mathbf {AX} +\mathbf {a} )=\mathbf {A} \,\operatorname {var} (\mathbf {X} )\,\mathbf {A} ^{\mathsf {T}}} If Y {\displaystyle \mathbf {Y} } is another random vector with the same dimension as X {\displaystyle \mathbf {X} } , then var ⁡ ⁡ ( X + Y ) = var ⁡ ⁡ ( X ) + cov ⁡ ⁡ ( X , Y ) + cov ⁡ ⁡ ( Y , X ) + var ⁡ ⁡ ( Y ) {\displaystyle \operatorname {var} (\mathbf {X} +\mathbf {Y} )=\operatorname {var} (\mathbf {X} )+\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )+\operatorname {cov} (\mathbf {Y} ,\mathbf {X} )+\operatorname {var} (\mathbf {Y} )} where cov ⁡ ⁡ ( X , Y ) {\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )} is the cross-covariance matrix of X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } .

Block matrices [ edit ] The joint mean μ μ {\displaystyle {\boldsymbol {\mu }}} and joint covariance matrix Σ Σ {\displaystyle {\boldsymbol {\Sigma }}} of X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } can be written in block form μ μ = [ μ μ X μ μ Y ] , Σ Σ = [ K X X K X Y K Y X K Y Y ] {\displaystyle {\boldsymbol {\mu }}={\begin{bmatrix}{\boldsymbol {\mu }}_{X}\\{\boldsymbol {\mu }}_{Y}\end{bmatrix}},\qquad {\boldsymbol {\Sigma }}={\begin{bmatrix}\operatorname {K} _{\mathbf {XX} }&\operatorname {K} _{\mathbf {XY} }\\\operatorname {K} _{\mathbf {YX} }&\operatorname {K} _{\mathbf {YY} }\end{bmatrix}}} where K X X = var ⁡ ⁡ ( X ) {\displaystyle \operatorname {K} _{\mathbf {XX} }=\operatorname {var} (\mathbf {X} )} , K Y Y = var ⁡ ⁡ ( Y ) {\displaystyle \operatorname {K} _{\mathbf {YY} }=\operatorname {var} (\mathbf {Y} )} and K X Y = K Y X T = cov ⁡ ⁡ ( X , Y ) {\displaystyle \operatorname {K} _{\mathbf {XY} }=\operatorname {K} _{\mathbf {YX} }^{\mathsf {T}}=\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )} .

K X X {\displaystyle \operatorname {K} _{\mathbf {XX} }} and K Y Y {\displaystyle \operatorname {K} _{\mathbf {YY} }} can be identified as the variance matrices of the marginal distributions for X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } respectively.

If X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are jointly normally distributed , X , Y ∼ ∼ N ( μ μ , Σ Σ ) , {\displaystyle \mathbf {X} ,\mathbf {Y} \sim \ {\mathcal {N}}({\boldsymbol {\mu }},\operatorname {\boldsymbol {\Sigma }} ),} then the conditional distribution for Y {\displaystyle \mathbf {Y} } given X {\displaystyle \mathbf {X} } is given by [ 5 ] Y ∣ ∣ X ∼ ∼ N ( μ μ Y | X , K Y | X ) , {\displaystyle \mathbf {Y} \mid \mathbf {X} \sim \ {\mathcal {N}}({\boldsymbol {\mu }}_{\mathbf {Y|X} },\operatorname {K} _{\mathbf {Y|X} }),} defined by conditional mean μ μ Y | X = μ μ Y + K Y X ⁡ ⁡ K X X − − 1 ⁡ ⁡ ( X − − μ μ X ) {\displaystyle {\boldsymbol {\mu }}_{\mathbf {Y} |\mathbf {X} }={\boldsymbol {\mu }}_{\mathbf {Y} }+\operatorname {K} _{\mathbf {YX} }\operatorname {K} _{\mathbf {XX} }^{-1}\left(\mathbf {X} -{\boldsymbol {\mu }}_{\mathbf {X} }\right)} and conditional variance K Y | X = K Y Y − − K Y X ⁡ ⁡ K X X − − 1 ⁡ ⁡ K X Y .

{\displaystyle \operatorname {K} _{\mathbf {Y|X} }=\operatorname {K} _{\mathbf {YY} }-\operatorname {K} _{\mathbf {YX} }\operatorname {K} _{\mathbf {XX} }^{-1}\operatorname {K} _{\mathbf {XY} }.} The matrix K Y X ⁡ ⁡ K X X − − 1 {\displaystyle \operatorname {K} _{\mathbf {YX} }\operatorname {K} _{\mathbf {XX} }^{-1}} is known as the matrix of regression coefficients, while in linear algebra K Y | X {\displaystyle \operatorname {K} _{\mathbf {Y|X} }} is the Schur complement of K X X {\displaystyle \operatorname {K} _{\mathbf {XX} }} in Σ Σ {\displaystyle {\boldsymbol {\Sigma }}} .

The matrix of regression coefficients may often be given in transpose form, K X X − − 1 ⁡ ⁡ K X Y {\displaystyle \operatorname {K} _{\mathbf {XX} }^{-1}\operatorname {K} _{\mathbf {XY} }} , suitable for post-multiplying a row vector of explanatory variables X T {\displaystyle \mathbf {X} ^{\mathsf {T}}} rather than pre-multiplying a column vector X {\displaystyle \mathbf {X} } . In this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares (OLS).

Partial covariance matrix [ edit ] A covariance matrix with all non-zero elements tells us that all the individual random variables are interrelated. This means that the variables are not only directly correlated, but also correlated via other variables indirectly. Often such indirect, common-mode correlations are trivial and uninteresting. They can be suppressed by calculating the partial covariance matrix, that is the part of covariance matrix that shows only the interesting part of correlations.

If two vectors of random variables X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are correlated via another vector I {\displaystyle \mathbf {I} } , the latter correlations are suppressed in a matrix [ 6 ] K X Y ∣ ∣ I = pcov ⁡ ⁡ ( X , Y ∣ ∣ I ) = cov ⁡ ⁡ ( X , Y ) − − cov ⁡ ⁡ ( X , I ) cov ⁡ ⁡ ( I , I ) − − 1 cov ⁡ ⁡ ( I , Y ) .

{\displaystyle \operatorname {K} _{\mathbf {XY\mid I} }=\operatorname {pcov} (\mathbf {X} ,\mathbf {Y} \mid \mathbf {I} )=\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )-\operatorname {cov} (\mathbf {X} ,\mathbf {I} )\operatorname {cov} (\mathbf {I} ,\mathbf {I} )^{-1}\operatorname {cov} (\mathbf {I} ,\mathbf {Y} ).} The partial covariance matrix K X Y ∣ ∣ I {\displaystyle \operatorname {K} _{\mathbf {XY\mid I} }} is effectively the simple covariance matrix K X Y {\displaystyle \operatorname {K} _{\mathbf {XY} }} as if the uninteresting random variables I {\displaystyle \mathbf {I} } were held constant.

Standard deviation matrix [ edit ] Main article: Standard deviation § Standard deviation matrix The standard deviation matrix S {\displaystyle \mathbf {S} } is the extension of the standard deviation to multiple dimensions.  It is the symmetric square root of the covariance matrix Σ Σ {\displaystyle \mathbf {\Sigma } } .

[ 7 ] Covariance matrix as a parameter of a distribution [ edit ] If a column vector X {\displaystyle \mathbf {X} } of n {\displaystyle n} possibly correlated random variables is jointly normally distributed , or more generally elliptically distributed , then its probability density function f ⁡ ⁡ ( X ) {\displaystyle \operatorname {f} (\mathbf {X} )} can be expressed in terms of the covariance matrix Σ Σ {\displaystyle {\boldsymbol {\Sigma }}} as follows [ 6 ] f ⁡ ⁡ ( X ) = ( 2 π π ) − − n / 2 | Σ Σ | − − 1 / 2 exp ⁡ ⁡ ( − − 1 2 ( X − − μ μ ) T Σ Σ − − 1 ( X − − μ μ ) ) , {\displaystyle \operatorname {f} (\mathbf {X} )=(2\pi )^{-n/2}|{\boldsymbol {\Sigma }}|^{-1/2}\exp \left(-{\tfrac {1}{2}}\mathbf {(X-\mu )^{\mathsf {T}}\Sigma ^{-1}(X-\mu )} \right),} where μ μ = E ⁡ ⁡ [ X ] {\displaystyle {\boldsymbol {\mu }}=\operatorname {E} [\mathbf {X} ]} and | Σ Σ | {\displaystyle |{\boldsymbol {\Sigma }}|} is the determinant of Σ Σ {\displaystyle {\boldsymbol {\Sigma }}} , the so-called generalized variance .

Covariance matrix as a linear operator [ edit ] Main article: Covariance operator Applied to one vector, the covariance matrix maps a linear combination c of the random variables X onto a vector of covariances with those variables: c T Σ Σ = cov ⁡ ⁡ ( c T X , X ) {\displaystyle \mathbf {c} ^{\mathsf {T}}\Sigma =\operatorname {cov} (\mathbf {c} ^{\mathsf {T}}\mathbf {X} ,\mathbf {X} )} . Treated as a bilinear form , it yields the covariance between the two linear combinations: d T Σ Σ c = cov ⁡ ⁡ ( d T X , c T X ) {\displaystyle \mathbf {d} ^{\mathsf {T}}{\boldsymbol {\Sigma }}\mathbf {c} =\operatorname {cov} (\mathbf {d} ^{\mathsf {T}}\mathbf {X} ,\mathbf {c} ^{\mathsf {T}}\mathbf {X} )} . The variance of a linear combination is then c T Σ Σ c {\displaystyle \mathbf {c} ^{\mathsf {T}}{\boldsymbol {\Sigma }}\mathbf {c} } , its covariance with itself.

Similarly, the (pseudo-)inverse covariance matrix provides an inner product ⟨ ⟨ c − − μ μ | Σ Σ + | c − − μ μ ⟩ ⟩ {\displaystyle \langle c-\mu |\Sigma ^{+}|c-\mu \rangle } , which induces the Mahalanobis distance , a measure of the "unlikelihood" of c .

[ citation needed ] Admissibility [ edit ] From basic property 4. above, let b {\displaystyle \mathbf {b} } be a ( p × × 1 ) {\displaystyle (p\times 1)} real-valued vector, then var ⁡ ⁡ ( b T X ) = b T var ⁡ ⁡ ( X ) b , {\displaystyle \operatorname {var} (\mathbf {b} ^{\mathsf {T}}\mathbf {X} )=\mathbf {b} ^{\mathsf {T}}\operatorname {var} (\mathbf {X} )\mathbf {b} ,\,} which must always be nonnegative, since it is the variance of a real-valued random variable, so a covariance matrix is always a positive-semidefinite matrix .

The above argument can be expanded as follows: w T E ⁡ ⁡ [ ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T ] w = E ⁡ ⁡ [ w T ( X − − E ⁡ ⁡ [ X ] ) ( X − − E ⁡ ⁡ [ X ] ) T w ] = E ⁡ ⁡ [ ( w T ( X − − E ⁡ ⁡ [ X ] ) ) 2 ] ≥ ≥ 0 , {\displaystyle {\begin{aligned}&w^{\mathsf {T}}\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {X} -\operatorname {E} [\mathbf {X} ])^{\mathsf {T}}\right]w=\operatorname {E} \left[w^{\mathsf {T}}(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {X} -\operatorname {E} [\mathbf {X} ])^{\mathsf {T}}w\right]\\&=\operatorname {E} {\big [}{\big (}w^{\mathsf {T}}(\mathbf {X} -\operatorname {E} [\mathbf {X} ]){\big )}^{2}{\big ]}\geq 0,\end{aligned}}} where the last inequality follows from the observation that w T ( X − − E ⁡ ⁡ [ X ] ) {\displaystyle w^{\mathsf {T}}(\mathbf {X} -\operatorname {E} [\mathbf {X} ])} is a scalar.

Conversely, every symmetric positive semi-definite matrix is a covariance matrix. To see this, suppose M {\displaystyle M} is a p × × p {\displaystyle p\times p} symmetric positive-semidefinite matrix. From the finite-dimensional case of the spectral theorem , it follows that M {\displaystyle M} has a nonnegative symmetric square root , which can be denoted by M 1/2 . Let X {\displaystyle \mathbf {X} } be any p × × 1 {\displaystyle p\times 1} column vector-valued random variable whose covariance matrix is the p × × p {\displaystyle p\times p} identity matrix. Then var ⁡ ⁡ ( M 1 / 2 X ) = M 1 / 2 var ⁡ ⁡ ( X ) M 1 / 2 = M .

{\displaystyle \operatorname {var} (\mathbf {M} ^{1/2}\mathbf {X} )=\mathbf {M} ^{1/2}\,\operatorname {var} (\mathbf {X} )\,\mathbf {M} ^{1/2}=\mathbf {M} .} Complex random vectors [ edit ] Further information: Complex random vector § Covariance matrix and pseudo-covariance matrix The variance of a complex scalar-valued random variable with expected value μ μ {\displaystyle \mu } is conventionally defined using complex conjugation : var ⁡ ⁡ ( Z ) = E ⁡ ⁡ [ ( Z − − μ μ Z ) ( Z − − μ μ Z ) ¯ ¯ ] , {\displaystyle \operatorname {var} (Z)=\operatorname {E} \left[(Z-\mu _{Z}){\overline {(Z-\mu _{Z})}}\right],} where the complex conjugate of a complex number z {\displaystyle z} is denoted z ¯ ¯ {\displaystyle {\overline {z}}} ; thus the variance of a complex random variable is a real number.

If Z = ( Z 1 , … … , Z n ) T {\displaystyle \mathbf {Z} =(Z_{1},\ldots ,Z_{n})^{\mathsf {T}}} is a column vector of complex-valued random variables, then the conjugate transpose Z H {\displaystyle \mathbf {Z} ^{\mathsf {H}}} is formed by both transposing and conjugating. In the following expression, the product of a vector with its conjugate transpose results in a square matrix called the covariance matrix , as its expectation: [ 8 ] : 293 K Z Z = cov ⁡ ⁡ [ Z , Z ] = E ⁡ ⁡ [ ( Z − − μ μ Z ) ( Z − − μ μ Z ) H ] , {\displaystyle \operatorname {K} _{\mathbf {Z} \mathbf {Z} }=\operatorname {cov} [\mathbf {Z} ,\mathbf {Z} ]=\operatorname {E} \left[(\mathbf {Z} -{\boldsymbol {\mu }}_{\mathbf {Z} })(\mathbf {Z} -{\boldsymbol {\mu }}_{\mathbf {Z} })^{\mathsf {H}}\right],} The matrix so obtained will be Hermitian positive-semidefinite , [ 9 ] with real numbers in the main diagonal and complex numbers off-diagonal.

Properties The covariance matrix is a Hermitian matrix , i.e.

K Z Z H = K Z Z {\displaystyle \operatorname {K} _{\mathbf {Z} \mathbf {Z} }^{\mathsf {H}}=\operatorname {K} _{\mathbf {Z} \mathbf {Z} }} .

[ 1 ] : 179 The diagonal elements of the covariance matrix are real.

[ 1 ] : 179 Pseudo-covariance matrix [ edit ] For complex random vectors, another kind of second central moment, the pseudo-covariance matrix (also called relation matrix ) is defined as follows: J Z Z = cov ⁡ ⁡ [ Z , Z ¯ ¯ ] = E ⁡ ⁡ [ ( Z − − μ μ Z ) ( Z − − μ μ Z ) T ] {\displaystyle \operatorname {J} _{\mathbf {Z} \mathbf {Z} }=\operatorname {cov} [\mathbf {Z} ,{\overline {\mathbf {Z} }}]=\operatorname {E} \left[(\mathbf {Z} -{\boldsymbol {\mu }}_{\mathbf {Z} })(\mathbf {Z} -{\boldsymbol {\mu }}_{\mathbf {Z} })^{\mathsf {T}}\right]} In contrast to the covariance matrix defined above, Hermitian transposition gets replaced by transposition in the definition.
Its diagonal elements may be complex valued; it is a complex symmetric matrix .

Estimation [ edit ] Main article: Estimation of covariance matrices If M X {\displaystyle \mathbf {M} _{\mathbf {X} }} and M Y {\displaystyle \mathbf {M} _{\mathbf {Y} }} are centered data matrices of dimension p × × n {\displaystyle p\times n} and q × × n {\displaystyle q\times n} respectively, i.e. with n columns of observations of p and q rows of variables, from which the row means have been subtracted, then, if the row means were estimated from the data, sample covariance matrices Q X X {\displaystyle \mathbf {Q} _{\mathbf {XX} }} and Q X Y {\displaystyle \mathbf {Q} _{\mathbf {XY} }} can be defined to be Q X X = 1 n − − 1 M X M X T , Q X Y = 1 n − − 1 M X M Y T {\displaystyle \mathbf {Q} _{\mathbf {XX} }={\frac {1}{n-1}}\mathbf {M} _{\mathbf {X} }\mathbf {M} _{\mathbf {X} }^{\mathsf {T}},\qquad \mathbf {Q} _{\mathbf {XY} }={\frac {1}{n-1}}\mathbf {M} _{\mathbf {X} }\mathbf {M} _{\mathbf {Y} }^{\mathsf {T}}} or, if the row means were known a priori, Q X X = 1 n M X M X T , Q X Y = 1 n M X M Y T .

{\displaystyle \mathbf {Q} _{\mathbf {XX} }={\frac {1}{n}}\mathbf {M} _{\mathbf {X} }\mathbf {M} _{\mathbf {X} }^{\mathsf {T}},\qquad \mathbf {Q} _{\mathbf {XY} }={\frac {1}{n}}\mathbf {M} _{\mathbf {X} }\mathbf {M} _{\mathbf {Y} }^{\mathsf {T}}.} These empirical sample covariance matrices are the most straightforward and most often used estimators for the covariance matrices, but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.

Applications [ edit ] The covariance matrix is a useful tool in many different areas. From it a transformation matrix can be derived, called a whitening transformation , that allows one to completely decorrelate the data [ 10 ] or, from a different point of view, to find an optimal basis for representing the data in a compact way [ citation needed ] (see Rayleigh quotient for a formal proof and additional properties of covariance matrices).
This is called principal component analysis (PCA) and the Karhunen–Loève transform (KL-transform).

The covariance matrix plays a key role in financial economics , especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model . The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis ) or are predicted to (in a positive analysis ) choose to hold in a context of diversification .

Use in optimization [ edit ] The evolution strategy , a particular family of Randomized Search Heuristics, fundamentally relies on a covariance matrix in its mechanism. The characteristic mutation operator draws the update step from a multivariate normal distribution using an evolving covariance matrix. There is a formal proof that the evolution strategy 's covariance matrix adapts to the inverse of the Hessian matrix of the search landscape, up to a scalar factor and small random fluctuations (proven for a single-parent strategy and a static model, as the population size increases, relying on the quadratic approximation).

[ 11 ] Intuitively, this result is supported by the rationale that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape, and so they maximize the progress rate.

Covariance mapping [ edit ] In covariance mapping the values of the cov ⁡ ⁡ ( X , Y ) {\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )} or pcov ⁡ ⁡ ( X , Y ∣ ∣ I ) {\displaystyle \operatorname {pcov} (\mathbf {X} ,\mathbf {Y} \mid \mathbf {I} )} matrix are plotted as a 2-dimensional map. When vectors X {\displaystyle \mathbf {X} } and Y {\displaystyle \mathbf {Y} } are discrete random functions , the map shows statistical relations between different regions of the random functions. Statistically independent regions of the functions show up on the map as zero-level flatland, while positive or negative correlations show up, respectively, as hills or valleys.

In practice the column vectors X , Y {\displaystyle \mathbf {X} ,\mathbf {Y} } , and I {\displaystyle \mathbf {I} } are acquired experimentally as rows of n {\displaystyle n} samples, e.g.

[ X 1 , X 2 , … … , X n ] = [ X 1 ( t 1 ) X 2 ( t 1 ) ⋯ ⋯ X n ( t 1 ) X 1 ( t 2 ) X 2 ( t 2 ) ⋯ ⋯ X n ( t 2 ) ⋮ ⋮ ⋮ ⋮ ⋱ ⋱ ⋮ ⋮ X 1 ( t m ) X 2 ( t m ) ⋯ ⋯ X n ( t m ) ] , {\displaystyle \left[\mathbf {X} _{1},\mathbf {X} _{2},\dots ,\mathbf {X} _{n}\right]={\begin{bmatrix}X_{1}(t_{1})&X_{2}(t_{1})&\cdots &X_{n}(t_{1})\\\\X_{1}(t_{2})&X_{2}(t_{2})&\cdots &X_{n}(t_{2})\\\\\vdots &\vdots &\ddots &\vdots \\\\X_{1}(t_{m})&X_{2}(t_{m})&\cdots &X_{n}(t_{m})\end{bmatrix}},} where X j ( t i ) {\displaystyle X_{j}(t_{i})} is the i -th discrete value in sample j of the random function X ( t ) {\displaystyle X(t)} . The expected values needed in the covariance formula are estimated using the sample mean , e.g.

⟨ ⟨ X ⟩ ⟩ = 1 n ∑ ∑ j = 1 n X j {\displaystyle \langle \mathbf {X} \rangle ={\frac {1}{n}}\sum _{j=1}^{n}\mathbf {X} _{j}} and the covariance matrix is estimated by the sample covariance matrix cov ⁡ ⁡ ( X , Y ) ≈ ≈ ⟨ ⟨ X Y T ⟩ ⟩ − − ⟨ ⟨ X ⟩ ⟩ ⟨ ⟨ Y T ⟩ ⟩ , {\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )\approx \langle \mathbf {XY^{\mathsf {T}}} \rangle -\langle \mathbf {X} \rangle \langle \mathbf {Y} ^{\mathsf {T}}\rangle ,} where the angular brackets denote sample averaging as before except that the Bessel's correction should be made to avoid bias . Using this estimation the partial covariance matrix can be calculated as pcov ⁡ ⁡ ( X , Y ∣ ∣ I ) = cov ⁡ ⁡ ( X , Y ) − − cov ⁡ ⁡ ( X , I ) ( cov ⁡ ⁡ ( I , I ) ∖ ∖ cov ⁡ ⁡ ( I , Y ) ) , {\displaystyle \operatorname {pcov} (\mathbf {X} ,\mathbf {Y} \mid \mathbf {I} )=\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )-\operatorname {cov} (\mathbf {X} ,\mathbf {I} )\left(\operatorname {cov} (\mathbf {I} ,\mathbf {I} )\backslash \operatorname {cov} (\mathbf {I} ,\mathbf {Y} )\right),} where the backslash denotes the left matrix division operator, which bypasses the requirement to invert a matrix and is available in some computational packages such as Matlab .

[ 12 ] Figure 1: Construction of a partial covariance map of N 2 molecules undergoing Coulomb explosion induced by a free-electron laser.

[ 13 ] Panels a and b map the two terms of the covariance matrix, which is shown in panel c . Panel d maps common-mode correlations via intensity fluctuations of the laser. Panel e maps the partial covariance matrix that is corrected for the intensity fluctuations. Panel f shows that 10% overcorrection improves the map and makes ion-ion correlations clearly visible. Owing to momentum conservation these correlations appear as lines approximately perpendicular to the autocorrelation line (and to the periodic modulations which are caused by detector ringing).

Fig. 1 illustrates how a partial covariance map is constructed on an example of an experiment performed at the FLASH free-electron laser in Hamburg.

[ 13 ] The random function X ( t ) {\displaystyle X(t)} is the time-of-flight spectrum of ions from a Coulomb explosion of nitrogen molecules multiply ionised by a laser pulse. Since only a few hundreds of molecules are ionised at each laser pulse, the single-shot spectra are highly fluctuating. However, collecting typically m = 10 4 {\displaystyle m=10^{4}} such spectra, X j ( t ) {\displaystyle \mathbf {X} _{j}(t)} , and averaging them over j {\displaystyle j} produces a smooth spectrum ⟨ ⟨ X ( t ) ⟩ ⟩ {\displaystyle \langle \mathbf {X} (t)\rangle } , which is shown in red at the bottom of Fig. 1. The average spectrum ⟨ ⟨ X ⟩ ⟩ {\displaystyle \langle \mathbf {X} \rangle } reveals several nitrogen ions in a form of peaks broadened by their kinetic energy, but to find the correlations between the ionisation stages and the ion momenta requires calculating a covariance map.

In the example of Fig. 1 spectra X j ( t ) {\displaystyle \mathbf {X} _{j}(t)} and Y j ( t ) {\displaystyle \mathbf {Y} _{j}(t)} are the same, except that the range of the time-of-flight t {\displaystyle t} differs. Panel a shows ⟨ ⟨ X Y T ⟩ ⟩ {\displaystyle \langle \mathbf {XY^{\mathsf {T}}} \rangle } , panel b shows ⟨ ⟨ X ⟩ ⟩ ⟨ ⟨ Y T ⟩ ⟩ {\displaystyle \langle \mathbf {X} \rangle \langle \mathbf {Y} ^{\mathsf {T}}\rangle } and panel c shows their difference, which is cov ⁡ ⁡ ( X , Y ) {\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )} (note a change in the colour scale). Unfortunately, this map is overwhelmed by uninteresting, common-mode correlations induced by laser intensity fluctuating from shot to shot. To suppress such correlations the laser intensity I j {\displaystyle I_{j}} is recorded at every shot, put into I {\displaystyle \mathbf {I} } and pcov ⁡ ⁡ ( X , Y ∣ ∣ I ) {\displaystyle \operatorname {pcov} (\mathbf {X} ,\mathbf {Y} \mid \mathbf {I} )} is calculated as panels d and e show. The suppression of the uninteresting correlations is, however, imperfect because there are other sources of common-mode fluctuations than the laser intensity and in principle all these sources should be monitored in vector I {\displaystyle \mathbf {I} } . Yet in practice it is often sufficient to overcompensate the partial covariance correction as panel f shows, where interesting correlations of ion momenta are now clearly visible as straight lines centred on ionisation stages of atomic nitrogen.

Two-dimensional infrared spectroscopy [ edit ] Two-dimensional infrared spectroscopy employs correlation analysis to obtain 2D spectra of the condensed phase . There are two versions of this analysis: synchronous and asynchronous . Mathematically, the former is expressed in terms of the sample covariance matrix and the technique is equivalent to covariance mapping.

[ 14 ] See also [ edit ] Covariance function Eigenvalue decomposition Gramian matrix Lewandowski-Kurowicka-Joe distribution Multivariate statistics Principal components Quadratic form (statistics) References [ edit ] ^ a b c Park, Kun Il (2018).

Fundamentals of Probability and Stochastic Processes with Applications to Communications . Springer.

ISBN 978-3-319-68074-3 .

^ William Feller (1971).

An introduction to probability theory and its applications . Wiley.

ISBN 978-0-471-25709-7 . Retrieved 10 August 2012 .

^ Wasserman, Larry (2004).

All of Statistics: A Concise Course in Statistical Inference . Springer.

ISBN 0-387-40272-1 .

^ Taboga, Marco (2010).

"Lectures on probability theory and mathematical statistics" .

^ Eaton, Morris L. (1983).

Multivariate Statistics: a Vector Space Approach . John Wiley and Sons. pp.

116– 117.

ISBN 0-471-02776-6 .

^ a b W J Krzanowski "Principles of Multivariate Analysis" (Oxford University Press, New York, 1988), Chap. 14.4; K V Mardia, J T Kent and J M Bibby "Multivariate Analysis (Academic Press, London, 1997), Chap. 6.5.3; T W Anderson "An Introduction to Multivariate Statistical Analysis" (Wiley, New York, 2003), 3rd ed., Chaps. 2.5.1 and 4.3.1.

^ Das, Abhranil; Wilson S Geisler (2020). "Methods to integrate multinormals and compute classification measures".

arXiv : 2012.14331 [ stat.ML ].

^ Lapidoth, Amos (2009).

A Foundation in Digital Communication . Cambridge University Press.

ISBN 978-0-521-19395-5 .

^ Brookes, Mike.

"The Matrix Reference Manual" .

^ Kessy, Agnan; Strimmer, Korbinian; Lewin, Alex (2018).

"Optimal Whitening and Decorrelation" .

The American Statistician .

72 (4). Taylor & Francis: 309– 314.

arXiv : 1512.00809 .

doi : 10.1080/00031305.2016.1277159 .

^ Shir, O.M.; A. Yehudayoff (2020).

"On the covariance-Hessian relation in evolution strategies" .

Theoretical Computer Science .

801 . Elsevier: 157– 174.

arXiv : 1806.03674 .

doi : 10.1016/j.tcs.2019.09.002 .

^ L J Frasinski "Covariance mapping techniques" J. Phys. B: At. Mol. Opt. Phys.

49 152004 (2016), doi : 10.1088/0953-4075/49/15/152004 ^ a b O Kornilov, M Eckstein, M Rosenblatt, C P Schulz, K Motomura, A Rouzée, J Klei, L Foucar, M Siano, A Lübcke, F. Schapper, P Johnsson, D M P Holland, T Schlatholter, T Marchenko, S Düsterer, K Ueda, M J J Vrakking and L J Frasinski "Coulomb explosion of diatomic molecules in intense XUV fields mapped by partial covariance" J. Phys. B: At. Mol. Opt. Phys.

46 164028 (2013), doi : 10.1088/0953-4075/46/16/164028 ^ Noda, I. (1993). "Generalized two-dimensional correlation method applicable to infrared, Raman, and other types of spectroscopy".

Appl. Spectrosc .

47 (9): 1329– 36.

Bibcode : 1993ApSpe..47.1329N .

doi : 10.1366/0003702934067694 .

Further reading [ edit ] "Covariance matrix" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] " Covariance Matrix Explained With Pictures ", an easy way to visualize covariance matrices!

Weisstein, Eric W.

"Covariance Matrix" .

MathWorld .

van Kampen, N. G. (1981).

Stochastic processes in physics and chemistry . New York: North-Holland.

ISBN 0-444-86200-5 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Matrix classes Explicitly constrained entries Alternant Anti-diagonal Anti-Hermitian Anti-symmetric Arrowhead Band Bidiagonal Bisymmetric Block-diagonal Block Block tridiagonal Boolean Cauchy Centrosymmetric Conference Complex Hadamard Copositive Diagonally dominant Diagonal Discrete Fourier Transform Elementary Equivalent Frobenius Generalized permutation Hadamard Hankel Hermitian Hessenberg Hollow Integer Logical Matrix unit Metzler Moore Nonnegative Pentadiagonal Permutation Persymmetric Polynomial Quaternionic Signature Skew-Hermitian Skew-symmetric Skyline Sparse Sylvester Symmetric Toeplitz Triangular Tridiagonal Vandermonde Walsh Z Constant Exchange Hilbert Identity Lehmer Of ones Pascal Pauli Redheffer Shift Zero Conditions on eigenvalues or eigenvectors Companion Convergent Defective Definite Diagonalizable Hurwitz-stable Positive-definite Stieltjes Satisfying conditions on products or inverses Congruent Idempotent or Projection Invertible Involutory Nilpotent Normal Orthogonal Unimodular Unipotent Unitary Totally unimodular Weighing With specific applications Adjugate Alternating sign Augmented Bézout Carleman Cartan Circulant Cofactor Commutation Confusion Coxeter Distance Duplication and elimination Euclidean distance Fundamental (linear differential equation) Generator Gram Hessian Householder Jacobian Moment Payoff Pick Random Rotation Routh-Hurwitz Seifert Shear Similarity Symplectic Totally positive Transformation Used in statistics Centering Correlation Covariance Design Doubly stochastic Fisher information Hat Precision Stochastic Transition Used in graph theory Adjacency Biadjacency Degree Edmonds Incidence Laplacian Seidel adjacency Tutte Used in science and engineering Cabibbo–Kobayashi–Maskawa Density Fundamental (computer vision) Fuzzy associative Gamma Gell-Mann Hamiltonian Irregular Overlap S State transition Substitution Z (chemistry) Related terms Jordan normal form Linear independence Matrix exponential Matrix representation of conic sections Perfect matrix Pseudoinverse Row echelon form Wronskian Mathematics portal List of matrices Category:Matrices (mathematics) NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐wq5gz
Cached time: 20250811235530
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.735 seconds
Real time usage: 1.150 seconds
Preprocessor visited node count: 4939/1000000
Revision size: 37831/2097152 bytes
Post‐expand include size: 219118/2097152 bytes
Template argument size: 5741/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 11/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 91071/5000000 bytes
Lua time usage: 0.333/10.000 seconds
Lua memory usage: 7779391/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  593.983      1 -total
 24.61%  146.183      1 Template:Reflist
 16.47%   97.857      6 Template:Cite_book
 15.47%   91.883      1 Template:Statistics
 15.11%   89.737      1 Template:Navbox_with_collapsible_groups
 14.89%   88.422      1 Template:Short_description
 11.80%   70.085      1 Template:Correlation_and_covariance
 11.53%   68.463      1 Template:Sidebar_with_collapsible_lists
 10.19%   60.517      2 Template:Pagetype
  9.45%   56.125     12 Template:Navbox Saved in parser cache with key enwiki:pcache:191752:|#|:idhash:canonical and timestamp 20250811235530 and revision id 1302390250. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Covariance_matrix&oldid=1302390250 " Categories : Covariance and correlation Matrices (mathematics) Summary statistics Hidden categories: Use American English from April 2019 All Wikipedia articles written in American English Articles with short description Short description matches Wikidata All accuracy disputes Articles with disputed statements from September 2024 All articles with unsourced statements Articles with unsourced statements from February 2012 This page was last edited on 25 July 2025, at 03:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Covariance matrix 23 languages Add topic

