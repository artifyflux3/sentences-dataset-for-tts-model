Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 History 3 Motivating properties Toggle Motivating properties subsection 3.1 Additional properties 4 Contrast with median 5 Generalizations Toggle Generalizations subsection 5.1 Weighted average 5.2 Functions 5.3 Continuous probability distributions 5.4 Angles 6 Symbols and encoding 7 See also 8 Notes 9 References 10 Further reading Toggle the table of contents Arithmetic mean 62 languages العربية Asturianu Azərbaycanca Беларуская Български Bosanski Català Чӑвашла Čeština Deutsch Eesti Español Esperanto Euskara فارسی Français Gaeilge Galego 한국어 Հայերեն हिन्दी Hrvatski Bahasa Indonesia עברית Қазақша Кыргызча Latina Latviešu Lietuvių Magyar Македонски Bahasa Melayu Nederlands 日本語 Norsk bokmål Norsk nynorsk Oʻzbekcha / ўзбекча ភាសាខ្មែរ Piemontèis Polski Português Română Русский Shqip සිංහල سنڌي Slovenčina Slovenščina کوردی Српски / srpski Srpskohrvatski / српскохрватски Sunda Suomi Svenska ไทย Türkçe Українська اردو Tiếng Việt 吴语 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Type of average of a collection of numbers "X̄" redirects here. For the character, see Macron (diacritic) .

For broader coverage of this topic, see Mean .

In mathematics and statistics , the arithmetic mean ( / ˌ æ r ɪ θ ˈ m ɛ t ɪ k / ⓘ arr-ith- MET -ik ), arithmetic average , or just the mean or average is the sum of a collection of numbers divided by the count of numbers in the collection.

[ 1 ] The collection is often a set of results from an experiment , an observational study , or a survey . The term "arithmetic mean" is preferred in some contexts in mathematics and statistics because it helps to distinguish it from other types of means, such as geometric and harmonic .

Arithmetic means are also frequently used in economics , anthropology , history , and almost every other academic field to some extent. For example, per capita income is the arithmetic average of the income of a nation 's population .

While the arithmetic mean is often used to report central tendencies , it is not a robust statistic : it is greatly influenced by outliers ( values much larger or smaller than most others). For skewed distributions , such as the distribution of income for which a few people's incomes are substantially higher than most people's, the arithmetic mean may not coincide with one's notion of "middle". In that case, robust statistics, such as the median , may provide a better description of central tendency .

Definition [ edit ] Further information: summation for an explanation of the summation operator The arithmetic mean of a set of observed data is equal to the sum of the numerical values of each observation, divided by the total number of observations. Symbolically, for a data set consisting of the values x 1 , … … , x n {\displaystyle x_{1},\dots ,x_{n}} , the arithmetic mean is defined by the formula: [ 2 ] x ¯ ¯ = 1 n ( ∑ ∑ i = 1 n x i ) = x 1 + x 2 + ⋯ ⋯ + x n n {\displaystyle {\bar {x}}={\frac {1}{n}}\left(\sum _{i=1}^{n}{x_{i}}\right)={\frac {x_{1}+x_{2}+\dots +x_{n}}{n}}} In simpler terms, the formula for the arithmetic mean is: Total of all numbers within the data Amount of total numbers within the data {\displaystyle {\frac {\text{Total of all numbers within the data}}{\text{Amount of total numbers within the data}}}} For example, if the monthly salaries of 5 {\displaystyle 5} employees are { 2500 , 2700 , 2300 , 2650 , 2450 } {\displaystyle \{2500,2700,2300,2650,2450\}} , then the arithmetic mean is: 2500 + 2700 + 2300 + 2650 + 2450 5 = 2520 {\displaystyle {\frac {2500+2700+2300+2650+2450}{5}}=2520} Example Person Salary A 2500 B 2700 C 2300 D 2650 E 2450 Average 2520 If the data set is a statistical population (i.e. consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean and denoted by the Greek letter μ μ {\displaystyle \mu } . If the data set is a statistical sample (a subset of the population), it is called the sample mean (which for a data set X {\displaystyle X} is denoted as X ¯ ¯ {\displaystyle {\overline {X}}} ).

The arithmetic mean can be similarly defined for vectors in multiple dimensions, not only scalar values; this is often referred to as a centroid . More generally, because the arithmetic mean is a convex combination (meaning its coefficients sum to 1 {\displaystyle 1} ), it can be defined on a convex space , not only a vector space .

History [ edit ] Statistician Churchill Eisenhart , senior researcher fellow at the U. S. National Bureau of Standards , traced the history of the arithmetic mean in detail. In the modern age, it started to be used as a way of combining various observations that should be identical, but were not such as estimates of the direction of magnetic north . In 1635, mathematician Henry Gellibrand described as "meane" the midpoint of a lowest and highest number, not quite the arithmetic mean. In 1668, a person known as "D. B." was quoted in the Transactions of the Royal Society describing "taking the mean" of five values: [ 3 ] In this Table, he [Capt. Sturmy] notes the greatest difference to be 14 minutes; and so taking the mean for the true Variation, he concludes it then and there to be just 1. deg. 27. min.

— D.B., p. 726 Motivating properties [ edit ] The arithmetic mean has several properties that make it interesting, especially as a measure of central tendency. These include: If numbers x 1 , … … , x n {\displaystyle x_{1},\dotsc ,x_{n}} have a mean x ¯ ¯ {\displaystyle {\bar {x}}} , then ( x 1 − − x ¯ ¯ ) + ⋯ ⋯ + ( x n − − x ¯ ¯ ) = 0 {\displaystyle (x_{1}-{\bar {x}})+\dotsb +(x_{n}-{\bar {x}})=0} . Since x i − − x ¯ ¯ {\displaystyle x_{i}-{\bar {x}}} is the distance from a given number to the mean, one way to interpret this property is by saying that the numbers to the left of the mean are balanced by the numbers to the right. The mean is the only number for which the residuals (deviations from the estimate) sum to zero.  This can also be interpreted as saying that the mean is translationally invariant in the sense that for any real number a {\displaystyle a} , x + a ¯ ¯ = x ¯ ¯ + a {\displaystyle {\overline {x+a}}={\bar {x}}+a} .

If it is required to use a single number as a "typical" value for a set of known numbers x 1 , … … , x n {\displaystyle x_{1},\dotsc ,x_{n}} , then the arithmetic mean of the numbers does this best since it minimizes the sum of squared deviations from the typical value: the sum of ( x i − − x ¯ ¯ ) 2 {\displaystyle (x_{i}-{\bar {x}})^{2}} . The sample mean is also the best single predictor because it has the lowest root mean squared error .

[ 4 ] If the arithmetic mean of a population of numbers is desired, then the estimate of it that is unbiased is the arithmetic mean of a sample drawn from the population.

The arithmetic mean is independent of scale of the units of measurement, in the sense that avg ( c a 1 , ⋯ ⋯ , c a n ) = c ⋅ ⋅ avg ( a 1 , ⋯ ⋯ , a n ) .

{\displaystyle {\text{avg}}(ca_{1},\cdots ,ca_{n})=c\cdot {\text{avg}}(a_{1},\cdots ,a_{n}).} So, for example, calculating a mean of liters and then converting to gallons is the same as converting to gallons first and then calculating the mean.  This is also called first order homogeneity .

Additional properties [ edit ] The arithmetic mean of a sample is always between the largest and smallest values in that sample.

The arithmetic mean of any amount of equal-sized number groups together is the arithmetic mean of the arithmetic means of each group.

Contrast with median [ edit ] Main article: Median This section needs expansion with: More applications needed (probably 1 or 2 more). You can help by adding to it .

( June 2025 ) The arithmetic mean may be contrasted with the median . The median is defined such that no more than half the values are larger, and no more than half are smaller than it. If elements in the data increase arithmetically when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample { 1 , 2 , 3 , 4 } {\displaystyle \{1,2,3,4\}} . The mean is 2.5 {\displaystyle 2.5} , as is the median. However, when we consider a sample that cannot be arranged to increase arithmetically, such as { 1 , 2 , 4 , 8 , 16 } {\displaystyle \{1,2,4,8,16\}} , the median and arithmetic average can differ significantly. In this case, the arithmetic average is 6.2 {\displaystyle 6.2} , while the median is 4 {\displaystyle 4} . The average value can vary considerably from most values in the sample and can be larger or smaller than most.

There are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.

[ 5 ] Another example is climate change : the frequency distribution of mean daily temperatures is usually approximately normal. However, the frequency distribution of annual (or monthly) totals of rainfall over a period of years may be 'skewed' with some years (months) having very large totals whereas most years (months) have low amounts. For such distributions, the median is a more representative average statistic.

[ 6 ] Generalizations [ edit ] Weighted average [ edit ] Main article: Weighted average A weighted average, or weighted mean, is an average in which some data points count more heavily than others in that they are given more weight in the calculation.

[ 7 ] For example, the arithmetic mean of 3 {\displaystyle 3} and 5 {\displaystyle 5} is 3 + 5 2 = 4 {\displaystyle {\frac {3+5}{2}}=4} , or equivalently 3 ⋅ ⋅ 1 2 + 5 ⋅ ⋅ 1 2 = 4 {\displaystyle 3\cdot {\frac {1}{2}}+5\cdot {\frac {1}{2}}=4} . In contrast, a weighted mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as 3 ⋅ ⋅ 2 3 + 5 ⋅ ⋅ 1 3 = 11 3 {\displaystyle 3\cdot {\frac {2}{3}}+5\cdot {\frac {1}{3}}={\frac {11}{3}}} . Here the weights, which necessarily sum to one, are 2 3 {\displaystyle {\frac {2}{3}}} and 1 3 {\displaystyle {\frac {1}{3}}} , the former being twice the latter. The arithmetic mean (sometimes called the "unweighted average" or "equally weighted average") can be interpreted as a special case of a weighted average in which all weights are equal to the same number ( 1 2 {\displaystyle {\frac {1}{2}}} in the above example and 1 n {\displaystyle {\frac {1}{n}}} in a situation with n {\displaystyle n} numbers being averaged).

Functions [ edit ] This section is an excerpt from Mean of a function .

[ edit ] In calculus , and especially multivariable calculus , the mean of a function is loosely defined as the ” average " value of the function over its domain .

Continuous probability distributions [ edit ] Comparison of two log-normal distributions with equal median, but different skewness , resulting in various means and modes If a numerical property, and any sample of data from it, can take on any value from a continuous range instead of, for example, just integers, then the probability of a number falling into some range of possible values can be described by integrating a continuous probability distribution across this range, even when the naive probability for a sample number taking one certain value from infinitely many is zero. In this context, the analog of a weighted average, in which there are infinitely many possibilities for the precise value of the variable in each range, is called the mean of the probability distribution . The most widely encountered probability distribution is called the normal distribution ; it has the property that all measures of its central tendency, including not just the mean but also the median mentioned above and the mode (the three Ms [ 8 ] ), are equal. This equality does not hold for other probability distributions, as illustrated for the log-normal distribution here.

Angles [ edit ] Main article: Mean of circular quantities Particular care is needed when using cyclic data, such as phases or angles . Taking the arithmetic mean of 1 ° and 359° yields a result of 180°. This is incorrect for two reasons: Angle measurements are only defined up to an additive constant of 360° ( 2 π π {\displaystyle 2\pi } or τ τ {\displaystyle \tau } , if measuring in radians ). Thus, these could easily be called 1° and -1°, or 361° and 719°, since each one of them produces a different average.

In this situation, 0° (or 360°) is geometrically a better average value: there is lower dispersion about it (the points are both 1° from it and 179° from 180°, the putative average).

In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (that is, define the mean as the central point: the point about which one has the lowest dispersion) and redefine the difference as a modular distance (i.e. the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).

Proof without words of the AM–GM inequality : PR is the diameter of a circle centered on O; its radius AO is the arithmetic mean of a and b . Triangle PGR is a right triangle from Thales's theorem , enabling use of the geometric mean theorem to show that its altitude GQ is the geometric mean . For any ratio a : b , AO ≥ GQ.

Symbols and encoding [ edit ] The arithmetic mean is often denoted by a bar ( vinculum or macron ), as in x ¯ ¯ {\displaystyle {\bar {x}}} .

[ 4 ] See also [ edit ] Geometric proof without words that max ( a , b ) > root mean square ( RMS ) or quadratic mean ( QM ) > arithmetic mean ( AM ) > geometric mean ( GM ) > harmonic mean ( HM ) > min ( a , b ) of two distinct positive numbers a and b [ note 1 ] Fréchet mean Generalized mean Inequality of arithmetic and geometric means Sample mean and covariance Standard deviation Standard error of the mean Summary statistics Notes [ edit ] ^ If NM = a and PM = b . AM = AM of a and b , and radius r = AQ = AG.

Using Pythagoras' theorem , QM² = AQ² + AM² ∴ QM = √ AQ² + AM² = QM .

Using Pythagoras' theorem, AM² = AG² + GM² ∴ GM = √ AM² − AG² = GM .

Using similar triangles , ⁠ HM / GM ⁠ = ⁠ GM / AM ⁠ ∴ HM = ⁠ GM² / AM ⁠ = HM .

References [ edit ] ^ Jacobs, Harold R. (1994).

Mathematics: A Human Endeavor (Third ed.).

W. H. Freeman . p. 547.

ISBN 0-7167-2426-X .

^ Weisstein, Eric W.

"Arithmetic Mean" .

mathworld.wolfram.com . Retrieved 21 August 2020 .

^ Eisenhart, Churchill (24 August 1971).

"The Development of the Concept of the Best Mean of a Set of Measurements from Antiquity to the Present Day" (PDF) . Presidential Address, 131st Annual Meeting of the American Statistical Association, Colorado State University. pp.

68– 69.

^ a b Medhi, Jyotiprasad (1992).

Statistical Methods: An Introductory Text . New Age International. pp.

53– 58.

ISBN 9788122404197 .

^ Krugman, Paul (4 June 2014) [Fall 1992].

"The Rich, the Right, and the Facts: Deconstructing the Income Distribution Debate" .

The American Prospect .

^ Barry, Roger Graham; Chorley, Richard John (2005).

Atmosphere, weather and climate (8th ed.). London: Routledge. p. 407.

ISBN 978-0-415-27170-7 .

^ "Mean | mathematics" .

Encyclopedia Britannica . Retrieved 21 August 2020 .

^ Thinkmap Visual Thesaurus (30 June 2010).

"The Three M's of Statistics: Mode, Median, Mean June 30, 2010" .

www.visualthesaurus.com . Retrieved 3 December 2018 .

Further reading [ edit ] Huff, Darrell (1993).

How to Lie with Statistics . W. W. Norton.

ISBN 978-0-393-31072-6 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Portal : Mathematics Authority control databases : National Germany Czech Republic NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐qmljg
Cached time: 20250812013750
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.642 seconds
Real time usage: 0.891 seconds
Preprocessor visited node count: 3474/1000000
Revision size: 15513/2097152 bytes
Post‐expand include size: 178773/2097152 bytes
Template argument size: 3743/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 14/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 60639/5000000 bytes
Lua time usage: 0.380/10.000 seconds
Lua memory usage: 7844172/52428800 bytes
Number of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  682.808      1 -total
 23.00%  157.067      2 Template:Reflist
 21.83%  149.030      1 Template:Statistics
 21.35%  145.786      1 Template:Navbox_with_collapsible_groups
 14.10%   96.273      4 Template:Cite_book
 10.28%   70.224      1 Template:Short_description
  9.37%   63.962     11 Template:Navbox
  7.20%   49.135      1 Template:Excerpt
  7.05%   48.114      1 Template:IPAc-en
  6.33%   43.189      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:612:|#|:idhash:canonical and timestamp 20250812013750 and revision id 1297733782. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Arithmetic_mean&oldid=1297733782 " Category : Means Hidden categories: Pages using the Phonos extension Articles with short description Short description is different from Wikidata Use dmy dates from December 2020 Pages including recorded pronunciations Pages using gadget Calculator Articles to be expanded from June 2025 All articles to be expanded Articles with excerpts This page was last edited on 28 June 2025, at 04:49 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Arithmetic mean 62 languages Add topic

