Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Properties 2 Generalization to modules 3 In functional analysis 4 Representation as matrix multiplication Toggle Representation as matrix multiplication subsection 4.1 Subspace properties 4.2 The row space of a matrix 4.3 Left null space 4.4 Nonhomogeneous systems of linear equations 5 Illustration 6 Examples 7 Computation by Gaussian elimination 8 Numerical computation Toggle Numerical computation subsection 8.1 Exact coefficients 8.2 Floating point computation 9 See also 10 Notes and references 11 Bibliography 12 External links Toggle the table of contents Kernel (linear algebra) 28 languages العربية Čeština Dansk Ελληνικά Español Esperanto Euskara فارسی Galego 한국어 Bahasa Indonesia עברית Latviešu 日本語 Polski Português Română Русский Slovenčina Slovenščina Suomi Svenska Türkçe Українська اردو Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Vectors mapped to 0 by a linear map For other uses, see Kernel (disambiguation) .

An example for a kernel- the linear operator L : ( x , y ) ⟶ ⟶ ( x , x ) {\displaystyle L:(x,y)\longrightarrow (x,x)} transforms all points on the ( x = 0 , y ) {\displaystyle (x=0,y)} line to the zero point ( 0 , 0 ) {\displaystyle (0,0)} , thus they form the kernel for the linear operator In mathematics , the kernel of a linear map , also known as the null space or nullspace , is the part of the domain which is mapped to the zero vector of the co-domain ; the kernel is always a linear subspace of the domain.

[ 1 ] That is, given a linear map L : V → W between two vector spaces V and W , the kernel of L is the vector space of all elements v of V such that L ( v ) = 0 , where 0 denotes the zero vector in W , [ 2 ] or more symbolically: ker ⁡ ⁡ ( L ) = { v ∈ ∈ V ∣ ∣ L ( v ) = 0 } = L − − 1 ( 0 ) .

{\displaystyle \ker(L)=\left\{\mathbf {v} \in V\mid L(\mathbf {v} )=\mathbf {0} \right\}=L^{-1}(\mathbf {0} ).} Properties [ edit ] Kernel and image of a linear map L from V to W The kernel of L is a linear subspace of the domain V .

[ 3 ] [ 2 ] In the linear map L : V → → W , {\displaystyle L:V\to W,} two elements of V have the same image in W if and only if their difference lies in the kernel of L , that is, L ( v 1 ) = L ( v 2 ) if and only if L ( v 1 − − v 2 ) = 0 .

{\displaystyle L\left(\mathbf {v} _{1}\right)=L\left(\mathbf {v} _{2}\right)\quad {\text{ if and only if }}\quad L\left(\mathbf {v} _{1}-\mathbf {v} _{2}\right)=\mathbf {0} .} From this, it follows by the first isomorphism theorem that the image of L is isomorphic to the quotient of V by the kernel: im ⁡ ⁡ ( L ) ≅ ≅ V / ker ⁡ ⁡ ( L ) .

{\displaystyle \operatorname {im} (L)\cong V/\ker(L).} In the case where V is finite-dimensional , this implies the rank–nullity theorem : dim ⁡ ⁡ ( ker ⁡ ⁡ L ) + dim ⁡ ⁡ ( im ⁡ ⁡ L ) = dim ⁡ ⁡ ( V ) .

{\displaystyle \dim(\ker L)+\dim(\operatorname {im} L)=\dim(V).} where the term rank refers to the dimension of the image of L , dim ⁡ ⁡ ( im ⁡ ⁡ L ) , {\displaystyle \dim(\operatorname {im} L),} while nullity refers to the dimension of the kernel of L , dim ⁡ ⁡ ( ker ⁡ ⁡ L ) .

{\displaystyle \dim(\ker L).} [ 4 ] That is, Rank ⁡ ⁡ ( L ) = dim ⁡ ⁡ ( im ⁡ ⁡ L ) and Nullity ⁡ ⁡ ( L ) = dim ⁡ ⁡ ( ker ⁡ ⁡ L ) , {\displaystyle \operatorname {Rank} (L)=\dim(\operatorname {im} L)\qquad {\text{ and }}\qquad \operatorname {Nullity} (L)=\dim(\ker L),} so that the rank–nullity theorem can be restated as Rank ⁡ ⁡ ( L ) + Nullity ⁡ ⁡ ( L ) = dim ⁡ ⁡ ( domain ⁡ ⁡ L ) .

{\displaystyle \operatorname {Rank} (L)+\operatorname {Nullity} (L)=\dim \left(\operatorname {domain} L\right).} When V is an inner product space , the quotient V / ker ⁡ ⁡ ( L ) {\displaystyle V/\ker(L)} can be identified with the orthogonal complement in V of ker ⁡ ⁡ ( L ) {\displaystyle \ker(L)} . This is the generalization to linear operators of the row space , or coimage , of a matrix.

Generalization to modules [ edit ] Main article: Module (mathematics) The notion of kernel also makes sense for homomorphisms of modules , which are generalizations of vector spaces where the scalars are elements of a ring , rather than a field . The domain of the mapping is a module, with the kernel constituting a submodule . Here, the concepts of rank and nullity do not necessarily apply.

In functional analysis [ edit ] Main article: Topological vector space If V and W are topological vector spaces such that W is finite-dimensional, then a linear operator L : V → W is continuous if and only if the kernel of L is a closed subspace of V .

Representation as matrix multiplication [ edit ] Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically R {\displaystyle \mathbb {R} } or C {\displaystyle \mathbb {C} } ), that is operating on column vectors x with n components over K .
The kernel of this linear map is the set of solutions to the equation A x = 0 , where 0 is understood as the zero vector . The dimension of the kernel of A is called the nullity of A . In set-builder notation , N ⁡ ⁡ ( A ) = Null ⁡ ⁡ ( A ) = ker ⁡ ⁡ ( A ) = { x ∈ ∈ K n ∣ ∣ A x = 0 } .

{\displaystyle \operatorname {N} (A)=\operatorname {Null} (A)=\operatorname {ker} (A)=\left\{\mathbf {x} \in K^{n}\mid A\mathbf {x} =\mathbf {0} \right\}.} The matrix equation is equivalent to a homogeneous system of linear equations : A x = 0 ⇔ ⇔ a 11 x 1 + a 12 x 2 + ⋯ ⋯ + a 1 n x n = 0 a 21 x 1 + a 22 x 2 + ⋯ ⋯ + a 2 n x n = 0 ⋮ ⋮ a m 1 x 1 + a m 2 x 2 + ⋯ ⋯ + a m n x n = 0 .

{\displaystyle A\mathbf {x} =\mathbf {0} \;\;\Leftrightarrow \;\;{\begin{alignedat}{7}a_{11}x_{1}&&\;+\;&&a_{12}x_{2}&&\;+\;\cdots \;+\;&&a_{1n}x_{n}&&\;=\;&&&0\\a_{21}x_{1}&&\;+\;&&a_{22}x_{2}&&\;+\;\cdots \;+\;&&a_{2n}x_{n}&&\;=\;&&&0\\&&&&&&&&&&\vdots \ \;&&&\\a_{m1}x_{1}&&\;+\;&&a_{m2}x_{2}&&\;+\;\cdots \;+\;&&a_{mn}x_{n}&&\;=\;&&&0{\text{.}}\\\end{alignedat}}} Thus the kernel of A is the same as the solution set to the above homogeneous equations.

Subspace properties [ edit ] The kernel of a m × n matrix A over a field K is a linear subspace of K n . That is, the kernel of A , the set Null( A ) , has the following three properties: Null( A ) always contains the zero vector , since A 0 = 0 .

If x ∈ Null( A ) and y ∈ Null( A ) , then x + y ∈ Null( A ) . This follows from the distributivity of matrix multiplication over addition.

If x ∈ Null( A ) and c is a scalar c ∈ K , then c x ∈ Null( A ) , since A ( c x ) = c ( A x ) = c 0 = 0 .

The row space of a matrix [ edit ] Main article: Rank–nullity theorem The product A x can be written in terms of the dot product of vectors as follows: A x = [ a 1 ⋅ ⋅ x a 2 ⋅ ⋅ x ⋮ ⋮ a m ⋅ ⋅ x ] .

{\displaystyle A\mathbf {x} ={\begin{bmatrix}\mathbf {a} _{1}\cdot \mathbf {x} \\\mathbf {a} _{2}\cdot \mathbf {x} \\\vdots \\\mathbf {a} _{m}\cdot \mathbf {x} \end{bmatrix}}.} Here, a 1 , ... , a m denote the rows of the matrix A . It follows that x is in the kernel of A , if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (since orthogonality is defined as having a dot product of 0).

The row space , or coimage, of a matrix A is the span of the row vectors of A . By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A , if and only if it is perpendicular to every vector in the row space of A .

The dimension of the row space of A is called the rank of A , and the dimension of the kernel of A is called the nullity of A . These quantities are related by the rank–nullity theorem [ 4 ] rank ⁡ ⁡ ( A ) + nullity ⁡ ⁡ ( A ) = n .

{\displaystyle \operatorname {rank} (A)+\operatorname {nullity} (A)=n.} Left null space [ edit ] The left null space , or cokernel , of a matrix A consists of all column vectors x such that x T A = 0 T , where T denotes the transpose of a matrix.  The left null space of A is the same as the kernel of A T . The left null space of A is the orthogonal complement to the column space of A , and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated with the matrix A .

Nonhomogeneous systems of linear equations [ edit ] The kernel also plays a role in the solution to a nonhomogeneous system of linear equations: A x = b or a 11 x 1 + a 12 x 2 + ⋯ ⋯ + a 1 n x n = b 1 a 21 x 1 + a 22 x 2 + ⋯ ⋯ + a 2 n x n = b 2 ⋮ ⋮ a m 1 x 1 + a m 2 x 2 + ⋯ ⋯ + a m n x n = b m {\displaystyle A\mathbf {x} =\mathbf {b} \quad {\text{or}}\quad {\begin{alignedat}{7}a_{11}x_{1}&&\;+\;&&a_{12}x_{2}&&\;+\;\cdots \;+\;&&a_{1n}x_{n}&&\;=\;&&&b_{1}\\a_{21}x_{1}&&\;+\;&&a_{22}x_{2}&&\;+\;\cdots \;+\;&&a_{2n}x_{n}&&\;=\;&&&b_{2}\\&&&&&&&&&&\vdots \ \;&&&\\a_{m1}x_{1}&&\;+\;&&a_{m2}x_{2}&&\;+\;\cdots \;+\;&&a_{mn}x_{n}&&\;=\;&&&b_{m}\\\end{alignedat}}} If u and v are two possible solutions to the above equation, then A ( u − − v ) = A u − − A v = b − − b = 0 {\displaystyle A(\mathbf {u} -\mathbf {v} )=A\mathbf {u} -A\mathbf {v} =\mathbf {b} -\mathbf {b} =\mathbf {0} } Thus, the difference of any two solutions to the equation A x = b lies in the kernel of A .

It follows that any solution to the equation A x = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation A x = b is { v + x ∣ ∣ A v = b ∧ ∧ x ∈ ∈ Null ⁡ ⁡ ( A ) } , {\displaystyle \left\{\mathbf {v} +\mathbf {x} \mid A\mathbf {v} =\mathbf {b} \land \mathbf {x} \in \operatorname {Null} (A)\right\},} Geometrically, this says that the solution set to A x = b is the translation of the kernel of A by the vector v . See also Fredholm alternative and flat (geometry) .

Illustration [ edit ] The following is a simple illustration of the computation of the kernel of a matrix (see § Computation by Gaussian elimination , below for methods better suited to more complex calculations). The illustration also touches on the row space and its relation to the kernel.

Consider the matrix A = [ 2 3 5 − − 4 2 3 ] .

{\displaystyle A={\begin{bmatrix}2&3&5\\-4&2&3\end{bmatrix}}.} The kernel of this matrix consists of all vectors ( x , y , z ) ∈ R 3 for which [ 2 3 5 − − 4 2 3 ] [ x y z ] = [ 0 0 ] , {\displaystyle {\begin{bmatrix}2&3&5\\-4&2&3\end{bmatrix}}{\begin{bmatrix}x\\y\\z\end{bmatrix}}={\begin{bmatrix}0\\0\end{bmatrix}},} which can be expressed as a homogeneous system of linear equations involving x , y , and z : 2 x + 3 y + 5 z = 0 , − − 4 x + 2 y + 3 z = 0.

{\displaystyle {\begin{aligned}2x+3y+5z&=0,\\-4x+2y+3z&=0.\end{aligned}}} The same linear equations can also be written in matrix form as: [ 2 3 5 0 − − 4 2 3 0 ] .

{\displaystyle \left[{\begin{array}{ccc|c}2&3&5&0\\-4&2&3&0\end{array}}\right].} Through Gauss–Jordan elimination , the matrix can be reduced to: [ 1 0 1 / 16 0 0 1 13 / 8 0 ] .

{\displaystyle \left[{\begin{array}{ccc|c}1&0&1/16&0\\0&1&13/8&0\end{array}}\right].} Rewriting the matrix in equation form yields: x = − − 1 16 z y = − − 13 8 z .

{\displaystyle {\begin{aligned}x&=-{\frac {1}{16}}z\\y&=-{\frac {13}{8}}z.\end{aligned}}} The elements of the kernel can be further expressed in parametric vector form , as follows: [ x y z ] = c [ − − 1 / 16 − − 13 / 8 1 ] ( where c ∈ ∈ R ) {\displaystyle {\begin{bmatrix}x\\y\\z\end{bmatrix}}=c{\begin{bmatrix}-1/16\\-13/8\\1\end{bmatrix}}\quad ({\text{where }}c\in \mathbb {R} )} Since c is a free variable ranging over all real numbers, this can be expressed equally well as: [ x y z ] = c [ − − 1 − − 26 16 ] .

{\displaystyle {\begin{bmatrix}x\\y\\z\end{bmatrix}}=c{\begin{bmatrix}-1\\-26\\16\end{bmatrix}}.} The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R 3 ). Here, the vector (−1,−26,16) T constitutes a basis of the kernel of A . The nullity of A is therefore 1, as it is spanned by a single vector.

The following dot products are zero: [ 2 3 5 ] [ − − 1 − − 26 16 ] = 0 a n d [ − − 4 2 3 ] [ − − 1 − − 26 16 ] = 0 , {\displaystyle {\begin{bmatrix}2&3&5\end{bmatrix}}{\begin{bmatrix}-1\\-26\\16\end{bmatrix}}=0\quad \mathrm {and} \quad {\begin{bmatrix}-4&2&3\end{bmatrix}}{\begin{bmatrix}-1\\-26\\16\end{bmatrix}}=0,} which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A .

These two (linearly independent) row vectors span the row space of A —a plane orthogonal to the vector (−1,−26,16) T .

With the rank 2 of A , the nullity 1 of A , and the dimension 3 of A , we have an illustration of the rank-nullity theorem.

Examples [ edit ] If L : R m → R n , then the kernel of L is the solution set to a homogeneous system of linear equations .  As in the above illustration, if L is the operator: L ( x 1 , x 2 , x 3 ) = ( 2 x 1 + 3 x 2 + 5 x 3 , − − 4 x 1 + 2 x 2 + 3 x 3 ) {\displaystyle L(x_{1},x_{2},x_{3})=(2x_{1}+3x_{2}+5x_{3},\;-4x_{1}+2x_{2}+3x_{3})} then the kernel of L is the set of solutions to the equations 2 x 1 + 3 x 2 + 5 x 3 = 0 − − 4 x 1 + 2 x 2 + 3 x 3 = 0 {\displaystyle {\begin{alignedat}{7}2x_{1}&\;+\;&3x_{2}&\;+\;&5x_{3}&\;=\;&0\\-4x_{1}&\;+\;&2x_{2}&\;+\;&3x_{3}&\;=\;&0\end{alignedat}}} Let C [0,1] denote the vector space of all continuous real-valued functions on the interval [0,1], and define L : C [0,1] → R by the rule L ( f ) = f ( 0.3 ) .

{\displaystyle L(f)=f(0.3).} Then the kernel of L consists of all functions f ∈ C [0,1] for which f (0.3) = 0 .

Let C ∞ ( R ) be the vector space of all infinitely differentiable functions R → R , and let D : C ∞ ( R ) → C ∞ ( R ) be the differentiation operator : D ( f ) = d f d x .

{\displaystyle D(f)={\frac {df}{dx}}.} Then the kernel of D consists of all functions in C ∞ ( R ) whose derivatives are zero, i.e. the set of all constant functions .

Let R ∞ be the direct product of infinitely many copies of R , and let s : R ∞ → R ∞ be the shift operator s ( x 1 , x 2 , x 3 , x 4 , … … ) = ( x 2 , x 3 , x 4 , … … ) .

{\displaystyle s(x_{1},x_{2},x_{3},x_{4},\ldots )=(x_{2},x_{3},x_{4},\ldots ).} Then the kernel of s is the one-dimensional subspace consisting of all vectors ( x 1 , 0, 0, 0, ...) .

If V is an inner product space and W is a subspace, the kernel of the orthogonal projection V → W is the orthogonal complement to W in V .

Computation by Gaussian elimination [ edit ] A basis of the kernel of a matrix may be computed by Gaussian elimination .

For this purpose, given an m × n matrix A , we construct first the row augmented matrix [ A I ] , {\displaystyle {\begin{bmatrix}A\\\hline I\end{bmatrix}},} where I is the n × n identity matrix .

Computing its column echelon form by Gaussian elimination (or any other suitable method), we get a matrix [ B C ] .

{\displaystyle {\begin{bmatrix}B\\\hline C\end{bmatrix}}.} A basis of the kernel of A consists in the non-zero columns of C such that the corresponding column of B is a zero column .

In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.

For example, suppose that A = [ 1 0 − − 3 0 2 − − 8 0 1 5 0 − − 1 4 0 0 0 1 7 − − 9 0 0 0 0 0 0 ] .

{\displaystyle A={\begin{bmatrix}1&0&-3&0&2&-8\\0&1&5&0&-1&4\\0&0&0&1&7&-9\\0&0&0&0&0&0\end{bmatrix}}.} Then [ A I ] = [ 1 0 − − 3 0 2 − − 8 0 1 5 0 − − 1 4 0 0 0 1 7 − − 9 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 ] .

{\displaystyle {\begin{bmatrix}A\\\hline I\end{bmatrix}}={\begin{bmatrix}1&0&-3&0&2&-8\\0&1&5&0&-1&4\\0&0&0&1&7&-9\\0&0&0&0&0&0\\\hline 1&0&0&0&0&0\\0&1&0&0&0&0\\0&0&1&0&0&0\\0&0&0&1&0&0\\0&0&0&0&1&0\\0&0&0&0&0&1\end{bmatrix}}.} Putting the upper part in column echelon form by column operations on the whole matrix gives [ B C ] = [ 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 3 − − 2 8 0 1 0 − − 5 1 − − 4 0 0 0 1 0 0 0 0 1 0 − − 7 9 0 0 0 0 1 0 0 0 0 0 0 1 ] .

{\displaystyle {\begin{bmatrix}B\\\hline C\end{bmatrix}}={\begin{bmatrix}1&0&0&0&0&0\\0&1&0&0&0&0\\0&0&1&0&0&0\\0&0&0&0&0&0\\\hline 1&0&0&3&-2&8\\0&1&0&-5&1&-4\\0&0&0&1&0&0\\0&0&1&0&-7&9\\0&0&0&0&1&0\\0&0&0&0&0&1\end{bmatrix}}.} The last three columns of B are zero columns. Therefore, the three last vectors of C , [ 3 − − 5 1 0 0 0 ] , [ − − 2 1 0 − − 7 1 0 ] , [ 8 − − 4 0 9 0 1 ] {\displaystyle \left[\!\!{\begin{array}{r}3\\-5\\1\\0\\0\\0\end{array}}\right],\;\left[\!\!{\begin{array}{r}-2\\1\\0\\-7\\1\\0\end{array}}\right],\;\left[\!\!{\begin{array}{r}8\\-4\\0\\9\\0\\1\end{array}}\right]} are a basis of the kernel of A .

Proof that the method computes the kernel: Since column operations correspond to post-multiplication by invertible matrices, the fact that [ A I ] {\displaystyle {\begin{bmatrix}A\\\hline I\end{bmatrix}}} reduces to [ B C ] {\displaystyle {\begin{bmatrix}B\\\hline C\end{bmatrix}}} means that there exists an invertible matrix P {\displaystyle P} such that [ A I ] P = [ B C ] , {\displaystyle {\begin{bmatrix}A\\\hline I\end{bmatrix}}P={\begin{bmatrix}B\\\hline C\end{bmatrix}},} with B {\displaystyle B} in column echelon form. Thus A P = B {\displaystyle AP=B} , I P = C {\displaystyle IP=C} , and A C = B {\displaystyle AC=B} .

A column vector v {\displaystyle \mathbf {v} } belongs to the kernel of A {\displaystyle A} (that is A v = 0 {\displaystyle A\mathbf {v} =\mathbf {0} } ) if and only if B w = 0 , {\displaystyle B\mathbf {w} =\mathbf {0} ,} where w = P − − 1 v = C − − 1 v {\displaystyle \mathbf {w} =P^{-1}\mathbf {v} =C^{-1}\mathbf {v} } .

As B {\displaystyle B} is in column echelon form, B w = 0 {\displaystyle B\mathbf {w} =\mathbf {0} } , if and only if the nonzero entries of w {\displaystyle \mathbf {w} } correspond to the zero columns of B {\displaystyle B} .

By multiplying by C {\displaystyle C} , one may deduce that this is the case if and only if v = C w {\displaystyle \mathbf {v} =C\mathbf {w} } is a linear combination of the corresponding columns of C {\displaystyle C} .

Numerical computation [ edit ] The problem of computing the kernel on a computer depends on the nature of the coefficients.

Exact coefficients [ edit ] If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed with Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem , which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).

[ citation needed ] For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity , but are faster and behave better with modern computer hardware .

[ citation needed ] Floating point computation [ edit ] For matrices whose entries are floating-point numbers , the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors , a floating-point matrix has almost always a full rank , even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned , i.e. it has a low condition number .

[ 5 ] [ citation needed ] Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed with any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.

[ citation needed ] See also [ edit ] Kernel (algebra) Zero set System of linear equations Row and column spaces Row reduction Four fundamental subspaces Vector space Linear subspace Linear operator Function space Fredholm alternative Notes and references [ edit ] ^ Weisstein, Eric W.

"Kernel" .

mathworld.wolfram.com . Retrieved 2019-12-09 .

^ a b "Kernel (Nullspace) | Brilliant Math & Science Wiki" .

brilliant.org . Retrieved 2019-12-09 .

^ Linear algebra, as discussed in this article, is a very well established mathematical discipline for which there are many sources. Almost all of the material in this article can be found in Lay 2005 , Meyer 2001 , and Strang's lectures.

^ a b Weisstein, Eric W.

"Rank-Nullity Theorem" .

mathworld.wolfram.com . Retrieved 2019-12-09 .

^ "Archived copy" (PDF) . Archived from the original (PDF) on 2017-08-29 . Retrieved 2015-04-14 .

{{ cite web }} :  CS1 maint: archived copy as title ( link ) Bibliography [ edit ] See also: Linear algebra § Further reading Axler, Sheldon Jay (1997), Linear Algebra Done Right (2nd ed.), Springer-Verlag, ISBN 0-387-98259-0 .

Lay, David C. (2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 .

Meyer, Carl D. (2001), Matrix Analysis and Applied Linear Algebra , Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8 , archived from the original on 2009-10-31.

Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Brooks/Cole, ISBN 0-534-99845-3 .

Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International.

Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall.

Lang, Serge (1987).

Linear Algebra . Springer.

ISBN 9780387964126 .

Trefethen, Lloyd N.; Bau, David III (1997), Numerical Linear Algebra , SIAM, ISBN 978-0-89871-361-9 .

External links [ edit ] Wikibooks has a book on the topic of: Linear Algebra/Null Spaces "Kernel of a matrix" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Khan Academy , Introduction to the Null Space of a Matrix v t e Linear algebra Outline Glossary Basic concepts Scalar Vector Vector space Scalar multiplication Vector projection Linear span Linear map Linear projection Linear independence Linear combination Multilinear map Basis Change of basis Row and column vectors Row and column spaces Kernel Eigenvalues and eigenvectors Transpose Linear equations Matrices Block Decomposition Invertible Minor Multiplication Rank Transformation Cramer's rule Gaussian elimination Productive matrix Gram matrix Bilinear Orthogonality Dot product Hadamard product Inner product space Outer product Kronecker product Gram–Schmidt process Multilinear algebra Determinant Cross product Triple product Seven-dimensional cross product Geometric algebra Exterior algebra Bivector Multivector Tensor Outermorphism Vector space constructions Dual Direct sum Function space Quotient Subspace Tensor product Numerical Floating-point Numerical stability Basic Linear Algebra Subprograms Sparse matrix Comparison of linear algebra libraries Category NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐d4jn7
Cached time: 20250812000726
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.622 seconds
Real time usage: 0.883 seconds
Preprocessor visited node count: 5960/1000000
Revision size: 25047/2097152 bytes
Post‐expand include size: 64834/2097152 bytes
Template argument size: 9154/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 51589/5000000 bytes
Lua time usage: 0.326/10.000 seconds
Lua memory usage: 8137631/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  569.035      1 -total
 22.81%  129.810      1 Template:Reflist
 15.46%   87.950      4 Template:Cite_web
 14.45%   82.240      1 Template:Linear_algebra
 14.15%   80.526      1 Template:Navbox
 12.92%   73.540      1 Template:Short_description
 10.65%   60.578     63 Template:Math
  9.04%   51.450      2 Template:Pagetype
  8.88%   50.533      4 Template:Citation_needed
  7.39%   42.027      4 Template:Fix Saved in parser cache with key enwiki:pcache:1072915:|#|:idhash:canonical and timestamp 20250812000726 and revision id 1302836466. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Kernel_(linear_algebra)&oldid=1302836466 " Categories : Linear algebra Functional analysis Matrices (mathematics) Numerical linear algebra Hidden categories: CS1 maint: archived copy as title Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from October 2014 Articles with unsourced statements from December 2019 This page was last edited on 27 July 2025, at 18:23 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Kernel (linear algebra) 28 languages Add topic

