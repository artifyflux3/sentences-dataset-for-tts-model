Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Common examples Toggle Common examples subsection 1.1 F -test of the equality of two variances 2 Formula and calculation Toggle Formula and calculation subsection 2.1 One-way analysis of variance 2.1.1 Advantages 2.1.2 Disadvantages 2.2 Multiple-comparison ANOVA problems 2.3 Regression problems 3 See also 4 References 5 Further reading 6 External links Toggle the table of contents F -test 25 languages العربية Català Čeština Deutsch Español فارسی Français 한국어 हिन्दी Italiano עברית Magyar Македонски Nederlands 日本語 Polski Português Română Русский Српски / srpski Sunda Türkçe Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical hypothesis test, mostly using multiple restrictions An f-test pdf with d1 and d2 = 10, at a significance level of 0.05. (Red shaded region indicates the critical region) An F-test is a statistical test that compares variances. It is used to determine if the variances of two samples, or if the ratios of variances among multiple samples, are significantly different. The test calculates a statistic , represented by the random variable F, and checks if it follows an F-distribution . This check is valid if the null hypothesis is true and standard assumptions about the errors (ε) in the data hold.

[ 1 ] F-tests are frequently used to compare different statistical models and find the one that best describes the population the data came from. When models are created using the least squares method, the resulting F-tests are often called "exact" F-tests. The F-statistic was developed by Ronald Fisher in the 1920s as the variance ratio and was later named in his honor by George W. Snedecor .

[ 2 ] Common examples [ edit ] Common examples of the use of F -tests include the study of the following cases One-way ANOVA table with 3 random groups that each has 30 observations. F value is being calculated in the second to last column The hypothesis that the means of a given set of normally distributed populations, all having the same standard deviation , are equal.  This is perhaps the best-known F -test, and plays an important role in the analysis of variance (ANOVA).

F test of analysis of variance (ANOVA) follows three assumptions Normality (statistics) Homogeneity of variance Independence of errors and random sampling The hypothesis that a proposed regression model fits the data well.  See Lack-of-fit sum of squares .

The hypothesis that a data set in a regression analysis follows the simpler of two proposed linear models that are nested within each other.

Multiple-comparison testing is conducted using needed data in already completed F-test, if F-test leads to rejection of null hypothesis and the factor under study has an impact on the dependent variable.

[ 1 ] " a priori comparisons"/ "planned comparisons"- a particular set of comparisons "pairwise comparisons"-all possible comparisons i.e. Fisher's least significant difference (LSD) test, Tukey's honestly significant difference (HSD) test , Newman Keuls test , Ducan's test " a posteriori comparisons "/ " post hoc comparisons "/ " exploratory comparisons "- choose comparisons after examining the data i.e.

Scheffé's method F -test of the equality of two variances [ edit ] Main article: F-test of equality of variances The F -test is sensitive to non-normality .

[ 3 ] [ 4 ] In the analysis of variance (ANOVA), alternative tests include Levene's test , Bartlett's test , and the Brown–Forsythe test . However, when any of these tests are conducted to test the underlying assumption of homoscedasticity ( i.e.

homogeneity of variance), as a preliminary step to testing for mean effects, there is an increase in the experiment-wise Type I error rate.

[ 5 ] Formula and calculation [ edit ] Most F -tests arise by considering a decomposition of the variability in a collection of data in terms of sums of squares .  The test statistic in an F -test is the ratio of two scaled sums of squares reflecting different sources of variability. These sums of squares are constructed so that the statistic tends to be greater when the null hypothesis is not true. In order for the statistic to follow the F -distribution under the null hypothesis, the sums of squares should be statistically independent , and each should follow a scaled χ²-distribution . The latter condition is guaranteed if the data values are independent and normally distributed with a common variance .

One-way analysis of variance [ edit ] The formula for the one-way ANOVA F -test statistic is F = explained variance unexplained variance , {\displaystyle F={\frac {\text{explained variance}}{\text{unexplained variance}}},} or F = between-group variability within-group variability .

{\displaystyle F={\frac {\text{between-group variability}}{\text{within-group variability}}}.} The "explained variance", or "between-group variability" is ∑ ∑ i = 1 K n i ( Y ¯ ¯ i ⋅ ⋅ − − Y ¯ ¯ ) 2 / ( K − − 1 ) {\displaystyle \sum _{i=1}^{K}n_{i}({\bar {Y}}_{i\cdot }-{\bar {Y}})^{2}/(K-1)} where Y ¯ ¯ i ⋅ ⋅ {\displaystyle {\bar {Y}}_{i\cdot }} denotes the sample mean in the i -th group, n i {\displaystyle n_{i}} is the number of observations in the i -th group, Y ¯ ¯ {\displaystyle {\bar {Y}}} denotes the overall mean of the data, and K {\displaystyle K} denotes the number of groups.

The "unexplained variance", or "within-group variability" is ∑ ∑ i = 1 K ∑ ∑ j = 1 n i ( Y i j − − Y ¯ ¯ i ⋅ ⋅ ) 2 / ( N − − K ) , {\displaystyle \sum _{i=1}^{K}\sum _{j=1}^{n_{i}}\left(Y_{ij}-{\bar {Y}}_{i\cdot }\right)^{2}/(N-K),} where Y i j {\displaystyle Y_{ij}} is the j th observation in the i th out of K {\displaystyle K} groups and N {\displaystyle N} is the overall sample size.  This F -statistic follows the F -distribution with degrees of freedom d 1 = K − − 1 {\displaystyle d_{1}=K-1} and d 2 = N − − K {\displaystyle d_{2}=N-K} under the null hypothesis.  The statistic will be large if the between-group variability is large relative to the within-group variability, which is unlikely to happen if the population means of the groups all have the same value.

F Table: Level 5% Critical values, containing degrees of freedoms for both denominator and numerator ranging from 1-20 The result of the F test can be determined by comparing calculated F value and critical F value with specific significance level (e.g. 5%). The F table serves as a reference guide containing critical F values for the distribution of the F-statistic under the assumption of a true null hypothesis. It is designed to help determine the threshold beyond which the F statistic is expected to exceed a controlled percentage of the time (e.g., 5%) when the null hypothesis is accurate. To locate the critical F value in the F table, one needs to utilize the respective degrees of freedom. This involves identifying the appropriate row and column in the F table that corresponds to the significance level being tested (e.g., 5%).

[ 6 ] How to use critical F values: If the F statistic < the critical F value Fail to reject null hypothesis Reject alternative hypothesis There is no significant differences among sample averages The observed differences among sample averages could be reasonably caused by random chance itself The result is not statistically significant If the F statistic > the critical F value Accept alternative hypothesis Reject null hypothesis There is significant differences among sample averages The observed differences among sample averages could not be reasonably caused by random chance itself The result is statistically significant Note that when there are only two groups for the one-way ANOVA F -test, F = t 2 {\displaystyle F=t^{2}} where t is the Student's t {\displaystyle t} statistic .

Advantages [ edit ] Multi-group comparison efficiency: facilitating simultaneous comparison of multiple groups, enhancing efficiency particularly in situations involving more than two groups.

Clarity in variance comparison: offering a straightforward interpretation of variance differences among groups, contributing to a clear understanding of the observed data patterns.

Versatility across disciplines: demonstrating broad applicability across diverse fields, including social sciences, natural sciences, and engineering.

Disadvantages [ edit ] Sensitivity to assumptions: the F-test is highly sensitive to certain assumptions, such as homogeneity of variance and normality which can affect the accuracy of test results.

Limited scope to group comparisons: the F-test is tailored for comparing variances between groups, making it less suitable for analyses beyond this specific scope.

Interpretation challenges: the F-test does not pinpoint specific group pairs with distinct variances. Careful interpretation is necessary, and additional post hoc tests are often essential for a more detailed understanding of group-wise differences.

Multiple-comparison ANOVA problems [ edit ] The F -test in one-way analysis of variance ( ANOVA ) is used to assess whether the expected values of a quantitative variable within several pre-defined groups differ from each other. For example, suppose that a medical trial compares four treatments.  The ANOVA F -test can be used to assess whether any of the treatments are on average superior, or inferior, to the others versus the null hypothesis that all four treatments yield the same mean response.  This is an example of an "omnibus" test, meaning that a single test is performed to detect any of several possible differences.  Alternatively, we could carry out pairwise tests among the treatments (for instance, in the medical trial example with four treatments we could carry out six tests among pairs of treatments).  The advantage of the ANOVA F -test is that we do not need to pre-specify which treatments are to be compared, and we do not need to adjust for making multiple comparisons .  The disadvantage of the ANOVA F -test is that if we reject the null hypothesis , we do not know which treatments can be said to be significantly different from the others, nor, if the F -test is performed at level α, can we state that the treatment pair with the greatest mean difference is significantly different at level α.

Regression problems [ edit ] Further information: Stepwise regression Consider two models, 1 and 2, where model 1 is 'nested' within model 2.  Model 1 is the restricted model, and model 2 is the unrestricted one.  That is, model 1 has p 1 parameters, and model 2 has p 2 parameters, where p 1 < p 2 , and for any choice of parameters in model 1, the same regression curve can be achieved by some choice of the parameters of model 2.

One common context in this regard is that of deciding whether a model fits the data significantly better than does a naive model, in which the only explanatory term is the intercept term, so that all predicted values for the dependent variable are set equal to that variable's sample mean. The naive model is the restricted model, since the coefficients of all potential explanatory variables are restricted to equal zero.

Another common context is deciding whether there is a structural break in the data: here the restricted model uses all data in one regression, while the unrestricted model uses separate regressions for two different subsets of the data. This use of the F-test is known as the Chow test .

The model with more parameters will always be able to fit the data at least as well as the model with fewer parameters.  Thus typically model 2 will give a better (i.e. lower error) fit to the data than model 1.  But one often wants to determine whether model 2 gives a significantly better fit to the data.  One approach to this problem is to use an F -test.

If there are n data points to estimate parameters of both models from, then one can calculate the F statistic, given by F = ( RSS 1 − − RSS 2 p 2 − − p 1 ) ( RSS 2 n − − p 2 ) = RSS 1 − − RSS 2 RSS 2 ⋅ ⋅ n − − p 2 p 2 − − p 1 , {\displaystyle F={\frac {\left({\frac {{\text{RSS}}_{1}-{\text{RSS}}_{2}}{p_{2}-p_{1}}}\right)}{\left({\frac {{\text{RSS}}_{2}}{n-p_{2}}}\right)}}={\frac {{\text{RSS}}_{1}-{\text{RSS}}_{2}}{{\text{RSS}}_{2}}}\cdot {\frac {n-p_{2}}{p_{2}-p_{1}}},} where RSS i is the residual sum of squares of model i . If the regression model has been calculated with weights, then replace RSS i with χ 2 , the weighted sum of squared residuals.  Under the null hypothesis that model 2 does not provide a significantly better fit than model 1, F will have an F distribution, with ( p 2 − p 1 , n − p 2 ) degrees of freedom .  The null hypothesis is rejected if the F calculated from the data is greater than the critical value of the F -distribution for some desired false-rejection probability (e.g. 0.05). Since F is a monotone function of the likelihood ratio statistic, the F -test is a likelihood ratio test .

See also [ edit ] Goodness of fit References [ edit ] ^ a b Berger, Paul D.; Maurer, Robert E.; Celli, Giovana B. (2018).

Experimental Design . Cham: Springer International Publishing. p. 108.

doi : 10.1007/978-3-319-64583-4 .

ISBN 978-3-319-64582-7 .

^ Lomax, Richard G. (2007).

Statistical Concepts: A Second Course . Lawrence Erlbaum Associates. p.

10 .

ISBN 978-0-8058-5850-1 .

^ Box, G. E. P.

(1953). "Non-Normality and Tests on Variances".

Biometrika .

40 (3/4): 318– 335.

doi : 10.1093/biomet/40.3-4.318 .

JSTOR 2333350 .

^ Markowski, Carol A; Markowski, Edward P. (1990). "Conditions for the Effectiveness of a Preliminary Test of Variance".

The American Statistician .

44 (4): 322– 326.

doi : 10.2307/2684360 .

JSTOR 2684360 .

^ Sawilowsky, S. (2002).

"Fermat, Schubert, Einstein, and Behrens–Fisher: The Probable Difference Between Two Means When σ 1 2 ≠ σ 2 2 " .

Journal of Modern Applied Statistical Methods .

1 (2): 461– 472.

doi : 10.22237/jmasm/1036109940 .

Archived from the original on 2015-04-03 . Retrieved 2015-03-30 .

^ Siegel, Andrew F. (2016-01-01), Siegel, Andrew F. (ed.), "Chapter 15 - ANOVA: Testing for Differences Among Many Samples and Much More" , Practical Business Statistics (Seventh Edition) , Academic Press, pp.

469– 492, doi : 10.1016/b978-0-12-804250-2.00015-8 , ISBN 978-0-12-804250-2 , retrieved 2023-12-10 Further reading [ edit ] Fox, Karl A. (1980).

Intermediate Economic Statistics (Second ed.). New York: John Wiley & Sons. pp.

290– 310.

ISBN 0-88275-521-8 .

Johnston, John (1972).

Econometric Methods (Second ed.). New York: McGraw-Hill. pp.

35– 38.

Kmenta, Jan (1986).

Elements of Econometrics (Second ed.). New York: Macmillan. pp.

147– 148.

ISBN 0-02-365070-2 .

Maddala, G. S.

; Lahiri, Kajal (2009).

Introduction to Econometrics (Fourth ed.). Chichester: Wiley. pp.

155– 160.

ISBN 978-0-470-01512-4 .

External links [ edit ] Table of F -test critical values Free calculator for F -testing The F -test for Linear Regression Econometrics lecture (topic: hypothesis testing) on YouTube by Mark Thoma v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐sn5h6
Cached time: 20250811235952
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.378 seconds
Real time usage: 0.531 seconds
Preprocessor visited node count: 2097/1000000
Revision size: 17009/2097152 bytes
Post‐expand include size: 168237/2097152 bytes
Template argument size: 2309/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 44878/5000000 bytes
Lua time usage: 0.214/10.000 seconds
Lua memory usage: 5631347/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  360.022      1 -total
 37.28%  134.228      1 Template:Statistics
 36.64%  131.908      1 Template:Navbox_with_collapsible_groups
 29.63%  106.683      1 Template:Reflist
 26.13%   94.081      6 Template:Cite_book
 17.10%   61.557      1 Template:Short_description
 13.69%   49.297     11 Template:Navbox
 10.52%   37.871      2 Template:Pagetype
  6.96%   25.044      1 Template:Hlist
  4.73%   17.027     15 Template:Main_other Saved in parser cache with key enwiki:pcache:318976:|#|:idhash:canonical and timestamp 20250811235952 and revision id 1292714067. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=F-test&oldid=1292714067 " Categories : Analysis of variance Statistical ratios Statistical tests Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 28 May 2025, at 12:02 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents F -test 25 languages Add topic

