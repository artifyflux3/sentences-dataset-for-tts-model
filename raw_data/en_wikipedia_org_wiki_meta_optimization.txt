Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Motivation 2 Methods 3 See also 4 References Toggle the table of contents Meta-optimization 2 languages Español Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Meta-optimization concept.

Meta-optimization from numerical optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson [ 1 ] for finding optimal parameter settings of a genetic algorithm .

Meta-optimization and related concepts are also known in the literature as meta-evolution, super-optimization, automated parameter calibration, hyper-heuristics , etc.

Motivation [ edit ] Performance landscape for differential evolution .

Optimization methods such as genetic algorithm and differential evolution have several parameters that govern their behaviour and efficiency in optimizing a given problem and these parameters must be chosen by the practitioner to achieve satisfactory results. Selecting the behavioural parameters by hand is a laborious task that is susceptible to human misconceptions of what makes the optimizer perform well.

The behavioural parameters of an optimizer can be varied and the optimization performance plotted as a landscape. This is computationally feasible for optimizers with few behavioural parameters and optimization problems that are fast to compute, but when the number of behavioural parameters increases the time usage for computing such a performance landscape increases exponentially. This is the curse of dimensionality for the search-space consisting of an optimizer's behavioural parameters. An efficient method is therefore needed to search the space of behavioural parameters.

Methods [ edit ] Meta-optimization of differential evolution .

A simple way of finding good behavioural parameters for an optimizer is to employ another overlaying optimizer, called the meta -optimizer. There are different ways of doing this depending on whether the behavioural parameters to be tuned are real-valued or discrete-valued , and depending on what performance measure is being used, etc.

Meta-optimizing the parameters of a genetic algorithm was done by Grefenstette [ 2 ] and Keane, [ 3 ] amongst others, and experiments with meta-optimizing both the parameters and the genetic operators were reported by Bäck.

[ 4 ] Meta-optimization of the COMPLEX-RF algorithm was done by Krus and Andersson, [ 5 ] and, [ 6 ] where performance index of optimization based on information theory was introduced and further developed. Meta-optimization of particle swarm optimization was done by Meissner et al., [ 7 ] Pedersen and Chipperfield, [ 8 ] and Mason et al.

[ 9 ] Pedersen and Chipperfield applied meta-optimization to differential evolution .

[ 10 ] Birattari et al.

[ 11 ] [ 12 ] meta-optimized ant colony optimization .

Statistical models have also been used to reveal more about the relationship between choices of behavioural parameters and optimization performance, see for example Francois and Lavergne, [ 13 ] and Nannen and Eiben.

[ 14 ] A comparison of various meta-optimization techniques was done by Smit and Eiben.

[ 15 ] See also [ edit ] Automated machine learning (AutoML) Hyper-heuristics References [ edit ] ^ Mercer, R.E.; Sampson, J.R. (1978). "Adaptive search using a reproductive metaplan".

Kybernetes .

7 (3): 215– 228.

doi : 10.1108/eb005486 .

^ Grefenstette, J.J. (1986). "Optimization of control parameters for genetic algorithms".

IEEE Transactions on Systems, Man, and Cybernetics .

16 (1): 122– 128.

doi : 10.1109/TSMC.1986.289288 .

S2CID 23313487 .

^ Keane, A.J. (1995). "Genetic algorithm optimization in multi-peak problems: studies in convergence and robustness".

Artificial Intelligence in Engineering .

9 (2): 75– 83.

doi : 10.1016/0954-1810(95)95751-Q .

^ Bäck, T. (1994). "Parallel optimization of evolutionary algorithms".

Proceedings of the International Conference on Evolutionary Computation . pp.

418– 427.

^ Krus, PK.; Andersson (Ölvander), J. (2003). "Optimizing optimization for design optimization".

Proceedings of DETC’03 2003 ASME Design Engineering Technical Conferences and Computers and Information in Engineering Conference Chicago, Illinois, USA .

^ Krus, PK.; Ölvander(Andersson), J. (2013).

"Performance index and meta-optimization of a direct search optimization method" (PDF) .

Engineering Optimization .

45 (10): 1167– 1185.

Bibcode : 2013EnOp...45.1167K .

doi : 10.1080/0305215X.2012.725052 .

S2CID 62731978 .

^ Meissner, M.; Schmuker, M.; Schneider, G. (2006).

"Optimized Particle Swarm Optimization (OPSO) and its application to artificial neural network training" .

BMC Bioinformatics .

7 (1): 125.

doi : 10.1186/1471-2105-7-125 .

PMC 1464136 .

PMID 16529661 .

^ Pedersen, M.E.H.; Chipperfield, A.J. (2010). "Simplifying particle swarm optimization".

Applied Soft Computing .

10 (2): 618– 628.

CiteSeerX 10.1.1.149.8300 .

doi : 10.1016/j.asoc.2009.08.029 .

^ Mason, Karl; Duggan, Jim; Howley, Enda (2018). "A Meta Optimisation Analysis of Particle Swarm Optimisation Velocity Update Equations for Watershed Management Learning".

Applied Soft Computing .

62 : 148– 161.

doi : 10.1016/j.asoc.2017.10.018 .

^ Pedersen, M.E.H. (2010).

Tuning & Simplifying Heuristical Optimization (PDF) (PhD thesis). University of Southampton, School of Engineering Sciences, Computational Engineering and Design Group.

S2CID 107805461 . Archived from the original (PDF) on 2020-02-13.

^ Birattari, M.; Stützle, T.; Paquete, L.; Varrentrapp, K. (2002).

"A racing algorithm for configuring metaheuristics" .

Proceedings of the Genetic and Evolutionary Computation Conference (GECCO) . pp.

11– 18.

^ Birattari, M. (2004).

The Problem of Tuning Metaheuristics as Seen from a Machine Learning Perspective (PDF) (PhD thesis). Université Libre de Bruxelles.

^ Francois, O.; Lavergne, C. (2001). "Design of evolutionary algorithms - a statistical perspective".

IEEE Transactions on Evolutionary Computation .

5 (2): 129– 148.

doi : 10.1109/4235.918434 .

^ Nannen, V.; Eiben, A.E. (2006).

"A method for parameter calibration and relevance estimation in evolutionary algorithms" (PDF) .

Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation (GECCO) . pp.

183– 190.

^ Smit, S.K.; Eiben, A.E. (2009).

"Comparing parameter tuning methods for evolutionary algorithms" (PDF) .

Proceedings of the IEEE Congress on Evolutionary Computation (CEC) . pp.

399– 406.

v t e Optimization : Algorithms , methods , and heuristics Unconstrained nonlinear Functions Golden-section search Powell's method Line search Nelder–Mead method Successive parabolic interpolation Gradients Convergence Trust region Wolfe conditions Quasi–Newton Berndt–Hall–Hall–Hausman Broyden–Fletcher–Goldfarb–Shanno and L-BFGS Davidon–Fletcher–Powell Symmetric rank-one (SR1) Other methods Conjugate gradient Gauss–Newton Gradient Mirror Levenberg–Marquardt Powell's dog leg method Truncated Newton Hessians Newton's method Optimization computes maxima and minima.

Constrained nonlinear General Barrier methods Penalty methods Differentiable Augmented Lagrangian methods Sequential quadratic programming Successive linear programming Convex optimization Convex minimization Cutting-plane method Reduced gradient (Frank–Wolfe) Subgradient method Linear and quadratic Interior point Affine scaling Ellipsoid algorithm of Khachiyan Projective algorithm of Karmarkar Basis- exchange Simplex algorithm of Dantzig Revised simplex algorithm Criss-cross algorithm Principal pivoting algorithm of Lemke Active-set method Combinatorial Paradigms Approximation algorithm Dynamic programming Greedy algorithm Integer programming Branch and bound / cut Graph algorithms Minimum spanning tree Borůvka Prim Kruskal Shortest path Bellman–Ford SPFA Dijkstra Floyd–Warshall Network flows Dinic Edmonds–Karp Ford–Fulkerson Push–relabel maximum flow Metaheuristics Evolutionary algorithm Hill climbing Local search Parallel metaheuristics Simulated annealing Spiral optimization algorithm Tabu search Software Retrieved from " https://en.wikipedia.org/w/index.php?title=Meta-optimization&oldid=1266456994 " Categories : Evolutionary computation Heuristics This page was last edited on 31 December 2024, at 18:27 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Meta-optimization 2 languages Add topic

