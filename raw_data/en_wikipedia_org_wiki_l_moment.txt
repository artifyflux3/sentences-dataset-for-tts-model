Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Population L-moments Toggle Population L-moments subsection 1.1 Analytic calculation 1.2 Sillitto's Theorem 2 Sample L-moments 3 L-moment ratios 4 Related quantities 5 Usage 6 Values for some common distributions 7 Extensions 8 See also 9 References 10 External links Toggle the table of contents L-moment 2 languages فارسی Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical sequence characterizing probability distributions In statistics , L-moments are a sequence of statistics used to summarize the shape of a probability distribution .

[ 1 ] [ 2 ] [ 3 ] [ 4 ] They are linear combinations of order statistics ( L-statistics ) analogous to conventional moments , and can be used to calculate quantities analogous to standard deviation , skewness and kurtosis , termed the L-scale, L-skewness and L-kurtosis respectively (the L-mean is identical to the conventional mean ). Standardized L-moments are called L-moment ratios and are analogous to standardized moments . Just as for conventional moments, a theoretical distribution has a set of population L-moments. Sample L-moments can be defined for a sample from the population, and can be used as estimators of the population L-moments.

Population L-moments [ edit ] For a random variable X , the r th population L-moment is [ 1 ] λ λ r = 1 r ∑ ∑ k = 0 r − − 1 ( − − 1 ) k ( r − − 1 k ) E ⁡ ⁡ [ X r − − k : r ] , {\displaystyle \lambda _{r}={\frac {1}{r}}\sum _{k=0}^{r-1}(-1)^{k}{\binom {r-1}{k}}\operatorname {\mathbb {E} } [X_{r-k:r}]\,,} where X k:n denotes the k th order statistic ( k th smallest value) in an independent sample of size n from the distribution of X and E {\displaystyle \mathbb {E} } denotes expected value operator . In particular, the first four population L-moments are λ λ 1 = E ⁡ ⁡ [ X ] λ λ 2 = 1 2 ( E ⁡ ⁡ [ X 2 : 2 ] − − E ⁡ ⁡ [ X 1 : 2 ] ) λ λ 3 = 1 3 ( E ⁡ ⁡ [ X 3 : 3 ] − − 2 E ⁡ ⁡ [ X 2 : 3 ] + E ⁡ ⁡ [ X 1 : 3 ] ) λ λ 4 = 1 4 ( E ⁡ ⁡ [ X 4 : 4 ] − − 3 E ⁡ ⁡ [ X 3 : 4 ] + 3 E ⁡ ⁡ [ X 2 : 4 ] − − E ⁡ ⁡ [ X 1 : 4 ] ) .

{\displaystyle {\begin{aligned}\lambda _{1}&=\operatorname {\mathbb {E} } [X]\\[4pt]\lambda _{2}&={\tfrac {1}{2}}\left(\operatorname {\mathbb {E} } [X_{2:2}]-\operatorname {\mathbb {E} } [X_{1:2}]\right)\\[4pt]\lambda _{3}&={\tfrac {1}{3}}\left(\operatorname {\mathbb {E} } [X_{3:3}]-2\operatorname {\mathbb {E} } [X_{2:3}]+\operatorname {\mathbb {E} } [X_{1:3}]\right)\\[4pt]\lambda _{4}&={\tfrac {1}{4}}\left(\operatorname {\mathbb {E} } [X_{4:4}]-3\operatorname {\mathbb {E} } [X_{3:4}]+3\operatorname {\mathbb {E} } [X_{2:4}]-\operatorname {\mathbb {E} } [X_{1:4}]\right).\end{aligned}}} Note that the coefficients of the r th L-moment are the same as in the r th term of the binomial transform , as used in the r -order finite difference (finite analog to the derivative).

The first two of these L-moments have conventional names: λ λ 1 {\displaystyle \lambda _{1}} is the "mean", "L-mean", or "L-location", λ λ 2 {\displaystyle \lambda _{2}} is the "L-scale".

The L-scale is equal to half the Mean absolute difference .

[ 5 ] Analytic calculation [ edit ] Expectations are often defined in terms of probability density functions , but the connection in terms of these between the order statistics X r : n {\displaystyle X_{r:n}} and their underlying random variable X {\displaystyle X} is rather remote. A closer connection can be found in terms of cumulative distribution functions (CDFs), since these (see this section ) satisfy F X r : n ( x ) = ∑ ∑ j = r n ( n j ) F X ( x ) j ( 1 − − F X ( x ) ) n − − j .

{\displaystyle F_{X_{r:n}}(x)=\sum _{j=r}^{n}{\binom {n}{j}}F_{X}(x)^{j}{\bigl (}1-F_{X}(x){\bigr )}^{n-j}.} In particular one may define polynomials b r : n ( y ) = ∑ ∑ j = r n ( n j ) y j ( 1 − − y ) n − − j {\displaystyle b_{r:n}(y)=\sum _{j=r}^{n}{\binom {n}{j}}y^{j}(1-y)^{n-j}} and express F X r : n = b r : n ∘ ∘ F X {\displaystyle F_{X_{r:n}}=b_{r:n}\circ F_{X}} .

Having a CDF F X {\displaystyle F_{X}} , the expectation E { X } {\displaystyle \mathbb {E} \{X\}} may be expressed using a Stieltjes integral as E { X } = ∫ ∫ R x d F X ( x ) , {\displaystyle \mathbb {E} \{X\}=\int _{\mathbb {R} }x\,dF_{X}(x),} thus E { X r : n } = ∫ ∫ R x d ( b r : n ∘ ∘ F X ) ( x ) = ∫ ∫ R x b r : n ′ ( F X ( x ) ) d F X ( x ) {\displaystyle \mathbb {E} \{X_{r:n}\}=\int _{\mathbb {R} }x\,d(b_{r:n}\circ F_{X})(x)=\int _{\mathbb {R} }xb_{r:n}'{\bigl (}F_{X}(x){\bigr )}\,dF_{X}(x)} where b r : n ′ {\displaystyle b_{r:n}'} is straight off the derivative of b r : n {\displaystyle b_{r:n}} . This integral can often be made more tractable by introducing the quantile function Q X {\displaystyle Q_{X}} via the change of variables y = F X ( x ) , x = Q X ( y ) {\displaystyle y=F_{X}(x),x=Q_{X}(y)} : E { X r : n } = ∫ ∫ R x b r : n ′ ( F X ( x ) ) d F X ( x ) = ∫ ∫ 0 1 Q X ( y ) b r : n ′ ( y ) d y .

{\displaystyle \mathbb {E} \{X_{r:n}\}=\int _{\mathbb {R} }xb_{r:n}'{\bigl (}F_{X}(x){\bigr )}\,dF_{X}(x)=\int _{0}^{1}Q_{X}(y)b_{r:n}'(y)\,dy.} Since the L-moments are linear combinations of such expectations, the corresponding integrals can be combined into one for each moment, where the integrand is Q X ( y ) {\displaystyle Q_{X}(y)} times a polynomial. We have [ 1 ] λ λ n = ∫ ∫ 0 1 Q X ( y ) P ~ ~ n − − 1 ( y ) d y {\displaystyle \lambda _{n}=\int _{0}^{1}Q_{X}(y){\widetilde {P}}_{n-1}(y)\,dy} where P ~ ~ m ( y ) = ∑ ∑ k = 0 m ( − − 1 ) m − − k ( m k ) ( m + k k ) y k {\displaystyle {\widetilde {P}}_{m}(y)=\sum _{k=0}^{m}(-1)^{m-k}{\binom {m}{k}}{\binom {m+k}{k}}y^{k}} are the shifted Legendre polynomials , orthogonal on [0,1] .

In particular λ λ 1 = ∫ ∫ 0 1 Q X ( y ) d y , λ λ 2 = ∫ ∫ 0 1 Q X ( y ) ( 2 y − − 1 ) d y , λ λ 3 = ∫ ∫ 0 1 Q X ( y ) ( 6 y 2 − − 6 y + 1 ) d y , λ λ 4 = ∫ ∫ 0 1 Q X ( y ) ( 20 y 3 − − 30 y 2 + 12 y − − 1 ) d y .

{\displaystyle {\begin{aligned}\lambda _{1}&=\int _{0}^{1}Q_{X}(y)\,dy,\\[2pt]\lambda _{2}&=\int _{0}^{1}Q_{X}(y)\left(2y-1\right)dy,\\[2pt]\lambda _{3}&=\int _{0}^{1}Q_{X}(y)\left(6y^{2}-6y+1\right)dy,\\[2pt]\lambda _{4}&=\int _{0}^{1}Q_{X}(y)\left(20y^{3}-30y^{2}+12y-1\right)dy.\end{aligned}}} Sillitto's Theorem [ edit ] The above integral formula for λ λ n {\displaystyle \lambda _{n}} has the form of a generalized Fourier coefficient , and they appeared as such in the literature years before being named moments. In the notation of this article, Sillitto [ 6 ] proved Theorem — Let X {\displaystyle X} be a real-valued continuous random variable with finite variance, quantile function Q X ( y ) {\displaystyle Q_{X}(y)} and L-moments { λ λ r } r = 1 ∞ ∞ {\displaystyle \{\lambda _{r}\}_{r=1}^{\infty }} . Then the representation Q X ( y ) = ∑ ∑ r = 1 ∞ ∞ ( 2 r − − 1 ) λ λ r P ~ ~ r − − 1 ( y ) for 0 < y < 1 {\displaystyle Q_{X}(y)=\sum _{r=1}^{\infty }(2r-1)\lambda _{r}{\widetilde {P}}_{r-1}(y)\qquad {\text{for }}0<y<1} is convergent in L 2 {\displaystyle L^{2}} norm .

However Hosking [ 1 ] cautions that partial sums of this series tend to give poor approximations for the tails of the distribution, and need not be monotonic. Similar problems arise with the Cornish–Fisher expansion of Q X {\displaystyle Q_{X}} in terms of the cumulants of X {\displaystyle X} .

Sample L-moments [ edit ] The sample L-moments can be computed as the population L-moments of the sample, summing over r -element subsets of the sample { x 1 < ⋯ ⋯ < x j < ⋯ ⋯ < x r } , {\displaystyle \left\{x_{1}<\cdots <x_{j}<\cdots <x_{r}\right\},} hence averaging by dividing by the binomial coefficient : λ λ r = 1 r ⋅ ⋅ ( n r ) ∑ ∑ x 1 < ⋯ ⋯ < x j < ⋯ ⋯ < x r ( − − 1 ) r − − j ( r − − 1 j ) x j .

{\displaystyle \lambda _{r}={\frac {1}{r\cdot {\tbinom {n}{r}}}}\,\sum _{x_{1}<\cdots <x_{j}<\cdots <x_{r}}(-1)^{r-j}{\binom {r-1}{j}}\,x_{j}\,.} Grouping these by order statistic counts the number of ways an element of an n element sample can be the j th element of an r element subset, and yields formulas of the form below. Direct estimators for the first four L-moments in a finite sample of n observations are: [ 7 ] ℓ ℓ 1 = 1 ( n 1 ) ∑ ∑ i = 1 n x ( i ) ℓ ℓ 2 = 1 2 ( n 2 ) ∑ ∑ i = 1 n [ ( i − − 1 1 ) − − ( n − − i 1 ) ] x ( i ) ℓ ℓ 3 = 1 3 ( n 3 ) ∑ ∑ i = 1 n [ ( i − − 1 2 ) − − 2 ( i − − 1 1 ) ( n − − i 1 ) + ( n − − i 2 ) ] x ( i ) ℓ ℓ 4 = 1 4 ( n 4 ) ∑ ∑ i = 1 n [ ( i − − 1 3 ) − − 3 ( i − − 1 2 ) ( n − − i 1 ) + 3 ( i − − 1 1 ) ( n − − i 2 ) − − ( n − − i 3 ) ] x ( i ) {\displaystyle {\begin{aligned}\ell _{1}&={\frac {1}{\tbinom {n}{1}}}\sum _{i=1}^{n}x_{(i)}\\[1ex]\ell _{2}&={\frac {1}{2{\tbinom {n}{2}}}}\sum _{i=1}^{n}\left[{\tbinom {i-1}{1}}-{\tbinom {n-i}{1}}\right]x_{(i)}\\[1ex]\ell _{3}&={\frac {1}{3{\tbinom {n}{3}}}}\sum _{i=1}^{n}\left[{\tbinom {i-1}{2}}-2{\tbinom {i-1}{1}}{\tbinom {n-i}{1}}+{\tbinom {n-i}{2}}\right]x_{(i)}\\[1ex]\ell _{4}&={\frac {1}{4{\tbinom {n}{4}}}}\sum _{i=1}^{n}\left[{\tbinom {i-1}{3}}-3{\tbinom {i-1}{2}}{\tbinom {n-i}{1}}+3{\tbinom {i-1}{1}}{\tbinom {n-i}{2}}-{\tbinom {n-i}{3}}\right]x_{(i)}\end{aligned}}} where x ( i ) is the i th order statistic and ( ⋅ ⋅ ⋅ ⋅ ) {\displaystyle {\tbinom {\boldsymbol {\cdot }}{\boldsymbol {\cdot }}}} is a binomial coefficient . Sample L-moments can also be defined indirectly in terms of probability weighted moments , [ 1 ] [ 8 ] [ 9 ] which leads to a more efficient algorithm for their computation.

[ 7 ] [ 10 ] L-moment ratios [ edit ] A set of L-moment ratios , or scaled L-moments, is defined by τ τ r = λ λ r / λ λ 2 , r = 3 , 4 , … … .

{\displaystyle \tau _{r}=\lambda _{r}/\lambda _{2},\qquad r=3,4,\dots ~.} The most useful of these are τ τ 3 , {\displaystyle \tau _{3},} called the L-skewness , and τ τ 4 , {\displaystyle \tau _{4},} the L-kurtosis .

L-moment ratios lie within the interval (−1, 1) .

Tighter bounds can be found for some specific L-moment ratios; in particular, the L-kurtosis τ τ 4 {\displaystyle \tau _{4}} lies in [−1 /4, 1) , and [ 1 ] 1 4 ( 5 τ τ 3 2 − − 1 ) ≤ ≤ τ τ 4 < 1 .

{\displaystyle {\tfrac {1}{4}}\left(5\tau _{3}^{2}-1\right)\leq \tau _{4}<1\,.} A quantity analogous to the coefficient of variation , but based on L-moments, can also be defined: τ τ = λ λ 2 / λ λ 1 , {\displaystyle \tau =\lambda _{2}/\lambda _{1}\,,} which is called the "coefficient of L-variation", or "L-CV". For a non-negative random variable, this lies in the interval (0, 1) [ 1 ] and is identical to the Gini coefficient .

[ 11 ] Related quantities [ edit ] L-moments are statistical quantities that are derived from probability weighted moments [ 12 ] (PWM) which were defined earlier (1979).

[ 8 ] PWM are used to efficiently estimate the parameters of distributions expressable in inverse form such as the Gumbel , [ 9 ] the Tukey lambda , and the Wakeby distributions.

Usage [ edit ] There are two common ways that L-moments are used, in both cases analogously to the conventional moments: As summary statistics for data.

To derive estimators for the parameters of probability distributions , applying the method of moments to the L-moments rather than conventional moments.

In addition to doing these with standard moments, the latter (estimation) is more commonly done using maximum likelihood methods; however using L-moments provides a number of advantages. Specifically, L-moments are more robust than conventional moments, and existence of higher L-moments only requires that the random variable have finite mean. One disadvantage of L-moment ratios for estimation is their typically smaller sensitivity. For instance, the Laplace distribution has a kurtosis of 6 and weak exponential tails, but a larger 4th L-moment ratio than e.g. the student-t distribution with d.f.=3, which has an infinite kurtosis and much heavier tails.

As an example consider a dataset with a few data points and one outlying data value. If the ordinary standard deviation of this data set is taken it will be highly influenced by this one point: however, if the L-scale is taken it will be far less sensitive to this data value. Consequently, L-moments are far more meaningful when dealing with outliers in data than conventional moments. However, there are also other better suited methods to achieve an even higher robustness than just replacing moments by L-moments. One example of this is using L-moments as summary statistics in extreme value theory (EVT). This application shows the limited robustness of L-moments, i.e. L-statistics are not resistant statistics , as a single extreme value can throw them off, but because they are only linear (not higher-order statistics ), they are less affected by extreme values than conventional moments.

Another advantage L-moments have over conventional moments is that their existence only requires the random variable to have finite mean, so the L-moments exist even if the higher conventional moments do not exist (for example, for Student's t distribution with low degrees of freedom ). A finite variance is required in addition in order for the standard errors of estimates of the L-moments to be finite.

[ 1 ] Some appearances of L-moments in the statistical literature include the book by David & Nagaraja (2003, Section 9.9) [ 13 ] and a number of papers.

[ 11 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] A number of favourable comparisons of L-moments with ordinary moments have been reported.

[ 19 ] [ 20 ] Values for some common distributions [ edit ] The table below gives expressions for the first two L moments and numerical values of the first two L-moment ratios of some common continuous probability distributions with constant L-moment ratios.

[ 1 ] [ 5 ] More complex expressions have been derived for some further distributions for which the L-moment ratios vary with one or more of the distributional parameters, including the log-normal , Gamma , generalized Pareto , generalized extreme value , and generalized logistic distributions.

[ 1 ] Distribution Parameters mean, λ 1 L-scale, λ 2 L-skewness, τ 3 L-kurtosis, τ 4 Uniform a , b ⁠ 1 / 2 ⁠ ( a + b ) ⁠ 1 / 6 ⁠ ( b – a ) 0 0 Logistic μ , s μ s 0 ⁠ 1 / 6 ⁠ = 0.1667 Normal μ , σ 2 μ ⁠ σ / √ π ⁠ 0 30 ⁠ θ m / π ⁠ - 9 = 0.1226 Laplace μ , b μ ⁠ 3 / 4 ⁠ b 0 ⁠ 1 / 3 √ 2 ⁠ = 0.2357 Student's t , 2 d.f.

ν = 2 0 ⁠ π / 2 √ 2 ⁠ = 1.111 0 ⁠ 3 / 8 ⁠ = 0.375 Student's t , 4 d.f.

ν = 4 0 ⁠ 15 / 64 ⁠ π = 0.7363 0 ⁠ 111 / 512 ⁠ = 0.2168 Exponential λ ⁠ 1 / λ ⁠ ⁠ 1 / 2 λ ⁠ ⁠ 1 / 3 ⁠ = 0.3333 ⁠ 1 / 6 ⁠ = 0.1667 Gumbel μ , β μ + γ e β β log 2 (3) 2 log 2 (3) − 3 = 0.1699 16 − 10 log 2 (3) = 0.1504 The notation for the parameters of each distribution is the same as that used in the linked article. In the expression for the mean of the Gumbel distribution , γ e is the Euler–Mascheroni constant 0.5772 1566 4901 ...

.

Extensions [ edit ] Trimmed L-moments are generalizations of L-moments that give zero weight to extreme observations. They are therefore more robust to the presence of outliers, and unlike L-moments they may be well-defined for distributions for which the mean does not exist, such as the Cauchy distribution .

[ 21 ] See also [ edit ] L-estimator References [ edit ] ^ a b c d e f g h i j Hosking, J.R.M. (1990). "L-moments: analysis and estimation of distributions using linear combinations of order statistics".

Journal of the Royal Statistical Society, Series B .

52 (1): 105– 124.

doi : 10.1111/j.2517-6161.1990.tb01775.x .

JSTOR 2345653 .

^ Hosking, J.R.M. (1992). "Moments or L moments? An example comparing two measures of distributional shape".

The American Statistician .

46 (3): 186– 189.

doi : 10.2307/2685210 .

JSTOR 2685210 .

^ Hosking, J.R.M. (2006). "On the characterization of distributions by their L-moments".

Journal of Statistical Planning and Inference .

136 : 193– 198.

doi : 10.1016/j.jspi.2004.06.004 .

^ Asquith, W.H. (2011) Distributional analysis with L-moment statistics using the R environment for statistical computing , Create Space Independent Publishing Platform, [print-on-demand], ISBN 1-463-50841-7 ^ a b Jones, M.C. (2002). "Student's simplest distribution".

Journal of the Royal Statistical Society, Series D .

51 (1): 41– 49.

doi : 10.1111/1467-9884.00297 .

JSTOR 3650389 .

^ Sillitto, G. P. (1969). "Derivation of approximants to the inverse distribution function of a continuous univariate population from the order statistics of a sample".

Biometrika .

56 (3): 641– 650.

doi : 10.1093/biomet/56.3.641 .

^ a b Wang, Q.J. (1996). "Direct sample estimators of L-moments".

Water Resources Research .

32 (12): 3617– 3619.

Bibcode : 1996WRR....32.3617W .

doi : 10.1029/96WR02675 .

^ a b Greenwood, J.A.; Landwehr, J.M.; Matalas, N.C.; Wallis, J.R. (1979).

"Probability weighted moments: Definition and relation to parameters of several distributions expressed in inverse form" (PDF) .

Water Resources Research .

15 (5): 1049– 1054.

doi : 10.1029/WR015i005p01049 .

S2CID 121955257 . Archived from the original (PDF) on 2020-02-10.

^ a b Landwehr, J.M.; Matalas, N.C.; Wallis, J.R. (1979). "Probability weighted moments compared with some traditional techniques in estimating Gumbel parameters and quantiles".

Water Resources Research .

15 (5): 1055– 1064.

Bibcode : 1979WRR....15.1055L .

doi : 10.1029/WR015i005p01055 .

^ "L moments" . NIST Dataplot.

itl.nist.gov (documentation).

National Institute of Standards and Technology . 6 January 2006 . Retrieved 19 January 2013 .

^ a b Valbuena, R.; Maltamo, M.; Mehtätalo, L.; Packalen, P. (2017).

"Key structural features of Boreal forests may be detected directly using L-moments from airborne lidar data" .

Remote Sensing of Environment .

194 : 437– 446.

Bibcode : 2017RSEnv.194..437V .

doi : 10.1016/j.rse.2016.10.024 .

^ Hosking, JRM; Wallis, JR (2005).

Regional Frequency Analysis: An Approach Based on L-moments . Cambridge University Press. p. 3.

ISBN 978-0521019408 . Retrieved 22 January 2013 .

^ David, H. A.; Nagaraja, H. N. (2003).

Order Statistics (3rd ed.). Wiley.

ISBN 978-0-471-38926-2 .

^ Serfling, R.; Xiao, P. (2007). "A contribution to multivariate L-moments: L-comoment matrices".

Journal of Multivariate Analysis .

98 (9): 1765– 1781.

CiteSeerX 10.1.1.62.4288 .

doi : 10.1016/j.jmva.2007.01.008 .

^ Delicado, P.; Goria, M. N. (2008). "A small sample comparison of maximum likelihood, moments and L-moments methods for the asymmetric exponential power distribution".

Computational Statistics & Data Analysis .

52 (3): 1661– 1673.

doi : 10.1016/j.csda.2007.05.021 .

^ Alkasasbeh, M. R.; Raqab, M. Z. (2009). "Estimation of the generalized logistic distribution parameters: comparative study".

Statistical Methodology .

6 (3): 262– 279.

doi : 10.1016/j.stamet.2008.10.001 .

^ Jones, M. C. (2004). "On some expressions for variance, covariance, skewness and L-moments".

Journal of Statistical Planning and Inference .

126 (1): 97– 106.

doi : 10.1016/j.jspi.2003.09.001 .

^ Jones, M. C. (2009). "Kumaraswamy's distribution: A beta-type distribution with some tractability advantages".

Statistical Methodology .

6 (1): 70– 81.

doi : 10.1016/j.stamet.2008.04.001 .

^ Royston, P. (1992). "Which measures of skewness and kurtosis are best?".

Statistics in Medicine .

11 (3): 333– 343.

doi : 10.1002/sim.4780110306 .

PMID 1609174 .

^ Ulrych, T. J.; Velis, D. R.; Woodbury, A. D.; Sacchi, M. D. (2000).

"L-moments and C-moments" .

Stochastic Environmental Research and Risk Assessment .

14 (1): 50– 68.

Bibcode : 2000SERRA..14...50U .

doi : 10.1007/s004770050004 .

S2CID 120542594 .

^ Elamir, Elsayed A. H.; Seheult, Allan H. (2003). "Trimmed L-moments".

Computational Statistics & Data Analysis .

43 (3): 299– 314.

doi : 10.1016/S0167-9473(02)00250-5 .

External links [ edit ] The L-moments page Jonathan R.M. Hosking, IBM Research L Moments.

Dataplot reference manual, vol. 1, auxiliary chapter.

National Institute of Standards and Technology , 2006. Accessed 2010-05-25.

Lmo lightweight Python includes functions for fast calculation of L-moments, trimmed L-moments, and multivariate L-comoments.

v t e Theory of probability distributions probability mass function (pmf) probability density function (pdf) cumulative distribution function (cdf) quantile function raw moment central moment mean variance standard deviation skewness kurtosis L-moment moment-generating function (mgf) characteristic function probability-generating function (pgf) cumulant combinant v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=L-moment&oldid=1301873326 " Categories : Moments (mathematics) Summary statistics Hidden categories: Articles with short description Short description matches Wikidata Use American English from January 2019 All Wikipedia articles written in American English This page was last edited on 22 July 2025, at 05:07 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents L-moment 2 languages Add topic

