Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Geometrical description Toggle Geometrical description subsection 1.1 Purpose and why it works 1.2 First-order process 1.3 Higher-order process 2 First-order example Toggle First-order example subsection 2.1 Using step size equal to 1 ( h = 1 ) 2.2 Using other step sizes 3 Higher-order example 4 Derivation 5 Local truncation error 6 Global truncation error Toggle Global truncation error subsection 6.1 Example 7 Numerical stability 8 Rounding errors 9 Modifications and extensions 10 In popular culture 11 See also 12 Notes 13 References 14 External links Toggle the table of contents Euler method 30 languages العربية Azərbaycanca Català Čeština Dansk Deutsch Ελληνικά Español Euskara Français 한국어 Íslenska Italiano עברית Latviešu Монгол Nederlands 日本語 Norsk bokmål Polski Português Русский Suomi Svenska Tagalog Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Euler approximations ) Approach to finding numerical solutions of ordinary differential equations For integrating with respect to the Euler characteristic, see Euler calculus . For Euler's method for factorizing an integer, see Euler's factorization method .

(Figure 1) Illustration of the Euler method. The unknown curve is in blue, and its polygonal approximation is in red.

Differential equations Scope Fields Natural sciences Engineering Astronomy Physics Chemistry Biology Geology Applied mathematics Continuum mechanics Chaos theory Dynamical systems Social sciences Economics Population dynamics List of named differential equations Classification Types Ordinary Partial Differential-algebraic Integro-differential Fractional Linear Non-linear By variable type Dependent and independent variables Autonomous Coupled / Decoupled Exact Homogeneous / Nonhomogeneous Features Order Operator Notation Relation to processes Difference (discrete analogue) Stochastic Stochastic partial Delay Solution Existence and uniqueness Picard–Lindelöf theorem Peano existence theorem Carathéodory's existence theorem Cauchy–Kowalevski theorem General topics Initial conditions Boundary values Dirichlet Neumann Robin Cauchy problem Wronskian Phase portrait Lyapunov / Asymptotic / Exponential stability Rate of convergence Series / Integral solutions Numerical integration Dirac delta function Solution methods Inspection Method of characteristics Euler Exponential response formula Finite difference ( Crank–Nicolson ) Finite element Infinite element Finite volume Galerkin Petrov–Galerkin Green's function Integrating factor Integral transforms Perturbation theory Runge–Kutta Separation of variables Undetermined coefficients Variation of parameters People List Isaac Newton Gottfried Leibniz Jacob Bernoulli Leonhard Euler Joseph-Louis Lagrange Józef Maria Hoene-Wroński Joseph Fourier Augustin-Louis Cauchy George Green Carl David Tolmé Runge Martin Kutta Rudolf Lipschitz Ernst Lindelöf Émile Picard Phyllis Nicolson John Crank v t e In mathematics and computational science , the Euler method (also called the forward Euler method ) is a first-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value . It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method . The Euler method is named after Leonhard Euler , who first proposed it in his book Institutionum calculi integralis (published 1768–1770).

[ 1 ] The Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, and the global error (error at a given time) is proportional to the step size. 
The Euler method often serves as the basis to construct more complex methods, e.g., predictor–corrector method .

Geometrical description [ edit ] Purpose and why it works [ edit ] Consider the problem of calculating the shape of an unknown curve which starts at a given point and satisfies a given differential equation. Here, a differential equation can be thought of as a formula by which the slope of the tangent line to the curve can be computed at any point on the curve, once the position of that point has been calculated.

The idea is that while the curve is initially unknown, its starting point, which we denote by A 0 , {\displaystyle A_{0},} is known (see Figure 1). Then, from the differential equation, the slope to the curve at A 0 {\displaystyle A_{0}} can be computed, and so, the tangent line.

Take a small step along that tangent line  up to a point A 1 .

{\displaystyle A_{1}.} Along this small step, the slope does not change too much, so A 1 {\displaystyle A_{1}} will be close to the curve. If we pretend that A 1 {\displaystyle A_{1}} is still on the curve, the same reasoning as for the point A 0 {\displaystyle A_{0}} above can be used. After several steps, a polygonal curve ( A 0 , A 1 , A 2 , A 3 , … … {\displaystyle A_{0},A_{1},A_{2},A_{3},\dots } ) is computed. In general, this curve does not diverge too far from the original unknown curve, and the error between the two curves can be made small if the step size is small enough and the interval of computation is finite.

[ 2 ] First-order process [ edit ] When given the values for t 0 {\displaystyle t_{0}} and y ( t 0 ) {\displaystyle y(t_{0})} , and the derivative of y {\displaystyle y} is a given function of t {\displaystyle t} and y {\displaystyle y} denoted as y ′ ( t ) = f ( t , y ( t ) ) {\displaystyle y'(t)=f{\bigl (}t,y(t){\bigr )}} . Begin the process by setting y 0 = y ( t 0 ) {\displaystyle y_{0}=y(t_{0})} . Next, choose a value h {\displaystyle h} for the size of every step along t-axis, and set t n = t 0 + n h {\displaystyle t_{n}=t_{0}+nh} (or equivalently t n + 1 = t n + h {\displaystyle t_{n+1}=t_{n}+h} ). Now, the Euler method is used to find y n + 1 {\displaystyle y_{n+1}} from y n {\displaystyle y_{n}} and t n {\displaystyle t_{n}} : [ 3 ] y n + 1 = y n + h f ( t n , y n ) .

{\displaystyle y_{n+1}=y_{n}+hf(t_{n},y_{n}).} The value of y n {\displaystyle y_{n}} is an approximation of the solution at time t n {\displaystyle t_{n}} , i.e., y n ≈ ≈ y ( t n ) {\displaystyle y_{n}\approx y(t_{n})} . The Euler method is explicit , i.e. the solution y n + 1 {\displaystyle y_{n+1}} is an explicit function of y i {\displaystyle y_{i}} for i ≤ ≤ n {\displaystyle i\leq n} .

Higher-order process [ edit ] While the Euler method integrates a first-order ODE, any ODE of order N {\displaystyle N} can be represented as a system of first-order ODEs. When given the ODE of order N {\displaystyle N} defined as y ( N + 1 ) ( t ) = f ( t , y ( t ) , y ′ ( t ) , … … , y ( N ) ( t ) ) , {\displaystyle y^{(N+1)}(t)=f\left(t,y(t),y'(t),\ldots ,y^{(N)}(t)\right),} as well as h {\displaystyle h} , t 0 {\displaystyle t_{0}} , and y 0 , y 0 ′ , … … , y 0 ( N ) {\displaystyle y_{0},y'_{0},\dots ,y_{0}^{(N)}} , we implement the following formula until we reach the approximation of the solution to the ODE at the desired time: y → → i + 1 = ( y i + 1 y i + 1 ′ ⋮ ⋮ y i + 1 ( N − − 1 ) y i + 1 ( N ) ) = ( y i + h ⋅ ⋅ y i ′ y i ′ + h ⋅ ⋅ y i ″ ⋮ ⋮ y i ( N − − 1 ) + h ⋅ ⋅ y i ( N ) y i ( N ) + h ⋅ ⋅ f ( t i , y i , y i ′ , … … , y i ( N ) ) ) {\displaystyle {\vec {y}}_{i+1}={\begin{pmatrix}y_{i+1}\\y'_{i+1}\\\vdots \\y_{i+1}^{(N-1)}\\y_{i+1}^{(N)}\end{pmatrix}}={\begin{pmatrix}y_{i}+h\cdot y'_{i}\\y'_{i}+h\cdot y''_{i}\\\vdots \\y_{i}^{(N-1)}+h\cdot y_{i}^{(N)}\\y_{i}^{(N)}+h\cdot f\left(t_{i},y_{i},y'_{i},\ldots ,y_{i}^{(N)}\right)\end{pmatrix}}} These first-order systems can be handled by Euler's method or, in fact, by any other scheme for first-order systems.

[ 4 ] First-order example [ edit ] Given the initial value problem y ′ = y , y ( 0 ) = 1 , {\displaystyle y'=y,\quad y(0)=1,} we would like to use the Euler method to approximate y ( 4 ) {\displaystyle y(4)} .

[ 5 ] Using step size equal to 1 ( h = 1 ) [ edit ] (Figure 2) Illustration of numerical integration for the equation y ′ = y , y ( 0 ) = 1.

{\displaystyle y'=y,y(0)=1.} Blue is the Euler method; green, the midpoint method ; red, the exact solution, y = e t .

{\displaystyle y=e^{t}.} The step size is h = 1.0.

{\displaystyle h=1.0.} The Euler method is y n + 1 = y n + h f ( t n , y n ) .

{\displaystyle y_{n+1}=y_{n}+hf(t_{n},y_{n}).} so first we must compute f ( t 0 , y 0 ) {\displaystyle f(t_{0},y_{0})} .  In this simple differential equation, the function f {\displaystyle f} is defined by f ( t , y ) = y {\displaystyle f(t,y)=y} . We have f ( t 0 , y 0 ) = f ( 0 , 1 ) = 1.

{\displaystyle f(t_{0},y_{0})=f(0,1)=1.} By doing the above step, we have found the slope of the line that is tangent to the solution curve at the point ( 0 , 1 ) {\displaystyle (0,1)} .  Recall that the slope is defined as the change in y {\displaystyle y} divided by the change in t {\displaystyle t} , or Δ Δ y Δ Δ t {\textstyle {\frac {\Delta y}{\Delta t}}} .

The next step is to multiply the above value by the step size h {\displaystyle h} , which we take equal to one here: h ⋅ ⋅ f ( y 0 ) = 1 ⋅ ⋅ 1 = 1.

{\displaystyle h\cdot f(y_{0})=1\cdot 1=1.} Since the step size is the change in t {\displaystyle t} , when we multiply the step size and the slope of the tangent, we get a change in y {\displaystyle y} value.  This value is then added to the initial y {\displaystyle y} value to obtain the next value to be used for computations.

y 0 + h f ( y 0 ) = y 1 = 1 + 1 ⋅ ⋅ 1 = 2.

{\displaystyle y_{0}+hf(y_{0})=y_{1}=1+1\cdot 1=2.} The above steps should be repeated to find y 2 {\displaystyle y_{2}} , y 3 {\displaystyle y_{3}} and y 4 {\displaystyle y_{4}} .

y 2 = y 1 + h f ( y 1 ) = 2 + 1 ⋅ ⋅ 2 = 4 , y 3 = y 2 + h f ( y 2 ) = 4 + 1 ⋅ ⋅ 4 = 8 , y 4 = y 3 + h f ( y 3 ) = 8 + 1 ⋅ ⋅ 8 = 16.

{\displaystyle {\begin{aligned}y_{2}&=y_{1}+hf(y_{1})=2+1\cdot 2=4,\\y_{3}&=y_{2}+hf(y_{2})=4+1\cdot 4=8,\\y_{4}&=y_{3}+hf(y_{3})=8+1\cdot 8=16.\end{aligned}}} Due to the repetitive nature of this algorithm, it can be helpful to organize computations in a chart form, as seen below, to avoid making errors.

n {\displaystyle n} y n {\displaystyle y_{n}} t n {\displaystyle t_{n}} f ( t n , y n ) {\displaystyle f(t_{n},y_{n})} h {\displaystyle h} Δ Δ y {\displaystyle \Delta y} y n + 1 {\displaystyle y_{n+1}} 0 1 0 1 1 1 2 1 2 1 2 1 2 4 2 4 2 4 1 4 8 3 8 3 8 1 8 16 The conclusion of this computation is that y 4 = 16 {\displaystyle y_{4}=16} . The exact solution of the differential equation is y ( t ) = e t {\displaystyle y(t)=e^{t}} , so y ( 4 ) = e 4 ≈ ≈ 54.598 {\displaystyle y(4)=e^{4}\approx 54.598} . Although the approximation of the Euler method was not very precise in this specific case, particularly due to a large value step size h {\displaystyle h} , its behaviour is qualitatively correct as the figure shows.

Using other step sizes [ edit ] (Figure 3) The same illustration for h = 0.25.

{\displaystyle h=0.25.} As suggested in the introduction, the Euler method is more accurate if the step size h {\displaystyle h} is smaller. The table below shows the result with different step sizes. The top row corresponds to the example in the previous section, and the second row is illustrated in the figure.

step size result of Euler's method error 1 16.00 38.60 0.25 35.53 19.07 0.1 45.26 0 9.34 0.05 49.56 0 5.04 0.025 51.98 0 2.62 0.0125 53.26 0 1.34 The error recorded in the last column of the table is the difference between the exact solution at t = 4 {\displaystyle t=4} and the Euler approximation. In the bottom of the table, the step size is half the step size in the previous row, and the error is also approximately half the error in the previous row. This suggests that the error is roughly proportional to the step size, at least for fairly small values of the step size. This is true in general, also for other equations; see the section Global truncation error for more details.

Other methods, such as the midpoint method also illustrated in the figures, behave more favourably: the global error of the midpoint method is roughly proportional to the square of the step size. For this reason, the Euler method is said to be a first-order method, while the midpoint method is second order.

We can extrapolate from the above table that the step size needed to get an answer that is correct to three decimal places is approximately 0.00001, meaning that we need 400,000 steps. This large number of steps entails a high computational cost. For this reason, higher-order methods are employed such as Runge–Kutta methods or linear multistep methods , especially if a high accuracy is desired.

[ 6 ] Higher-order example [ edit ] For this third-order example, assume that the following information is given: y ‴ + 4 t y ″ − − t 2 y ′ − − ( cos ⁡ ⁡ t ) y = sin ⁡ ⁡ t t 0 = 0 y 0 = y ( t 0 ) = 2 y 0 ′ = y ′ ( t 0 ) = − − 1 y 0 ″ = y ″ ( t 0 ) = 3 h = 0.5 {\displaystyle {\begin{aligned}&y'''+4ty''-t^{2}y'-(\cos {t})y=\sin {t}\\&t_{0}=0\\&y_{0}=y(t_{0})=2\\&y'_{0}=y'(t_{0})=-1\\&y''_{0}=y''(t_{0})=3\\&h=0.5\end{aligned}}} From this we can isolate y''' to get the equation: f ( t , y , y ′ , y ″ ) = y ‴ = sin ⁡ ⁡ t + ( cos ⁡ ⁡ t ) y + t 2 y ′ − − 4 t y ″ {\displaystyle f\left(t,y,y',y''\right)=y'''=\sin {t}+(\cos {t})y+t^{2}y'-4ty''} Using that we can get the solution for y → → 1 {\displaystyle {\vec {y}}_{1}} : y → → 1 = ( y 1 y 1 ′ y 1 ″ ) = ( y 0 + h ⋅ ⋅ y 0 ′ y 0 ′ + h ⋅ ⋅ y 0 ″ y 0 ″ + h ⋅ ⋅ f ( t 0 , y 0 , y 0 ′ , y 0 ″ ) ) = ( 2 + 0.5 ⋅ ⋅ − − 1 − − 1 + 0.5 ⋅ ⋅ 3 3 + 0.5 ⋅ ⋅ ( sin ⁡ ⁡ 0 + ( cos ⁡ ⁡ 0 ) ⋅ ⋅ 2 + 0 2 ⋅ ⋅ ( − − 1 ) − − 4 ⋅ ⋅ 0 ⋅ ⋅ 3 ) ) = ( 1.5 0.5 4 ) {\displaystyle {\vec {y}}_{1}={\begin{pmatrix}y_{1}\\y_{1}'\\y_{1}''\end{pmatrix}}={\begin{pmatrix}y_{0}+h\cdot y'_{0}\\y'_{0}+h\cdot y''_{0}\\y''_{0}+h\cdot f\left(t_{0},y_{0},y'_{0},y''_{0}\right)\end{pmatrix}}={\begin{pmatrix}2+0.5\cdot -1\\-1+0.5\cdot 3\\3+0.5\cdot \left(\sin {0}+(\cos {0})\cdot 2+0^{2}\cdot (-1)-4\cdot 0\cdot 3\right)\end{pmatrix}}={\begin{pmatrix}1.5\\0.5\\4\end{pmatrix}}} And using the solution for y → → 1 {\displaystyle {\vec {y}}_{1}} , we can get the solution for y → → 2 {\displaystyle {\vec {y}}_{2}} : y → → 2 = ( y 2 y 2 ′ y 2 ″ ) = ( y 1 + h ⋅ ⋅ y 1 ′ y 1 ′ + h ⋅ ⋅ y 1 ″ y 1 ″ + h ⋅ ⋅ f ( t 1 , y 1 , y 1 ′ , y 1 ″ ) ) = ( 1.5 + 0.5 ⋅ ⋅ 0.5 0.5 + 0.5 ⋅ ⋅ 4 4 + 0.5 ⋅ ⋅ ( sin ⁡ ⁡ 0.5 + ( cos ⁡ ⁡ 0.5 ) ⋅ ⋅ 1.5 + 0.5 2 ⋅ ⋅ 0.5 − − 4 ⋅ ⋅ 0.5 ⋅ ⋅ 4 ) ) = ( 1.75 2.5 0.9604...

) {\displaystyle {\vec {y}}_{2}={\begin{pmatrix}y_{2}\\y_{2}'\\y_{2}''\end{pmatrix}}={\begin{pmatrix}y_{1}+h\cdot y'_{1}\\y'_{1}+h\cdot y''_{1}\\y''_{1}+h\cdot f\left(t_{1},y_{1},y'_{1},y''_{1}\right)\end{pmatrix}}={\begin{pmatrix}1.5+0.5\cdot 0.5\\0.5+0.5\cdot 4\\4+0.5\cdot \left(\sin {0.5}+(\cos {0.5})\cdot 1.5+0.5^{2}\cdot 0.5-4\cdot 0.5\cdot 4\right)\end{pmatrix}}={\begin{pmatrix}1.75\\2.5\\0.9604...\end{pmatrix}}} We can continue this process using the same formula as long as necessary to find whichever y → → i {\displaystyle {\vec {y}}_{i}} desired.

Derivation [ edit ] The Euler method can be derived in a number of ways.

(1) Firstly, there is the geometrical description above.

(2) Another possibility is to consider the Taylor expansion of the function y {\displaystyle y} around t 0 {\displaystyle t_{0}} : y ( t 0 + h ) = y ( t 0 ) + h y ′ ( t 0 ) + 1 2 h 2 y ″ ( t 0 ) + O ( h 3 ) .

{\displaystyle y(t_{0}+h)=y(t_{0})+hy'(t_{0})+{\tfrac {1}{2}}h^{2}y''(t_{0})+O\left(h^{3}\right).} The differential equation states that y ′ = f ( t , y ) {\displaystyle y'=f(t,y)} . If this is substituted in the Taylor expansion and the quadratic and higher-order terms are ignored, the Euler method arises.

[ 7 ] The Taylor expansion is used below to analyze the error committed by the Euler method, and it can be extended to produce Runge–Kutta methods .

(3) A closely related derivation is to substitute the forward finite difference formula for the derivative, y ′ ( t 0 ) ≈ ≈ y ( t 0 + h ) − − y ( t 0 ) h {\displaystyle y'(t_{0})\approx {\frac {y(t_{0}+h)-y(t_{0})}{h}}} in the differential equation y ′ = f ( t , y ) {\displaystyle y'=f(t,y)} . Again, this yields the Euler method.

[ 8 ] A similar computation leads to the midpoint method and the backward Euler method .

(4) Finally, one can integrate the differential equation from t 0 {\displaystyle t_{0}} to t 0 + h {\displaystyle t_{0}+h} and apply the fundamental theorem of calculus to get: y ( t 0 + h ) − − y ( t 0 ) = ∫ ∫ t 0 t 0 + h f ( t , y ( t ) ) d t .

{\displaystyle y(t_{0}+h)-y(t_{0})=\int _{t_{0}}^{t_{0}+h}f{\bigl (}t,y(t){\bigr )}\,\mathrm {d} t.} Now approximate the integral by the left-hand rectangle method (with only one rectangle): ∫ ∫ t 0 t 0 + h f ( t , y ( t ) ) d t ≈ ≈ h f ( t 0 , y ( t 0 ) ) .

{\displaystyle \int _{t_{0}}^{t_{0}+h}f{\bigl (}t,y(t){\bigr )}\,\mathrm {d} t\approx hf{\bigl (}t_{0},y(t_{0}){\bigr )}.} Combining both equations, one finds again the Euler method.

[ 9 ] This line of thought can be continued to arrive at various linear multistep methods .

Local truncation error [ edit ] The local truncation error of the Euler method is the error made in a single step. It is the difference between the numerical solution after one step, y 1 {\displaystyle y_{1}} , and the exact solution at time t 1 = t 0 + h {\displaystyle t_{1}=t_{0}+h} . The numerical solution is given by y 1 = y 0 + h f ( t 0 , y 0 ) .

{\displaystyle y_{1}=y_{0}+hf(t_{0},y_{0}).} For the exact solution, we use the Taylor expansion mentioned in the section Derivation above: y ( t 0 + h ) = y ( t 0 ) + h y ′ ( t 0 ) + 1 2 h 2 y ″ ( t 0 ) + O ( h 3 ) .

{\displaystyle y(t_{0}+h)=y(t_{0})+hy'(t_{0})+{\tfrac {1}{2}}h^{2}y''(t_{0})+O\left(h^{3}\right).} The local truncation error (LTE) introduced by the Euler method is given by the difference between these equations: L T E = y ( t 0 + h ) − − y 1 = 1 2 h 2 y ″ ( t 0 ) + O ( h 3 ) .

{\displaystyle \mathrm {LTE} =y(t_{0}+h)-y_{1}={\tfrac {1}{2}}h^{2}y''(t_{0})+O\left(h^{3}\right).} This result is valid if y {\displaystyle y} has a bounded third derivative.

[ 10 ] This shows that for small h {\displaystyle h} , the local truncation error is approximately proportional to h 2 {\displaystyle h^{2}} . This makes the Euler method less accurate than higher-order techniques such as Runge–Kutta methods and linear multistep methods , for which the local truncation error is proportional to a higher power of the step size.

A slightly different formulation for the local truncation error can be obtained by using the Lagrange form for the remainder term in Taylor's theorem . If y {\displaystyle y} has a continuous second derivative, then there exists a ξ ξ ∈ ∈ [ t 0 , t 0 + h ] {\displaystyle \xi \in [t_{0},t_{0}+h]} such that L T E = y ( t 0 + h ) − − y 1 = 1 2 h 2 y ″ ( ξ ξ ) .

{\displaystyle \mathrm {LTE} =y(t_{0}+h)-y_{1}={\tfrac {1}{2}}h^{2}y''(\xi ).} [ 11 ] In the above expressions for the error, the second derivative of the unknown exact solution y {\displaystyle y} can be replaced by an expression involving the right-hand side of the differential equation. Indeed, it follows from the equation y ′ = f ( t , y ) {\displaystyle y'=f(t,y)} that [ 12 ] y ″ ( t 0 ) = ∂ ∂ f ∂ ∂ t ( t 0 , y ( t 0 ) ) + ∂ ∂ f ∂ ∂ y ( t 0 , y ( t 0 ) ) f ( t 0 , y ( t 0 ) ) .

{\displaystyle y''(t_{0})={\frac {\partial f}{\partial t}}{\bigl (}t_{0},y(t_{0}){\bigr )}+{\frac {\partial f}{\partial y}}{\bigl (}t_{0},y(t_{0}){\bigr )}\,f{\bigl (}t_{0},y(t_{0}){\bigr )}.} Global truncation error [ edit ] The global truncation error is the error at a fixed time t i {\displaystyle t_{i}} , after however many steps the method needs to take to reach that time from the initial time. The global truncation error is the cumulative effect of the local truncation errors committed in each step.

[ 13 ] The number of steps is easily determined to be t i − − t 0 h {\textstyle {\frac {t_{i}-t_{0}}{h}}} , which is proportional to 1 h {\textstyle {\frac {1}{h}}} , and the error committed in each step is proportional to h 2 {\displaystyle h^{2}} (see the previous section). Thus, it is to be expected that the global truncation error will be proportional to h {\displaystyle h} .

[ 14 ] This intuitive reasoning can be made precise. If the solution y {\displaystyle y} has a bounded second derivative and f {\displaystyle f} is Lipschitz continuous in its second argument, then the global truncation error (denoted as | y ( t i ) − − y i | {\displaystyle |y(t_{i})-y_{i}|} ) is bounded by | y ( t i ) − − y i | ≤ ≤ h M 2 L ( e L ( t i − − t 0 ) − − 1 ) {\displaystyle |y(t_{i})-y_{i}|\leq {\frac {hM}{2L}}\left(e^{L(t_{i}-t_{0})}-1\right)} where M {\displaystyle M} is an upper bound on the second derivative of y {\displaystyle y} on the given interval and L {\displaystyle L} is the Lipschitz constant of f {\displaystyle f} .

[ 15 ] Or more simply, when y ′ ( t ) = f ( t , y ) {\displaystyle y'(t)=f(t,y)} , the value L = max ( | d d y [ f ( t , y ) ] | ) {\textstyle L={\text{max}}{\bigl (}|{\frac {d}{dy}}{\bigl [}f(t,y){\bigr ]}|{\bigr )}} (such that t {\displaystyle t} is treated as a constant). In contrast, M = max ( | d 2 d t 2 [ y ( t ) ] | ) {\textstyle M={\text{max}}{\bigl (}|{\frac {d^{2}}{dt^{2}}}{\bigl [}y(t){\bigr ]}|{\bigr )}} where function y ( t ) {\displaystyle y(t)} is the exact solution which only contains the t {\displaystyle t} variable.

The precise form of this bound is of little practical importance, as in most cases the bound vastly overestimates the actual error committed by the Euler method.

[ 16 ] What is important is that it shows that the global truncation error is (approximately) proportional to h {\displaystyle h} . For this reason, the Euler method is said to be first order.

[ 17 ] Example [ edit ] If we have the differential equation y ′ = 1 + ( t − − y ) 2 {\displaystyle y'=1+(t-y)^{2}} , and the exact solution y = t + 1 t − − 1 {\displaystyle y=t+{\frac {1}{t-1}}} , and we want to find M {\displaystyle M} and L {\displaystyle L} for when 2 ≤ ≤ t ≤ ≤ 3 {\displaystyle 2\leq t\leq 3} .

L = max ( | d d y [ f ( t , y ) ] | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | d d y [ 1 + ( t − − y ) 2 ] | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | 2 ( t − − y ) | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | 2 ( t − − [ t + 1 t − − 1 ] ) | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | − − 2 t − − 1 | ) = 2 {\displaystyle L={\text{max}}{\bigl (}|{\frac {d}{dy}}{\bigl [}f(t,y){\bigr ]}|{\bigr )}=\max _{2\leq t\leq 3}{\bigl (}|{\frac {d}{dy}}{\bigl [}1+(t-y)^{2}{\bigr ]}|{\bigr )}=\max _{2\leq t\leq 3}{\bigl (}|2(t-y)|{\bigr )}=\max _{2\leq t\leq 3}{\bigl (}|2(t-[t+{\frac {1}{t-1}}])|{\bigr )}=\max _{2\leq t\leq 3}{\bigl (}|-{\frac {2}{t-1}}|{\bigr )}=2} M = max ( | d 2 d t 2 [ y ( t ) ] | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | d 2 d t 2 [ t + 1 1 − − t ] | ) = max 2 ≤ ≤ t ≤ ≤ 3 ( | 2 ( − − t + 1 ) 3 | ) = 2 {\displaystyle M={\text{max}}{\bigl (}|{\frac {d^{2}}{dt^{2}}}{\bigl [}y(t){\bigr ]}|{\bigr )}=\max _{2\leq t\leq 3}\left(|{\frac {d^{2}}{dt^{2}}}{\bigl [}t+{\frac {1}{1-t}}{\bigr ]}|\right)=\max _{2\leq t\leq 3}\left(|{\frac {2}{(-t+1)^{3}}}|\right)=2} Thus we can find the error bound at t=2.5 and h=0.5: error bound = h M 2 L ( e L ( t i − − t 0 ) − − 1 ) = 0.5 ⋅ ⋅ 2 2 ⋅ ⋅ 2 ( e 2 ( 2.5 − − 2 ) − − 1 ) = 0.42957 {\displaystyle {\text{error bound}}={\frac {hM}{2L}}\left(e^{L(t_{i}-t_{0})}-1\right)={\frac {0.5\cdot 2}{2\cdot 2}}\left(e^{2(2.5-2)}-1\right)=0.42957} Notice that t 0 is equal to 2 because it is the lower bound for t in 2 ≤ ≤ t ≤ ≤ 3 {\displaystyle 2\leq t\leq 3} .

Numerical stability [ edit ] (Figure 4) Solution of y ′ = − − 2.3 y {\displaystyle y'=-2.3y} computed with the Euler method with step size h = 1 {\displaystyle h=1} (blue squares) and h = 0.7 {\displaystyle h=0.7} (red circles). The black curve shows the exact solution.

The Euler method can also be numerically unstable , especially for stiff equations , meaning that the numerical solution grows very large for equations where the exact solution does not. This can be illustrated using the linear equation y ′ = − − 2.3 y , y ( 0 ) = 1.

{\displaystyle y'=-2.3y,\qquad y(0)=1.} The exact solution is y ( t ) = e − − 2.3 t {\displaystyle y(t)=e^{-2.3t}} , which decays to zero as t → → ∞ ∞ {\displaystyle t\to \infty } . However, if the Euler method is applied to this equation with step size h = 1 {\displaystyle h=1} , then the numerical solution is qualitatively wrong: It oscillates and grows (see the figure). This is what it means to be unstable. If a smaller step size is used, for instance h = 0.7 {\displaystyle h=0.7} , then the numerical solution does decay to zero.

(Figure 5) The pink disk shows the stability region for the Euler method.

If the Euler method is applied to the linear equation y ′ = k y {\displaystyle y'=ky} , then the numerical solution is unstable if the product h k {\displaystyle hk} is outside the region { z ∈ ∈ C | | z + 1 | ≤ ≤ 1 } , {\displaystyle {\bigl \{}z\in \mathbf {C} \,{\big |}\,|z+1|\leq 1{\bigr \}},} illustrated on the right. This region is called the (linear) stability region .

[ 18 ] In the example, k = − − 2.3 {\displaystyle k=-2.3} , so if h = 1 {\displaystyle h=1} then h k = − − 2.3 {\displaystyle hk=-2.3} which is outside the stability region, and thus the numerical solution is unstable.

This limitation — along with its slow convergence of error with h {\displaystyle h} — means that the Euler method is not often used, except as a simple example of numerical integration [ citation needed ] . Frequently models of physical systems contain terms representing fast-decaying elements (i.e. with large negative exponential arguments). Even when these are not of interest in the overall solution, the instability they can induce means that an exceptionally small timestep would be required if the Euler method is used.

Rounding errors [ edit ] In step n {\displaystyle n} of the Euler method, the rounding error is roughly of the magnitude ε ε y n {\displaystyle \varepsilon y_{n}} where ε ε {\displaystyle \varepsilon } is the machine epsilon . Assuming that the rounding errors are independent random variables, the expected total rounding error is proportional to ε ε h {\textstyle {\frac {\varepsilon }{\sqrt {h}}}} .

[ 19 ] Thus, for extremely small values of the step size the truncation error will be small but the effect of rounding error may be big. Most of the effect of rounding error can be easily avoided if compensated summation is used in the formula for the Euler method.

[ 20 ] Modifications and extensions [ edit ] A simple modification of the Euler method which eliminates the stability problems noted above is the backward Euler method : y n + 1 = y n + h f ( t n + 1 , y n + 1 ) .

{\displaystyle y_{n+1}=y_{n}+hf(t_{n+1},y_{n+1}).} This differs from the (standard, or forward) Euler method in that the function f {\displaystyle f} is evaluated at the end point of the step, instead of the starting point. The backward Euler method is an implicit method , meaning that the formula for the backward Euler method has y n + 1 {\displaystyle y_{n+1}} on both sides, so when applying the backward Euler method we have to solve an equation. This makes the implementation more costly.

Other modifications of the Euler method that help with stability yield the exponential Euler method or the semi-implicit Euler method .

More complicated methods can achieve a higher order (and more accuracy). One possibility is to use more function evaluations. This is illustrated by the midpoint method which is already mentioned in this article: y n + 1 = y n + h f ( t n + 1 2 h , y n + 1 2 h f ( t n , y n ) ) {\displaystyle y_{n+1}=y_{n}+hf\left(t_{n}+{\tfrac {1}{2}}h,y_{n}+{\tfrac {1}{2}}hf(t_{n},y_{n})\right)} .

This leads to the family of Runge–Kutta methods .

The other possibility is to use more past values, as illustrated by the two-step Adams–Bashforth method: y n + 1 = y n + 3 2 h f ( t n , y n ) − − 1 2 h f ( t n − − 1 , y n − − 1 ) .

{\displaystyle y_{n+1}=y_{n}+{\tfrac {3}{2}}hf(t_{n},y_{n})-{\tfrac {1}{2}}hf(t_{n-1},y_{n-1}).} This leads to the family of linear multistep methods . There are other modifications which uses techniques from compressive sensing to minimize memory usage [ 21 ] In popular culture [ edit ] In the film Hidden Figures , Katherine Johnson resorts to the Euler method in calculating the re-entry of astronaut John Glenn from Earth orbit.

[ 22 ] See also [ edit ] Crank–Nicolson method Gradient descent similarly uses finite steps, here to find minima of functions List of Runge–Kutta methods Linear multistep method Numerical integration (for calculating definite integrals) Numerical methods for ordinary differential equations Notes [ edit ] ^ Butcher 2003 , p. 45; Hairer, Nørsett & Wanner 1993 , p. 35 ^ Atkinson 1989 , p. 342; Butcher 2003 , p. 60 ^ Butcher 2003 , p. 45; Hairer, Nørsett & Wanner 1993 , p. 36 ^ Butcher 2003 , p. 3; Hairer, Nørsett & Wanner 1993 , p. 2 ^ See also Atkinson 1989 , p. 344 ^ Hairer, Nørsett & Wanner 1993 , p. 40 ^ Atkinson 1989 , p. 342; Hairer, Nørsett & Wanner 1993 , p. 36 ^ Atkinson 1989 , p. 342 ^ Atkinson 1989 , p. 343 ^ Butcher 2003 , p. 60 ^ Atkinson 1989 , p. 342 ^ Stoer & Bulirsch 2002 , p. 474 ^ Atkinson 1989 , p. 344 ^ Butcher 2003 , p. 49 ^ Atkinson 1989 , p. 346; Lakoba 2012 , equation (1.16) ^ Iserles 1996 , p. 7 ^ Butcher 2003 , p. 63 ^ Butcher 2003 , p. 70; Iserles 1996 , p. 57 ^ Butcher 2003 , pp. 74–75 ^ Butcher 2003 , pp. 75–78 ^ Unni, M. P.; Chandra, M. G.; Kumar, A. A. (March 2017). "Memory reduction for numerical solution of differential equations using compressive sensing".

2017 IEEE 13th International Colloquium on Signal Processing & its Applications (CSPA) . pp.

79– 84.

doi : 10.1109/CSPA.2017.8064928 .

ISBN 978-1-5090-1184-1 .

S2CID 13082456 .

^ Khan, Amina (9 January 2017).

"Meet the 'Hidden Figures' mathematician who helped send Americans into space" .

Los Angeles Times . Retrieved 12 February 2017 .

References [ edit ] Atkinson, Kendall A. (1989).

An Introduction to Numerical Analysis (2nd ed.). New York: John Wiley & Sons .

ISBN 978-0-471-50023-0 .

Ascher, Uri M.; Petzold, Linda R.

(1998).

Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations . Philadelphia: Society for Industrial and Applied Mathematics .

ISBN 978-0-89871-412-8 .

Butcher, John C.

(2003).

Numerical Methods for Ordinary Differential Equations . New York: John Wiley & Sons .

ISBN 978-0-471-96758-3 .

Hairer, Ernst; Nørsett, Syvert Paul; Wanner, Gerhard (1993).

Solving ordinary differential equations I: Nonstiff problems . Berlin, New York: Springer-Verlag .

ISBN 978-3-540-56670-0 .

Iserles, Arieh (1996).

A First Course in the Numerical Analysis of Differential Equations .

Cambridge University Press .

ISBN 978-0-521-55655-2 .

Stoer, Josef; Bulirsch, Roland (2002).

Introduction to Numerical Analysis (3rd ed.). Berlin, New York: Springer-Verlag .

ISBN 978-0-387-95452-3 .

Lakoba, Taras I. (2012), Simple Euler method and its modifications (PDF) (Lecture notes for MATH334), University of Vermont , retrieved 29 February 2012 Unni, M P. (2017). "Memory reduction for numerical solution of differential equations using compressive sensing".

2017 IEEE 13th International Colloquium on Signal Processing & its Applications (CSPA) .

IEEE CSPA . pp.

79– 84.

doi : 10.1109/CSPA.2017.8064928 .

ISBN 978-1-5090-1184-1 .

S2CID 13082456 .

External links [ edit ] The Wikibook Calculus has a page on the topic of: Euler's Method Media related to Euler method at Wikimedia Commons Euler method implementations in different languages by Rosetta Code "Euler method" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] v t e Numerical methods for ordinary differential equations First-order methods Euler method Backward Euler Semi-implicit Euler Exponential Euler Second-order methods Verlet integration Velocity Verlet Trapezoidal rule Beeman's algorithm Midpoint method Heun's method Newmark-beta method Leapfrog integration Higher-order methods Exponential integrator Runge–Kutta methods List of Runge–Kutta methods Linear multistep method General linear methods Backward differentiation formula Yoshida Gauss–Legendre method Theory Symplectic integrator Related Numerical methods for partial differential equations Numerical integration v t e Leonhard Euler Euler–Lagrange equation Euler–Lotka equation Euler–Maclaurin formula Euler–Maruyama method Euler–Mascheroni constant Euler–Poisson–Darboux equation Euler–Rodrigues formula Euler–Tricomi equation Euler's continued fraction formula Euler's critical load Euler's formula Euler's four-square identity Euler's identity Euler's pump and turbine equation Euler's rotation theorem Euler's sum of powers conjecture Euler's theorem Euler equations (fluid dynamics) Euler function Euler method Euler numbers Euler number (physics) Euler–Bernoulli beam theory Namesakes Category Retrieved from " https://en.wikipedia.org/w/index.php?title=Euler_method&oldid=1302899022 " Categories : Numerical differential equations Runge–Kutta methods First order methods Leonhard Euler Hidden categories: Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from May 2021 Commons category link from Wikidata Articles with example R code Articles with example MATLAB/Octave code This page was last edited on 28 July 2025, at 01:13 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Euler method 30 languages Add topic

