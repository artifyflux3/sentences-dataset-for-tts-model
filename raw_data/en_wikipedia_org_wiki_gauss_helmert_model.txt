Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Formulation 2 Solution Toggle Solution subsection 2.1 Computation 3 Applications 4 Related concepts 5 Extensions 6 References Toggle the table of contents Least-squares adjustment 2 languages Deutsch 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Gauss-Helmert model ) This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( June 2014 ) ( Learn how and when to remove this message ) Least-squares adjustment is a model for the solution of an overdetermined system of equations based on the principle of least squares of observation residuals . It is used extensively in the disciplines of surveying , geodesy , and photogrammetry —the field of geomatics , collectively.

Formulation [ edit ] There are three forms of least squares adjustment: parametric , conditional , and combined : In parametric adjustment , one can find an observation equation h ( X ) = Y relating observations Y explicitly in terms of parameters X (leading to the A-model below).

In conditional adjustment , there exists a condition equation which is g ( Y ) = 0 involving only observations Y (leading to the B-model below) — with no parameters X at all.

Finally, in a combined adjustment , both parameters X and observations Y are involved implicitly in a mixed-model equation f ( X , Y ) = 0 .

Clearly, parametric and conditional adjustments correspond to the more general combined case when f ( X , Y ) = h ( X ) - Y and f ( X , Y ) = g ( Y ) , respectively. Yet the special cases warrant simpler solutions, as detailed below. Often in the literature, Y may be denoted L .

Solution [ edit ] The equalities above only hold for the estimated parameters X ^ ^ {\displaystyle {\hat {X}}} and observations Y ^ ^ {\displaystyle {\hat {Y}}} , thus f ( X ^ ^ , Y ^ ^ ) = 0 {\displaystyle f\left({\hat {X}},{\hat {Y}}\right)=0} . In contrast, measured observations Y ~ ~ {\displaystyle {\tilde {Y}}} and approximate parameters X ~ ~ {\displaystyle {\tilde {X}}} produce a nonzero misclosure : w ~ ~ = f ( X ~ ~ , Y ~ ~ ) .

{\displaystyle {\tilde {w}}=f\left({\tilde {X}},{\tilde {Y}}\right).} One can proceed to Taylor series expansion of the equations, which results in the Jacobians or design matrices : the first one, A = ∂ ∂ f / ∂ ∂ X ; {\displaystyle A=\partial {f}/\partial {X};} and the second one, B = ∂ ∂ f / ∂ ∂ Y .

{\displaystyle B=\partial {f}/\partial {Y}.} The linearized model then reads: w ~ ~ + A x ^ ^ + B y ^ ^ = 0 , {\displaystyle {\tilde {w}}+A{\hat {x}}+B{\hat {y}}=0,} where x ^ ^ = X ^ ^ − − X ~ ~ {\displaystyle {\hat {x}}={\hat {X}}-{\tilde {X}}} are estimated parameter corrections to the a priori values, and y ^ ^ = Y ^ ^ − − Y ~ ~ {\displaystyle {\hat {y}}={\hat {Y}}-{\tilde {Y}}} are post-fit observation residuals .

In the parametric adjustment, the second design matrix is an identity, B =- I , and the misclosure vector can be interpreted as the pre-fit residuals, y ~ ~ = w ~ ~ = h ( X ~ ~ ) − − Y ~ ~ {\displaystyle {\tilde {y}}={\tilde {w}}=h({\tilde {X}})-{\tilde {Y}}} , so the system simplifies to: A x ^ ^ = y ^ ^ − − y ~ ~ , {\displaystyle A{\hat {x}}={\hat {y}}-{\tilde {y}},} which is in the form of ordinary least squares . 
In the conditional adjustment, the first design matrix is null, A = 0 .
For the more general cases, Lagrange multipliers are introduced to relate the two Jacobian matrices, and transform the constrained least squares problem into an unconstrained one (albeit a larger one). In any case, their manipulation leads to the X ^ ^ {\displaystyle {\hat {X}}} and Y ^ ^ {\displaystyle {\hat {Y}}} vectors as well as the respective parameters and observations a posteriori covariance matrices.

Computation [ edit ] Given the matrices and vectors above, their solution is found via standard least-squares methods; e.g., forming the normal matrix and applying Cholesky decomposition , applying the QR factorization directly to the Jacobian matrix, iterative methods for very large systems, etc.

Applications [ edit ] Leveling , traverse , and control networks Bundle adjustment Triangulation , Trilateration , Triangulateration GPS / GNSS positioning Helmert transformation Related concepts [ edit ] Parametric adjustment is similar to most of regression analysis and coincides with the Gauss–Markov model Combined adjustment, also known as the Gauss–Helmert model (named after German mathematicians/geodesists C.F. Gauss and F.R. Helmert ), [ 1 ] [ 2 ] is related to the errors-in-variables models and total least squares .

[ 3 ] [ 4 ] The use of a priori parameter covariance matrix is akin to Tikhonov regularization Extensions [ edit ] If rank deficiency is encountered, it can often be rectified by the inclusion of additional equations imposing constraints on the parameters and/or observations, leading to constrained least squares .

References [ edit ] ^ Kotz, Samuel; Read, Campbell B.; Balakrishnan, N.; Vidakovic, Brani; Johnson, Norman L. (2004-07-15). "Gauss-Helmert Model".

Encyclopedia of Statistical Sciences . Hoboken, NJ, USA: John Wiley & Sons, Inc.

doi : 10.1002/0471667196.ess0854.pub2 .

ISBN 978-0-471-66719-3 .

^ Förstner, Wolfgang; Wrobel, Bernhard P. (2016). "Estimation".

Photogrammetric Computer Vision . Geometry and Computing. Vol. 11. Cham: Springer International Publishing. pp.

75– 190.

doi : 10.1007/978-3-319-11550-4_4 .

ISBN 978-3-319-11549-8 .

ISSN 1866-6795 .

^ Schaffrin, Burkhard; Snow, Kyle (2010).

"Total Least-Squares regularization of Tykhonov type and an ancient racetrack in Corinth" .

Linear Algebra and Its Applications .

432 (8). Elsevier BV: 2061– 2076.

doi : 10.1016/j.laa.2009.09.014 .

ISSN 0024-3795 .

^ Neitzel, Frank (2010-09-17). "Generalization of total least-squares on example of unweighted and weighted 2D similarity transformation".

Journal of Geodesy .

84 (12). Springer Science and Business Media LLC: 751– 762.

Bibcode : 2010JGeod..84..751N .

doi : 10.1007/s00190-010-0408-0 .

ISSN 0949-7714 .

S2CID 123207786 .

Retrieved from " https://en.wikipedia.org/w/index.php?title=Least-squares_adjustment&oldid=1306529701#Gauss–Helmert_model " Categories : Curve fitting Least squares Geodesy Surveying Photogrammetry Hidden categories: Articles lacking in-text citations from June 2014 All articles lacking in-text citations This page was last edited on 18 August 2025, at 07:02 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Least-squares adjustment 2 languages Add topic

