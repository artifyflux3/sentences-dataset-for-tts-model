Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Cumulative distribution function 1.2 Probability density function 2 Properties Toggle Properties subsection 2.1 Moments of the noncentral t -distribution 2.2 Asymmetry 2.3 Mode 3 Related distributions Toggle Related distributions subsection 3.1 Special cases 4 Occurrence and applications Toggle Occurrence and applications subsection 4.1 Use in power analysis 4.2 Use in tolerance intervals 5 See also 6 References 7 External links Toggle the table of contents Noncentral t -distribution 5 languages Català Español فارسی 日本語 Slovenščina Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Noncentral Student's t Probability density function Parameters ν > 0 degrees of freedom μ μ ∈ ∈ ℜ ℜ {\displaystyle \mu \in \Re \,\!} noncentrality parameter Support x ∈ ∈ ( − − ∞ ∞ ; + ∞ ∞ ) {\displaystyle x\in (-\infty ;+\infty )\,\!} PDF see text CDF see text Mean see text Mode see text Variance see text Skewness see text Excess kurtosis see text The noncentral t -distribution generalizes Student's t -distribution using a noncentrality parameter . Whereas the central probability distribution describes how a test statistic t is distributed when the difference tested is null, the noncentral distribution describes how t is distributed when the null is false. This leads to its use in statistics, especially calculating statistical power . The noncentral t -distribution is also known as the singly noncentral t -distribution, and in addition to its primary use in statistical inference , is also used in robust modeling for data .

Definitions [ edit ] If Z is a standard normal random variable, and V is a chi-squared distributed random variable with ν degrees of freedom that is independent of Z , then T = Z + μ μ V / ν ν {\displaystyle T={\frac {Z+\mu }{\sqrt {V/\nu }}}} is a noncentral t -distributed random variable with ν degrees of freedom and noncentrality parameter μ ≠ 0. Note that the noncentrality parameter may be negative.

Cumulative distribution function [ edit ] The cumulative distribution function of noncentral t -distribution with ν degrees of freedom and noncentrality parameter μ can be expressed as [ 1 ] F ν ν , μ μ ( x ) = { F ~ ~ ν ν , μ μ ( x ) , if x ≥ ≥ 0 ; 1 − − F ~ ~ ν ν , − − μ μ ( x ) , if x < 0 , {\displaystyle F_{\nu ,\mu }(x)={\begin{cases}{\tilde {F}}_{\nu ,\mu }(x),&{\mbox{if }}x\geq 0;\\1-{\tilde {F}}_{\nu ,-\mu }(x),&{\mbox{if }}x<0,\end{cases}}} where F ~ ~ ν ν , μ μ ( x ) = Φ Φ ( − − μ μ ) + 1 2 ∑ ∑ j = 0 ∞ ∞ [ p j I y ( j + 1 2 , ν ν 2 ) + q j I y ( j + 1 , ν ν 2 ) ] , {\displaystyle {\tilde {F}}_{\nu ,\mu }(x)=\Phi (-\mu )+{\frac {1}{2}}\sum _{j=0}^{\infty }\left[p_{j}I_{y}\left(j+{\frac {1}{2}},{\frac {\nu }{2}}\right)+q_{j}I_{y}\left(j+1,{\frac {\nu }{2}}\right)\right],} I y ( a , b ) {\displaystyle I_{y}\,\!(a,b)} is the regularized incomplete beta function , y = x 2 x 2 + ν ν , {\displaystyle y={\frac {x^{2}}{x^{2}+\nu }},} p j = 1 j !

exp ⁡ ⁡ { − − μ μ 2 2 } ( μ μ 2 2 ) j , {\displaystyle p_{j}={\frac {1}{j!}}\exp \left\{-{\frac {\mu ^{2}}{2}}\right\}\left({\frac {\mu ^{2}}{2}}\right)^{j},} q j = μ μ 2 Γ Γ ( j + 3 / 2 ) exp ⁡ ⁡ { − − μ μ 2 2 } ( μ μ 2 2 ) j , {\displaystyle q_{j}={\frac {\mu }{{\sqrt {2}}\Gamma (j+3/2)}}\exp \left\{-{\frac {\mu ^{2}}{2}}\right\}\left({\frac {\mu ^{2}}{2}}\right)^{j},} and Φ is the cumulative distribution function of the standard normal distribution .

Alternatively, the noncentral t -distribution CDF can be expressed as [ citation needed ] : F v , μ μ ( x ) = { 1 2 ∑ ∑ j = 0 ∞ ∞ 1 j !

( − − μ μ 2 ) j e − − μ μ 2 2 Γ Γ ( j + 1 2 ) π π I ( v v + x 2 ; v 2 , j + 1 2 ) , x ≥ ≥ 0 1 − − 1 2 ∑ ∑ j = 0 ∞ ∞ 1 j !

( − − μ μ 2 ) j e − − μ μ 2 2 Γ Γ ( j + 1 2 ) π π I ( v v + x 2 ; v 2 , j + 1 2 ) , x < 0 {\displaystyle F_{v,\mu }(x)={\begin{cases}{\frac {1}{2}}\sum _{j=0}^{\infty }{\frac {1}{j!}}(-\mu {\sqrt {2}})^{j}e^{\frac {-\mu ^{2}}{2}}{\frac {\Gamma ({\frac {j+1}{2}})}{\sqrt {\pi }}}I\left({\frac {v}{v+x^{2}}};{\frac {v}{2}},{\frac {j+1}{2}}\right),&x\geq 0\\1-{\frac {1}{2}}\sum _{j=0}^{\infty }{\frac {1}{j!}}(-\mu {\sqrt {2}})^{j}e^{\frac {-\mu ^{2}}{2}}{\frac {\Gamma ({\frac {j+1}{2}})}{\sqrt {\pi }}}I\left({\frac {v}{v+x^{2}}};{\frac {v}{2}},{\frac {j+1}{2}}\right),&x<0\end{cases}}} where Γ is the gamma function and I is the regularized incomplete beta function .

Although there are other forms of the cumulative distribution function, the first form presented above is very easy to evaluate through recursive computing .

[ 1 ] In statistical software R , the cumulative distribution function is implemented as pt .

Probability density function [ edit ] The probability density function (pdf) for the noncentral t -distribution with ν > 0 degrees of freedom and noncentrality parameter μ can be expressed in several forms.

The confluent hypergeometric function form of the density function is f ( x ) = Γ Γ ( ν ν + 1 2 ) ν ν π π Γ Γ ( ν ν 2 ) ( 1 + x 2 ν ν ) − − ν ν + 1 2 ⏟ ⏟ StudentT ( x ; μ μ = 0 ) exp ⁡ ⁡ ( − − μ μ 2 2 ) { A ν ν ( x ; μ μ ) + B ν ν ( x ; μ μ ) } , {\displaystyle f(x)=\underbrace {{\frac {\Gamma ({\frac {\nu +1}{2}})}{{\sqrt {\nu \pi }}\Gamma ({\frac {\nu }{2}})}}\left(1+{\frac {x^{2}}{\nu }}\right)^{-{\tfrac {\nu +1}{2}}}} _{{\text{StudentT}}(x\,;\,\mu =0)}\exp {\big (}-{\tfrac {\mu ^{2}}{2}}{\big )}{\Big \{}A_{\nu }(x\,;\,\mu )+B_{\nu }(x\,;\,\mu ){\Big \}},} where A ν ν ( x ; μ μ ) = 1 F 1 ( ν ν + 1 2 ; 1 2 ; μ μ 2 x 2 2 ( x 2 + ν ν ) ) , B ν ν ( x ; μ μ ) = 2 μ μ x x 2 + ν ν Γ Γ ( ν ν 2 + 1 ) Γ Γ ( ν ν + 1 2 ) 1 F 1 ( ν ν 2 + 1 ; 3 2 ; μ μ 2 x 2 2 ( x 2 + ν ν ) ) , {\displaystyle {\begin{aligned}A_{\nu }(x\,;\,\mu )&={_{1}F}_{1}\left({\frac {\nu +1}{2}}\,;\,{\frac {1}{2}}\,;\,{\frac {\mu ^{2}x^{2}}{2(x^{2}+\nu )}}\right),\\B_{\nu }(x\,;\,\mu )&={\frac {{\sqrt {2}}\mu x}{\sqrt {x^{2}+\nu }}}{\frac {\Gamma ({\frac {\nu }{2}}+1)}{\Gamma ({\frac {\nu +1}{2}})}}{_{1}F}_{1}\left({\frac {\nu }{2}}+1\,;\,{\frac {3}{2}}\,;\,{\frac {\mu ^{2}x^{2}}{2(x^{2}+\nu )}}\right),\end{aligned}}} and where 1 F 1 is a confluent hypergeometric function .

An alternative integral form is [ 2 ] f ( x ) = ν ν ν ν 2 exp ⁡ ⁡ ( − − ν ν μ μ 2 2 ( x 2 + ν ν ) ) π π Γ Γ ( ν ν 2 ) 2 ν ν − − 1 2 ( x 2 + ν ν ) ν ν + 1 2 ∫ ∫ 0 ∞ ∞ y ν ν exp ⁡ ⁡ ( − − 1 2 ( y − − μ μ x x 2 + ν ν ) 2 ) d y .

{\displaystyle f(x)={\frac {\nu ^{\frac {\nu }{2}}\exp \left(-{\frac {\nu \mu ^{2}}{2(x^{2}+\nu )}}\right)}{{\sqrt {\pi }}\Gamma ({\frac {\nu }{2}})2^{\frac {\nu -1}{2}}(x^{2}+\nu )^{\frac {\nu +1}{2}}}}\int _{0}^{\infty }y^{\nu }\exp \left(-{\frac {1}{2}}\left(y-{\frac {\mu x}{\sqrt {x^{2}+\nu }}}\right)^{2}\right)dy.} A third form of the density is obtained using its cumulative distribution functions, as follows.

f ( x ) = { ν ν x { F ν ν + 2 , μ μ ( x 1 + 2 ν ν ) − − F ν ν , μ μ ( x ) } , if x ≠ ≠ 0 ; Γ Γ ( ν ν + 1 2 ) π π ν ν Γ Γ ( ν ν 2 ) exp ⁡ ⁡ ( − − μ μ 2 2 ) , if x = 0.

{\displaystyle f(x)={\begin{cases}{\frac {\nu }{x}}\left\{F_{\nu +2,\mu }\left(x{\sqrt {1+{\frac {2}{\nu }}}}\right)-F_{\nu ,\mu }(x)\right\},&{\mbox{if }}x\neq 0;\\{\frac {\Gamma ({\frac {\nu +1}{2}})}{{\sqrt {\pi \nu }}\Gamma ({\frac {\nu }{2}})}}\exp \left(-{\frac {\mu ^{2}}{2}}\right),&{\mbox{if }}x=0.\end{cases}}} This is the approach implemented by the dt function in R .

Properties [ edit ] Moments of the noncentral t -distribution [ edit ] In general, the k th raw moment of the noncentral t -distribution is [ 3 ] E [ T k ] = { ( ν ν 2 ) k 2 Γ Γ ( ν ν − − k 2 ) Γ Γ ( ν ν 2 ) exp ( − − μ μ 2 2 ) d k d μ μ k exp ( μ μ 2 2 ) , if ν ν > k ; Does not exist , if ν ν ≤ ≤ k .

{\displaystyle {\mbox{E}}\left[T^{k}\right]={\begin{cases}\left({\frac {\nu }{2}}\right)^{\frac {k}{2}}{\frac {\Gamma \left({\frac {\nu -k}{2}}\right)}{\Gamma \left({\frac {\nu }{2}}\right)}}{\mbox{exp}}\left(-{\frac {\mu ^{2}}{2}}\right){\frac {d^{k}}{d\mu ^{k}}}{\mbox{exp}}\left({\frac {\mu ^{2}}{2}}\right),&{\mbox{if }}\nu >k;\\{\mbox{Does not exist}},&{\mbox{if }}\nu \leq k.\\\end{cases}}} In particular, the mean and variance of the noncentral t -distribution are E [ T ] = { μ μ ν ν 2 Γ Γ ( ( ν ν − − 1 ) / 2 ) Γ Γ ( ν ν / 2 ) , if ν ν > 1 ; Does not exist , if ν ν ≤ ≤ 1 , Var [ T ] = { ν ν ( 1 + μ μ 2 ) ν ν − − 2 − − μ μ 2 ν ν 2 ( Γ Γ ( ( ν ν − − 1 ) / 2 ) Γ Γ ( ν ν / 2 ) ) 2 , if ν ν > 2 ; Does not exist , if ν ν ≤ ≤ 2.

{\displaystyle {\begin{aligned}{\mbox{E}}\left[T\right]&={\begin{cases}\mu {\sqrt {\frac {\nu }{2}}}{\frac {\Gamma ((\nu -1)/2)}{\Gamma (\nu /2)}},&{\mbox{if }}\nu >1;\\{\mbox{Does not exist}},&{\mbox{if }}\nu \leq 1,\\\end{cases}}\\{\mbox{Var}}\left[T\right]&={\begin{cases}{\frac {\nu (1+\mu ^{2})}{\nu -2}}-{\frac {\mu ^{2}\nu }{2}}\left({\frac {\Gamma ((\nu -1)/2)}{\Gamma (\nu /2)}}\right)^{2},&{\mbox{if }}\nu >2;\\{\mbox{Does not exist}},&{\mbox{if }}\nu \leq 2.\\\end{cases}}\end{aligned}}} An excellent approximation to ν ν 2 Γ Γ ( ( ν ν − − 1 ) / 2 ) Γ Γ ( ν ν / 2 ) {\displaystyle {\sqrt {\frac {\nu }{2}}}{\frac {\Gamma ((\nu -1)/2)}{\Gamma (\nu /2)}}} is ( 1 − − 3 4 ν ν − − 1 ) − − 1 {\displaystyle \left(1-{\frac {3}{4\nu -1}}\right)^{-1}} , which can be used in both formulas.

[ 4 ] [ 5 ] Asymmetry [ edit ] The non-central t -distribution is asymmetric unless μ is zero, i.e., a central t -distribution. In addition, the asymmetry becomes smaller the larger degree of freedom. The right tail will be heavier than the left when μ > 0, and vice versa. However, the usual skewness is not generally a good measure of asymmetry for this distribution, because if the degrees of freedom is not larger than 3, the third moment does not exist at all. Even if the degrees of freedom is greater than 3, the sample estimate of the skewness is still very unstable unless the sample size is very large.

Mode [ edit ] The noncentral t -distribution is always unimodal and bell shaped, but the mode is not analytically available, although for μ ≠ 0 we have [ 6 ] ν ν ν ν + ( 5 / 2 ) < m o d e μ μ < ν ν ν ν + 1 {\displaystyle {\sqrt {\frac {\nu }{\nu +(5/2)}}}<{\frac {\mathrm {mode} }{\mu }}<{\sqrt {\frac {\nu }{\nu +1}}}} In particular, the mode always has the same sign as the noncentrality parameter μ. Moreover, the negative of the mode is exactly the mode for a noncentral t -distribution with the same number of degrees of freedom ν but noncentrality parameter −μ.

The mode is strictly increasing with μ (it always moves in the same direction as μ is adjusted in). In the limit, when μ → 0, the mode is approximated by ν ν 2 Γ Γ ( ν ν + 2 2 ) Γ Γ ( ν ν + 3 2 ) μ μ ; {\displaystyle {\sqrt {\frac {\nu }{2}}}{\frac {\Gamma \left({\frac {\nu +2}{2}}\right)}{\Gamma \left({\frac {\nu +3}{2}}\right)}}\mu ;\,} and when μ → ∞, the mode is approximated by ν ν ν ν + 1 μ μ .

{\displaystyle {\sqrt {\frac {\nu }{\nu +1}}}\mu .} Related distributions [ edit ] Central t -distribution: the central t -distribution can be converted into a location / scale family.  This family of distributions is used in data modeling to capture various tail behaviors.  The location/scale generalization of the central t -distribution is a different distribution from the noncentral t -distribution discussed in this article. In particular, this approximation does not respect the asymmetry of the noncentral t -distribution. However, the central t -distribution can be used as an approximation to the noncentral t -distribution.

[ 7 ] If T is noncentral t -distributed with ν degrees of freedom and noncentrality parameter μ and F = T 2 , then F has a noncentral F -distribution with 1 numerator degree of freedom, ν denominator degrees of freedom, and noncentrality parameter μ 2 .

If T is noncentral t -distributed with ν degrees of freedom and noncentrality parameter μ and Z = lim ν ν → → ∞ ∞ T {\displaystyle Z=\lim _{\nu \rightarrow \infty }T} , then Z has a normal distribution with mean μ and unit variance.

When the denominator noncentrality parameter of a doubly noncentral t -distribution is zero, then it becomes a noncentral t -distribution.

Special cases [ edit ] When μ = 0, the noncentral t -distribution becomes the central (Student's) t -distribution with the same degrees of freedom.

Occurrence and applications [ edit ] Use in power analysis [ edit ] Suppose we have an independent and identically distributed sample X 1 , ..., X n each of which is normally distributed with mean θ and variance σ 2 , and we are interested in testing the null hypothesis θ = 0 vs. the alternative hypothesis θ ≠ 0. We can perform a one sample t -test using the test statistic T = X ¯ ¯ σ σ ^ ^ / n = X ¯ ¯ − − θ θ ( σ σ / n ) + θ θ ( σ σ / n ) ( σ σ ^ ^ 2 σ σ 2 / ( n − − 1 ) ) / ( n − − 1 ) {\displaystyle T={\frac {\bar {X}}{{\hat {\sigma }}/{\sqrt {n}}}}={\frac {{\frac {{\bar {X}}-\theta }{(\sigma /{\sqrt {n}})}}+{\frac {\theta }{(\sigma /{\sqrt {n}})}}}{\sqrt {\left.\left({\frac {{\hat {\sigma }}^{2}}{\sigma ^{2}/(n-1)}}\right)\right/(n-1)}}}} where X ¯ ¯ {\displaystyle {\bar {X}}} is the sample mean and σ σ ^ ^ 2 {\displaystyle {\hat {\sigma }}^{2}\,\!} is the unbiased sample variance . Since the right hand side of the second equality exactly matches the characterization of a noncentral t -distribution as described above, T has a noncentral t -distribution with n −1 degrees of freedom and noncentrality parameter n θ θ / σ σ {\displaystyle {\sqrt {n}}\theta /\sigma \,\!} .

If the test procedure rejects the null hypothesis whenever | T | > t 1 − − α α / 2 {\displaystyle |T|>t_{1-\alpha /2}\,\!} , where t 1 − − α α / 2 {\displaystyle t_{1-\alpha /2}\,\!} is the upper α/2 quantile of the (central) Student's t -distribution for a pre-specified α ∈ (0, 1), then the power of this test is given by 1 − − F n − − 1 , n θ θ / σ σ ( t 1 − − α α / 2 ) + F n − − 1 , n θ θ / σ σ ( − − t 1 − − α α / 2 ) .

{\displaystyle 1-F_{n-1,{\sqrt {n}}\theta /\sigma }(t_{1-\alpha /2})+F_{n-1,{\sqrt {n}}\theta /\sigma }(-t_{1-\alpha /2}).} Similar applications of the noncentral t -distribution can be found in the power analysis of the general normal-theory linear models , which includes the above one sample t -test as a special case.

Use in tolerance intervals [ edit ] One-sided normal tolerance intervals have an exact solution in terms of the sample mean and sample variance based on the noncentral t -distribution.

[ 8 ] This enables the calculation of a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls.

See also [ edit ] Noncentral F -distribution References [ edit ] ^ a b Lenth, Russell V (1989). "Algorithm AS 243: Cumulative Distribution Function of the Non-central t Distribution".

Journal of the Royal Statistical Society, Series C .

38 (1): 185– 189.

JSTOR 2347693 .

^ Scharf, L. (1991).

Statistical Signal Processing . Reading: Addison-Wesley. p. 177.

ISBN 0-201-19038-9 .

^ Hogben, D; Pinkham, RS; Wilk, MB (1961). "The moments of the non-central t -distribution".

Biometrika .

48 ( 3– 4): 465– 468.

doi : 10.1093/biomet/48.3-4.465 .

hdl : 2027/coo.31924001119068 .

JSTOR 2332772 .

^ Hedges, Larry V. (June 1981). "Distribution Theory for Glass's Estimator of Effect size and Related Estimators".

Journal of Educational Statistics .

6 (2): 107– 128.

doi : 10.3102/2F10769986006002107 .

^ Tothfalusi, Laszlo; Endrenyi, Laszlo (1 March 2016).

"An Exact Procedure for the Evaluation of Reference-Scaled Average Bioequivalence" .

The AAPS Journal .

18 (2): 476– 489.

doi : 10.1208/s12248-016-9873-6 .

PMC 4779113 .

^ van Aubel, A; Gawronski, W (2003). "Analytic properties of noncentral distributions".

Applied Mathematics and Computation .

141 : 3– 12.

doi : 10.1016/S0096-3003(02)00316-8 .

^ Helena Chmura Kraemer; Minja Paik (1979). "A Central t Approximation to the Noncentral t Distribution".

Technometrics .

21 (3): 357– 360.

doi : 10.1080/00401706.1979.10489781 .

JSTOR 1267759 .

^ Derek S. Young (August 2010).

"tolerance: An R Package for Estimating Tolerance Intervals" .

Journal of Statistical Software .

36 (5): 1– 39.

ISSN 1548-7660 . Retrieved 19 February 2013 .

, p.23 External links [ edit ] Eric W. Weisstein. "Noncentral Student's t -Distribution." From MathWorld—A Wolfram Web Resource High accuracy calculation for life or science.: Noncentral t -distribution From Casio company.

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Noncentral_t-distribution&oldid=1251317434 " Category : Continuous distributions Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from April 2014 This page was last edited on 15 October 2024, at 14:57 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Noncentral t -distribution 5 languages Add topic

