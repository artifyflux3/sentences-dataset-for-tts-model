Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Density 2 Properties Toggle Properties subsection 2.1 Mode 2.2 Characteristic function and other related functions 3 Related distributions 4 Statistical Inference Toggle Statistical Inference subsection 4.1 Estimation of parameters 5 See also 6 References 7 External links Toggle the table of contents Folded normal distribution 3 languages Català فارسی Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Probability density function μ =1, σ =1 Cumulative distribution function μ =1, σ =1 Parameters μ ∈ R ( location ) σ 2 > 0 ( scale ) Support x ∈ [0,∞) PDF 1 σ σ 2 π π e − − ( x − − μ μ ) 2 2 σ σ 2 + 1 σ σ 2 π π e − − ( x + μ μ ) 2 2 σ σ 2 {\displaystyle {\frac {1}{\sigma {\sqrt {2\pi }}}}\,e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}+{\frac {1}{\sigma {\sqrt {2\pi }}}}\,e^{-{\frac {(x+\mu )^{2}}{2\sigma ^{2}}}}} CDF 1 2 [ erf ( x + μ μ σ σ 2 ) + erf ( x − − μ μ σ σ 2 ) ] {\displaystyle {\frac {1}{2}}\left[{\mbox{erf}}\left({\frac {x+\mu }{\sigma {\sqrt {2}}}}\right)+{\mbox{erf}}\left({\frac {x-\mu }{\sigma {\sqrt {2}}}}\right)\right]} Mean μ μ Y = σ σ 2 π π e ( − − μ μ 2 / 2 σ σ 2 ) + μ μ ( 1 − − 2 Φ Φ ( − − μ μ σ σ ) ) {\displaystyle \mu _{Y}=\sigma {\sqrt {\tfrac {2}{\pi }}}\,e^{(-\mu ^{2}/2\sigma ^{2})}+\mu \left(1-2\,\Phi (-{\tfrac {\mu }{\sigma }})\right)} Variance σ σ Y 2 = μ μ 2 + σ σ 2 − − μ μ Y 2 {\displaystyle \sigma _{Y}^{2}=\mu ^{2}+\sigma ^{2}-\mu _{Y}^{2}} The folded normal distribution is a probability distribution related to the normal distribution . Given a normally distributed random variable X with mean μ and variance σ 2 , the random variable Y = | X | has a folded normal distribution.  Such a case may be encountered if only the magnitude of some variable is recorded, but not its sign. The distribution is called "folded" because probability mass to the left of x = 0 is folded over by taking the absolute value . In the physics of heat conduction , the folded normal distribution is a fundamental solution of the heat equation on the half space; it corresponds to having a perfect insulator on a hyperplane through the origin.

Definitions [ edit ] Density [ edit ] The probability density function (PDF) is given by f Y ( x ; μ μ , σ σ 2 ) = 1 2 π π σ σ 2 e − − ( x − − μ μ ) 2 2 σ σ 2 + 1 2 π π σ σ 2 e − − ( x + μ μ ) 2 2 σ σ 2 {\displaystyle f_{Y}(x;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\,e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}+{\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\,e^{-{\frac {(x+\mu )^{2}}{2\sigma ^{2}}}}} for x ≥ 0, and 0 everywhere else. An alternative formulation is given by f ( x ) = 2 π π σ σ 2 e − − ( x 2 + μ μ 2 ) 2 σ σ 2 cosh ⁡ ⁡ ( μ μ x σ σ 2 ) {\displaystyle f\left(x\right)={\sqrt {\frac {2}{\pi \sigma ^{2}}}}e^{-{\frac {\left(x^{2}+\mu ^{2}\right)}{2\sigma ^{2}}}}\cosh {\left({\frac {\mu x}{\sigma ^{2}}}\right)}} , where cosh is the Hyperbolic cosine function . It follows that the cumulative distribution function (CDF) is given by: F Y ( x ; μ μ , σ σ 2 ) = 1 2 [ erf ( x + μ μ 2 σ σ 2 ) + erf ( x − − μ μ 2 σ σ 2 ) ] {\displaystyle F_{Y}(x;\mu ,\sigma ^{2})={\frac {1}{2}}\left[{\mbox{erf}}\left({\frac {x+\mu }{\sqrt {2\sigma ^{2}}}}\right)+{\mbox{erf}}\left({\frac {x-\mu }{\sqrt {2\sigma ^{2}}}}\right)\right]} for x ≥ 0, where erf() is the error function . This expression reduces to the CDF of the half-normal distribution when μ = 0.

The mean of the folded distribution is then μ μ Y = σ σ 2 π π exp ⁡ ⁡ ( − − μ μ 2 2 σ σ 2 ) + μ μ erf ( μ μ 2 σ σ 2 ) {\displaystyle \mu _{Y}=\sigma {\sqrt {\frac {2}{\pi }}}\,\,\exp \left({\frac {-\mu ^{2}}{2\sigma ^{2}}}\right)+\mu \,{\mbox{erf}}\left({\frac {\mu }{\sqrt {2\sigma ^{2}}}}\right)} or μ μ Y = 2 π π σ σ e − − μ μ 2 2 σ σ 2 + μ μ [ 1 − − 2 Φ Φ ( − − μ μ σ σ ) ] {\displaystyle \mu _{Y}={\sqrt {\frac {2}{\pi }}}\sigma e^{-{\frac {\mu ^{2}}{2\sigma ^{2}}}}+\mu \left[1-2\Phi \left(-{\frac {\mu }{\sigma }}\right)\right]} where Φ Φ {\displaystyle \Phi } is the normal cumulative distribution function : Φ Φ ( x ) = 1 2 [ 1 + erf ⁡ ⁡ ( x 2 ) ] .

{\displaystyle \Phi (x)\;=\;{\frac {1}{2}}\left[1+\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)\right].} The variance then is expressed easily in terms of the mean: σ σ Y 2 = μ μ 2 + σ σ 2 − − μ μ Y 2 .

{\displaystyle \sigma _{Y}^{2}=\mu ^{2}+\sigma ^{2}-\mu _{Y}^{2}.} Both the mean ( μ ) and variance ( σ 2 ) of X in the original normal distribution can be interpreted as the location and scale parameters of Y in the folded distribution.

Properties [ edit ] Mode [ edit ] The mode of the distribution is the value of x {\displaystyle x} for which the density is maximised. In order to find this value, we take the first derivative of the density with respect to x {\displaystyle x} and set it equal to zero. Unfortunately, there is no closed form. We can, however, write the derivative in a better way and end up with a non-linear equation d f ( x ) d x = 0 ⇒ ⇒ − − ( x − − μ μ ) σ σ 2 e − − 1 2 ( x − − μ μ ) 2 σ σ 2 − − ( x + μ μ ) σ σ 2 e − − 1 2 ( x + μ μ ) 2 σ σ 2 = 0 {\displaystyle {\frac {df(x)}{dx}}=0\Rightarrow -{\frac {\left(x-\mu \right)}{\sigma ^{2}}}e^{-{\frac {1}{2}}{\frac {\left(x-\mu \right)^{2}}{\sigma ^{2}}}}-{\frac {\left(x+\mu \right)}{\sigma ^{2}}}e^{-{\frac {1}{2}}{\frac {\left(x+\mu \right)^{2}}{\sigma ^{2}}}}=0} x [ e − − 1 2 ( x − − μ μ ) 2 σ σ 2 + e − − 1 2 ( x + μ μ ) 2 σ σ 2 ] − − μ μ [ e − − 1 2 ( x − − μ μ ) 2 σ σ 2 − − e − − 1 2 ( x + μ μ ) 2 σ σ 2 ] = 0 {\displaystyle x\left[e^{-{\frac {1}{2}}{\frac {\left(x-\mu \right)^{2}}{\sigma ^{2}}}}+e^{-{\frac {1}{2}}{\frac {\left(x+\mu \right)^{2}}{\sigma ^{2}}}}\right]-\mu \left[e^{-{\frac {1}{2}}{\frac {\left(x-\mu \right)^{2}}{\sigma ^{2}}}}-e^{-{\frac {1}{2}}{\frac {\left(x+\mu \right)^{2}}{\sigma ^{2}}}}\right]=0} x ( 1 + e − − 2 μ μ x σ σ 2 ) − − μ μ ( 1 − − e − − 2 μ μ x σ σ 2 ) = 0 {\displaystyle x\left(1+e^{-{\frac {2\mu x}{\sigma ^{2}}}}\right)-\mu \left(1-e^{-{\frac {2\mu x}{\sigma ^{2}}}}\right)=0} ( μ μ + x ) e − − 2 μ μ x σ σ 2 = μ μ − − x {\displaystyle \left(\mu +x\right)e^{-{\frac {2\mu x}{\sigma ^{2}}}}=\mu -x} x = − − σ σ 2 2 μ μ log ⁡ ⁡ μ μ − − x μ μ + x {\displaystyle x=-{\frac {\sigma ^{2}}{2\mu }}\log {\frac {\mu -x}{\mu +x}}} .

Tsagris et al. (2014) saw from numerical investigation that when μ μ < σ σ {\displaystyle \mu <\sigma } , the maximum is met when x = 0 {\displaystyle x=0} , and when μ μ {\displaystyle \mu } becomes greater than 3 σ σ {\displaystyle 3\sigma } , the maximum approaches μ μ {\displaystyle \mu } . This is of course something to be expected, since, in this case, the folded normal converges to the normal distribution. In order to avoid any trouble with negative variances, the exponentiation of the parameter is suggested. Alternatively, you can add a constraint, such as if the optimiser goes for a negative variance the value of the log-likelihood is NA or something very small.

Characteristic function and other related functions [ edit ] The characteristic function is given by φ φ x ( t ) = e − − σ σ 2 t 2 2 + i μ μ t Φ Φ ( μ μ σ σ + i σ σ t ) + e − − σ σ 2 t 2 2 − − i μ μ t Φ Φ ( − − μ μ σ σ + i σ σ t ) {\displaystyle \varphi _{x}\left(t\right)=e^{{\frac {-\sigma ^{2}t^{2}}{2}}+i\mu t}\Phi \left({\frac {\mu }{\sigma }}+i\sigma t\right)+e^{-{\frac {\sigma ^{2}t^{2}}{2}}-i\mu t}\Phi \left(-{\frac {\mu }{\sigma }}+i\sigma t\right)} .

The moment generating function is given by M x ( t ) = φ φ x ( − − i t ) = e σ σ 2 t 2 2 + μ μ t Φ Φ ( μ μ σ σ + σ σ t ) + e σ σ 2 t 2 2 − − μ μ t Φ Φ ( − − μ μ σ σ + σ σ t ) {\displaystyle M_{x}\left(t\right)=\varphi _{x}\left(-it\right)=e^{{\frac {\sigma ^{2}t^{2}}{2}}+\mu t}\Phi \left({\frac {\mu }{\sigma }}+\sigma t\right)+e^{{\frac {\sigma ^{2}t^{2}}{2}}-\mu t}\Phi \left(-{\frac {\mu }{\sigma }}+\sigma t\right)} .

The cumulant generating function is given by K x ( t ) = log ⁡ ⁡ M x ( t ) = ( σ σ 2 t 2 2 + μ μ t ) + log ⁡ ⁡ { 1 − − Φ Φ ( − − μ μ σ σ − − σ σ t ) + e σ σ 2 t 2 2 − − μ μ t [ 1 − − Φ Φ ( μ μ σ σ − − σ σ t ) ] } {\displaystyle K_{x}\left(t\right)=\log {M_{x}\left(t\right)}=\left({\frac {\sigma ^{2}t^{2}}{2}}+\mu t\right)+\log {\left\lbrace 1-\Phi \left(-{\frac {\mu }{\sigma }}-\sigma t\right)+e^{{\frac {\sigma ^{2}t^{2}}{2}}-\mu t}\left[1-\Phi \left({\frac {\mu }{\sigma }}-\sigma t\right)\right]\right\rbrace }} .

The Laplace transformation is given by E ( e − − t x ) = e σ σ 2 t 2 2 − − μ μ t [ 1 − − Φ Φ ( − − μ μ σ σ + σ σ t ) ] + e σ σ 2 t 2 2 + μ μ t [ 1 − − Φ Φ ( μ μ σ σ + σ σ t ) ] {\displaystyle E\left(e^{-tx}\right)=e^{{\frac {\sigma ^{2}t^{2}}{2}}-\mu t}\left[1-\Phi \left(-{\frac {\mu }{\sigma }}+\sigma t\right)\right]+e^{{\frac {\sigma ^{2}t^{2}}{2}}+\mu t}\left[1-\Phi \left({\frac {\mu }{\sigma }}+\sigma t\right)\right]} .

The Fourier transform is given by f ^ ^ ( t ) = φ φ x ( − − 2 π π t ) = e − − 4 π π 2 σ σ 2 t 2 2 − − i 2 π π μ μ t [ 1 − − Φ Φ ( − − μ μ σ σ − − i 2 π π σ σ t ) ] + e − − 4 π π 2 σ σ 2 t 2 2 + i 2 π π μ μ t [ 1 − − Φ Φ ( μ μ σ σ − − i 2 π π σ σ t ) ] {\displaystyle {\hat {f}}\left(t\right)=\varphi _{x}\left(-2\pi t\right)=e^{{\frac {-4\pi ^{2}\sigma ^{2}t^{2}}{2}}-i2\pi \mu t}\left[1-\Phi \left(-{\frac {\mu }{\sigma }}-i2\pi \sigma t\right)\right]+e^{-{\frac {4\pi ^{2}\sigma ^{2}t^{2}}{2}}+i2\pi \mu t}\left[1-\Phi \left({\frac {\mu }{\sigma }}-i2\pi \sigma t\right)\right]} .

Related distributions [ edit ] When μ = 0 , the distribution of Y is a half-normal distribution .

The random variable ( Y / σ ) 2 has a noncentral chi-squared distribution with 1 degree of freedom and noncentrality equal to ( μ / σ ) 2 .

The folded normal distribution can also be seen as the limit of the folded non-standardized t distribution as the degrees of freedom go to infinity.

There is a bivariate version developed by Psarakis and Panaretos (2001) as well as a multivariate version developed by Chakraborty and Chatterjee (2013).

The Rice distribution is a multivariate generalization of the folded normal distribution.

Modified half-normal distribution [ 1 ] with the pdf on ( 0 , ∞ ∞ ) {\displaystyle (0,\infty )} is given as f ( x ) = 2 β β α α 2 x α α − − 1 exp ⁡ ⁡ ( − − β β x 2 + γ γ x ) Ψ Ψ ( α α 2 , γ γ β β ) {\displaystyle f(x)={\frac {2\beta ^{\frac {\alpha }{2}}x^{\alpha -1}\exp(-\beta x^{2}+\gamma x)}{\Psi {\left({\frac {\alpha }{2}},{\frac {\gamma }{\sqrt {\beta }}}\right)}}}} , where Ψ Ψ ( α α , z ) = 1 Ψ Ψ 1 ( ( α α , 1 2 ) ( 1 , 0 ) ; z ) {\displaystyle \Psi (\alpha ,z)={}_{1}\Psi _{1}\left({\begin{matrix}\left(\alpha ,{\frac {1}{2}}\right)\\(1,0)\end{matrix}};z\right)} denotes the Fox–Wright Psi function .

Statistical Inference [ edit ] Estimation of parameters [ edit ] There are a few ways of estimating the parameters of the folded normal. All of them are essentially the maximum likelihood estimation procedure, but in some cases, a numerical maximization is performed, whereas in other cases, the root of an equation is being searched. The log-likelihood of the folded normal when a sample x i {\displaystyle x_{i}} of size n {\displaystyle n} is available can be written in the following way l = − − n 2 log ⁡ ⁡ 2 π π σ σ 2 + ∑ ∑ i = 1 n log ⁡ ⁡ [ e − − ( x i − − μ μ ) 2 2 σ σ 2 + e − − ( x i + μ μ ) 2 2 σ σ 2 ] {\displaystyle l=-{\frac {n}{2}}\log {2\pi \sigma ^{2}}+\sum _{i=1}^{n}\log {\left[e^{-{\frac {\left(x_{i}-\mu \right)^{2}}{2\sigma ^{2}}}}+e^{-{\frac {\left(x_{i}+\mu \right)^{2}}{2\sigma ^{2}}}}\right]}} l = − − n 2 log ⁡ ⁡ 2 π π σ σ 2 + ∑ ∑ i = 1 n log ⁡ ⁡ [ e − − ( x i − − μ μ ) 2 2 σ σ 2 ( 1 + e − − ( x i + μ μ ) 2 2 σ σ 2 e ( x i − − μ μ ) 2 2 σ σ 2 ) ] {\displaystyle l=-{\frac {n}{2}}\log {2\pi \sigma ^{2}}+\sum _{i=1}^{n}\log {\left[e^{-{\frac {\left(x_{i}-\mu \right)^{2}}{2\sigma ^{2}}}}\left(1+e^{-{\frac {\left(x_{i}+\mu \right)^{2}}{2\sigma ^{2}}}}e^{\frac {\left(x_{i}-\mu \right)^{2}}{2\sigma ^{2}}}\right)\right]}} l = − − n 2 log ⁡ ⁡ 2 π π σ σ 2 − − ∑ ∑ i = 1 n ( x i − − μ μ ) 2 2 σ σ 2 + ∑ ∑ i = 1 n log ⁡ ⁡ ( 1 + e − − 2 μ μ x i σ σ 2 ) {\displaystyle l=-{\frac {n}{2}}\log {2\pi \sigma ^{2}}-\sum _{i=1}^{n}{\frac {\left(x_{i}-\mu \right)^{2}}{2\sigma ^{2}}}+\sum _{i=1}^{n}\log {\left(1+e^{-{\frac {2\mu x_{i}}{\sigma ^{2}}}}\right)}} In R (programming language) , using the package Rfast one can obtain the MLE really fast (command foldnorm.mle ). Alternatively, the command optim or nlm will fit this distribution. The maximisation is easy, since two parameters ( μ μ {\displaystyle \mu } and σ σ 2 {\displaystyle \sigma ^{2}} ) are involved. Note, that both positive and negative values for μ μ {\displaystyle \mu } are acceptable, since μ μ {\displaystyle \mu } belongs to the real line of numbers, hence, the sign is not important because the distribution is symmetric with respect to it. The next code is written in R folded <- function ( y ) { ## y is a vector with positive data n <- length ( y ) ## sample size sy2 <- sum ( y ^ 2 ) sam <- function ( para , n , sy2 ) { me <- para [ 1 ] ; se <- exp ( para [ 2 ] ) f <- - n / 2 * log ( 2 / pi / se ) + n * me ^ 2 / 2 / se + sy2 / 2 / se - sum ( log ( cosh ( me * y / se ) ) ) f } mod <- optim ( c ( mean ( y ), sd ( y ) ), n = n , sy2 = sy2 , sam , control = list ( maxit = 2000 ) ) mod <- optim ( mod $ par , sam , n = n , sy2 = sy2 , control = list ( maxit = 20000 ) ) result <- c ( - mod $ value , mod $ par [ 1 ], exp ( mod $ par [ 2 ]) ) names ( result ) <- c ( "log-likelihood" , "mu" , "sigma squared" ) result } The partial derivatives of the log-likelihood are written as ∂ ∂ l ∂ ∂ μ μ = ∑ ∑ i = 1 n ( x i − − μ μ ) σ σ 2 − − 2 σ σ 2 ∑ ∑ i = 1 n x i e − − 2 μ μ x i σ σ 2 1 + e − − 2 μ μ x i σ σ 2 {\displaystyle {\frac {\partial l}{\partial \mu }}={\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)}{\sigma ^{2}}}-{\frac {2}{\sigma ^{2}}}\sum _{i=1}^{n}{\frac {x_{i}e^{\frac {-2\mu x_{i}}{\sigma ^{2}}}}{1+e^{\frac {-2\mu x_{i}}{\sigma ^{2}}}}}} ∂ ∂ l ∂ ∂ μ μ = ∑ ∑ i = 1 n ( x i − − μ μ ) σ σ 2 − − 2 σ σ 2 ∑ ∑ i = 1 n x i 1 + e 2 μ μ x i σ σ 2 and {\displaystyle {\frac {\partial l}{\partial \mu }}={\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)}{\sigma ^{2}}}-{\frac {2}{\sigma ^{2}}}\sum _{i=1}^{n}{\frac {x_{i}}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}\ \ {\text{and}}} ∂ ∂ l ∂ ∂ σ σ 2 = − − n 2 σ σ 2 + ∑ ∑ i = 1 n ( x i − − μ μ ) 2 2 σ σ 4 + 2 μ μ σ σ 4 ∑ ∑ i = 1 n x i e − − 2 μ μ x i σ σ 2 1 + e − − 2 μ μ x i σ σ 2 {\displaystyle {\frac {\partial l}{\partial \sigma ^{2}}}=-{\frac {n}{2\sigma ^{2}}}+{\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)^{2}}{2\sigma ^{4}}}+{\frac {2\mu }{\sigma ^{4}}}\sum _{i=1}^{n}{\frac {x_{i}e^{-{\frac {2\mu x_{i}}{\sigma ^{2}}}}}{1+e^{-{\frac {2\mu x_{i}}{\sigma ^{2}}}}}}} ∂ ∂ l ∂ ∂ σ σ 2 = − − n 2 σ σ 2 + ∑ ∑ i = 1 n ( x i − − μ μ ) 2 2 σ σ 4 + 2 μ μ σ σ 4 ∑ ∑ i = 1 n x i 1 + e 2 μ μ x i σ σ 2 {\displaystyle {\frac {\partial l}{\partial \sigma ^{2}}}=-{\frac {n}{2\sigma ^{2}}}+{\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)^{2}}{2\sigma ^{4}}}+{\frac {2\mu }{\sigma ^{4}}}\sum _{i=1}^{n}{\frac {x_{i}}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}} .

By equating the first partial derivative of the log-likelihood to zero, we obtain a nice relationship ∑ ∑ i = 1 n x i 1 + e 2 μ μ x i σ σ 2 = ∑ ∑ i = 1 n ( x i − − μ μ ) 2 {\displaystyle \sum _{i=1}^{n}{\frac {x_{i}}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}={\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)}{2}}} .

Note that the above equation has three solutions, one at zero and two more with the opposite sign. By substituting the above equation, to the partial derivative of the log-likelihood w.r.t σ σ 2 {\displaystyle \sigma ^{2}} and equating it to zero, we get the following expression for the variance σ σ 2 = ∑ ∑ i = 1 n ( x i − − μ μ ) 2 n + 2 μ μ ∑ ∑ i = 1 n ( x i − − μ μ ) n = ∑ ∑ i = 1 n ( x i 2 − − μ μ 2 ) n = ∑ ∑ i = 1 n x i 2 n − − μ μ 2 {\displaystyle \sigma ^{2}={\frac {\sum _{i=1}^{n}\left(x_{i}-\mu \right)^{2}}{n}}+{\frac {2\mu \sum _{i=1}^{n}\left(x_{i}-\mu \right)}{n}}={\frac {\sum _{i=1}^{n}\left(x_{i}^{2}-\mu ^{2}\right)}{n}}={\frac {\sum _{i=1}^{n}x_{i}^{2}}{n}}-\mu ^{2}} , which is the same formula as in the normal distribution . A main difference here is that μ μ {\displaystyle \mu } and σ σ 2 {\displaystyle \sigma ^{2}} are not statistically independent. The above relationships can be used to obtain maximum likelihood estimates in an efficient recursive way. We start with an initial value for σ σ 2 {\displaystyle \sigma ^{2}} and find the positive root ( μ μ {\displaystyle \mu } ) of the last equation. Then, we get an updated value of σ σ 2 {\displaystyle \sigma ^{2}} . The procedure is being repeated until the change in the log-likelihood value is negligible. Another easier and more efficient way is to perform a search algorithm. Let us write the last equation in a more elegant way 2 ∑ ∑ i = 1 n x i 1 + e 2 μ μ x i σ σ 2 − − ∑ ∑ i = 1 n x i ( 1 + e 2 μ μ x i σ σ 2 ) 1 + e 2 μ μ x i σ σ 2 + n μ μ = 0 {\displaystyle 2\sum _{i=1}^{n}{\frac {x_{i}}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}-\sum _{i=1}^{n}{\frac {x_{i}\left(1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}\right)}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}+n\mu =0} ∑ ∑ i = 1 n x i ( 1 − − e 2 μ μ x i σ σ 2 ) 1 + e 2 μ μ x i σ σ 2 + n μ μ = 0 {\displaystyle \sum _{i=1}^{n}{\frac {x_{i}\left(1-e^{\frac {2\mu x_{i}}{\sigma ^{2}}}\right)}{1+e^{\frac {2\mu x_{i}}{\sigma ^{2}}}}}+n\mu =0} .

It becomes clear that the optimization the log-likelihood with respect to the two parameters has turned into a root search of a function. This of course is identical to the previous root search. Tsagris et al. (2014) spotted that there are three roots to this equation for μ μ {\displaystyle \mu } , i.e. there are three possible values of μ μ {\displaystyle \mu } that satisfy this equation. The − − μ μ {\displaystyle -\mu } and + μ μ {\displaystyle +\mu } , which are the maximum likelihood estimates and 0, which corresponds to the minimum log-likelihood.

See also [ edit ] Folded cumulative distribution Half-normal distribution Modified half-normal distribution [ 1 ] with the pdf on ( 0 , ∞ ∞ ) {\displaystyle (0,\infty )} is given as f ( x ) = 2 β β α α 2 x α α − − 1 exp ⁡ ⁡ ( − − β β x 2 + γ γ x ) Ψ Ψ ( α α 2 , γ γ β β ) {\displaystyle f(x)={\frac {2\beta ^{\frac {\alpha }{2}}x^{\alpha -1}\exp(-\beta x^{2}+\gamma x)}{\Psi {\left({\frac {\alpha }{2}},{\frac {\gamma }{\sqrt {\beta }}}\right)}}}} , where Ψ Ψ ( α α , z ) = 1 Ψ Ψ 1 ( ( α α , 1 2 ) ( 1 , 0 ) ; z ) {\displaystyle \Psi (\alpha ,z)={}_{1}\Psi _{1}\left({\begin{matrix}\left(\alpha ,{\frac {1}{2}}\right)\\(1,0)\end{matrix}};z\right)} denotes the Fox–Wright Psi function .

Truncated normal distribution References [ edit ] ^ a b Sun, Jingchao; Kong, Maiying; Pal, Subhadip (22 June 2021).

"The Modified-Half-Normal distribution: Properties and an efficient sampling scheme" (PDF) .

Communications in Statistics - Theory and Methods .

52 (5): 1591– 1613.

doi : 10.1080/03610926.2021.1934700 .

ISSN 0361-0926 .

S2CID 237919587 .

Tsagris, M.; Beneki, C.; Hassani, H. (2014).

"On the folded normal distribution" .

Mathematics .

2 (1): 12– 28.

arXiv : 1402.3559 .

doi : 10.3390/math2010012 .

Leone FC, Nottingham RB, Nelson LS (1961). "The Folded Normal Distribution".

Technometrics .

3 (4): 543– 550.

doi : 10.2307/1266560 .

hdl : 2027/mdp.39015095248541 .

JSTOR 1266560 .

Johnson NL (1962). "The folded normal distribution: accuracy of the estimation by maximum likelihood".

Technometrics .

4 (2): 249– 256.

doi : 10.2307/1266622 .

JSTOR 1266622 .

Nelson LS (1980). "The Folded Normal Distribution".

J Qual Technol .

12 (4): 236– 238.

doi : 10.1080/00224065.1980.11980971 .

hdl : 2027/mdp.39015095248541 .

Elandt RC (1961). "The folded normal distribution: two methods of estimating parameters from moments".

Technometrics .

3 (4): 551– 562.

doi : 10.2307/1266561 .

JSTOR 1266561 .

Lin PC (2005). "Application of the generalized folded-normal distribution to the process capability measures".

Int J Adv Manuf Technol .

26 ( 7– 8): 825– 830.

doi : 10.1007/s00170-003-2043-x .

S2CID 123589207 .

Psarakis, S.; Panaretos, J. (1990). "The folded t distribution".

Communications in Statistics - Theory and Methods .

19 (7): 2717– 2734.

doi : 10.1080/03610929008830342 .

S2CID 121332770 .

Psarakis, S.; Panaretos, J. (2001). "On some bivariate extensions of the folded normal and the folded-t distributions".

Journal of Applied Statistical Science .

10 (2): 119– 136.

Chakraborty, A. K.; Chatterjee, M. (2013). "On multivariate folded normal distribution".

Sankhyā: The Indian Journal of Statistics, Series B .

75 (1): 1– 15.

JSTOR 42003783 .

External links [ edit ] Random (formerly Virtual Laboratories): The Folded Normal Distribution v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Folded_normal_distribution&oldid=1237907072 " Categories : Continuous distributions Normal distribution Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 1 August 2024, at 02:55 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Folded normal distribution 3 languages Add topic

