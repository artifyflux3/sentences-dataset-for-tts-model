Title: Transpose

URL Source: https://en.wikipedia.org/wiki/Transpose

Published Time: 2003-01-24T14:46:56Z

Markdown Content:
From Wikipedia, the free encyclopedia

[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Matrix_transpose.gif/200px-Matrix_transpose.gif)](https://en.wikipedia.org/wiki/File:Matrix_transpose.gif)

The transpose **A**T of a matrix **A** can be obtained by reflecting the elements along its main diagonal. Repeating the process on the transposed matrix returns the elements to their original position.

In [linear algebra](https://en.wikipedia.org/wiki/Linear_algebra "Linear algebra"), the **transpose** of a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics) "Matrix (mathematics)") is an operator which flips a matrix over its diagonal; that is, it switches the row and column indices of the matrix **A** by producing another matrix, often denoted by **A**T (among other notations).[[1]](https://en.wikipedia.org/wiki/Transpose#cite_note-1)

The transpose of a matrix was introduced in 1858 by the British mathematician [Arthur Cayley](https://en.wikipedia.org/wiki/Arthur_Cayley "Arthur Cayley").[[2]](https://en.wikipedia.org/wiki/Transpose#cite_note-2)

Transpose of a matrix
---------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=1 "Edit section: Transpose of a matrix")]

This article assumes that matrices are taken over a commutative ring. These results may not hold in the non-commutative case.

The transpose of a matrix **A**, denoted by **A**T,[[3]](https://en.wikipedia.org/wiki/Transpose#cite_note-Whitelaw1991-3)T**A**, **A**tr, t**A** or **A**t, may be constructed by any one of the following methods:

1.   [Reflect](https://en.wikipedia.org/wiki/Reflection_(mathematics) "Reflection (mathematics)")**A** over its [main diagonal](https://en.wikipedia.org/wiki/Main_diagonal "Main diagonal") (which runs from top-left to bottom-right) to obtain **A**T
2.   Write the rows of **A** as the columns of **A**T
3.   Write the columns of **A** as the rows of **A**T

Formally, the i th row, j th column element of **A**T is the j th row, i th column element of **A**:

![Image 2: {\displaystyle \left[\mathbf {A} ^{\text{T}}\right]_{ij}=\left[\mathbf {A} \right]_{ji}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5f120b7c3b828199d9486c36abcc04d2d2337f4f)
If **A** is an _m_ × _n_ matrix, then **A**T is an _n_ × _m_ matrix.

#### Matrix definitions involving transposition

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=3 "Edit section: Matrix definitions involving transposition")]

A square matrix whose transpose is equal to itself is called a _[symmetric matrix](https://en.wikipedia.org/wiki/Symmetric\_matrix "Symmetric matrix")_; that is, **A** is symmetric if

![Image 3: {\displaystyle \mathbf {A} ^{\text{T}}=\mathbf {A} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2fcb3ecaea0552d0ae5025ed544a1ed1fcc51c19)
A square matrix whose transpose is equal to its negative is called a _[skew-symmetric matrix](https://en.wikipedia.org/wiki/Skew-symmetric\_matrix "Skew-symmetric matrix")_; that is, **A** is skew-symmetric if

![Image 4: {\displaystyle \mathbf {A} ^{\text{T}}=-\mathbf {A} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/931d7baf99b63acafc142273a288107f2f6ad7aa)
A square [complex](https://en.wikipedia.org/wiki/Complex_number "Complex number") matrix whose transpose is equal to the matrix with every entry replaced by its [complex conjugate](https://en.wikipedia.org/wiki/Complex_conjugate "Complex conjugate") (denoted here with an overline) is called a _[Hermitian matrix](https://en.wikipedia.org/wiki/Hermitian\_matrix "Hermitian matrix")_ (equivalent to the matrix being equal to its [conjugate transpose](https://en.wikipedia.org/wiki/Conjugate_transpose "Conjugate transpose")); that is, **A** is Hermitian if

![Image 5: {\displaystyle \mathbf {A} ^{\text{T}}={\overline {\mathbf {A} }}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f2f70d84046cb2f475165bd4dee9c274a0670b90)
A square [complex](https://en.wikipedia.org/wiki/Complex_number "Complex number") matrix whose transpose is equal to the negation of its complex conjugate is called a _[skew-Hermitian matrix](https://en.wikipedia.org/wiki/Skew-Hermitian\_matrix "Skew-Hermitian matrix")_; that is, **A** is skew-Hermitian if

![Image 6: {\displaystyle \mathbf {A} ^{\text{T}}=-{\overline {\mathbf {A} }}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d9f0562001a88c95c812e642c8b9fb9708d3393)
A square matrix whose transpose is equal to its [inverse](https://en.wikipedia.org/wiki/Inverse_matrix "Inverse matrix") is called an _[orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal\_matrix "Orthogonal matrix")_; that is, **A** is orthogonal if

![Image 7: {\displaystyle \mathbf {A} ^{\text{T}}=\mathbf {A} ^{-1}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/06bfed021524684df3e9bb9836e86b1a8a147dff)
A square complex matrix whose transpose is equal to its conjugate inverse is called a _[unitary matrix](https://en.wikipedia.org/wiki/Unitary\_matrix "Unitary matrix")_; that is, **A** is unitary if

![Image 8: {\displaystyle \mathbf {A} ^{\text{T}}={\overline {\mathbf {A} ^{-1}}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/98f11c53847d25d5ad0ac40dcaa122ee919e50df)
*   ![Image 9: {\displaystyle {\begin{bmatrix}1&2\end{bmatrix}}^{\text{T}}=\,{\begin{bmatrix}1\\2\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3eecd050791abaf025fe8fa6b00f939c7117ac1d)
*   ![Image 10: {\displaystyle {\begin{bmatrix}1&2\\3&4\end{bmatrix}}^{\text{T}}={\begin{bmatrix}1&3\\2&4\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/378d08047d8eca553bef941534065ac73e4bd2aa)
*   ![Image 11: {\displaystyle {\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}}^{\text{T}}={\begin{bmatrix}1&3&5\\2&4&6\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5be9f0e3d0dce8a1d1efa34f4ede92c92fe7feac)

Let **A** and **B** be matrices and c be a [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics) "Scalar (mathematics)").

*   ![Image 12: {\displaystyle \left(\mathbf {A} ^{\text{T}}\right)^{\text{T}}=\mathbf {A} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a70d671da41dd7713b972f53f86341fab427853a)The operation of taking the transpose is an [involution](https://en.wikipedia.org/wiki/Involution_(mathematics) "Involution (mathematics)") (self-[inverse](https://en.wikipedia.org/wiki/Inverse_matrix "Inverse matrix")).
*   ![Image 13: {\displaystyle \left(\mathbf {A} +\mathbf {B} \right)^{\text{T}}=\mathbf {A} ^{\text{T}}+\mathbf {B} ^{\text{T}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cadeebe8007fa7f72dfd0d2c6eeb4020ea4d875e)The transpose respects [addition](https://en.wikipedia.org/wiki/Matrix_addition "Matrix addition").
*   ![Image 14: {\displaystyle \left(c\mathbf {A} \right)^{\text{T}}=c(\mathbf {A} ^{\text{T}}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9b491949d7433bb7c17107dc10cebbb9f9364c8c)The transpose of a scalar is the same scalar. Together with the preceding property, this implies that the transpose is a [linear map](https://en.wikipedia.org/wiki/Linear_map "Linear map") from the [space](https://en.wikipedia.org/wiki/Vector_space "Vector space") of _m_ × _n_ matrices to the space of the _n_ × _m_ matrices.
*   ![Image 15: {\displaystyle \left(\mathbf {AB} \right)^{\text{T}}=\mathbf {B} ^{\text{T}}\mathbf {A} ^{\text{T}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/117656bdd47a84f48b1d08f92e8b5bfb2bfacc36)The order of the factors reverses. By induction, this result extends to the general case of multiple matrices, so (**A**1**A**2...**A**_k_−1**A**_k_)T=**A**_k_ T**A**_k_−1 T…**A**2 T**A**1 T.
*   ![Image 16: {\displaystyle \det \left(\mathbf {A} ^{\text{T}}\right)=\det(\mathbf {A} ).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2c0b220c06912da6a31ad51d7f90f1a78ed6a156)The [determinant](https://en.wikipedia.org/wiki/Determinant "Determinant") of a square matrix is the same as the determinant of its transpose.
*   The [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product") of two column vectors **a** and **b** can be computed as the single entry of the matrix product![Image 17: {\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {a} ^{\text{T}}\mathbf {b} .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/324b0fd8e650c4e722a4dcbe57ad1c9105b04a53)
*   If **A** has only real entries, then **A**T**A** is a [positive-semidefinite matrix](https://en.wikipedia.org/wiki/Positive-semidefinite_matrix "Positive-semidefinite matrix").
*   ![Image 18: {\displaystyle \left(\mathbf {A} ^{\text{T}}\right)^{-1}=\left(\mathbf {A} ^{-1}\right)^{\text{T}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43fba4b80d5ab759b162685111d6be75a85da54e)The transpose of an invertible matrix is also invertible, and its inverse is the transpose of the inverse of the original matrix.

The notation **A**−T is sometimes used to represent either of these equivalent expressions.
*   If **A** is a square matrix, then its [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace "Eigenvalue, eigenvector and eigenspace") are equal to the eigenvalues of its transpose, since they share the same [characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial "Characteristic polynomial").
*   ![Image 19: {\displaystyle \left(\mathbf {A} \mathbf {a} \right)\cdot \mathbf {b} =\mathbf {a} \cdot \left(\mathbf {A} ^{\text{T}}\mathbf {b} \right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6f13767d72c94958878b101e5fbed786d0bcca0c) for two column vectors ![Image 20: {\displaystyle \mathbf {a} ,\mathbf {b} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6f1488994015f56ee267a2dfadf01a1d067e7d6) and the standard [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product").
*   Over any field ![Image 21: {\displaystyle k}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40), a square matrix ![Image 22: {\displaystyle \mathbf {A} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1) is [similar](https://en.wikipedia.org/wiki/Matrix_similarity "Matrix similarity") to ![Image 23: {\displaystyle \mathbf {A} ^{\text{T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43ec27bc146618b55c9b51ab0a5b5d2c6c9d0498). This implies that ![Image 24: {\displaystyle \mathbf {A} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1) and ![Image 25: {\displaystyle \mathbf {A} ^{\text{T}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43ec27bc146618b55c9b51ab0a5b5d2c6c9d0498) have the same [invariant factors](https://en.wikipedia.org/wiki/Invariant_factors "Invariant factors"), which implies they share the same minimal polynomial, characteristic polynomial, and eigenvalues, among other properties.A proof of this property uses the following two observations. 

If **A** is an _m_ × _n_ matrix and **A**T is its transpose, then the result of [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication") with these two matrices gives two square matrices: **A A**T is _m_ × _m_ and **A**T**A** is _n_ × _n_. Furthermore, these products are [symmetric matrices](https://en.wikipedia.org/wiki/Symmetric_matrices "Symmetric matrices"). Indeed, the matrix product **A A**T has entries that are the [inner product](https://en.wikipedia.org/wiki/Inner_product "Inner product") of a row of **A** with a column of **A**T. But the columns of **A**T are the rows of **A**, so the entry corresponds to the inner product of two rows of **A**. If _p_ _ij_ is the entry of the product, it is obtained from rows i and j in **A**. The entry _p_ _ji_ is also obtained from these rows, thus _p_ _ij_ = _p_ _ji_, and the product matrix (_p_ _ij_) is symmetric. Similarly, the product **A**T**A** is a symmetric matrix.

A quick proof of the symmetry of **A A**T results from the fact that it is its own transpose:

![Image 26: {\displaystyle \left(\mathbf {A} \mathbf {A} ^{\text{T}}\right)^{\text{T}}=\left(\mathbf {A} ^{\text{T}}\right)^{\text{T}}\mathbf {A} ^{\text{T}}=\mathbf {A} \mathbf {A} ^{\text{T}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1895e5dc268a0bc30e709c657ce2d476cb996050)[[4]](https://en.wikipedia.org/wiki/Transpose#cite_note-4)

### Implementation of matrix transposition on computers

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=7 "Edit section: Implementation of matrix transposition on computers")]

[![Image 27](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Row_and_column_major_order.svg/250px-Row_and_column_major_order.svg.png)](https://en.wikipedia.org/wiki/File:Row_and_column_major_order.svg)

Illustration of [row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order "Row- and column-major order")

On a [computer](https://en.wikipedia.org/wiki/Computer "Computer"), one can often avoid explicitly transposing a matrix in [memory](https://en.wikipedia.org/wiki/Random_access_memory "Random access memory") by simply accessing the same data in a different order. For example, [software libraries](https://en.wikipedia.org/wiki/Software_libraries "Software libraries") for [linear algebra](https://en.wikipedia.org/wiki/Linear_algebra "Linear algebra"), such as [BLAS](https://en.wikipedia.org/wiki/BLAS "BLAS"), typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.

However, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in [row-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order "Row- and column-major order"), the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a [fast Fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform "Fast Fourier transform") algorithm, transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing [memory locality](https://en.wikipedia.org/wiki/Memory_locality "Memory locality").

Ideally, one might hope to transpose a matrix with minimal additional storage. This leads to the problem of transposing an _n_ × _m_ matrix [in-place](https://en.wikipedia.org/wiki/In-place "In-place"), with [O(1)](https://en.wikipedia.org/wiki/Big_O_notation "Big O notation") additional storage or at most storage much less than _mn_. For _n_ ≠ _m_, this involves a complicated [permutation](https://en.wikipedia.org/wiki/Permutation "Permutation") of the data elements that is non-trivial to implement in-place. Therefore, efficient [in-place matrix transposition](https://en.wikipedia.org/wiki/In-place_matrix_transposition "In-place matrix transposition") has been the subject of numerous research publications in [computer science](https://en.wikipedia.org/wiki/Computer_science "Computer science"), starting in the late 1950s, and several algorithms have been developed.

Transposes of linear maps and bilinear forms
--------------------------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=8 "Edit section: Transposes of linear maps and bilinear forms")]

As the main use of matrices is to represent linear maps between [finite-dimensional vector spaces](https://en.wikipedia.org/wiki/Finite-dimensional_vector_space "Finite-dimensional vector space"), the transpose is an operation on matrices that may be seen as the representation of some operation on linear maps.

This leads to a much more general definition of the transpose that works on every linear map, even when linear maps cannot be represented by matrices (such as in the case of infinite dimensional vector spaces). In the finite dimensional case, the matrix representing the transpose of a linear map is the transpose of the matrix representing the linear map, independently of the [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra) "Basis (linear algebra)") choice.

### Transpose of a linear map

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=9 "Edit section: Transpose of a linear map")]

Let _X_# denote the [algebraic dual space](https://en.wikipedia.org/wiki/Algebraic_dual_space "Algebraic dual space") of an R-[module](https://en.wikipedia.org/wiki/Module_(mathematics) "Module (mathematics)")X. Let X and Y be R-modules. If _u_: _X_ → _Y_ is a [linear map](https://en.wikipedia.org/wiki/Linear_map "Linear map"), then its **algebraic adjoint** or **dual**,[[5]](https://en.wikipedia.org/wiki/Transpose#cite_note-FOOTNOTESchaeferWolff1999128-5) is the map _u_#: _Y_# → _X_# defined by _f_ ↦ _f_ ∘ _u_. The resulting functional _u_#(_f_) is called the **[pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry) "Pullback (differential geometry)")** of f by u. The following [relation](https://en.wikipedia.org/wiki/Relation_(math) "Relation (math)") characterizes the algebraic adjoint of u[[6]](https://en.wikipedia.org/wiki/Transpose#cite_note-6)

⟨_u_#(_f_), _x_⟩ = ⟨_f_, _u_(_x_)⟩ for all _f_ ∈ _Y_# and _x_ ∈ _X_
where ⟨•, •⟩ is the [natural pairing](https://en.wikipedia.org/wiki/Natural_pairing "Natural pairing") (i.e. defined by ⟨_h_, _z_⟩:= _h_(_z_)). This definition also applies unchanged to left modules and to vector spaces.[[7]](https://en.wikipedia.org/wiki/Transpose#cite_note-7)

The definition of the transpose may be seen to be independent of any bilinear form on the modules, unlike the adjoint ([below](https://en.wikipedia.org/wiki/Transpose#Adjoint)).

The [continuous dual space](https://en.wikipedia.org/wiki/Continuous_dual_space "Continuous dual space") of a [topological vector space](https://en.wikipedia.org/wiki/Topological_vector_space "Topological vector space") (TVS) X is denoted by _X_′. If X and Y are TVSs then a linear map _u_: _X_ → _Y_ is **weakly continuous** if and only if _u_#(_Y_′) ⊆ _X_′, in which case we let t _u_: _Y_′ → _X_′ denote the restriction of _u_# to _Y_′. The map t _u_ is called the **transpose**[[8]](https://en.wikipedia.org/wiki/Transpose#cite_note-FOOTNOTETr%C3%A8ves2006240-8) of u.

If the matrix **A** describes a linear map with respect to [bases](https://en.wikipedia.org/wiki/Basis_(linear_algebra) "Basis (linear algebra)") of V and W, then the matrix **A**T describes the transpose of that linear map with respect to the [dual bases](https://en.wikipedia.org/wiki/Dual_basis "Dual basis").

### Transpose of a bilinear form

[[edit](https://en.wikipedia.org/w/index.php?title=Transpose&action=edit&section=10 "Edit section: Transpose of a bilinear form")]

Every linear map to the dual space _u_: _X_ → _X_# defines a bilinear form _B_: _X_ × _X_ → _F_, with the relation _B_(_x_, _y_) = _u_(_x_)(_y_). By defining the transpose of this bilinear form as the bilinear form t _B_ defined by the transpose t _u_: _X_## → _X_# i.e. t _B_(_y_, _x_) = t _u_(Ψ(_y_))(_x_), we find that _B_(_x_, _y_) = t _B_(_y_, _x_). Here, Ψ is the natural [homomorphism](https://en.wikipedia.org/wiki/Homomorphism "Homomorphism")_X_ → _X_## into the [double dual](https://en.wikipedia.org/wiki/Double_dual "Double dual").

If the vector spaces X and Y have respectively [nondegenerate](https://en.wikipedia.org/wiki/Nondegenerate_form "Nondegenerate form")[bilinear forms](https://en.wikipedia.org/wiki/Bilinear_form "Bilinear form")_B_ _X_ and _B_ _Y_, a concept known as the **adjoint**, which is closely related to the transpose, may be defined:

If _u_: _X_ → _Y_ is a [linear map](https://en.wikipedia.org/wiki/Linear_map "Linear map") between [vector spaces](https://en.wikipedia.org/wiki/Vector_space "Vector space")X and Y, we define g as the **adjoint** of u if _g_: _Y_ → _X_ satisfies

![Image 28: {\displaystyle B_{X}{\big (}x,g(y){\big )}=B_{Y}{\big (}u(x),y{\big )}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d9e1f7d7b26922fab10c0b2d9358d686bd013674) for all _x_ ∈ _X_ and _y_ ∈ _Y_.
These bilinear forms define an [isomorphism](https://en.wikipedia.org/wiki/Isomorphism "Isomorphism") between X and _X_#, and between _Y_ and _Y_#, resulting in an isomorphism between the transpose and adjoint of u. The matrix of the adjoint of a map is the transposed matrix only if the [bases](https://en.wikipedia.org/wiki/Basis_(linear_algebra) "Basis (linear algebra)") are [orthonormal](https://en.wikipedia.org/wiki/Orthonormality "Orthonormality") with respect to their bilinear forms. In this context, many authors however, use the term transpose to refer to the adjoint as defined here.

The adjoint allows us to consider whether _g_: _Y_ → _X_ is equal to _u_ −1: _Y_ → _X_. In particular, this allows the [orthogonal group](https://en.wikipedia.org/wiki/Orthogonal_group "Orthogonal group") over a vector space X with a quadratic form to be defined without reference to matrices (nor the components thereof) as the set of all linear maps _X_ → _X_ for which the adjoint equals the inverse.

Over a complex vector space, one often works with [sesquilinear forms](https://en.wikipedia.org/wiki/Sesquilinear_form "Sesquilinear form") (conjugate-linear in one argument) instead of bilinear forms. The [Hermitian adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint "Hermitian adjoint") of a map between such spaces is defined similarly, and the matrix of the Hermitian adjoint is given by the conjugate transpose matrix if the bases are orthonormal.

*   [Adjugate matrix](https://en.wikipedia.org/wiki/Adjugate_matrix "Adjugate matrix"), the transpose of the [cofactor matrix](https://en.wikipedia.org/wiki/Cofactor_matrix "Cofactor matrix")
*   [Conjugate transpose](https://en.wikipedia.org/wiki/Conjugate_transpose "Conjugate transpose")
*   [Converse relation](https://en.wikipedia.org/wiki/Converse_relation "Converse relation")
*   [Moore–Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse "Moore–Penrose pseudoinverse")
*   [Projection (linear algebra)](https://en.wikipedia.org/wiki/Projection_(linear_algebra) "Projection (linear algebra)")

1.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-1 "Jump up")**Nykamp, Duane. ["The transpose of a matrix"](https://mathinsight.org/matrix_transpose). _Math Insight_. Retrieved September 8, 2020.
2.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-2 "Jump up")**Arthur Cayley (1858) ["A memoir on the theory of matrices"](https://books.google.com/books?id=flFFAAAAcAAJ&pg=PA31), _Philosophical Transactions of the Royal Society of London_, **148**: 17–37. The transpose (or "transposition") is defined on page 31.
3.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-Whitelaw1991_3-0 "Jump up")**T.A. Whitelaw (1 April 1991). [_Introduction to Linear Algebra, 2nd edition_](https://books.google.com/books?id=6M_kDzA7-qIC&q=transpose). CRC Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-7514-0159-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-7514-0159-2 "Special:BookSources/978-0-7514-0159-2").
4.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-4 "Jump up")**[Gilbert Strang](https://en.wikipedia.org/wiki/Gilbert_Strang "Gilbert Strang") (2006) _Linear Algebra and its Applications_ 4th edition, page 51, Thomson [Brooks/Cole](https://en.wikipedia.org/wiki/Brooks/Cole "Brooks/Cole")[ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-03-010567-6](https://en.wikipedia.org/wiki/Special:BookSources/0-03-010567-6 "Special:BookSources/0-03-010567-6")
5.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-FOOTNOTESchaeferWolff1999128_5-0 "Jump up")**[Schaefer & Wolff 1999](https://en.wikipedia.org/wiki/Transpose#CITEREFSchaeferWolff1999), p.128.
6.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-6 "Jump up")**[Halmos 1974](https://en.wikipedia.org/wiki/Transpose#CITEREFHalmos1974), §44
7.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-7 "Jump up")**[Bourbaki 1989](https://en.wikipedia.org/wiki/Transpose#CITEREFBourbaki1989), II §2.5
8.   **[^](https://en.wikipedia.org/wiki/Transpose#cite_ref-FOOTNOTETr%C3%A8ves2006240_8-0 "Jump up")**[Trèves 2006](https://en.wikipedia.org/wiki/Transpose#CITEREFTr%C3%A8ves2006), p.240.

*   [Bourbaki, Nicolas](https://en.wikipedia.org/wiki/Nicolas_Bourbaki "Nicolas Bourbaki") (1989) [1970]. [_Algebra I Chapters 1-3_](http://www.cmat.edu.uy/~marclan/TM/Algebra%20i%20-%20Bourbaki.pdf) [_Algèbre: Chapitres 1 à 3_] (PDF). [Éléments de mathématique](https://en.wikipedia.org/wiki/%C3%89l%C3%A9ments_de_math%C3%A9matique "Éléments de mathématique"). Berlin New York: Springer Science & Business Media. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-3-540-64243-5](https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-64243-5 "Special:BookSources/978-3-540-64243-5"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[18588156](https://search.worldcat.org/oclc/18588156).

*   [Halmos, Paul](https://en.wikipedia.org/wiki/Paul_Halmos "Paul Halmos") (1974), _Finite dimensional vector spaces_, Springer, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-387-90093-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-90093-3 "Special:BookSources/978-0-387-90093-3").
*   Maruskin, Jared M. (2012). [_Essential Linear Algebra_](https://books.google.com/books?id=aOF3-hx3u1kC&pg=PA122). San José: Solar Crest. pp.122–132. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-9850627-3-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-9850627-3-6 "Special:BookSources/978-0-9850627-3-6").
*   [Schaefer, Helmut H.](https://en.wikipedia.org/wiki/Helmut_H._Schaefer "Helmut H. Schaefer"); Wolff, Manfred P. (1999). _Topological Vector Spaces_. [GTM](https://en.wikipedia.org/wiki/Graduate_Texts_in_Mathematics "Graduate Texts in Mathematics"). Vol.8 (Second ed.). New York, NY: Springer New York Imprint Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-1-4612-7155-0](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4612-7155-0 "Special:BookSources/978-1-4612-7155-0"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[840278135](https://search.worldcat.org/oclc/840278135).
*   [Trèves, François](https://en.wikipedia.org/wiki/Fran%C3%A7ois_Tr%C3%A8ves "François Trèves") (2006) [1967]. _Topological Vector Spaces, Distributions and Kernels_. Mineola, N.Y.: Dover Publications. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-486-45352-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-486-45352-1 "Special:BookSources/978-0-486-45352-1"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)")[853623322](https://search.worldcat.org/oclc/853623322).
*   Schwartz, Jacob T. (2001). [_Introduction to Matrices and Vectors_](https://books.google.com/books?id=fMG3y1z9Jw0C&pg=PA126). Mineola: Dover. pp.126–132. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[0-486-42000-0](https://en.wikipedia.org/wiki/Special:BookSources/0-486-42000-0 "Special:BookSources/0-486-42000-0").

*   Gilbert Strang (Spring 2010) [Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/) from MIT Open Courseware
