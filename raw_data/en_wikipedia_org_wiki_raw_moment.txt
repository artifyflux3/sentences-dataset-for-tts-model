Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Significance of the moments Toggle Significance of the moments subsection 1.1 Standardized moments 1.2 Notable moments 1.2.1 Mean 1.2.2 Variance 1.2.3 Skewness 1.2.4 Kurtosis 1.3 Higher moments 1.4 Mixed moments 2 Properties of moments Toggle Properties of moments subsection 2.1 Transformation of center 2.2 The moment of a convolution of function 3 Cumulants 4 Sample moments 5 Problem of moments 6 Partial moments 7 Central moments in metric spaces 8 See also 9 References 10 Further reading 11 External links Toggle the table of contents Moment (mathematics) 32 languages العربية Беларуская Català Čeština Deutsch Ελληνικά Español Esperanto Euskara فارسی Français 한국어 Italiano עברית ಕನ್ನಡ Magyar Македонски Nederlands 日本語 Polski Português Русский Slovenčina Slovenščina Sunda Suomi Svenska Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia (Redirected from Raw moment ) In mathematics, a quantitative measure of the shape of a set of points For the physical concept, see Moment (physics) .

In mathematics , the moments of a function are certain quantitative measures related to the shape of the function's graph .  For example: If the function represents mass density, then the zeroth moment is the total mass, the first moment (normalized by total mass) is the center of mass , and the second moment is the moment of inertia . If the function is a probability distribution , then the first moment is the expected value , the second central moment is the variance , the third standardized moment is the skewness , and the fourth standardized moment is the kurtosis .

For a distribution of mass or probability on a bounded interval , the collection of all the moments (of all orders, from 0 to ∞ ) uniquely determines the distribution ( Hausdorff moment problem ).  The same is not true on unbounded intervals ( Hamburger moment problem ).

In the mid-nineteenth century, Pafnuty Chebyshev became the first person to think systematically in terms of the moments of random variables .

[ 1 ] Significance of the moments [ edit ] The n -th raw moment (i.e., moment about zero) of a random variable X {\displaystyle X} with density function f ( x ) {\displaystyle f(x)} is defined by [ 2 ] μ μ n ′ = ⟨ ⟨ X n ⟩ ⟩ = d e f { ∑ ∑ i x i n f ( x i ) , discrete distribution ∫ ∫ x n f ( x ) d x , continuous distribution {\displaystyle \mu '_{n}=\langle X^{n}\rangle ~{\overset {\mathrm {def} }{=}}~{\begin{cases}\sum _{i}x_{i}^{n}f(x_{i}),&{\text{discrete distribution}}\\[1.2ex]\int x^{n}f(x)\,dx,&{\text{continuous distribution}}\end{cases}}} The n -th moment of a real -valued continuous random variable with density function f ( x ) {\displaystyle f(x)} about a value c {\displaystyle c} is the integral μ μ n = ∫ ∫ − − ∞ ∞ ∞ ∞ ( x − − c ) n f ( x ) d x .

{\displaystyle \mu _{n}=\int _{-\infty }^{\infty }(x-c)^{n}\,f(x)\,\mathrm {d} x.} It is possible to define moments for random variables in a more general fashion than moments for real-valued functions — see moments in metric spaces . The moment of a function, without further explanation, usually refers to the above expression with c = 0 {\displaystyle c=0} .
For the second and higher moments, the central moment (moments about the mean, with c being the mean) are usually used rather than the moments about zero, because they provide clearer information about the distribution's shape.

Other moments may also be defined. For example, the n th inverse moment about zero is E ⁡ ⁡ [ X − − n ] {\displaystyle \operatorname {E} \left[X^{-n}\right]} and the n -th logarithmic moment about zero is E ⁡ ⁡ [ ln n ⁡ ⁡ ( X ) ] .

{\displaystyle \operatorname {E} \left[\ln ^{n}(X)\right].} The n -th moment about zero of a probability density function f ( x ) {\displaystyle f(x)} is the expected value of X n {\displaystyle X^{n}} and is called a raw moment or crude moment .

[ 3 ] The moments about its mean μ μ {\displaystyle \mu } are called central moments ; these describe the shape of the function, independently of translation .

If f {\displaystyle f} is a probability density function , then the value of the integral above is called the n -th moment of the probability distribution . More generally, if F is a cumulative probability distribution function of any probability distribution, which may not have a density function, then the n -th moment of the probability distribution is given by the Riemann–Stieltjes integral μ μ n ′ = E ⁡ ⁡ [ X n ] = ∫ ∫ − − ∞ ∞ ∞ ∞ x n d F ( x ) {\displaystyle \mu '_{n}=\operatorname {E} \left[X^{n}\right]=\int _{-\infty }^{\infty }x^{n}\,\mathrm {d} F(x)} where X is a random variable that has this cumulative distribution F , and E is the expectation operator or mean.
When E ⁡ ⁡ [ | X n | ] = ∫ ∫ − − ∞ ∞ ∞ ∞ | x n | d F ( x ) = ∞ ∞ {\displaystyle \operatorname {E} \left[\left|X^{n}\right|\right]=\int _{-\infty }^{\infty }\left|x^{n}\right|\,\mathrm {d} F(x)=\infty } the moment is said not to exist. If the n -th moment about any point exists, so does the ( n − 1) -th moment (and thus, all lower-order moments) about every point.
The zeroth moment of any probability density function is 1, since the area under any probability density function must be equal to one.

Significance of moments (raw, central, standardised) and cumulants (raw, normalised), in connection with named properties of distributions Moment ordinal Moment Cumulant Raw Central Standardized Raw Normalized 1 Mean 0 0 Mean — 2 – Variance 1 Variance 1 3 – – Skewness – Skewness 4 – – (Non-excess or historical) kurtosis – Excess kurtosis 5 – – Hyperskewness – – 6 – – Hypertailedness – – 7+ – – – – – Standardized moments [ edit ] Main article: Standardized moment The normalised n -th central moment or standardised moment is the n -th central moment divided by σ n ; the normalised n -th central moment of the random variable X is μ μ n σ σ n = E ⁡ ⁡ [ ( X − − μ μ ) n ] σ σ n = E ⁡ ⁡ [ ( X − − μ μ ) n ] E ⁡ ⁡ [ ( X − − μ μ ) 2 ] n 2 .

{\displaystyle {\frac {\mu _{n}}{\sigma ^{n}}}={\frac {\operatorname {E} \left[(X-\mu )^{n}\right]}{\sigma ^{n}}}={\frac {\operatorname {E} \left[(X-\mu )^{n}\right]}{\operatorname {E} \left[(X-\mu )^{2}\right]^{\frac {n}{2}}}}.} These normalised central moments are dimensionless quantities , which represent the distribution independently of any linear change of scale.

Notable moments [ edit ] Mean [ edit ] Main article: Mean The first raw moment is the mean , usually denoted μ μ ≡ ≡ E ⁡ ⁡ [ X ] .

{\displaystyle \mu \equiv \operatorname {E} [X].} Variance [ edit ] Main article: Variance The second central moment is the variance . The positive square root of the variance is the standard deviation σ σ ≡ ≡ ( E ⁡ ⁡ [ ( x − − μ μ ) 2 ] ) 1 2 .

{\displaystyle \sigma \equiv \left(\operatorname {E} \left[(x-\mu )^{2}\right]\right)^{\frac {1}{2}}.} Skewness [ edit ] Main article: Skewness The third central moment is the measure of the lopsidedness of the distribution; any symmetric distribution will have a third central moment, if defined, of zero. The normalised third central moment is called the skewness , often γ . A distribution that is skewed to the left (the tail of the distribution is longer on the left) will have a negative skewness. A distribution that is skewed to the right (the tail of the distribution is longer on the right), will have a positive skewness.

For distributions that are not too different from the normal distribution , the median will be somewhere near μ − γσ /6 ; the mode about μ − γσ /2 .

Kurtosis [ edit ] Main article: Kurtosis The fourth central moment is a measure of the heaviness of the tail of the distribution. Since it is the expectation of a fourth power, the fourth central moment, where defined, is always nonnegative; and except for a point distribution , it is always strictly positive. The fourth central moment of a normal distribution is 3 σ 4 .

The kurtosis κ is defined to be the standardized fourth central moment. (Equivalently, as in the next section, excess kurtosis is the fourth cumulant divided by the square of the second cumulant .) [ 4 ] [ 5 ] If a distribution has heavy tails, the kurtosis will be high (sometimes called leptokurtic); conversely, light-tailed distributions (for example, bounded distributions such as the uniform) have low kurtosis (sometimes called platykurtic).

The kurtosis can be positive without limit, but κ must be greater than or equal to γ 2 + 1 ; equality only holds for binary distributions . For unbounded skew distributions not too far from normal, κ tends to be somewhere in the area of γ 2 and 2 γ 2 .

The inequality can be proven by considering E ⁡ ⁡ [ ( T 2 − − a T − − 1 ) 2 ] {\displaystyle \operatorname {E} \left[\left(T^{2}-aT-1\right)^{2}\right]} where T = ( X − μ )/ σ . This is the expectation of a square, so it is non-negative for all a ; however it is also a quadratic polynomial in a . Its discriminant must be non-positive, which gives the required relationship.

Higher moments [ edit ] High-order moments are moments beyond 4th-order moments.

As with variance, skewness, and kurtosis, these are higher-order statistics , involving non-linear combinations of the data, and can be used for description or estimation of further shape parameters . The higher the moment, the harder it is to estimate, in the sense that larger samples are required in order to obtain estimates of similar quality. This is due to the excess degrees of freedom consumed by the higher orders. Further, they can be subtle to interpret, often being most easily understood in terms of lower order moments – compare the higher-order derivatives of jerk and jounce in physics . For example, just as the 4th-order moment (kurtosis) can be interpreted as "relative importance of tails as compared to shoulders in contribution to dispersion" (for a given amount of dispersion, higher kurtosis corresponds to thicker tails, while lower kurtosis corresponds to broader shoulders), the 5th-order moment can be interpreted as measuring "relative importance of tails as compared to center ( mode and shoulders) in contribution to skewness" (for a given amount of skewness, higher 5th moment corresponds to higher skewness in the tail portions and little skewness of mode, while lower 5th moment corresponds to more skewness in shoulders).

Mixed moments [ edit ] Mixed moments are moments involving multiple variables.

The value E [ X k ] {\displaystyle E[X^{k}]} is called the moment of order k {\displaystyle k} (moments are also defined for non-integral k {\displaystyle k} ). The moments of the joint distribution of random variables X 1 .

.

.

X n {\displaystyle X_{1}...X_{n}} are defined similarly. For any integers k i ≥ ≥ 0 {\displaystyle k_{i}\geq 0} , the mathematical expectation E [ X 1 k 1 ⋯ ⋯ X n k n ] {\displaystyle E[{X_{1}}^{k_{1}}\cdots {X_{n}}^{k_{n}}]} is called a mixed moment of order k {\displaystyle k} (where k = k 1 + .

.

.

+ k n {\displaystyle k=k_{1}+...+k_{n}} ), and E [ ( X 1 − − E [ X 1 ] ) k 1 ⋯ ⋯ ( X n − − E [ X n ] ) k n ] {\displaystyle E[(X_{1}-E[X_{1}])^{k_{1}}\cdots (X_{n}-E[X_{n}])^{k_{n}}]} is called a central mixed moment of order k {\displaystyle k} . The mixed moment E [ ( X 1 − − E [ X 1 ] ) ( X 2 − − E [ X 2 ] ) ] {\displaystyle E[(X_{1}-E[X_{1}])(X_{2}-E[X_{2}])]} is called the covariance and is one of the basic characteristics of dependency between random variables.

Some examples are covariance , coskewness and cokurtosis .  While there is a unique covariance, there are multiple co-skewnesses and co-kurtoses.

Properties of moments [ edit ] Transformation of center [ edit ] Since ( x − − b ) n = ( x − − a + a − − b ) n = ∑ ∑ i = 0 n ( n i ) ( x − − a ) i ( a − − b ) n − − i {\displaystyle (x-b)^{n}=(x-a+a-b)^{n}=\sum _{i=0}^{n}{n \choose i}(x-a)^{i}(a-b)^{n-i}} where ( n i ) {\textstyle {\binom {n}{i}}} is the binomial coefficient , it follows that the moments about b can be calculated from the moments about a by: E [ ( x − − b ) n ] = ∑ ∑ i = 0 n ( n i ) E [ ( x − − a ) i ] ( a − − b ) n − − i .

{\displaystyle E\left[(x-b)^{n}\right]=\sum _{i=0}^{n}{n \choose i}E\left[(x-a)^{i}\right](a-b)^{n-i}.} The moment of a convolution of function [ edit ] Main article: Convolution The raw moment of a convolution h ( t ) = ( f ∗ ∗ g ) ( t ) = ∫ ∫ − − ∞ ∞ ∞ ∞ f ( τ τ ) g ( t − − τ τ ) d τ τ {\textstyle h(t)=(f*g)(t)=\int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau } reads μ μ n [ h ] = ∑ ∑ i = 0 n ( n i ) μ μ i [ f ] μ μ n − − i [ g ] {\displaystyle \mu _{n}[h]=\sum _{i=0}^{n}{n \choose i}\mu _{i}[f]\mu _{n-i}[g]} where μ μ n [ ⋅ ⋅ ] {\displaystyle \mu _{n}[\,\cdot \,]} denotes the n {\displaystyle n} -th moment of the function given in the brackets. This identity follows by the convolution theorem for moment generating function and applying the chain rule for differentiating a product.

Cumulants [ edit ] Main article: Cumulant The first raw moment and the second and third unnormalized central moments are additive in the sense that if X and Y are independent random variables then m 1 ( X + Y ) = m 1 ( X ) + m 1 ( Y ) Var ⁡ ⁡ ( X + Y ) = Var ⁡ ⁡ ( X ) + Var ⁡ ⁡ ( Y ) μ μ 3 ( X + Y ) = μ μ 3 ( X ) + μ μ 3 ( Y ) {\displaystyle {\begin{aligned}m_{1}(X+Y)&=m_{1}(X)+m_{1}(Y)\\\operatorname {Var} (X+Y)&=\operatorname {Var} (X)+\operatorname {Var} (Y)\\\mu _{3}(X+Y)&=\mu _{3}(X)+\mu _{3}(Y)\end{aligned}}} (These can also hold for variables that satisfy weaker conditions than independence. The first always holds; if the second holds, the variables are called uncorrelated ).

In fact, these are the first three cumulants and all cumulants share this additivity property.

Sample moments [ edit ] For all k , the k -th raw moment of a population can be estimated using the k -th raw sample  moment 1 n ∑ ∑ i = 1 n X i k {\displaystyle {\frac {1}{n}}\sum _{i=1}^{n}X_{i}^{k}} applied to a sample X 1 , ..., X n drawn from the population.

It can be shown that the expected value of the raw sample moment is equal to the k -th raw moment of the population, if that moment exists, for any sample size n . It is thus an unbiased estimator. This contrasts with the situation for central moments, whose computation uses up a degree of freedom by using the sample mean. So for example an unbiased estimate of the population variance (the second central moment) is given by 1 n − − 1 ∑ ∑ i = 1 n ( X i − − X ¯ ¯ ) 2 {\displaystyle {\frac {1}{n-1}}\sum _{i=1}^{n}\left(X_{i}-{\bar {X}}\right)^{2}} in which the previous denominator n has been replaced by the degrees of freedom n − 1 , and in which X ¯ ¯ {\displaystyle {\bar {X}}} refers to the sample mean. This estimate of the population moment is greater than the unadjusted observed sample moment by a factor of n n − − 1 , {\displaystyle {\tfrac {n}{n-1}},} and it is referred to as the "adjusted sample variance" or sometimes simply the "sample variance".

Problem of moments [ edit ] Main article: Moment problem Problems of determining a probability distribution from its sequence of moments are called problem of moments . Such problems were first discussed by P.L. Chebyshev (1874) [ 6 ] in connection with research on limit theorems. In order that the probability distribution of a random variable X {\displaystyle X} be uniquely defined by its moments α α k = E [ X k ] {\displaystyle \alpha _{k}=E\left[X^{k}\right]} it is sufficient, for example, that Carleman's condition be satisfied: ∑ ∑ k = 1 ∞ ∞ 1 α α 2 k 1 / 2 k = ∞ ∞ {\displaystyle \sum _{k=1}^{\infty }{\frac {1}{\alpha _{2k}^{1/2k}}}=\infty } A similar result even holds for moments of random vectors. The problem of moments seeks characterizations of sequences μ μ n ′ : n = 1 , 2 , 3 , … … {\displaystyle {{\mu _{n}}':n=1,2,3,\dots }} that are sequences of moments of some function f, all moments α α k ( n ) {\displaystyle \alpha _{k}(n)} of which are finite, and for each integer k ≥ ≥ 1 {\displaystyle k\geq 1} let α α k ( n ) → → α α k , n → → ∞ ∞ , {\displaystyle \alpha _{k}(n)\rightarrow \alpha _{k},n\rightarrow \infty ,} where α α k {\displaystyle \alpha _{k}} is finite. Then there is a sequence μ μ n ′ {\displaystyle {\mu _{n}}'} that weakly converges to a distribution function μ μ {\displaystyle \mu } having α α k {\displaystyle \alpha _{k}} as its moments. If the moments determine μ μ {\displaystyle \mu } uniquely, then the sequence μ μ n ′ {\displaystyle {\mu _{n}}'} weakly converges to μ μ {\displaystyle \mu } .

Partial moments [ edit ] Partial moments are sometimes referred to as "one-sided moments." The n -th order lower and upper partial moments with respect to a reference point r may be expressed as μ μ n − − ( r ) = ∫ ∫ − − ∞ ∞ r ( r − − x ) n f ( x ) d x , {\displaystyle \mu _{n}^{-}(r)=\int _{-\infty }^{r}(r-x)^{n}\,f(x)\,\mathrm {d} x,} μ μ n + ( r ) = ∫ ∫ r ∞ ∞ ( x − − r ) n f ( x ) d x .

{\displaystyle \mu _{n}^{+}(r)=\int _{r}^{\infty }(x-r)^{n}\,f(x)\,\mathrm {d} x.} If the integral function does not converge, the partial moment does not exist.

Partial moments are normalized by being raised to the power 1/ n . The upside potential ratio may be expressed as a ratio of a first-order upper partial moment to a normalized second-order lower partial moment.

Central moments in metric spaces [ edit ] Let ( M , d ) be a metric space , and let B( M ) be the Borel σ -algebra on M , the σ -algebra generated by the d - open subsets of M . (For technical reasons, it is also convenient to assume that M is a separable space with respect to the metric d .) Let 1 ≤ p ≤ ∞ .

The p -th central moment of a measure μ on the measurable space ( M , B( M )) about a given point x 0 ∈ M is defined to be ∫ ∫ M d ( x , x 0 ) p d μ μ ( x ) .

{\displaystyle \int _{M}d\left(x,x_{0}\right)^{p}\,\mathrm {d} \mu (x).} μ is said to have finite p -th central moment if the p -th central moment of μ about x 0 is finite for some x 0 ∈ M .

This terminology for measures carries over to random variables in the usual way: if (Ω, Σ, P ) is a probability space and X : Ω → M is a random variable, then the p -th central moment of X about x 0 ∈ M is defined to be ∫ ∫ M d ( x , x 0 ) p d ( X ∗ ∗ ( P ) ) ( x ) = ∫ ∫ Ω Ω d ( X ( ω ω ) , x 0 ) p d P ( ω ω ) = E ⁡ ⁡ [ d ( X , x 0 ) p ] , {\displaystyle \int _{M}d\left(x,x_{0}\right)^{p}\,\mathrm {d} \left(X_{*}\left(\mathbf {P} \right)\right)(x)=\int _{\Omega }d\left(X(\omega ),x_{0}\right)^{p}\,\mathrm {d} \mathbf {P} (\omega )=\operatorname {\mathbf {E} } [d(X,x_{0})^{p}],} and X has finite p -th central moment if the p -th central moment of X about x 0 is finite for some x 0 ∈ M .

See also [ edit ] Energy (signal processing) Factorial moment Generalised mean Image moment L-moment Method of moments (probability theory) Method of moments (statistics) Moment-generating function Moment measure Second moment method Standardised moment Stieltjes moment problem Taylor expansions for the moments of functions of random variables References [ edit ] Text was copied from Moment at the Encyclopedia of Mathematics, which is released under a Creative Commons Attribution-Share Alike 3.0 (Unported) (CC-BY-SA 3.0) license and the GNU Free Documentation License .

^ George Mackey (July 1980). "HARMONIC ANALYSIS AS THE EXPLOITATION OF SYMMETRY - A HISTORICAL SURVEY".

Bulletin of the American Mathematical Society . New Series.

3 (1): 549.

^ Papoulis, A. (1984).

Probability, Random Variables, and Stochastic Processes, 2nd ed . New York: McGraw Hill . pp.

145– 149.

^ "Raw Moment -- from Wolfram MathWorld" .

Archived from the original on 2009-05-28 . Retrieved 2009-06-24 .

Raw Moments at Math-world ^ Casella, George ; Berger, Roger L.

(2002).

Statistical Inference (2 ed.). Pacific Grove: Duxbury.

ISBN 0-534-24312-6 .

^ Ballanda, Kevin P.; MacGillivray, H. L.

(1988). "Kurtosis: A Critical Review".

The American Statistician .

42 (2). American Statistical Association: 111– 119.

doi : 10.2307/2684482 .

JSTOR 2684482 .

^ Feller, W. (1957-1971).

An introduction to probability theory and its applications.

New York: John Wiley & Sons. 419 p.

Further reading [ edit ] Spanos, Aris (1999).

Probability Theory and Statistical Inference . New York: Cambridge University Press. pp.

109 –130.

ISBN 0-521-42408-9 .

Walker, Helen M.

(1929).

Studies in the history of statistical method, with special reference to certain educational problems . Baltimore, Williams & Wilkins Co. p.

71 .

External links [ edit ] "Moment" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Moments at Mathworld v t e Theory of probability distributions probability mass function (pmf) probability density function (pdf) cumulative distribution function (cdf) quantile function raw moment central moment mean variance standard deviation skewness kurtosis L-moment moment-generating function (mgf) characteristic function probability-generating function (pgf) cumulant combinant v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐hmvcn
Cached time: 20250811235741
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.456 seconds
Real time usage: 0.609 seconds
Preprocessor visited node count: 3463/1000000
Revision size: 21483/2097152 bytes
Post‐expand include size: 172616/2097152 bytes
Template argument size: 3640/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 11/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 49986/5000000 bytes
Lua time usage: 0.229/10.000 seconds
Lua memory usage: 5296825/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  385.709      1 -total
 29.73%  114.668     12 Template:Navbox
 25.49%   98.300      1 Template:Reflist
 18.64%   71.884      1 Template:Theory_of_probability_distributions
 18.28%   70.499      2 Template:Cite_journal
 16.24%   62.637      1 Template:Statistics
 15.65%   60.349      1 Template:Navbox_with_collapsible_groups
 15.53%   59.888      1 Template:Short_description
  9.72%   37.505      2 Template:Pagetype
  5.46%   21.049      4 Template:Cite_book Saved in parser cache with key enwiki:pcache:368684:|#|:idhash:canonical and timestamp 20250811235741 and revision id 1302500337. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Moment_(mathematics)&oldid=1302500337 " Categories : Moments (mathematics) Moment (physics) Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 25 July 2025, at 20:19 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Moment (mathematics) 32 languages Add topic

