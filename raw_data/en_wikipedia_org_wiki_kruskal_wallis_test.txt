Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Method 2 Exact probability tables 3 Exact distribution of H 4 Example Toggle Example subsection 4.1 Test for differences in ozone levels by month 5 Implementation 6 See also 7 References 8 Further reading 9 External links Toggle the table of contents Kruskal–Wallis test 17 languages العربية Azərbaycanca Català Čeština Deutsch Español Euskara فارسی Français 한국어 Italiano Magyar Nederlands Polski Português Русский Türkçe Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Non-parametric method for testing whether samples originate from the same distribution Difference between ANOVA and Kruskal–Wallis test with ranks The Kruskal–Wallis test by ranks, Kruskal–Wallis H {\displaystyle H} test (named after William Kruskal and W. Allen Wallis ), or one-way ANOVA on ranks is a non-parametric statistical test for testing whether samples originate from the same distribution.

[ 1 ] [ 2 ] [ 3 ] It is used for comparing two or more independent samples of equal or different sample sizes. It extends the Mann–Whitney U test , which is used for comparing only two groups. The parametric equivalent of the Kruskal–Wallis test is the one-way analysis of variance (ANOVA).

A significant Kruskal–Wallis test indicates that at least one sample stochastically dominates one other sample. The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. For analyzing the specific sample pairs for stochastic dominance, Dunn's test, [ 4 ] pairwise Mann–Whitney tests with Bonferroni correction , [ 5 ] or the more powerful but less well known Conover–Iman test [ 5 ] are sometimes used.

It is supposed that the treatments significantly affect the response level and then there is an order among the treatments: one tends to give the lowest response, another gives the next lowest response is second, and so forth.

[ 6 ] Since it is a nonparametric method, the Kruskal–Wallis test does not assume a normal distribution of the residuals, unlike the analogous one-way analysis of variance. If the researcher can make the assumptions of an identically shaped and scaled distribution for all groups, except for any difference in medians, then the null hypothesis is that the medians of all groups are equal, and the alternative hypothesis is that at least one population median of one group is different from the population median of at least one other group. Otherwise, it is impossible to say, whether the rejection of the null hypothesis comes from the shift in locations or group dispersions. This is the same issue that happens also with the Mann-Whitney test.

[ 7 ] [ 8 ] [ 9 ] If the data contains potential outliers, if the population distributions have heavy tails, or if the population distributions are significantly skewed, the Kruskal-Wallis test is more powerful at detecting differences among treatments than ANOVA F-test . On the other hand, if the population distributions are normal or are light-tailed and symmetric, then ANOVA F-test will generally have greater power which is the probability of rejecting the null hypothesis when it indeed should be rejected.

[ 10 ] [ 11 ] Method [ edit ] An illustration of how to assign any tied values the average of the rank Rank all data from all groups together; i.e., rank the data from 1 to N ignoring group membership. Assign any tied values the average of the ranks they would have received had they not been tied.

The test statistic is given by H = ( N − − 1 ) ∑ ∑ i = 1 g n i ( r ¯ ¯ i ⋅ ⋅ − − r ¯ ¯ ) 2 ∑ ∑ i = 1 g ∑ ∑ j = 1 n i ( r i j − − r ¯ ¯ ) 2 , {\displaystyle \definecolor {Orange}{rgb}{1,0.5019607843137255,0}\definecolor {ChromeYellow}{rgb}{1,0.6549019607843137,0.011764705882352941}\definecolor {Green}{rgb}{0,0.5019607843137255,0}\definecolor {green}{rgb}{0,0.5019607843137255,0}\definecolor {Blue}{rgb}{0,0,1}\definecolor {Purple}{rgb}{0.5019607843137255,0,0.5019607843137255}H=({\color {Red}N}-1){\frac {\sum _{i=1}^{\color {Orange}g}{\color {ChromeYellow}n_{i}}({\color {Blue}{\bar {r}}_{i\cdot }}-{\color {Purple}{\bar {r}}})^{2}}{\sum _{i=1}^{\color {Orange}g}\sum _{j=1}^{\color {ChromeYellow}n_{i}}({\color {Green}r_{ij}}-{\color {Purple}{\bar {r}}})^{2}}},} where N {\textstyle \color {Red}N} is the total number of observations across all groups g {\textstyle \definecolor {Orange}{rgb}{1,0.5019607843137255,0}\color {Orange}g} is the number of groups n i {\textstyle \definecolor {ChromeYellow}{rgb}{1,0.6549019607843137,0.011764705882352941}\color {ChromeYellow}n_{i}} is the number of observations in group i {\displaystyle i} r i j {\displaystyle \definecolor {Green}{rgb}{0,0.5019607843137255,0}\definecolor {green}{rgb}{0,0.5019607843137255,0}\color {Green}r_{ij}} is the rank (among all observations) of observation j {\displaystyle j} from group i {\displaystyle i} r ¯ ¯ i ⋅ ⋅ = ∑ ∑ j = 1 n i r i j n i {\displaystyle \definecolor {blue}{rgb}{0,0,1}{\color {blue}{\bar {r}}_{i\cdot }}={\frac {\sum _{j=1}^{n_{i}}{r_{ij}}}{n_{i}}}} is the average rank of all observations in group i {\displaystyle i} r ¯ ¯ = 1 2 ( N + 1 ) {\textstyle \definecolor {Purple}{rgb}{0.5019607843137255,0,0.5019607843137255}{\color {Purple}{\bar {r}}}={\tfrac {1}{2}}(N+1)} is the average of all the r i j {\textstyle \definecolor {Green}{rgb}{0,0.5019607843137255,0}\definecolor {green}{rgb}{0,0.5019607843137255,0}\color {Green}r_{ij}} .

If the data contain no ties, the denominator of the expression for H {\displaystyle H} is exactly ( N − − 1 ) N ( N + 1 ) / 12 {\displaystyle (N-1)N(N+1)/12} and r ¯ ¯ = N + 1 2 {\displaystyle {\bar {r}}={\tfrac {N+1}{2}}} . Thus H = 12 N ( N + 1 ) ∑ ∑ i = 1 g n i ( r ¯ ¯ i ⋅ ⋅ − − N + 1 2 ) 2 = 12 N ( N + 1 ) ∑ ∑ i = 1 g n i r ¯ ¯ i ⋅ ⋅ 2 − − 3 ( N + 1 ) {\displaystyle {\begin{aligned}H&={\frac {12}{N(N+1)}}\sum _{i=1}^{g}n_{i}\left({\bar {r}}_{i\cdot }-{\frac {N+1}{2}}\right)^{2}\\&={\frac {12}{N(N+1)}}\sum _{i=1}^{g}n_{i}{\bar {r}}_{i\cdot }^{2}-\ 3(N+1)\end{aligned}}} The last formula contains only the squares of the average ranks.

A correction for ties if using the short-cut formula described in the previous point can be made by dividing H {\displaystyle H} by 1 − − ∑ ∑ i = 1 G ( t i 3 − − t i ) N 3 − − N {\displaystyle 1-{\frac {\sum _{i=1}^{G}(t_{i}^{3}-t_{i})}{N^{3}-N}}} , where G {\textstyle G} is the number of groupings of different tied ranks, and t i {\textstyle t_{i}} is the number of tied values within group i {\textstyle i} that are tied at a particular value. This correction usually makes little difference in the value of H {\textstyle H} unless there are a large number of ties.

When performing multiple sample comparisons, the type I error tends to become inflated. Therefore, the Bonferroni procedure is used to adjust the significance level, that is, a ¯ ¯ = α α k k {\displaystyle {\bar {a}}={\frac {\alpha }{\Bbbk }}} , where a ¯ ¯ {\displaystyle {\bar {a}}} is the adjusted significance level, α α {\displaystyle \alpha } is the initial significance level, and k k {\displaystyle \Bbbk } is the number of contrasts.

[ 12 ] Finally, the decision to reject or accept the null hypothesis is made by comparing H {\displaystyle H} to a critical value H c {\displaystyle H_{c}} (obtained from a table or software) for a given significance or alpha level. If H {\displaystyle H} is bigger than H c {\displaystyle H_{c}} , the null hypothesis is rejected. If possible (no ties, sample not too big) one should compare H {\displaystyle H} to the critical value obtained from the exact distribution of H {\displaystyle H} . Otherwise, the distribution of H can be approximated by a chi-squared distribution with g − − 1 {\textstyle g-1} degrees of freedom. If some n i {\displaystyle n_{i}} values are small (i.e., less than 5) the exact probability distribution of H {\displaystyle H} can be quite different from this chi-squared distribution . If a table of the chi-squared probability distribution is available, the critical value of chi-squared, χ χ α α : g − − 1 2 {\displaystyle \chi _{\alpha :g-1}^{2}} , can be found by entering the table at g − − 1 {\textstyle g-1} degrees of freedom and looking under the desired significance or alpha level.

[ 13 ] If the statistic is not significant, there is no evidence of stochastic dominance among the samples. However, if the test is significant then at least one sample stochastically dominates another sample. Then, a researcher might use sample contrasts between individual sample pairs, or post hoc tests using Dunn's test, which (1) properly employs the same rankings as the Kruskal–Wallis test, and (2) properly employs the pooled variance implied by the null hypothesis of the Kruskal–Wallis test in order to determine which of the sample pairs are significantly different.

[ 4 ] When performing multiple sample contrasts or tests, the Type I error rate tends to become inflated, raising concerns about multiple comparisons .

Exact probability tables [ edit ] A large amount of computing resources is required to compute exact probabilities for the Kruskal–Wallis test. Existing software only provides exact probabilities for sample sizes of less than about 30 participants. These software programs rely on the asymptotic approximation for larger sample sizes. Exact probability values for larger sample sizes are available. Spurrier (2003) published exact probability tables for samples as large as 45 participants.

[ 14 ] Meyer and Seaman (2006) produced exact probability distributions for samples as large as 105 participants.

[ 15 ] Exact distribution of H [ edit ] Choi et al.

[ 16 ] made a review of two methods that had been developed to compute the exact distribution of H {\displaystyle H} , proposed a new one, and compared the exact distribution to its chi-squared approximation.

Example [ edit ] Test for differences in ozone levels by month [ edit ] The following example uses data from Chambers et al.

[ 17 ] on daily readings of ozone for May 1 to September 30, 1973, in New York City. The data are in the R data set airquality , and the analysis is included in the documentation for the R function kruskal.test . Boxplots of ozone values by month are shown in the figure.

The Kruskal-Wallis test finds a significant difference (p = 6.901e-06) indicating that ozone differs among the 5 months.

kruskal.test ( Ozone ~ Month , data = airquality ) Kruskal - Wallis rank sum test data : Ozone by Month Kruskal - Wallis chi - squared = 29.267 , df = 4 , p - value = 6.901e-06 To determine which months differ, post-hoc tests may be performed using a Wilcoxon test for each pair of months, with a Bonferroni (or other) correction for multiple hypothesis testing.

pairwise.wilcox.test ( airquality $ Ozone , airquality $ Month , p.adjust.method = "bonferroni" ) Pairwise comparisons using Wilcoxon rank sum test data : airquality $ Ozone and airquality $ Month 5 6 7 8 6 1.0000 - - - 7 0.0003 0.1414 - - 8 0.0012 0.2591 1.0000 - 9 1.0000 1.0000 0.0074 0.0325 P value adjustment method : bonferroni The post-hoc tests indicate that, after Bonferroni correction for multiple testing, the following differences are significant (adjusted p < 0.05).

Month 5 vs Months 7 and 8 Month 9 vs Months 7 and 8 Implementation [ edit ] The Kruskal-Wallis test can be implemented in many programming tools and languages. We list here only the open source free software packages: In Python 's SciPy package, the function scipy.stats.kruskal can return the test result and p -value .

[ 18 ] R base-package has an implement of this test using kruskal.test .

[ 19 ] Java has the implement provided by provided by Apache Commons .

[ 20 ] In Julia , the package HypothesisTests.jl has the function KruskalWallisTest(groups::AbstractVector{<:Real}...) to compute the p-value .

[ 21 ] See also [ edit ] One-way ANOVA Mann–Whitney U tests Bonferroni test Friedman test Jonckheere's trend test Mood's Median test References [ edit ] ^ Kruskal; Wallis (1952). "Use of ranks in one-criterion variance analysis".

Journal of the American Statistical Association .

47 (260): 583– 621.

doi : 10.1080/01621459.1952.10483441 .

^ Corder, Gregory W.; Foreman, Dale I. (2009).

Nonparametric Statistics for Non-Statisticians . Hoboken: John Wiley & Sons. pp.

99 –105.

ISBN 9780470454619 .

^ Siegel; Castellan (1988).

Nonparametric Statistics for the Behavioral Sciences (Second ed.). New York: McGraw–Hill.

ISBN 0070573573 .

^ a b Dunn, Olive Jean (1964). "Multiple comparisons using rank sums".

Technometrics .

6 (3): 241– 252.

doi : 10.2307/1266041 .

^ a b Conover, W. Jay; Iman, Ronald L. (1979).

"On multiple-comparisons procedures" (PDF) (Report). Los Alamos Scientific Laboratory . Retrieved 2016-10-28 .

^ Lehmann, E. L., & D'Abrera, H. J. (1975).

Nonparametrics: Statistical methods based on ranks.

Holden-Day.

^ Divine; Norton; Barón; Juarez-Colunga (2018). "The Wilcoxon–Mann–Whitney Procedure Fails as a Test of Medians". The American Statistician.

doi : 10.1080/00031305.2017.1305291 .

^ Hart (2001). "Mann-Whitney test is not just a test of medians: differences in spread can be important". BMJ.

doi : 10.1136/bmj.323.7309.391 .

^ Bruin (2006). "FAQ: Why is the Mann-Whitney significant when the medians are equal?". UCLA: Statistical Consulting Group.

^ Higgins, James J.; Jeffrey Higgins, James (2004).

An introduction to modern nonparametric statistics . Duxbury advanced series. Pacific Gove, CA: Brooks-Cole; Thomson Learning.

ISBN 978-0-534-38775-4 .

^ Berger, Paul D.; Maurer, Robert E.; Celli, Giovana B. (2018).

Experimental Design . Cham: Springer International Publishing.

doi : 10.1007/978-3-319-64583-4 .

ISBN 978-3-319-64582-7 .

^ Corder, G.W. & Foreman, D.I. (2010). Nonparametric Statistics for Non-statisticians: A Step-by-Step Approach. Hoboken, NJ: Wiley.

^ Montgomery, Douglas C.; Runger, George C. (2018).

Applied statistics and probability for engineers . EMEA edition (Seventh ed.). Hoboken, NJ: Wiley.

ISBN 978-1-119-40036-3 .

^ Spurrier, J. D. (2003). "On the null distribution of the Kruskal–Wallis statistic".

Journal of Nonparametric Statistics .

15 (6): 685– 691.

doi : 10.1080/10485250310001634719 .

^ Meyer; Seaman (April 2006). "Expanded tables of critical values for the Kruskal–Wallis H statistic".

Paper presented at the annual meeting of the American Educational Research Association, San Francisco .

Critical value tables and exact probabilities from Meyer and Seaman are available for download at http://faculty.virginia.edu/kruskal-wallis/ Archived 2018-10-17 at the Wayback Machine . A paper describing their work may also be found there.

^ Won Choi, Jae Won Lee, Myung-Hoe Huh, and Seung-Ho Kang (2003). "An Algorithm for Computing the Exact Distribution of the Kruskal–Wallis Test".

Communications in Statistics - Simulation and Computation (32, number 4): 1029– 1040.

doi : 10.1081/SAC-120023876 .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) ^ John M. Chambers, William S. Cleveland,  Beat Kleiner, and Paul A. Tukey (1983).

Graphical Methods for Data Analysis . Belmont, Calif: Wadsworth International Group, Duxbury Press.

ISBN 053498052X .

{{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ "scipy.stats.kruskal — SciPy v1.11.4 Manual" .

docs.scipy.org . Retrieved 2023-12-06 .

^ "kruskal.test function - RDocumentation" .

www.rdocumentation.org . Retrieved 2023-12-06 .

^ "Math – The Commons Math User Guide - Statistics" .

commons.apache.org . Retrieved 2023-12-06 .

^ "Nonparametric tests · HypothesisTests.jl" .

juliastats.org . Retrieved 2023-12-06 .

Further reading [ edit ] Daniel, Wayne W. (1990).

"Kruskal–Wallis one-way analysis of variance by ranks" .

Applied Nonparametric Statistics (2nd ed.). Boston: PWS-Kent. pp.

226– 234.

ISBN 0-534-91976-6 .

External links [ edit ] An online version of the test v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Kruskal–Wallis_test&oldid=1248232841 " Categories : Statistical tests Analysis of variance Nonparametric statistics Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list Articles with short description Short description matches Wikidata This page was last edited on 28 September 2024, at 11:16 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Kruskal–Wallis test 17 languages Add topic

