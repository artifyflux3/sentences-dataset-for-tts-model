Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Example 2 Application and purpose 3 Kernel density estimation 4 See also 5 References 6 External links Toggle the table of contents Density estimation 4 languages Català فارسی Sunda Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Estimate of an unobservable underlying probability density function For the signal processing concept, see spectral density estimation .

This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Density estimation" – news · newspapers · books · scholar · JSTOR ( August 2012 ) ( Learn how and when to remove this message ) Demonstration of density estimation using Kernel density estimation : The true density is a mixture of two Gaussians centered around 0 and 3, shown with a solid blue curve. In each frame, 100 samples are generated from the distribution, shown in red. Centered on each sample, a Gaussian kernel is drawn in gray. Averaging the Gaussians yields the density estimate shown in the dashed black curve.

In statistics , probability density estimation or simply density estimation is the construction of an estimate , based on observed data , of an unobservable underlying probability density function .  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.

[ 1 ] A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization . The most basic form of density estimation is a rescaled histogram .

Example [ edit ] Estimated density of p (glu | diabetes=1) (red), p (glu | diabetes=0) (blue), and p (glu) (black) Estimated probability of p (diabetes=1 | glu) Estimated probability of p (diabetes=1 | glu) We will consider records of the incidence of diabetes . The following is quoted verbatim from the data set description: A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona,  was tested for diabetes mellitus according to World Health Organization criteria.  The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records.

[ 2 ] [ 3 ] In this example, we construct three density estimates for "glu" ( plasma glucose concentration), one conditional on the presence of diabetes,
the second conditional on the absence of diabetes, and the third not conditional on diabetes.
The conditional density estimates are then used to construct the probability of diabetes conditional on "glu".

The "glu" data were obtained from the MASS package [ 4 ] of the R programming language . Within R, ?Pima.tr and ?Pima.te give a fuller account of the data.

The mean of "glu" in the diabetes cases is 143.1 and the standard deviation is 31.26.
The mean of "glu" in the non-diabetes cases is 110.0 and the standard deviation is 24.29.
From this we see that, in this data set, diabetes cases are associated with greater levels of "glu".
This will be made clearer by plots of the estimated density functions.

The first figure shows density estimates of p (glu | diabetes=1), p (glu | diabetes=0), and p (glu).
The density estimates are kernel density estimates using a Gaussian kernel. That is, a Gaussian density function is placed at each data point, and the sum of the density functions is computed over the range of the data.

From the density of "glu" conditional on diabetes, we can obtain the probability of diabetes conditional on "glu" via Bayes' rule . For brevity, "diabetes" is abbreviated "db." in this formula.

p ( diabetes = 1 | glu ) = p ( glu | db.

= 1 ) p ( db.

= 1 ) p ( glu | db.

= 1 ) p ( db.

= 1 ) + p ( glu | db.

= 0 ) p ( db.

= 0 ) {\displaystyle p({\mbox{diabetes}}=1|{\mbox{glu}})={\frac {p({\mbox{glu}}|{\mbox{db.}}=1)\,p({\mbox{db.}}=1)}{p({\mbox{glu}}|{\mbox{db.}}=1)\,p({\mbox{db.}}=1)+p({\mbox{glu}}|{\mbox{db.}}=0)\,p({\mbox{db.}}=0)}}} The second figure shows the estimated posterior probability p (diabetes=1 | glu). From these data, it appears that an increased level of "glu" is associated with diabetes.

Application and purpose [ edit ] A very natural use of density estimates is in the informal investigation of the properties of a given set of data. Density estimates can give a valuable indication of such features as skewness and multimodality in the data. In some cases they will yield conclusions that may then be regarded as self-evidently true, while in others all they will do is to point the way to further analysis and/or data collection.

[ 5 ] Histogram and density function for a Gumbel distribution [ 6 ] An important aspect of statistics is often the presentation of data back to the client in order to provide explanation and illustration of conclusions that may possibly have been obtained by other means. Density estimates are ideal for this purpose, for the simple reason that they are fairly easily comprehensible to non-mathematicians.

More examples illustrating the use of density estimates for exploratory and presentational purposes, including the important case of bivariate data.

[ 7 ] Density estimation is also frequently used in anomaly detection or novelty detection : [ 8 ] if an observation lies in a very low-density region, it is likely to be an anomaly or a novelty.

In hydrology the histogram and estimated density function of rainfall and river discharge data, analysed with a probability distribution , are used to gain insight in their behaviour and frequency of occurrence.

[ 9 ] An example is shown in the blue figure.

Kernel density estimation [ edit ] This section is an excerpt from Kernel density estimation .

[ edit ] Kernel density estimation of 100 normally distributed random numbers using different smoothing bandwidths.

In statistics , kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation , i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights . KDE answers a fundamental data smoothing problem where inferences about the population are made based on a finite data sample . In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method,  after Emanuel Parzen and Murray Rosenblatt , who are usually credited with independently creating it in its current form.

[ 10 ] [ 11 ] One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier , which can improve its prediction accuracy.

[ 12 ] See also [ edit ] Kernel density estimation Mean integrated squared error Histogram Multivariate kernel density estimation Spectral density estimation Kernel embedding of distributions Generative model Application of Order Statistics: Non-parametric Density Estimation Probability distribution fitting References [ edit ] ^ Alberto Bernacchia, Simone Pigolotti, Self-Consistent Method for Density Estimation, Journal of the Royal Statistical Society Series B: Statistical Methodology, Volume 73, Issue 3, June 2011, Pages 407–422, https://doi.org/10.1111/j.1467-9868.2011.00772.x ^ "Diabetes in Pima Indian Women - R documentation" .

^ Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and Johannes, R. S. (1988). R. A. Greenes (ed.).

"Using the ADAP learning algorithm to forecast the onset of diabetes mellitus" .

Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988) . Los Alamitos, CA: 261– 265.

PMC 2245318 .

{{ cite journal }} :  CS1 maint: multiple names: authors list ( link ) ^ "Support Functions and Datasets for Venables and Ripley's MASS" .

^ Silverman, B. W. (1986).

Density Estimation for Statistics and Data Analysis . Chapman and Hall.

ISBN 978-0412246203 .

^ A calculator for probability distributions and density functions ^ Geof H., Givens (2013). Computational Statistics. Wiley. p. 330.

ISBN 978-0-470-53331-4 .

^ Pimentel, Marco A.F.; Clifton, David A.; Clifton, Lei; Tarassenko, Lionel (2 January 2014). "A review of novelty detection".

Signal Processing .

99 (June 2014): 215– 249.

doi : 10.1016/j.sigpro.2013.12.026 .

^ An illustration of histograms and probability density functions ^ Rosenblatt, M.

(1956).

"Remarks on Some Nonparametric Estimates of a Density Function" .

The Annals of Mathematical Statistics .

27 (3): 832– 837.

doi : 10.1214/aoms/1177728190 .

^ Parzen, E.

(1962).

"On Estimation of a Probability Density Function and Mode" .

The Annals of Mathematical Statistics .

33 (3): 1065– 1076.

doi : 10.1214/aoms/1177704472 .

JSTOR 2237880 .

^ Hastie, Trevor ; Tibshirani, Robert ; Friedman, Jerome H.

(2001).

The Elements of Statistical Learning : Data Mining, Inference, and Prediction : with 200 full-color illustrations . New York: Springer.

ISBN 0-387-95284-5 .

OCLC 46809224 .

Sources Brian D. Ripley (1996).

Pattern Recognition and Neural Networks . Cambridge: Cambridge University Press.

ISBN 978-0521460866 .

Trevor Hastie , Robert Tibshirani , and Jerome Friedman.

The Elements of Statistical Learning . New York: Springer, 2001.

ISBN 0-387-95284-5 .

(See Chapter 6.) Qi Li and Jeffrey S. Racine.

Nonparametric Econometrics: Theory and Practice . Princeton University Press, 2007, ISBN 0-691-12161-3 .

(See Chapter 1.) D.W. Scott.

Multivariate Density Estimation. Theory, Practice and Visualization . New York: Wiley, 1992.

B.W. Silverman .

Density Estimation . London: Chapman and Hall, 1986.

ISBN 978-0-412-24620-3 External links [ edit ] CREEM: Centre for Research Into Ecological and Environmental Modelling Downloads for free density estimation software packages Distance 4 (from Research Unit for Wildlife Population Assessment "RUWPA") and WiSP .

UCI Machine Learning Repository Content Summary (See "Pima Indians Diabetes Database" for the original data set of 732 records, and additional notes.) MATLAB code for one dimensional and two dimensional density estimation libAGF C++ software for variable kernel density estimation .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject Retrieved from " https://en.wikipedia.org/w/index.php?title=Density_estimation&oldid=1288299021 " Categories : Estimation of densities Nonparametric statistics Hidden categories: CS1 maint: multiple names: authors list Articles with short description Short description is different from Wikidata Articles needing additional references from August 2012 All articles needing additional references Articles with excerpts This page was last edited on 1 May 2025, at 19:10 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Density estimation 4 languages Add topic

