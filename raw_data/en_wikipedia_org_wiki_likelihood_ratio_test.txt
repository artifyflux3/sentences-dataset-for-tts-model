Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 General 1.2 Case of simple hypotheses 2 Interpretation Toggle Interpretation subsection 2.1 An example 3 Asymptotic distribution: Wilks’ theorem 4 See also 5 References 6 Further reading 7 External links Toggle the table of contents Likelihood-ratio test 15 languages Català Deutsch Ελληνικά فارسی Français Italiano עברית Nederlands 日本語 Polski Português Русский Sunda Suomi Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistical test that compares goodness of fit This article is about the statistical test that compares goodness of fit. For a general description of the likelihood ratio, see Likelihood ratio . For the use of likelihood ratios in interpreting diagnostic tests, see Likelihood ratios in diagnostic testing .

In statistics , the likelihood-ratio test is a hypothesis test that involves comparing the goodness of fit of two competing statistical models , typically one found by maximization over the entire parameter space and another found after imposing some constraint , based on the ratio of their likelihoods . If the more constrained model (i.e., the null hypothesis ) is supported by the observed data , the two likelihoods should not differ by more than sampling error .

[ 1 ] Thus the likelihood-ratio test tests whether this ratio is significantly different from one, or equivalently whether its natural logarithm is significantly different from zero.

The likelihood-ratio test, also known as Wilks test , [ 2 ] is the oldest of the three classical approaches to hypothesis testing, together with the Lagrange multiplier test and the Wald test .

[ 3 ] In fact, the latter two can be conceptualized as approximations to the likelihood-ratio test, and are asymptotically equivalent.

[ 4 ] [ 5 ] [ 6 ] In the case of comparing two models each of which has no unknown parameters , use of the likelihood-ratio test can be justified by the Neyman–Pearson lemma . The lemma demonstrates that the test has the highest power among all competitors.

[ 7 ] Definition [ edit ] General [ edit ] Suppose that we have a statistical model with parameter space Θ Θ {\displaystyle \Theta } . A null hypothesis is often stated by saying that the parameter θ θ {\displaystyle \theta } lies in a specified subset Θ Θ 0 {\displaystyle \Theta _{0}} of Θ Θ {\displaystyle \Theta } . The alternative hypothesis is thus that θ θ {\displaystyle \theta } lies in the complement of Θ Θ 0 {\displaystyle \Theta _{0}} , i.e. in Θ Θ ∖ ∖ Θ Θ 0 {\displaystyle \Theta ~\backslash ~\Theta _{0}} , which is denoted by Θ Θ 0 c {\displaystyle \Theta _{0}^{\text{c}}} . The likelihood ratio test statistic for the null hypothesis H 0 : θ θ ∈ ∈ Θ Θ 0 {\displaystyle H_{0}\,:\,\theta \in \Theta _{0}} is given by: [ 8 ] λ λ LR = − − 2 ln ⁡ ⁡ [ sup θ θ ∈ ∈ Θ Θ 0 L ( θ θ ) sup θ θ ∈ ∈ Θ Θ L ( θ θ ) ] {\displaystyle \lambda _{\text{LR}}=-2\ln \left[{\frac {~\sup _{\theta \in \Theta _{0}}{\mathcal {L}}(\theta )~}{~\sup _{\theta \in \Theta }{\mathcal {L}}(\theta )~}}\right]} where the quantity inside the brackets is called the likelihood ratio. Here, the sup {\displaystyle \sup } notation refers to the supremum . As all likelihoods are positive, and as the constrained maximum cannot exceed the unconstrained maximum, the likelihood ratio is bounded between zero and one.

Often the likelihood-ratio test statistic is expressed as a difference between the log-likelihoods λ λ LR = − − 2 [ ℓ ℓ ( θ θ 0 ) − − ℓ ℓ ( θ θ ^ ^ ) ] {\displaystyle \lambda _{\text{LR}}=-2\left[~\ell (\theta _{0})-\ell ({\hat {\theta }})~\right]} where ℓ ℓ ( θ θ ^ ^ ) ≡ ≡ ln ⁡ ⁡ [ sup θ θ ∈ ∈ Θ Θ L ( θ θ ) ] {\displaystyle \ell ({\hat {\theta }})\equiv \ln \left[~\sup _{\theta \in \Theta }{\mathcal {L}}(\theta )~\right]~} is the logarithm of the maximized likelihood function L {\displaystyle {\mathcal {L}}} , and ℓ ℓ ( θ θ 0 ) {\displaystyle \ell (\theta _{0})} is the maximal value in the special case that the null hypothesis is true (but not necessarily a value that maximizes L {\displaystyle {\mathcal {L}}} for the sampled data) and θ θ 0 ∈ ∈ Θ Θ 0 and θ θ ^ ^ ∈ ∈ Θ Θ {\displaystyle \theta _{0}\in \Theta _{0}\qquad {\text{ and }}\qquad {\hat {\theta }}\in \Theta ~} denote the respective arguments of the maxima and the allowed ranges they're embedded in. Multiplying by −2 ensures mathematically that (by Wilks' theorem ) λ λ LR {\displaystyle \lambda _{\text{LR}}} converges asymptotically to being χ ²-distributed if the null hypothesis happens to be true.

[ 9 ] The finite-sample distributions of likelihood-ratio statistics are generally unknown.

[ 10 ] The likelihood-ratio test requires that the models be nested – i.e. the more complex model can be transformed into the simpler model by imposing constraints on the former's parameters. Many common test statistics are tests for nested models and can be phrased as log-likelihood ratios or approximations thereof: e.g. the Z -test , the F -test , the G -test , and Pearson's chi-squared test ; for an illustration with the one-sample t -test , see below.

If the models are not nested, then instead of the likelihood-ratio test, there is a generalization of the test that can usually be used: for details, see relative likelihood .

Case of simple hypotheses [ edit ] Main article: Neyman–Pearson lemma A simple-vs.-simple hypothesis test has completely specified models under both the null hypothesis and the alternative hypothesis, which for convenience are written in terms of fixed values of a notional parameter θ θ {\displaystyle \theta } : H 0 : θ θ = θ θ 0 , H 1 : θ θ = θ θ 1 .

{\displaystyle {\begin{aligned}H_{0}&:&\theta =\theta _{0},\\H_{1}&:&\theta =\theta _{1}.\end{aligned}}} In this case, under either hypothesis, the distribution of the data is fully specified: there are no unknown parameters to estimate. For this case, a variant of the likelihood-ratio test is available: [ 11 ] [ 12 ] Λ Λ ( x ) = L ( θ θ 0 ∣ ∣ x ) L ( θ θ 1 ∣ ∣ x ) .

{\displaystyle \Lambda (x)={\frac {~{\mathcal {L}}(\theta _{0}\mid x)~}{~{\mathcal {L}}(\theta _{1}\mid x)~}}.} Some older references may use the reciprocal of the function above as the definition.

[ 13 ] Thus, the likelihood ratio is small if the alternative model is better than the null model.

The likelihood-ratio test provides the decision rule as follows: If Λ Λ > c {\displaystyle ~\Lambda >c~} , do not reject H 0 {\displaystyle H_{0}} ; If Λ Λ < c {\displaystyle ~\Lambda <c~} , reject H 0 {\displaystyle H_{0}} ; If Λ Λ = c {\displaystyle ~\Lambda =c~} , reject H 0 {\displaystyle H_{0}} with probability q {\displaystyle ~q~} .

The values c {\displaystyle c} and q {\displaystyle q} are usually chosen to obtain a specified significance level α α {\displaystyle \alpha } , via the relation q {\displaystyle ~q~} P ⁡ ⁡ ( Λ Λ = c ∣ ∣ H 0 ) + P ⁡ ⁡ ( Λ Λ < c ∣ ∣ H 0 ) = α α .

{\displaystyle \operatorname {P} (\Lambda =c\mid H_{0})~+~\operatorname {P} (\Lambda <c\mid H_{0})~=~\alpha ~.} The Neyman–Pearson lemma states that this likelihood-ratio test is the most powerful among all level α α {\displaystyle \alpha } tests for this case.

[ 7 ] [ 12 ] Interpretation [ edit ] The likelihood ratio is a function of the data x {\displaystyle x} ; therefore, it is a statistic , although unusual in that the statistic's value depends on a parameter, θ θ {\displaystyle \theta } . The likelihood-ratio test rejects the null hypothesis if the value of this statistic is too small. How small is too small depends on the significance level of the test, i.e. on what probability of Type I error is considered tolerable (Type I errors consist of the rejection of a null hypothesis that is true).

The numerator corresponds to the likelihood of an observed outcome under the null hypothesis . The denominator corresponds to the maximum likelihood of an observed outcome, varying parameters over the whole parameter space. The numerator of this ratio is less than the denominator; so, the likelihood ratio is between 0 and 1. Low values of the likelihood ratio mean that the observed result was much less likely to occur under the null hypothesis as compared to the alternative. High values of the statistic mean that the observed outcome was nearly as likely to occur under the null hypothesis as the alternative, and so the null hypothesis cannot be rejected.

An example [ edit ] The following example is adapted and abridged from Stuart, Ord & Arnold (1999 , §22.2).

Suppose that we have a random sample, of size n , from a population that is normally-distributed. Both the mean, μ , and the standard deviation, σ , of the population are unknown. We want to test whether the mean is equal to a given value, μ 0 .

Thus, our null hypothesis is H 0 : μ = μ 0 and our alternative hypothesis is H 1 : μ ≠ μ 0 . The likelihood function is L ( μ μ , σ σ ∣ ∣ x ) = ( 2 π π σ σ 2 ) − − n / 2 exp ⁡ ⁡ ( − − ∑ ∑ i = 1 n ( x i − − μ μ ) 2 2 σ σ 2 ) .

{\displaystyle {\mathcal {L}}(\mu ,\sigma \mid x)=\left(2\pi \sigma ^{2}\right)^{-n/2}\exp \left(-\sum _{i=1}^{n}{\frac {(x_{i}-\mu )^{2}}{2\sigma ^{2}}}\right)\,.} With some calculation (omitted here), it can then be shown that λ λ L R = n ln ⁡ ⁡ [ 1 + t 2 n − − 1 ] {\displaystyle \lambda _{LR}=n\ln \left[1+{\frac {t^{2}}{n-1}}\right]} where t is the t -statistic with n − 1 degrees of freedom. Hence we may use the known exact distribution of t n −1 to draw inferences.

Asymptotic distribution: Wilks’ theorem [ edit ] Main article: Wilks' theorem If the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions (to sustain or reject the null hypothesis). In most cases, however, the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine.

[ citation needed ] Assuming H 0 is true, there is a fundamental result by Samuel S. Wilks : As the sample size n {\displaystyle n} approaches ∞ ∞ {\displaystyle \infty } , and if the null hypothesis lies strictly within the interior of the parameter space, the test statistic λ λ LR {\displaystyle \lambda _{\text{LR}}} defined above will be asymptotically chi-squared distributed ( χ χ 2 {\displaystyle \chi ^{2}} ) with degrees of freedom equal to the difference in dimensionality of Θ Θ {\displaystyle \Theta } and Θ Θ 0 {\displaystyle \Theta _{0}} .

[ 14 ] This implies that for a great variety of hypotheses, we can calculate the likelihood ratio λ λ {\displaystyle \lambda } for the data and then compare the observed λ λ LR {\displaystyle \lambda _{\text{LR}}} to the χ χ 2 {\displaystyle \chi ^{2}} value corresponding to a desired statistical significance as an approximate statistical test. Other extensions exist.

[ which?

] See also [ edit ] Akaike information criterion Bayes factor Johansen test Model selection Vuong's closeness test Sup-LR test Error exponents in hypothesis testing References [ edit ] ^ King, Gary (1989).

Unifying Political Methodology : The Likelihood Theory of Statistical Inference . New York: Cambridge University Press. p. 84.

ISBN 0-521-36697-6 .

^ Li, Bing; Babu, G. Jogesh (2019).

A Graduate Course on Statistical Inference . Springer. p. 331.

ISBN 978-1-4939-9759-6 .

^ Maddala, G. S.

; Lahiri, Kajal (2010).

Introduction to Econometrics (Fourth ed.). New York: Wiley. p. 200.

^ Buse, A. (1982). "The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note".

The American Statistician .

36 (3a): 153– 157.

doi : 10.1080/00031305.1982.10482817 .

^ Pickles, Andrew (1985).

An Introduction to Likelihood Analysis . Norwich: W. H. Hutchins & Sons. pp.

24–27 .

ISBN 0-86094-190-6 .

^ Severini, Thomas A. (2000).

Likelihood Methods in Statistics . New York: Oxford University Press. pp.

120– 121.

ISBN 0-19-850650-3 .

^ a b Neyman, J.

; Pearson, E. S.

(1933), "On the problem of the most efficient tests of statistical hypotheses" (PDF) , Philosophical Transactions of the Royal Society of London A , 231 ( 694– 706): 289– 337, Bibcode : 1933RSPTA.231..289N , doi : 10.1098/rsta.1933.0009 , JSTOR 91247 ^ Koch, Karl-Rudolf (1988).

Parameter Estimation and Hypothesis Testing in Linear Models . New York: Springer. p.

306 .

ISBN 0-387-18840-1 .

^ Silvey, S.D. (1970).

Statistical Inference . London: Chapman & Hall. pp.

112– 114.

ISBN 0-412-13820-4 .

^ Mittelhammer, Ron C.

; Judge, George G.

; Miller, Douglas J. (2000).

Econometric Foundations . New York: Cambridge University Press. p. 66.

ISBN 0-521-62394-4 .

^ Mood, A.M.; Graybill, F.A.; Boes, D.C. (1974).

Introduction to the Theory of Statistics (3rd ed.).

McGraw-Hill . §9.2.

^ a b Stuart, A.; Ord, K.; Arnold, S. (1999), Kendall's Advanced Theory of Statistics , vol. 2A, Arnold , §§20.10–20.13 ^ Cox, D. R.

; Hinkley, D. V.

(1974), Theoretical Statistics , Chapman & Hall , p. 92, ISBN 0-412-12420-3 ^ Wilks, S.S.

(1938).

"The large-sample distribution of the likelihood ratio for testing composite hypotheses" .

Annals of Mathematical Statistics .

9 (1): 60– 62.

doi : 10.1214/aoms/1177732360 .

Further reading [ edit ] Glover, Scott; Dixon, Peter (2004), "Likelihood ratios: A simple and flexible statistic for empirical psychologists", Psychonomic Bulletin & Review , 11 (5): 791– 806, doi : 10.3758/BF03196706 , PMID 15732688 Held, Leonhard; Sabanés Bové, Daniel (2014), Applied Statistical Inference—Likelihood and Bayes , Springer Kalbfleisch, J. G.

(1985), Probability and Statistical Inference , vol. 2, Springer-Verlag Perlman, Michael D.; Wu, Lang (1999), "The emperor's new tests", Statistical Science , 14 (4): 355– 381, doi : 10.1214/ss/1009212517 Perneger, Thomas V. (2001), "Sifting the evidence: Likelihood ratios are alternatives to P values", The BMJ , 322 (7295): 1184– 5, doi : 10.1136/bmj.322.7295.1184 , PMC 1120301 , PMID 11379590 Pinheiro, José C.; Bates, Douglas M. (2000), Mixed-Effects Models in S and S-PLUS , Springer-Verlag , pp.

82– 93 Richard, Mark; Vecer, Jan (2021).

"Efficiency Testing of Prediction Markets: Martingale Approach, Likelihood Ratio and Bayes Factor Analysis" .

Risks .

9 (2): 31.

doi : 10.3390/risks9020031 .

hdl : 10419/258120 .

Solomon, Daniel L. (1975), "A note on the non-equivalence of the Neyman-Pearson and generalized likelihood ratio tests for testing a simple null versus a simple alternative hypothesis" (PDF) , The American Statistician , 29 (2): 101– 102, doi : 10.1080/00031305.1975.10477383 , hdl : 1813/32605 External links [ edit ] Practical application of likelihood ratio test described R Package: Wald's Sequential Probability Ratio Test Richard Lowry's Predictive Values and Likelihood Ratios Online Clinical Calculator v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐m7595
Cached time: 20250812000602
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.630 seconds
Real time usage: 0.845 seconds
Preprocessor visited node count: 3193/1000000
Revision size: 17462/2097152 bytes
Post‐expand include size: 190139/2097152 bytes
Template argument size: 3463/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 81040/5000000 bytes
Lua time usage: 0.383/10.000 seconds
Lua memory usage: 7804627/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  604.723      1 -total
 29.91%  180.885      1 Template:Reflist
 27.10%  163.852      1 Template:Statistics
 26.65%  161.169      1 Template:Navbox_with_collapsible_groups
 21.32%  128.934      9 Template:Cite_book
 13.62%   82.365      1 Template:Short_description
 10.51%   63.541     10 Template:Citation
  9.64%   58.274     11 Template:Navbox
  7.86%   47.512      2 Template:Pagetype
  6.58%   39.804      1 Template:Hlist Saved in parser cache with key enwiki:pcache:45035:|#|:idhash:canonical and timestamp 20250812000602 and revision id 1235627009. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Likelihood-ratio_test&oldid=1235627009 " Categories : Statistical ratios Statistical tests Hidden categories: Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from September 2018 All articles with specifically marked weasel-worded phrases Articles with specifically marked weasel-worded phrases from March 2019 This page was last edited on 20 July 2024, at 09:11 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Likelihood-ratio test 15 languages Add topic

