Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Special cases Toggle Special cases subsection 2.1 Hartley or max-entropy 2.2 Shannon entropy 2.3 Collision entropy 2.4 Min-entropy 3 Inequalities for different orders α 4 Rényi divergence 5 Financial interpretation 6 Properties specific to α = 1 7 Exponential families 8 Physical meaning 9 See also 10 Notes 11 References Toggle the table of contents Rényi entropy 8 languages Boarisch Deutsch Español Français 한국어 עברית Русский Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Concept in information theory In information theory , the Rényi entropy is a quantity that generalizes various notions of entropy , including Hartley entropy , Shannon entropy , collision entropy , and min-entropy . The Rényi entropy is named after Alfréd Rényi , who looked for the most general way to quantify information while preserving additivity for independent events.

[ 1 ] [ 2 ] In the context of fractal dimension estimation, the Rényi entropy forms the basis of the concept of generalized dimensions .

[ 3 ] The Rényi entropy is important in ecology and statistics as index of diversity .  The Rényi entropy is also important in quantum information , where it can be used as a measure of entanglement .  In the Heisenberg XY spin chain model, the Rényi entropy as a function of α can be calculated explicitly because it is an automorphic function with respect to a particular subgroup of the modular group .

[ 4 ] [ 5 ] In theoretical computer science , the min-entropy is used in the context of randomness extractors .

Definition [ edit ] The Rényi entropy of order ⁠ α α {\displaystyle \alpha } ⁠ , where 0 < α α < ∞ ∞ {\displaystyle 0<\alpha <\infty } and ⁠ α α ≠ ≠ 1 {\displaystyle \alpha \neq 1} ⁠ , is defined as [ 1 ] H α α ( X ) = 1 1 − − α α log ⁡ ⁡ ( ∑ ∑ i = 1 n p i α α ) .

{\displaystyle \mathrm {H} _{\alpha }(X)={\frac {1}{1-\alpha }}\log \left(\sum _{i=1}^{n}p_{i}^{\alpha }\right).} It is further defined at α α = 0 , 1 , ∞ ∞ {\displaystyle \alpha =0,1,\infty } as H α α ( X ) = lim γ γ → → α α H γ γ ( X ) .

{\displaystyle \mathrm {H} _{\alpha }(X)=\lim _{\gamma \to \alpha }\mathrm {H} _{\gamma }(X).} Here, X {\displaystyle X} is a discrete random variable with possible outcomes in the set A = { x 1 , x 2 , .

.

.

, x n } {\displaystyle {\mathcal {A}}=\{x_{1},x_{2},...,x_{n}\}} and corresponding probabilities p i ≐ ≐ Pr ( X = x i ) {\displaystyle p_{i}\doteq \Pr(X=x_{i})} for ⁠ i = 1 , … … , n {\displaystyle i=1,\dots ,n} ⁠ .  The resulting unit of information is determined by the base of the logarithm , e.g.

shannon for base 2, or nat for base e .
If the probabilities are p i = 1 / n {\displaystyle p_{i}=1/n} for all ⁠ i = 1 , … … , n {\displaystyle i=1,\dots ,n} ⁠ , then all the Rényi entropies of the distribution are equal: ⁠ H α α ( X ) = log ⁡ ⁡ n {\displaystyle \mathrm {H} _{\alpha }(X)=\log n} ⁠ .
In general, for all discrete random variables ⁠ X {\displaystyle X} ⁠ , H α α ( X ) {\displaystyle \mathrm {H} _{\alpha }(X)} is a non-increasing function in ⁠ α α {\displaystyle \alpha } ⁠ .

Applications often exploit the following relation between the Rényi entropy and the α -norm of the vector of probabilities: H α α ( X ) = α α 1 − − α α log ⁡ ⁡ ( ‖ P ‖ α α ) .

{\displaystyle \mathrm {H} _{\alpha }(X)={\frac {\alpha }{1-\alpha }}\log \left({\left\|P\right\|}_{\alpha }\right).} Here, the discrete probability distribution P = ( p 1 , … … , p n ) {\displaystyle P=(p_{1},\dots ,p_{n})} is interpreted as a vector in R n {\displaystyle \mathbb {R} ^{n}} with p i ≥ ≥ 0 {\displaystyle p_{i}\geq 0} and ∑ ∑ i = 1 n p i = 1 {\textstyle \sum _{i=1}^{n}p_{i}=1} .

The Rényi entropy for any α α ≥ ≥ 0 {\displaystyle \alpha \geq 0} is Schur concave . Proven by the Schur–Ostrowski criterion.

Special cases [ edit ] Rényi entropy of a random variable with two possible outcomes against p 1 , where P = ( p 1 , 1 − p 1 ) . Shown are Η 0 , Η 1 , Η 2 and Η ∞ , with the unit on the vertical axis being the shannon .

As α α {\displaystyle \alpha } approaches zero, the Rényi entropy increasingly weighs all events with nonzero probability more equally, regardless of their probabilities. In the limit for ⁠ α α → → 0 {\displaystyle \alpha \to 0} ⁠ , the Rényi entropy is just the logarithm of the size of the support of X . The limit for α α → → 1 {\displaystyle \alpha \to 1} is the Shannon entropy . As α α {\displaystyle \alpha } approaches infinity, the Rényi entropy is increasingly determined by the events of highest probability.

Hartley or max-entropy [ edit ] H 0 ( X ) {\displaystyle \mathrm {H} _{0}(X)} is log ⁡ ⁡ n {\displaystyle \log n} where n {\displaystyle n} is the number of non-zero probabilities.

[ 6 ] If the probabilities are all nonzero, it is simply the logarithm of the cardinality of the alphabet ( ⁠ A {\displaystyle {\mathcal {A}}} ⁠ ) of ⁠ X {\displaystyle X} ⁠ , sometimes called the Hartley entropy of ⁠ X {\displaystyle X} ⁠ , H 0 ( X ) = log ⁡ ⁡ n = log ⁡ ⁡ | A | {\displaystyle \mathrm {H} _{0}(X)=\log n=\log |{\mathcal {A}}|\,} Shannon entropy [ edit ] The limiting value of H α α {\displaystyle \mathrm {H} _{\alpha }} as α α → → 1 {\displaystyle \alpha \to 1} is the Shannon entropy : [ 7 ] H 1 ( X ) ≡ ≡ lim α α → → 1 H α α ( X ) = − − ∑ ∑ i = 1 n p i log ⁡ ⁡ p i {\displaystyle \mathrm {H} _{1}(X)\equiv \lim _{\alpha \to 1}\mathrm {H} _{\alpha }(X)=-\sum _{i=1}^{n}p_{i}\log p_{i}} Collision entropy [ edit ] Collision entropy , sometimes just called "Rényi entropy", refers to the case ⁠ α α = 2 {\displaystyle \alpha =2} ⁠ , H 2 ( X ) = − − log ⁡ ⁡ ∑ ∑ i = 1 n p i 2 = − − log ⁡ ⁡ P ( X = Y ) , {\displaystyle \mathrm {H} _{2}(X)=-\log \sum _{i=1}^{n}p_{i}^{2}=-\log P(X=Y),} where X {\displaystyle X} and Y {\displaystyle Y} are independent and identically distributed . The collision entropy is related to the index of coincidence . It is the negative logarithm of the Simpson diversity index .

Min-entropy [ edit ] Main article: Min-entropy In the limit as ⁠ α α → → ∞ ∞ {\displaystyle \alpha \rightarrow \infty } ⁠ , the Rényi entropy H α α {\displaystyle \mathrm {H} _{\alpha }} converges to the min-entropy ⁠ H ∞ ∞ {\displaystyle \mathrm {H} _{\infty }} ⁠ : H ∞ ∞ ( X ) ≐ ≐ min i ( − − log ⁡ ⁡ p i ) = − − ( max i log ⁡ ⁡ p i ) = − − log ⁡ ⁡ max i p i .

{\displaystyle \mathrm {H} _{\infty }(X)\doteq \min _{i}(-\log p_{i})=-(\max _{i}\log p_{i})=-\log \max _{i}p_{i}\,.} Equivalently, the min-entropy H ∞ ∞ ( X ) {\displaystyle \mathrm {H} _{\infty }(X)} is the largest real number b such that all events occur with probability at most ⁠ 2 − − b {\displaystyle 2^{-b}} ⁠ .

The name min-entropy stems from the fact that it is the smallest entropy measure in the family of Rényi entropies.
In this sense, it is the strongest way to measure the information content of a discrete random variable.
In particular, the min-entropy is never larger than the Shannon entropy .

The min-entropy has important applications for randomness extractors in theoretical computer science :
Extractors are able to extract randomness from random sources that have a large min-entropy; merely having a large Shannon entropy does not suffice for this task.

Inequalities for different orders α [ edit ] That H α α {\displaystyle \mathrm {H} _{\alpha }} is non-increasing in α α {\displaystyle \alpha } for any given distribution of probabilities ⁠ p i {\displaystyle p_{i}} ⁠ ,
which can be proven by differentiation, [ 8 ] as − − d H α α d α α = 1 ( 1 − − α α ) 2 ∑ ∑ i = 1 n z i log ⁡ ⁡ ( z i / p i ) = 1 ( 1 − − α α ) 2 D K L ( z ‖ ‖ p ) {\displaystyle -{\frac {d\mathrm {H} _{\alpha }}{d\alpha }}={\frac {1}{(1-\alpha )^{2}}}\sum _{i=1}^{n}z_{i}\log(z_{i}/p_{i})={\frac {1}{(1-\alpha )^{2}}}D_{KL}(z\|p)} which is proportional to Kullback–Leibler divergence (which is always non-negative), where z i = p i α α / ∑ ∑ j = 1 n p j α α {\textstyle z_{i}=p_{i}^{\alpha }/\sum _{j=1}^{n}p_{j}^{\alpha }} . In particular, it is strictly positive except when the distribution is uniform.

At the α α → → 1 {\displaystyle \alpha \to 1} limit, we have − − d H α α d α α → → 1 2 ∑ ∑ i p i ( ln ⁡ ⁡ p i + H ( p ) ) 2 {\textstyle -{\frac {d\mathrm {H} _{\alpha }}{d\alpha }}\to {\frac {1}{2}}\sum _{i}p_{i}{\left(\ln p_{i}+H(p)\right)}^{2}} .

In particular cases inequalities can be proven also by Jensen's inequality : [ 9 ] [ 10 ] log ⁡ ⁡ n = H 0 ≥ ≥ H 1 ≥ ≥ H 2 ≥ ≥ H ∞ ∞ .

{\displaystyle \log n=\mathrm {H} _{0}\geq \mathrm {H} _{1}\geq \mathrm {H} _{2}\geq \mathrm {H} _{\infty }.} For values of ⁠ α α > 1 {\displaystyle \alpha >1} ⁠ , inequalities in the other direction also hold. In particular, we have [ 11 ] [ 12 ] H 2 ≤ ≤ 2 H ∞ ∞ .

{\displaystyle \mathrm {H} _{2}\leq 2\mathrm {H} _{\infty }.} On the other hand, the Shannon entropy H 1 {\displaystyle \mathrm {H} _{1}} can be arbitrarily high for a random variable X {\displaystyle X} that has a given min-entropy. An example of this is given by the sequence of random variables X n ∼ ∼ { 0 , … … , n } {\displaystyle X_{n}\sim \{0,\ldots ,n\}} for n ≥ ≥ 1 {\displaystyle n\geq 1} such that P ( X n = 0 ) = 1 / 2 {\displaystyle P(X_{n}=0)=1/2} and P ( X n = x ) = 1 / ( 2 n ) {\displaystyle P(X_{n}=x)=1/(2n)} since H ∞ ∞ ( X n ) = log ⁡ ⁡ 2 {\displaystyle \mathrm {H} _{\infty }(X_{n})=\log 2} but ⁠ H 1 ( X n ) = ( log ⁡ ⁡ 2 + log ⁡ ⁡ 2 n ) / 2 {\displaystyle \mathrm {H} _{1}(X_{n})=(\log 2+\log 2n)/2} ⁠ .

Rényi divergence [ edit ] As well as the absolute Rényi entropies, Rényi also defined a spectrum of divergence measures generalising the Kullback–Leibler divergence .

[ 13 ] The Rényi divergence of order ⁠ α α {\displaystyle \alpha } ⁠ or alpha-divergence of a distribution P from a distribution Q is defined to be D α α ( P ‖ ‖ Q ) = 1 α α − − 1 log ⁡ ⁡ ( ∑ ∑ i = 1 n p i α α q i α α − − 1 ) = 1 α α − − 1 log ⁡ ⁡ E i ∼ ∼ p [ ( p i / q i ) α α − − 1 ] {\displaystyle {\begin{aligned}D_{\alpha }(P\Vert Q)&={\frac {1}{\alpha -1}}\log \left(\sum _{i=1}^{n}{\frac {p_{i}^{\alpha }}{q_{i}^{\alpha -1}}}\right)\\[1ex]&={\frac {1}{\alpha -1}}\log \mathbb {E} _{i\sim p}\left[{\left(p_{i}/q_{i}\right)}^{\alpha -1}\right]\,\end{aligned}}} when ⁠ 0 < α α < ∞ ∞ {\displaystyle 0<\alpha <\infty } ⁠ and ⁠ α α ≠ ≠ 1 {\displaystyle \alpha \neq 1} ⁠ .   We can define the Rényi divergence for the special values α = 0, 1, ∞ by taking a limit, and in particular the limit α → 1 gives the Kullback–Leibler divergence.

Some special cases: ⁠ D 0 ( P ‖ ‖ Q ) = − − log ⁡ ⁡ Q ( { i : p i > 0 } ) {\displaystyle D_{0}(P\Vert Q)=-\log Q(\{i:p_{i}>0\})} ⁠ : minus the log probability under Q that p i > 0 ; ⁠ D 1 / 2 ( P ‖ ‖ Q ) = − − 2 log ⁡ ⁡ ∑ ∑ i = 1 n p i q i {\displaystyle D_{1/2}(P\Vert Q)=-2\log \sum _{i=1}^{n}{\sqrt {p_{i}q_{i}}}} ⁠ : minus twice the logarithm of the Bhattacharyya coefficient ; ( Nielsen & Boltz (2010) ) ⁠ D 1 ( P ‖ ‖ Q ) = ∑ ∑ i = 1 n p i log ⁡ ⁡ p i q i {\displaystyle D_{1}(P\Vert Q)=\sum _{i=1}^{n}p_{i}\log {\frac {p_{i}}{q_{i}}}} ⁠ : the Kullback–Leibler divergence ; ⁠ D 2 ( P ‖ ‖ Q ) = log ⁡ ⁡ ⟨ p i q i ⟩ {\displaystyle D_{2}(P\Vert Q)=\log {\Big \langle }{\frac {p_{i}}{q_{i}}}{\Big \rangle }} ⁠ : the log of the expected ratio of the probabilities; ⁠ D ∞ ∞ ( P ‖ ‖ Q ) = log ⁡ ⁡ sup i p i q i {\displaystyle D_{\infty }(P\Vert Q)=\log \sup _{i}{\frac {p_{i}}{q_{i}}}} ⁠ : the log of the maximum ratio of the probabilities.

The Rényi divergence is indeed a divergence , meaning simply that D α α ( P ‖ ‖ Q ) {\displaystyle D_{\alpha }(P\|Q)} is greater than or equal to zero, and zero only when P = Q .  For any fixed distributions P and Q , the Rényi divergence is nondecreasing as a function of its order α , and it is continuous on the set of α for which it is finite, [ 13 ] or for the sake of brevity, the information of order α obtained if the distribution P is replaced by the distribution Q .

[ 1 ] Financial interpretation [ edit ] A pair of probability distributions can be viewed as a game of chance in which one of the distributions defines official odds and the other contains the actual probabilities. Knowledge of the actual probabilities allows a player to profit from the game. The expected profit rate is connected to the Rényi divergence as follows [ 14 ] E x p e c t e d R a t e = 1 R D 1 ( b ‖ ‖ m ) + R − − 1 R D 1 / R ( b ‖ ‖ m ) , {\displaystyle {\rm {ExpectedRate}}={\frac {1}{R}}\,D_{1}(b\|m)+{\frac {R-1}{R}}\,D_{1/R}(b\|m)\,,} where m {\displaystyle m} is the distribution defining the official odds (i.e. the "market") for the game, b {\displaystyle b} is the investor-believed distribution and R {\displaystyle R} is the investor's risk aversion (the Arrow–Pratt relative risk aversion ).

If the true distribution is p {\displaystyle p} (not necessarily coinciding with the investor's belief ⁠ b {\displaystyle b} ⁠ ), the long-term realized rate converges to the true expectation which has a similar mathematical structure [ 14 ] R e a l i z e d R a t e = 1 R ( D 1 ( p ‖ ‖ m ) − − D 1 ( p ‖ ‖ b ) ) + R − − 1 R D 1 / R ( b ‖ ‖ m ) .

{\displaystyle {\rm {RealizedRate}}={\frac {1}{R}}\,{\Big (}D_{1}(p\|m)-D_{1}(p\|b){\Big )}+{\frac {R-1}{R}}\,D_{1/R}(b\|m)\,.} Properties specific to α = 1 [ edit ] The value ⁠ α α = 1 {\displaystyle \alpha =1} ⁠ , which gives the Shannon entropy and the Kullback–Leibler divergence , is the only value at which the chain rule of conditional probability holds exactly: H ( A , X ) = H ( A ) + E a ∼ ∼ A [ H ( X | A = a ) ] {\displaystyle \mathrm {H} (A,X)=\mathrm {H} (A)+\mathbb {E} _{a\sim A}{\big [}\mathrm {H} (X|A=a){\big ]}} for the absolute entropies, and D K L ( p ( x | a ) p ( a ) ‖ ‖ m ( x , a ) ) = D K L ( p ( a ) ‖ ‖ m ( a ) ) + E p ( a ) { D K L ( p ( x | a ) ‖ ‖ m ( x | a ) ) } , {\displaystyle D_{\mathrm {KL} }(p(x|a)p(a)\|m(x,a))=D_{\mathrm {KL} }(p(a)\|m(a))+\mathbb {E} _{p(a)}\{D_{\mathrm {KL} }(p(x|a)\|m(x|a))\},} for the relative entropies.

The latter in particular means that if we seek a distribution p ( x , a ) which minimizes the divergence from some underlying prior measure m ( x , a ) , and we acquire new information which only affects the distribution of a , then the distribution of p ( x | a ) remains m ( x | a ) , unchanged.

The other Rényi divergences satisfy the criteria of being positive and continuous, being invariant under 1-to-1 co-ordinate transformations, and of combining additively when A and X are independent, so that if p ( A , X ) = p ( A ) p ( X ) , then H α α ( A , X ) = H α α ( A ) + H α α ( X ) {\displaystyle \mathrm {H} _{\alpha }(A,X)=\mathrm {H} _{\alpha }(A)+\mathrm {H} _{\alpha }(X)\;} and D α α ( P ( A ) P ( X ) ‖ ‖ Q ( A ) Q ( X ) ) = D α α ( P ( A ) ‖ ‖ Q ( A ) ) + D α α ( P ( X ) ‖ ‖ Q ( X ) ) .

{\displaystyle D_{\alpha }(P(A)P(X)\|Q(A)Q(X))=D_{\alpha }(P(A)\|Q(A))+D_{\alpha }(P(X)\|Q(X)).} The stronger properties of the α α = 1 {\displaystyle \alpha =1} quantities allow the definition of conditional information and mutual information from communication theory.

Exponential families [ edit ] The Rényi entropies and divergences for an exponential family admit simple expressions [ 15 ] H α α ( p F ( x ; θ θ ) ) = 1 1 − − α α ( F ( α α θ θ ) − − α α F ( θ θ ) + log ⁡ ⁡ E p [ e ( α α − − 1 ) k ( x ) ] ) {\displaystyle \mathrm {H} _{\alpha }(p_{F}(x;\theta ))={\frac {1}{1-\alpha }}\left(F(\alpha \theta )-\alpha F(\theta )+\log E_{p}\left[e^{(\alpha -1)k(x)}\right]\right)} and D α α ( p : q ) = J F , α α ( θ θ : θ θ ′ ) 1 − − α α {\displaystyle D_{\alpha }(p:q)={\frac {J_{F,\alpha }(\theta :\theta ')}{1-\alpha }}} where J F , α α ( θ θ : θ θ ′ ) = α α F ( θ θ ) + ( 1 − − α α ) F ( θ θ ′ ) − − F ( α α θ θ + ( 1 − − α α ) θ θ ′ ) {\displaystyle J_{F,\alpha }(\theta :\theta ')=\alpha F(\theta )+(1-\alpha )F(\theta ')-F(\alpha \theta +(1-\alpha )\theta ')} is a Jensen difference divergence.

Physical meaning [ edit ] The Rényi entropy in quantum physics is not considered to be an observable , due to its nonlinear dependence on the density matrix .  (This nonlinear dependence applies even in the special case of the Shannon entropy.)  It can, however, be given an operational meaning through the two-time measurements (also known as full counting statistics) of energy transfers [ citation needed ] .

The limit of the quantum mechanical Rényi entropy as α α → → 1 {\displaystyle \alpha \to 1} is the von Neumann entropy .

See also [ edit ] Diversity indices Tsallis entropy Generalized entropy index Notes [ edit ] ^ a b c Rényi (1961) ^ Rioul (2021) ^ Barros, Vanessa; Rousseau, Jérôme (2021-06-01).

"Shortest Distance Between Multiple Orbits and Generalized Fractal Dimensions" .

Annales Henri Poincaré .

22 (6): 1853– 1885.

arXiv : 1912.07516 .

Bibcode : 2021AnHP...22.1853B .

doi : 10.1007/s00023-021-01039-y .

ISSN 1424-0661 .

S2CID 209376774 .

^ Franchini, Its & Korepin (2008) ^ Its & Korepin (2010) ^ RFC 4086, page 6 ^ Bromiley, Thacker & Bouhova-Thacker (2004) ^ Beck & Schlögl (1993) ^ H 1 ≥ ≥ H 2 {\displaystyle \textstyle \mathrm {H} _{1}\geq \mathrm {H} _{2}} holds because ⁠ ∑ ∑ i = 1 M p i log ⁡ ⁡ p i ≤ ≤ log ⁡ ⁡ ∑ ∑ i = 1 M p i 2 {\displaystyle \textstyle \sum \limits _{i=1}^{M}{p_{i}\log p_{i}}\leq \log \sum \limits _{i=1}^{M}{p_{i}^{2}}} ⁠ .

^ H ∞ ∞ ≤ ≤ H 2 {\displaystyle \mathrm {H} _{\infty }\leq \mathrm {H} _{2}} holds because ⁠ log ⁡ ⁡ ∑ ∑ i = 1 n p i 2 ≤ ≤ log ⁡ ⁡ sup i p i ( ∑ ∑ i = 1 n p i ) = log ⁡ ⁡ sup i p i {\displaystyle \textstyle \log \sum \limits _{i=1}^{n}{p_{i}^{2}}\leq \log \sup _{i}p_{i}\left({\sum \limits _{i=1}^{n}{p_{i}}}\right)=\log \sup _{i}p_{i}} ⁠ .

^ H 2 ≤ ≤ 2 H ∞ ∞ {\displaystyle \mathrm {H} _{2}\leq 2\mathrm {H} _{\infty }} holds because log ⁡ ⁡ ∑ ∑ i = 1 n p i 2 ≥ ≥ log ⁡ ⁡ sup i p i 2 = 2 log ⁡ ⁡ sup i p i {\displaystyle \textstyle \log \sum \limits _{i=1}^{n}{p_{i}^{2}}\geq \log \sup _{i}p_{i}^{2}=2\log \sup _{i}p_{i}} ^ Devroye, Luc; Györfi, Laszlo; Lugosi, Gabor (1996-04-04).

A Probabilistic Theory of Pattern Recognition (Corrected ed.). New York, NY: Springer.

ISBN 978-0-387-94618-4 .

^ a b Van Erven, Tim; Harremoës, Peter (2014). "Rényi Divergence and Kullback–Leibler Divergence".

IEEE Transactions on Information Theory .

60 (7): 3797– 3820.

arXiv : 1206.2459 .

doi : 10.1109/TIT.2014.2320500 .

S2CID 17522805 .

^ a b Soklakov (2018) ^ Nielsen & Nock (2011) References [ edit ] Beck, Christian; Schlögl, Friedrich (1993).

Thermodynamics of chaotic systems: an introduction . Cambridge University Press.

ISBN 0521433673 .

Jizba, P.; Arimitsu, T. (2004). "The world according to Rényi: Thermodynamics of multifractal systems".

Annals of Physics .

312 (1): 17– 59.

arXiv : cond-mat/0207707 .

Bibcode : 2004AnPhy.312...17J .

doi : 10.1016/j.aop.2004.01.002 .

S2CID 119704502 .

Jizba, P.; Arimitsu, T. (2004). "On observability of Rényi's entropy".

Physical Review E .

69 (2): 026128.

arXiv : cond-mat/0307698 .

Bibcode : 2004PhRvE..69b6128J .

doi : 10.1103/PhysRevE.69.026128 .

PMID 14995541 .

S2CID 39231939 .

Bromiley, P.A.; Thacker, N.A.; Bouhova-Thacker, E. (2004), Shannon Entropy, Rényi Entropy, and Information , CiteSeerX 10.1.1.330.9856 Franchini, F.; Its, A. R.; Korepin, V. E. (2008). "Rényi entropy as a measure of entanglement in quantum spin chain".

Journal of Physics A: Mathematical and Theoretical .

41 (25302): 025302.

arXiv : 0707.2534 .

Bibcode : 2008JPhA...41b5302F .

doi : 10.1088/1751-8113/41/2/025302 .

S2CID 119672750 .

"Rényi test" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Hero, A. O.; Michael, O.; Gorman, J. (2002).

Alpha-divergence for Classification, Indexing and Retrieval (PDF) (Technical Report CSPL-328). Communications and Signal Processing Laboratory, University of Michigan.

CiteSeerX 10.1.1.373.2763 .

Its, A. R.; Korepin, V. E. (2010). "Generalized entropy of the Heisenberg spin chain".

Theoretical and Mathematical Physics .

164 (3): 1136– 1139.

Bibcode : 2010TMP...164.1136I .

doi : 10.1007/s11232-010-0091-6 .

S2CID 119525704 .

Nielsen, F.; Boltz, S. (2010). "The Burbea-Rao and Bhattacharyya centroids".

IEEE Transactions on Information Theory .

57 (8): 5455– 5466.

arXiv : 1004.5049 .

doi : 10.1109/TIT.2011.2159046 .

S2CID 14238708 .

Nielsen, Frank; Nock, Richard (2012). "A closed-form expression for the Sharma–Mittal entropy of exponential families".

Journal of Physics A .

45 (3): 032003.

arXiv : 1112.4221 .

Bibcode : 2012JPhA...45c2003N .

doi : 10.1088/1751-8113/45/3/032003 .

S2CID 8653096 .

Nielsen, Frank; Nock, Richard (2011). "On Rényi and Tsallis entropies and divergences for exponential families".

Journal of Physics A .

45 (3): 032003.

arXiv : 1105.3259 .

Bibcode : 2012JPhA...45c2003N .

doi : 10.1088/1751-8113/45/3/032003 .

S2CID 8653096 .

Rényi, Alfréd (1961).

"On measures of information and entropy" (PDF) .

Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability 1960 . pp.

547– 561.

Rosso, O. A. (2006). "EEG analysis using wavelet-based information tools".

Journal of Neuroscience Methods .

153 (2): 163– 182.

doi : 10.1016/j.jneumeth.2005.10.009 .

PMID 16675027 .

S2CID 7134638 .

Zachos, C. K. (2007). "A classical bound on quantum entropy".

Journal of Physics A .

40 (21): F407 – F412 .

arXiv : hep-th/0609148 .

Bibcode : 2007JPhA...40..407Z .

doi : 10.1088/1751-8113/40/21/F02 .

S2CID 1619604 .

Nazarov, Y. (2011). "Flows of Rényi entropies".

Physical Review B .

84 (10): 205437.

arXiv : 1108.3537 .

Bibcode : 2015PhRvB..91j4303A .

doi : 10.1103/PhysRevB.91.104303 .

S2CID 40312624 .

Ansari, Mohammad H.; Nazarov, Yuli V. (2015). "Rényi entropy flows from quantum heat engines".

Physical Review B .

91 (10): 104303.

arXiv : 1408.3910 .

Bibcode : 2015PhRvB..91j4303A .

doi : 10.1103/PhysRevB.91.104303 .

S2CID 40312624 .

Ansari, Mohammad H.; Nazarov, Yuli V. (2015). "Exact correspondence between Rényi entropy flows and physical flows".

Physical Review B .

91 (17): 174307.

arXiv : 1502.08020 .

Bibcode : 2015PhRvB..91q4307A .

doi : 10.1103/PhysRevB.91.174307 .

S2CID 36847902 .

Soklakov, A. N. (2020).

"Economics of Disagreement—Financial Intuition for the Rényi Divergence" .

Entropy .

22 (8): 860.

arXiv : 1811.08308 .

Bibcode : 2020Entrp..22..860S .

doi : 10.3390/e22080860 .

PMC 7517462 .

PMID 33286632 .

Ansari, Mohammad H.; van Steensel, Alwin; Nazarov, Yuli V. (2019).

"Entropy Production in Quantum is Different" .

Entropy .

21 (9): 854.

arXiv : 1907.09241 .

doi : 10.3390/e21090854 .

S2CID 198148019 .

Rioul, Olivier (2021).

"This is IT: A Primer on Shannon's Entropy and Information" (PDF) .

Information Theory . Progress in Mathematical Physics. Vol. 78. Birkhäuser. pp.

49– 86.

doi : 10.1007/978-3-030-81480-9_2 .

ISBN 978-3-030-81479-3 .

S2CID 204783328 .

v t e Statistical mechanics Theory Principle of maximum entropy ergodic theory Statistical thermodynamics Ensembles partition functions equations of state thermodynamic potential : U H F G Maxwell relations Models Ferromagnetism models Ising Potts Heisenberg percolation Particles with force field depletion force Lennard-Jones potential Mathematical approaches Boltzmann equation H-theorem Vlasov equation BBGKY hierarchy stochastic process mean-field theory and conformal field theory Critical phenomena Phase transition Critical exponents correlation length size scaling Entropy Boltzmann Shannon Tsallis Rényi von Neumann Applications Statistical field theory elementary particle superfluidity Condensed matter physics Complex system chaos information theory Boltzmann machine Retrieved from " https://en.wikipedia.org/w/index.php?title=Rényi_entropy&oldid=1287254942 " Categories : Information theory Entropy and information Hidden categories: Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from July 2022 This page was last edited on 25 April 2025, at 01:18 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Rényi entropy 8 languages Add topic

