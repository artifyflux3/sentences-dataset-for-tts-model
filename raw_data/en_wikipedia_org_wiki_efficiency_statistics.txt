Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Estimators Toggle Estimators subsection 1.1 Efficient estimators 1.1.1 Finite-sample efficiency 1.2 Asymptotic efficiency 1.2.1 Example: Median 1.3 Dominant estimators 1.4 Relative efficiency 1.4.1 Estimators of the mean of u.i.d. variables 1.5 Robustness 1.6 Efficiency in statistics 1.6.1 Uses of inefficient estimators 2 Hypothesis tests 3 Experimental design 4 See also 5 Notes 6 References 7 Further reading Toggle the table of contents Efficiency (statistics) 10 languages Català Deutsch Español فارسی Français Italiano Norsk bokmål Simple English Sunda 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Quality measure of a statistical method In statistics, efficiency is a measure of quality of an estimator , of an experimental design, [ 1 ] or of a hypothesis testing procedure.

[ 2 ] Essentially, a more efficient estimator needs fewer input data or observations than a less efficient one to achieve the Cramér–Rao bound . 
An efficient estimator is characterized by having the smallest possible variance , indicating that there is a small deviance between the estimated value and the "true" value in the L2 norm sense.

[ 1 ] The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure.

Estimators [ edit ] The efficiency of an unbiased estimator , T , of a parameter θ is defined as [ 3 ] e ( T ) = 1 / I ( θ θ ) var ⁡ ⁡ ( T ) {\displaystyle e(T)={\frac {1/{\mathcal {I}}(\theta )}{\operatorname {var} (T)}}} where I ( θ θ ) {\displaystyle {\mathcal {I}}(\theta )} is the Fisher information of the sample. Thus e ( T ) is the minimum possible variance for an unbiased estimator divided by its actual variance. The Cramér–Rao bound can be used to prove that e ( T ) ≤ 1.

Efficient estimators [ edit ] An efficient estimator is an estimator that estimates the quantity of interest in some “best possible” manner. The notion of “best possible” relies upon the choice of a particular loss function — the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes. The most common choice of the loss function is quadratic , resulting in the mean squared error criterion of optimality.

[ 4 ] In general, the spread of an estimator around the parameter θ is a measure of estimator efficiency and performance. This performance can be calculated by finding the mean squared error. More formally, let T be an estimator for the parameter θ . The mean squared error of T is the value MSE ⁡ ⁡ ( T ) = E [ ( T − − θ θ ) 2 ] {\displaystyle \operatorname {MSE} (T)=E[(T-\theta )^{2}]} , which can be decomposed as a sum of its variance and bias: MSE ⁡ ⁡ ( T ) = E ⁡ ⁡ [ ( T − − θ θ ) 2 ] = E ⁡ ⁡ [ ( T − − E ⁡ ⁡ [ T ] + E ⁡ ⁡ [ T ] − − θ θ ) 2 ] = E ⁡ ⁡ [ ( T − − E ⁡ ⁡ [ T ] ) 2 ] + 2 E [ T − − E [ T ] ] ( E ⁡ ⁡ [ T ] − − θ θ ) + ( E ⁡ ⁡ [ T ] − − θ θ ) 2 = var ⁡ ⁡ ( T ) + ( E ⁡ ⁡ [ T ] − − θ θ ) 2 {\displaystyle {\begin{aligned}\operatorname {MSE} (T)&=\operatorname {E} [(T-\theta )^{2}]=\operatorname {E} [(T-\operatorname {E} [T]+\operatorname {E} [T]-\theta )^{2}]\\[5pt]&=\operatorname {E} [(T-\operatorname {E} [T])^{2}]+2E[T-E[T]](\operatorname {E} [T]-\theta )+(\operatorname {E} [T]-\theta )^{2}\\[5pt]&=\operatorname {var} (T)+(\operatorname {E} [T]-\theta )^{2}\end{aligned}}} An estimator T 1 performs better than an estimator T 2 if MSE ⁡ ⁡ ( T 1 ) < MSE ⁡ ⁡ ( T 2 ) {\displaystyle \operatorname {MSE} (T_{1})<\operatorname {MSE} (T_{2})} .

[ 5 ] For a more specific case, if T 1 and T 2 are two unbiased estimators for the same parameter θ, then the variance can be compared to determine performance. In this case, T 2 is more efficient than T 1 if the variance of T 2 is smaller than the variance of T 1 , i.e.

var ⁡ ⁡ ( T 1 ) > var ⁡ ⁡ ( T 2 ) {\displaystyle \operatorname {var} (T_{1})>\operatorname {var} (T_{2})} for all values of θ . This relationship can be determined by simplifying the more general case above for mean squared error; since the expected value of an unbiased estimator is equal to the parameter value, E ⁡ ⁡ [ T ] = θ θ {\displaystyle \operatorname {E} [T]=\theta } . Therefore, for an unbiased estimator, MSE ⁡ ⁡ ( T ) = var ⁡ ⁡ ( T ) {\displaystyle \operatorname {MSE} (T)=\operatorname {var} (T)} , as the ( E ⁡ ⁡ [ T ] − − θ θ ) 2 {\displaystyle (\operatorname {E} [T]-\theta )^{2}} term drops out for being equal to 0.

[ 5 ] If an unbiased estimator of a parameter θ attains e ( T ) = 1 {\displaystyle e(T)=1} for all values of the parameter, then the estimator is called efficient.

[ 3 ] Equivalently, the estimator achieves equality in the Cramér–Rao inequality for all θ . The Cramér–Rao lower bound is a lower bound of the variance of an unbiased estimator, representing the "best" an unbiased estimator can be.

An efficient estimator is also the minimum variance unbiased estimator (MVUE). This is because an efficient estimator maintains equality on the Cramér–Rao inequality for all parameter values, which means it attains the minimum variance for all parameters (the definition of the MVUE). The MVUE estimator, even if it exists, is not necessarily efficient, because "minimum" does not mean equality holds on the Cramér–Rao inequality.

Thus an efficient estimator need not exist, but if it does, it is the MVUE.

Finite-sample efficiency [ edit ] Suppose { P θ | θ ∈ Θ } is a parametric model and X = ( X 1 , …, X n ) are the data sampled from this model. Let T = T ( X ) be an estimator for the parameter θ . If this estimator is unbiased (that is, E[ T ] = θ ), then the Cramér–Rao inequality states the variance of this estimator is bounded from below: var ⁡ ⁡ [ T ] ≥ ≥ I θ θ − − 1 , {\displaystyle \operatorname {var} [\,T\,]\ \geq \ {\mathcal {I}}_{\theta }^{-1},} where I θ θ {\displaystyle \scriptstyle {\mathcal {I}}_{\theta }} is the Fisher information matrix of the model at point θ . Generally, the variance measures the degree of dispersion of a random variable around its mean. Thus estimators with small variances are more concentrated, they estimate the parameters more precisely. We say that the estimator is a finite-sample efficient estimator (in the class of unbiased estimators) if it reaches the lower bound in the Cramér–Rao inequality above, for all θ ∈ Θ . Efficient estimators are always minimum variance unbiased estimators . However the converse is false: There exist point-estimation problems for which the minimum-variance mean-unbiased estimator is inefficient.

[ 6 ] Historically, finite-sample efficiency was an early optimality criterion. However this criterion has some limitations: Finite-sample efficient estimators are extremely rare. In fact, it was proved that efficient estimation is possible only in an exponential family , and only for the natural parameters of that family.

[ 7 ] This notion of efficiency is sometimes restricted to the class of unbiased estimators. (Often it is not.

[ 8 ] ) Since there are no good theoretical reasons to require that estimators are unbiased, this restriction is inconvenient. In fact, if we use mean squared error as a selection criterion, many biased estimators will slightly outperform the “best” unbiased ones. For example, in multivariate statistics for dimension three or more, the mean-unbiased estimator, sample mean , is inadmissible : Regardless of the outcome, its performance is worse than for example the James–Stein estimator .

[ citation needed ] Finite-sample efficiency is based on the variance, as a criterion according to which the estimators are judged. A more general approach is to use loss functions other than quadratic ones, in which case the finite-sample efficiency can no longer be formulated.

[ citation needed ] [ dubious – discuss ] As an example, among the models encountered in practice, efficient estimators exist for: the mean μ of the normal distribution (but not the variance σ 2 ), parameter λ of the Poisson distribution , the probability p in the binomial or multinomial distribution .

Consider the model of a normal distribution with unknown mean but known variance: { P θ = N ( θ , σ 2 ) | θ ∈ R }.

The data consists of n independent and identically distributed observations from this model: X = ( x 1 , …, x n ) . We estimate the parameter θ using the sample mean of all observations: T ( X ) = 1 n ∑ ∑ i = 1 n x i .

{\displaystyle T(X)={\frac {1}{n}}\sum _{i=1}^{n}x_{i}\ .} This estimator has mean θ and variance of σ 2 / n , which is equal to the reciprocal of the Fisher information from the sample. Thus, the sample mean is a finite-sample efficient estimator for the mean of the normal distribution.

Asymptotic efficiency [ edit ] Asymptotic efficiency requires Consistency (statistics) , asymptotically normal distribution of the estimator, and an asymptotic variance-covariance matrix no worse than that of any other estimator.

[ 9 ] Example: Median [ edit ] Consider a sample of size N {\displaystyle N} drawn from a normal distribution of mean μ μ {\displaystyle \mu } and unit variance , i.e., X n ∼ ∼ N ( μ μ , 1 ) .

{\displaystyle X_{n}\sim {\mathcal {N}}(\mu ,1).} The sample mean , X ¯ ¯ {\displaystyle {\overline {X}}} , of the sample X 1 , X 2 , … … , X N {\displaystyle X_{1},X_{2},\ldots ,X_{N}} , defined as X ¯ ¯ = 1 N ∑ ∑ n = 1 N X n ∼ ∼ N ( μ μ , 1 N ) .

{\displaystyle {\overline {X}}={\frac {1}{N}}\sum _{n=1}^{N}X_{n}\sim {\mathcal {N}}\left(\mu ,{\frac {1}{N}}\right).} The variance of the mean, 1/ N (the square of the standard error ) is equal to the reciprocal of the Fisher information from the sample and thus, by the Cramér–Rao inequality , the sample mean is efficient in the sense that its efficiency is unity (100%).

Now consider the sample median , X ~ ~ {\displaystyle {\widetilde {X}}} .  This is an unbiased and consistent estimator for μ μ {\displaystyle \mu } .  For large N {\displaystyle N} the sample median is approximately normally distributed with mean μ μ {\displaystyle \mu } and variance π π / 2 N , {\displaystyle {\pi }/{2N},} [ 10 ] X ~ ~ ∼ ∼ N ( μ μ , π π 2 N ) .

{\displaystyle {\widetilde {X}}\sim {\mathcal {N}}\left(\mu ,{\frac {\pi }{2N}}\right).} The efficiency of the median for large N {\displaystyle N} is thus e ( X ~ ~ ) = ( 1 N ) ( π π 2 N ) − − 1 = 2 / π π ≈ ≈ 0.64.

{\displaystyle e\left({\widetilde {X}}\right)=\left({\frac {1}{N}}\right)\left({\frac {\pi }{2N}}\right)^{-1}=2/\pi \approx 0.64.} In other words, the relative variance of the median will be π π / 2 ≈ ≈ 1.57 {\displaystyle \pi /2\approx 1.57} , or 57% greater than the variance of the mean – the standard error of the median will be 25% greater than that of the mean.

[ 11 ] Note that this is the asymptotic efficiency — that is, the efficiency in the limit as sample size N {\displaystyle N} tends to infinity. For finite values of N , {\displaystyle N,} the efficiency is higher than this (for example, a sample size of 3 gives an efficiency of about 74%).

[ citation needed ] The sample mean is thus more efficient than the sample median in this example. However, there may be measures by which the median performs better. For example, the median is far more robust to outliers , so that if the Gaussian model is questionable or approximate, there may advantages to using the median (see Robust statistics ).

Dominant estimators [ edit ] If T 1 {\displaystyle T_{1}} and T 2 {\displaystyle T_{2}} are estimators for the parameter θ θ {\displaystyle \theta } , then T 1 {\displaystyle T_{1}} is said to dominate T 2 {\displaystyle T_{2}} if: its mean squared error (MSE) is smaller for at least some value of θ θ {\displaystyle \theta } the MSE does not exceed that of T 2 {\displaystyle T_{2}} for any value of θ.

Formally, T 1 {\displaystyle T_{1}} dominates T 2 {\displaystyle T_{2}} if E ⁡ ⁡ [ ( T 1 − − θ θ ) 2 ] ≤ ≤ E ⁡ ⁡ [ ( T 2 − − θ θ ) 2 ] {\displaystyle \operatorname {E} [(T_{1}-\theta )^{2}]\leq \operatorname {E} [(T_{2}-\theta )^{2}]} holds for all θ θ {\displaystyle \theta } , with strict inequality holding somewhere.

Relative efficiency [ edit ] The relative efficiency of two unbiased estimators is defined as [ 12 ] e ( T 1 , T 2 ) = E ⁡ ⁡ [ ( T 2 − − θ θ ) 2 ] E ⁡ ⁡ [ ( T 1 − − θ θ ) 2 ] = var ⁡ ⁡ ( T 2 ) var ⁡ ⁡ ( T 1 ) {\displaystyle e(T_{1},T_{2})={\frac {\operatorname {E} [(T_{2}-\theta )^{2}]}{\operatorname {E} [(T_{1}-\theta )^{2}]}}={\frac {\operatorname {var} (T_{2})}{\operatorname {var} (T_{1})}}} Although e {\displaystyle e} is in general a function of θ θ {\displaystyle \theta } , in many cases the dependence drops out; if this is so, e {\displaystyle e} being greater than one would indicate that T 1 {\displaystyle T_{1}} is preferable, regardless of the true value of θ θ {\displaystyle \theta } .

An alternative to relative efficiency for comparing estimators, is the Pitman closeness criterion . This replaces the comparison of mean-squared-errors with comparing how often one estimator produces estimates closer to the true value than another estimator.

Estimators of the mean of u.i.d. variables [ edit ] In estimating the mean of uncorrelated, identically distributed variables we can take advantage of the fact that the variance of the sum is the sum of the variances . In this case efficiency can be defined as the square of the coefficient of variation , i.e., [ 13 ] e ≡ ≡ ( σ σ μ μ ) 2 {\displaystyle e\equiv \left({\frac {\sigma }{\mu }}\right)^{2}} Relative efficiency of two such estimators can thus be interpreted as the relative sample size of one required to achieve the certainty of the other.  Proof: e 1 e 2 = s 1 2 s 2 2 .

{\displaystyle {\frac {e_{1}}{e_{2}}}={\frac {s_{1}^{2}}{s_{2}^{2}}}.} Now because s 1 2 = n 1 σ σ 2 , s 2 2 = n 2 σ σ 2 {\displaystyle s_{1}^{2}=n_{1}\sigma ^{2},\,s_{2}^{2}=n_{2}\sigma ^{2}} we have e 1 e 2 = n 1 n 2 {\displaystyle {\frac {e_{1}}{e_{2}}}={\frac {n_{1}}{n_{2}}}} , so the relative efficiency expresses the relative sample size of the first estimator needed to match the variance of the second.

Robustness [ edit ] Efficiency of an estimator may change significantly if the distribution changes, often dropping. This is one of the motivations of robust statistics – an estimator such as the sample mean is an efficient estimator of the population mean of a normal distribution, for example, but can be an inefficient estimator of a mixture distribution of two normal distributions with the same mean and different variances. For example, if a distribution is a combination of 98% N ( μ, σ ) and 2% N ( μ, 10 σ ), the presence of extreme values from the latter distribution (often "contaminating outliers") significantly reduces the efficiency of the sample mean as an estimator of μ.

By contrast, the trimmed mean is less efficient for a normal distribution, but is more robust (i.e., less affected) by changes in the distribution, and thus may be more efficient for a mixture distribution. Similarly, the shape of a distribution , such as skewness or heavy tails , can significantly reduce the efficiency of estimators that assume a symmetric distribution or thin tails.

Efficiency in statistics [ edit ] Efficiency in statistics is important because it allows the performance of various estimators to be compared. Although an unbiased estimator is usually favored over a biased one, a more efficient biased estimator can sometimes be more valuable than a less efficient unbiased estimator. For example, this can occur when the values of the biased estimator gathers around a number closer to the true value. Thus, estimator performance can be predicted easily by comparing their mean squared errors or variances.

Uses of inefficient estimators [ edit ] Further information: L-estimator § Applications While efficiency is a desirable quality of an estimator, it must be weighed against other considerations, and an estimator that is efficient for certain distributions may well be inefficient for other distributions. Most significantly, estimators that are efficient for clean data from a simple distribution, such as the normal distribution (which is symmetric, unimodal, and has thin tails) may not be robust to contamination by outliers, and may be inefficient for more complicated distributions. In robust statistics , more importance is placed on robustness and applicability to a wide variety of distributions, rather than efficiency on a single distribution.

M-estimators are a general class of estimators motivated by these concerns. They can be designed to yield both robustness and high relative efficiency, though possibly lower efficiency than traditional estimators for some cases. They can be very computationally complicated, however.

A more traditional alternative are L-estimators , which are very simple statistics that are easy to compute and interpret, in many cases robust, and often sufficiently efficient for initial estimates. See applications of L-estimators for further discussion. Inefficient statistics in this sense are discussed in detail in The Atomic Nucleus by R. D. Evans, written before the advent of computers, when efficiently estimating even the arithmetic mean of a sorted series of measurements was laborious.

[ 14 ] Hypothesis tests [ edit ] For comparing significance tests , a meaningful measure of efficiency can be defined based on the sample size required for the test to achieve a given task power .

[ 15 ] Pitman efficiency [ 16 ] and Bahadur efficiency (or Hodges–Lehmann efficiency ) [ 17 ] [ 18 ] [ 19 ] relate to the comparison of the performance of statistical hypothesis testing procedures.

Experimental design [ edit ] Further information: Optimal design For experimental designs, efficiency relates to the ability of a design to achieve the objective of the study with minimal expenditure of resources such as time and money. In simple cases, the relative efficiency of designs can be expressed as the ratio of the sample sizes required to achieve a given objective.

[ 20 ] See also [ edit ] Bayes estimator Consistent estimator Hodges' estimator Optimal instruments Notes [ edit ] ^ a b Everitt 2002 , p. 128.

^ Nikulin, M.S. (2001) [1994], "Efficiency of a statistical procedure" , Encyclopedia of Mathematics , EMS Press ^ a b Fisher, R (1921). "On the Mathematical Foundations of Theoretical Statistics".

Philosophical Transactions of the Royal Society of London A .

222 : 309– 368.

JSTOR 91208 .

^ Everitt 2002 , p.

128 .

^ a b Dekking, F.M. (2007).

A Modern Introduction to Probability and Statistics: Understanding Why and How . Springer. pp.

303 –305.

ISBN 978-1852338961 .

^ Romano, Joseph P.; Siegel, Andrew F. (1986).

Counterexamples in Probability and Statistics . Chapman and Hall. p. 194.

^ Van Trees, Harry L. (2013).

Detection estimation and modulation theory . Kristine L. Bell, Zhi Tian (Second ed.). Hoboken, N.J.

ISBN 978-1-299-66515-6 .

OCLC 851161356 .

{{ cite book }} :  CS1 maint: location missing publisher ( link ) ^ DeGroot; Schervish (2002).

Probability and Statistics (3rd ed.). pp.

440– 441.

^ Greene, William H. (2012).

Econometric analysis (7th ed., international ed.). Boston: Pearson.

ISBN 978-0-273-75356-8 .

OCLC 726074601 .

^ Williams, D. (2001).

Weighing the Odds . Cambridge University Press. p.

165 .

ISBN 052100618X .

^ Maindonald, John; Braun, W. John (2010-05-06).

Data Analysis and Graphics Using R: An Example-Based Approach . Cambridge University Press. p. 104.

ISBN 978-1-139-48667-5 .

^ Wackerly, Dennis D.; Mendenhall, William; Scheaffer, Richard L. (2008).

Mathematical statistics with applications (Seventh ed.). Belmont, CA: Thomson Brooks/Cole. p.

445 .

ISBN 9780495110811 .

OCLC 183886598 .

^ Grubbs, Frank (1965).

Statistical Measures of Accuracy for Riflemen and Missile Engineers . pp.

26– 27.

^ Evans, Robley D. (1955).

The Atomic Nucleus (PDF) . McGraw Hill. pp. 746, Appendix G, p902 - Some Useful Inefficient Statistics.

^ Everitt 2002 , p. 321.

^ Nikitin, Ya.Yu. (2001) [1994], "Efficiency, asymptotic" , Encyclopedia of Mathematics , EMS Press ^ "Bahadur efficiency - Encyclopedia of Mathematics" .

^ Arcones M. A.

"Bahadur efficiency of the likelihood ratio test" preprint ^ Canay I. A. & Otsu, T.

"Hodges–Lehmann Optimality for Testing Moment Condition Models" ^ Dodge, Y. (2006).

The Oxford Dictionary of Statistical Terms . Oxford University Press.

ISBN 0-19-920613-9 .

References [ edit ] Everitt, Brian S. (2002).

The Cambridge Dictionary of Statistics . Cambridge University Press.

ISBN 0-521-81099-X .

Lehmann, Erich L.

(1998).

Elements of Large-Sample Theory . New York: Springer Verlag.

ISBN 978-0-387-98595-4 .

Further reading [ edit ] Lehmann, E.L.

; Casella, G. (1998).

Theory of Point Estimation (2nd ed.). Springer.

ISBN 0-387-98502-6 .

Pfanzagl, Johann ; with the assistance of R. Hamböker (1994).

Parametric Statistical Theory . Berlin: Walter de Gruyter.

ISBN 3-11-013863-8 .

MR 1291393 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐hj469
Cached time: 20250812014342
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.647 seconds
Real time usage: 0.888 seconds
Preprocessor visited node count: 3594/1000000
Revision size: 22718/2097152 bytes
Post‐expand include size: 188222/2097152 bytes
Template argument size: 4783/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 78690/5000000 bytes
Lua time usage: 0.386/10.000 seconds
Lua memory usage: 7862737/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  589.823      1 -total
 32.22%  190.050      1 Template:Reflist
 28.58%  168.565      1 Template:Statistics
 28.13%  165.929      1 Template:Navbox_with_collapsible_groups
 16.14%   95.205      2 Template:SpringerEOM
 14.01%   82.660     15 Template:Cite_book
 12.13%   71.529      1 Template:Short_description
  9.68%   57.115     11 Template:Navbox
  9.45%   55.737      1 Template:Hlist
  8.10%   47.762      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:27250866:|#|:idhash:canonical and timestamp 20250812014342 and revision id 1300987313. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Efficiency_(statistics)&oldid=1300987313 " Category : Estimation theory Hidden categories: CS1 maint: location missing publisher Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from December 2011 Articles with unsourced statements from February 2012 All accuracy disputes Articles with disputed statements from February 2012 Articles with unsourced statements from April 2013 This page was last edited on 17 July 2025, at 14:02 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Efficiency (statistics) 10 languages Add topic

