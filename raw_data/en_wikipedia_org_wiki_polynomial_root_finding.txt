Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Overview Toggle Overview subsection 1.1 Closed-form formulas 1.2 Numerical algorithms 2 History Toggle History subsection 2.1 Closed-form formulas 2.1.1 The quadratics 2.1.2 The cubics and the quartics 2.1.3 Insolvability of the quintics 2.1.4 General solution using combinatorics 2.2 Numerical methods 2.2.1 Iterative methods 2.2.2 Real-root isolation methods 2.3 Mechanical methods 3 Common root-finding algorithms Toggle Common root-finding algorithms subsection 3.1 Finding one root 3.2 Finding all complex roots 3.2.1 Methods using complex-number arithmetic 3.2.2 Methods using linear algebra 3.2.3 Limitations of iterative methods for finding all roots 3.3 Finding all real roots 3.4 Finding roots in a restricted domain 3.5 Finding complex roots in pairs 3.6 Polynomials with rational coefficients 4 See also 5 References Toggle the table of contents Polynomial root-finding Add languages Add links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia For broader coverage of this topic, see Root-finding algorithms .

Finding the roots of polynomials is a long-standing problem that has been extensively studied throughout the history and substantially influenced the development of mathematics. It involves determining either a numerical approximation or a closed-form expression of the roots of a univariate polynomial, i.e., determining approximate or closed form solutions of x {\displaystyle x} in the equation a 0 + a 1 x + a 2 x 2 + ⋯ ⋯ + a n x n = 0 {\displaystyle a_{0}+a_{1}x+a_{2}x^{2}+\cdots +a_{n}x^{n}=0} where a i {\displaystyle a_{i}} are either real or complex numbers .

Efforts to understand and solve polynomial equations led to the development of important mathematical concepts, including irrational and complex numbers, as well as foundational structures in modern algebra such as fields , rings , and groups .

Despite being historically important, finding the roots of higher degree polynomials no longer play a central role in mathematics and computational mathematics, with one major exception in computer algebra .

[ 1 ] Overview [ edit ] Closed-form formulas [ edit ] Closed-form formulas for polynomial roots exist only when the degree of the polynomial is less than 5. The quadratic formula has been known since antiquity, and the cubic and quartic formulas were discovered in full generality during the 16th century.

When the degree of polynomial is at least 5, a closed-form expression for the roots by the polynomial coefficients does not exist in general, if we only uses additions, subtractions, multiplications, divisions, and radicals (taking n-th roots) in the formula. This is due to the celebrated Abel-Ruffini theorem . On the other hand, the fundamental theorem of algebra shows that all nonconstant polynomials have at least one root. Therefore, root-finding algorithms consists of finding numerical solutions in most cases.

Numerical algorithms [ edit ] Root-finding algorithms can be broadly categorized according to the goal of the computation. Some methods aim to find a single root, while others are designed to find all complex roots at once. In certain cases, the objective may be to find roots within a specific region of the complex plane. It is often desirable and even necessary to select algorithms specific to the computational task due to efficiency and accuracy reasons. See Root Finding Methods for a summary of the existing methods available in each case.

History [ edit ] Closed-form formulas [ edit ] The root-finding problem of polynomials was first recognized by the Sumerians and then the Babylonians. Since then, the search for closed-form formulas for polynomial equations lasted for thousands of years.

The quadratics [ edit ] The Babylonions and Egyptians were able to solve specific quadratic equations  in the second millennium BCE, and their solutions essentially correspond to the quadratic formula.

[ 2 ] However, it took 2 millennia of effort to state the quadratic formula in an explicit form similar to the modern formulation, provided by Indian Mathematician Brahmagupta in his book Brāhmasphuṭasiddhānta 625 CE. The full recognition of the quadratic formula requires the introduction of  complex numbers, which took another a millennia.

The cubics and the quartics [ edit ] The first breakthrough in a closed-form formula of polynomials with degree higher than two took place in Italy. In the early 16th century, the Italian mathematician Scipione del Ferro found a closed-form formula for cubic equations of the form x 3 + m x = n {\displaystyle x^{3}+mx=n} , where m , n {\displaystyle m,n} are nonnegative numbers. Later, Niccolò Tartaglia also discovered methods to solve such cubic equations, and Gerolamo Cardano summarized and published their work in his book Ars Magna in 1545.

Meanwhile, Cardano's student Lodovico Ferrari discovered the closed-form formula of the quartic equations in 1540. His solution is based on the closed-form formula of the cubic equations, thus had to wait until the cubic formula to be published.

In Ars Magna, Cardano noticed that Tartaglia's method sometimes involves extracting the square root of a negative number. In fact, this could happen even if the roots are real themselves . Later, the Italian mathematician Rafael Bombelli investigated further into these mathematical objects by giving an explicit arithmetic rules in his book Algebra published in 1569. These mathematical objects are now known as the complex numbers , which are foundational in mathematics, physics, and engineering.

Insolvability of the quintics [ edit ] Since the discovery of cubic and quartic formulas, solving quintic equations in a closed form had been a major problem in algebra. The French lawyer Viete , who first formulated the root formula for cubics in modern language and applied trigonometric methods to root-solving, believed that his methods generalize to a closed-form formula in radicals for polynomial with arbitrary degree.

Descartes also held the same opinion.

[ 3 ] However, Lagrange noticed the flaws in these arguments in his 1771 paper Reflections on the Algebraic Theory of Equations , where he analyzed why the methods used to solve the cubics and quartics would not work to solve the quintics. His argument involves studying the permutation of the roots of polynomial equations. Nevertheless, Lagrange still believed that closed-form formula in radicals of the quintics exist. Gauss seems to have been the first prominent mathematician who suspected the insolvability of the quintics, stated in his 1799 doctoral dissertation.

The first serious attempt at proving the insolvability of the quintic was given by the Italian mathematician Paolo Ruffini. He published six versions of his proof between 1799 and 1813, yet his proof was not widely accepted as the writing was long and difficult to understand, and turned out to have a gap.

The first rigorous and accepted proof of the insolvability of the quintic was famously given by Niels Henrik Abel in 1824, which made essential use of the Galois theory of field extensions. In the paper, Abel proved that polynomials with degree more than 4 do not have a closed-form root formula by radicals in general. This puts an end in the search of closed form formulas of the roots of polynomials by radicals of the polynomial coefficients.

General solution using combinatorics [ edit ] In 2025, Norman Wildberger and Dean Rubine introduced a general solution for arbitrary degree, involving a formal power series . The equation 1 − − x + a 2 x 2 + a 3 x 3 + a 4 x 4 + .

.

.

{\displaystyle 1-x+a_{2}x^{2}+a_{3}x^{3}+a_{4}x^{4}+...} has a solution x = ∑ ∑ m 2 , m 3 , .

.

.

≥ ≥ 0 ( 2 m 2 + 3 m 3 + 4 m 4 + .

.

.

) !

( 1 + m 2 + 2 m 3 + .

.

.

) !

m 2 !

m 3 !

.

.

.

a 2 m 2 a 3 m 3 .

.

.

{\displaystyle x=\sum _{m_{2},m_{3},...\geq 0}{{\frac {(2m_{2}+3m_{3}+4m_{4}+...)!}{(1+m_{2}+2m_{3}+...)!m_{2}!m_{3}!...}}a_{2}^{m_{2}}a_{3}^{m_{3}}...}} This is a generalization of a solution for the quadratics using Catalan numbers C n {\displaystyle C_{n}} , for which it reduces to x = ∑ ∑ n ≥ ≥ 0 C n t n {\displaystyle x=\sum _{n\geq 0}C_{n}t^{n}} . For the quintic, this is closely related to the Eisenstein series .

[ 4 ] Numerical methods [ edit ] Since finding a closed-form formula of higher degree polynomials is significantly harder than that of quadratic equations, the earliest attempts to solve cubic equations are either geometrical or numerical.  Also, for practical purposes, numerical solutions are necessary.

Iterative methods [ edit ] The earliest iterative approximation methods of root-finding were developed to compute square roots. In Heron of Alexandria 's book Metrica (1st-2nd century CE), approximate values of square roots were computed by iteratively improving an initial estimate.

[ 5 ] Jamshīd al-Kāshī presented a generalized version of the method to compute n {\displaystyle n} th root s.

A similar method was also found in Henry Briggs 's publication Trigonometria Britannica in 1633.

Franciscus Vieta also developed an approximation method that is almost identical to Newton's method.

Newton further generalized the method to compute the roots of arbitrary polynomials  in De analysi per aequationes numero terminorum infinitas (written in 1669, published in 1711), now known as Newton's method . In 1690, Joseph Raphson published a refinement of Newton's method, presenting it in a form that more closely aligned with the modern version used today.

[ 6 ] In 1879, the English mathematician Arthur Cayley noticed the difficulties in generalizing Newton's method to complex roots of polynomials with degree greater than 2 and complex initial values in his paper The Newton–Fourier imaginary problem.

This opened the way to the study of the theory of iterations of rational functions.

Real-root isolation methods [ edit ] A class of methods of finding numerical value of real roots is based on real-root isolation . The first example of such method is given by René Descartes in 1637. It counts the roots of a polynomial by examining sign changes in its coefficients. In 1807, the French mathematician François Budan de Boislaurent generalized Descarte's result into Budan's theorem which counts the real roots in a half-open interval ( a , b ]. However, both methods are not suitable as an effective algorithm.

The first complete real-root isolation algorithm was given by Jacques Charles François Sturm in 1829, known as the Sturm's theorem .

In 1836, Alexandre Joseph Hidulphe Vincent proposed a method for isolating real roots of polynomials using continued fractions, a result now known as Vincent's theorem . The work was largely forgotten until it was rediscovered over a century later by J. V. Uspensky , who included it in his 1948 textbook Theory of Equations . The theorem was subsequently brought to wider academic attention by the American mathematician Alkiviadis G. Akritas , who recognized its significance while studying Uspensky's account.

[ 7 ] [ 8 ] The first implimentation of real-root isolation method by modern computer is given by G.E. Collins and Alkiviadis G. Akritas in 1976, where they proved an effective version of Vincent's theorem. Variants of the algorithm were subsequently studied.

[ 9 ] Mechanical methods [ edit ] Before electronic computers were invented, people used mechanical computers to automate the polynomial-root solving problems. In 1758, the Hungarian scientist J.A. De Segner proposed a design of root-solving machine in his paper, which operates by drawing the graph of the polynomial on a plane and find the roots as the intersections of the graph with x-axis. In 1770, the English mathematician Jack Rowning investigated the possibility of drawing the graph of polynomials via local motions.

[ 10 ] In 1845, the English mathematician Francis Bushforth proposed to use trignometric methods to simplify the root finding problem. Given a polynomial a 0 + a 1 x + .

.

.

+ a n x n = 0 {\displaystyle a_{0}+a_{1}x+...+a_{n}x^{n}=0} , substitute x = cos ⁡ ⁡ t {\displaystyle x=\cos t} . Since cos n ⁡ ⁡ t {\displaystyle \cos ^{n}t} can be written as a linear combination of cos ⁡ ⁡ k t , k ∈ ∈ Z {\displaystyle \cos kt,k\in \mathbb {Z} } (See Chebyshev polynomials ), the polynomial can be reformulated into the following form b 0 + b 1 cos ⁡ ⁡ t + b 2 cos ⁡ ⁡ ( 2 t ) + .

.

.

+ b n cos ⁡ ⁡ ( n t ) {\displaystyle b_{0}+b_{1}\cos t+b_{2}\cos(2t)+...+b_{n}\cos(nt)} Such curves can be drawn by a harmonic analyzer (also known as tide predicting machines).

[ 11 ] The first harmonic analyzer was built by Lord Kelvin in 1872, while Bashforth envisioned such machine in his paper 27 years ago.

[ 12 ] The Spanish engineer and mathematician Leonardo Torres Quevedo built several machines for solving real and complex roots of polynomials between 1893-1900. His machine employs a logarithmic algorithm, and has a mechanical component called the Endless principle to the value of log ⁡ ⁡ ( a + b ) {\displaystyle \log(a+b)} from log ⁡ ⁡ a , log ⁡ ⁡ b {\displaystyle \log a,\log b} with high accuracy. This allow him to achieve high accuracy in polynomial root-finding: the machine computes the roots of deg 8 polynomials with an accuracy of 10 − − 3 {\displaystyle 10^{-3}} .

[ 13 ] Common root-finding algorithms [ edit ] Finding one root [ edit ] The most widely used method for computing a root of any differentiable function f {\displaystyle f} is Newton's method , in which an initial guess x 0 {\displaystyle x_{0}} is iteratively refined. At each iteration the tangent line to f {\displaystyle f} at x n {\displaystyle x_{n}} is used as a linear approximation to f {\displaystyle f} , and its root is used as the succeeding guess x n + 1 {\displaystyle x_{n+1}} : x n + 1 = x n − − f ( x n ) f ′ ( x n ) , {\displaystyle x_{n+1}=x_{n}-{\frac {f(x_{n})}{f'(x_{n})}},} In general, the value of x n {\displaystyle x_{n}} will converge to a root of f {\displaystyle f} .

In particular, the method can be applied to compute a root of polynomial functions. In this case, the computations in Newton's method can be accelerated using Horner's method or evaluation with preprocessing for computing the polynomial and its derivative in each iteration.

Though the rate of convergence of Newton's method is generally quadratic , it might converge much slowly or even not converge at all. In particular, if the polynomial has no real root, and x 0 {\displaystyle x_{0}} is chosen to be a real number, then Newton's method cannot converge. However, if the polynomial has a real root, which is larger than the larger real root of its derivative, then Newton's method converges quadratically to this largest root if x 0 {\displaystyle x_{0}} is larger than this larger root (there are easy ways for computing an upper bound of the roots, see Properties of polynomial roots ). This is the starting point of Horner's method for computing the roots.

Closely related to Newton's method are Halley's method and Laguerre's method . Both use the polynomial and its two first derivations for an iterative process that has a cubic convergence . Combining two consecutive steps of these methods into a single test, one gets a rate of convergence of 9, at the cost of 6 polynomial evaluations (with Horner's rule). On the other hand, combining three steps of Newtons method gives a rate of convergence of 8 at the cost of the same number of polynomial evaluation. This gives a slight advantage to these methods (less clear for Laguerre's method, as a square root has to be computed at each step).

When applying these methods to polynomials with real coefficients and real starting points, Newton's and Halley's method stay inside the real number line. One has to choose complex starting points to find complex roots. In contrast, the Laguerre method with a square root in its evaluation will leave the real axis of its own accord.

Finding all complex roots [ edit ] Methods using complex-number arithmetic [ edit ] Both the Aberth method and the similar yet simpler Durand–Kerner method simultaneously find all of the roots using only simple complex number arithmetic. The Aberth method is presently the most efficient method. Accelerated algorithms for multi-point evaluation and interpolation similar to the fast Fourier transform can help speed them up for large degrees of the polynomial.

A free implementation of Aberth's method is available under the name of MPSolve . This is a reference implementation, which can find routinely the roots of polynomials of degree larger than 1,000, with more than 1,000 significant decimal digits.

Another method with this style is the Dandelin–Gräffe method (sometimes also ascribed to Lobachevsky ), which uses polynomial transformations to repeatedly and implicitly square the roots. This greatly magnifies variances in the roots. Applying Viète's formulas , one obtains easy approximations for the modulus of the roots, and with some more effort, for the roots themselves.

Methods using linear algebra [ edit ] Arguably, the most reliable method to find all roots of a polynomial is to find the eigenvalues of the companion matrix of monic polynomial, which coincides with the roots of the polynomial. There are plenty of algorithms for computing the eigenvalue of matrices. The standard method for finding all roots of a polynomial in MATLAB uses the Francis QR algorithm to compute the eigenvalues of the corresponding companion matrix of the polynomial.

[ 14 ] By the use of the sparse structure of the companion matrix, some special QR iteration methods give all complex roots in O(n^3) arithmetics
and O(n) storage [ 15 ] [ 16 ] .

In principle, one can use any eigenvalue algorithm to find the roots of the polynomial. However, for efficiency reasons one prefers methods that employ the structure of the matrix, that is, can be implemented in matrix-free form. Among these methods are the power method , whose application to the transpose of the companion matrix is the classical Bernoulli's method to find the root of greatest modulus. The inverse power method with shifts, which finds some smallest root first, is what drives the complex ( cpoly ) variant of the Jenkins–Traub algorithm and gives it its numerical stability. Additionally, it has fast convergence with order 1 + φ φ ≈ ≈ 2.6 {\displaystyle 1+\varphi \approx 2.6} (where φ φ {\displaystyle \varphi } is the golden ratio ) even in the presence of clustered roots. This fast convergence comes with a cost of three polynomial evaluations per step, resulting in a residual of O (| f ( x )| 2+3 φ ) , that is a slower convergence than with three steps of Newton's method.

Limitations of iterative methods for finding all roots [ edit ] The oldest method of finding all roots is to start by finding a single root. When a root r has been found, it can be removed from the polynomial by dividing out the binomial x – r . The resulting polynomial contains the remaining roots, which can be found by iterating on this process. This idea, despite being common in theoretical deriviations, does not work well in numerical computations because of the phenomenon of numerical instability : Wilkinson's polynomial shows that a very small modification of one coefficient may change dramatically not only the value of the roots, but also their nature (real or complex). Also, even with a good approximation, when one evaluates a polynomial at an approximate root, one may get a result that is far to close to zero. For example, if a polynomial of degree 20 (the degree of Wilkinson's polynomial) has a root close to 10, the derivative of the polynomial at the root may be of the order of 10 20 ; {\displaystyle 10^{20};} this implies that an error of 10 − − 10 {\displaystyle 10^{-10}} on the value of the root may produce a value of the polynomial at the approximate root that is of the order of 10 10 .

{\displaystyle 10^{10}.} Finding all real roots [ edit ] Main article: Real-root isolation Finding the real roots of a polynomial with real coefficients is a problem that has received much attention since the beginning of 19th century, and is still an active domain of research.

Methods for finding all complex roots can provide the real roots. However, because of the numerical instability of polynomials, it may need arbitrary-precision arithmetic to decide whether a root with a small imaginary part is real or not. Moreover, as the number of the real roots is, on the average, proportional to the logarithm of the degree, [ 17 ] it is a waste of computer resources to compute the non-real roots when one is interested in real roots.

The standard way of computing real roots is to compute first disjoint intervals, called isolating intervals , such that each one contains exactly one real root, and together they contain all the roots. This computation is called real-root isolation . Having an isolating interval, one may use fast numerical methods, such as Newton's method for improving the precision of the result.

The oldest complete algorithm for real-root isolation results from Sturm's theorem . However, it appears to be much less efficient than the methods based on Descartes' rule of signs and its extensions— Budan's and Vincent's theorems . These methods divide into two main classes, one using continued fractions and the other using bisection. Both method have been dramatically improved since the beginning of 21st century. With these improvements they reach a computational complexity that is similar to that of the best algorithms for computing all the roots (even when all roots are real).

These algorithms have been implemented and are available in Mathematica (continued fraction method) and Maple (bisection method), as well as in other main computer algebra systems ( SageMath , PARI/GP ) . Both implementations can routinely find the real roots of polynomials of degree higher than 1,000.

Finding roots in a restricted domain [ edit ] Several fast tests exist that tell if a segment of the real line or a region of the complex plane contains no roots. By bounding the modulus of the roots and recursively subdividing the initial region indicated by these bounds, one can isolate small regions that may contain roots and then apply other methods to locate them exactly.

All these methods involve finding the coefficients of shifted and scaled versions of the polynomial. For large degrees, FFT -based accelerated methods become viable.

The Lehmer–Schur algorithm uses the Schur–Cohn test for circles; a variant, Wilf's global bisection algorithm uses a winding number computation for rectangular regions in the complex plane.

The splitting circle method uses FFT-based polynomial transformations to find large-degree factors corresponding to clusters of roots. The precision of the factorization is maximized using a Newton-type iteration. This method is useful for finding the roots of polynomials of high degree to arbitrary precision; it has almost optimal complexity in this setting.

[ citation needed ] Finding complex roots in pairs [ edit ] If the given polynomial only has real coefficients, one may wish to avoid computations with complex numbers. To that effect, one has to find quadratic factors for pairs of conjugate complex roots. The application of the multidimensional Newton's method to this task results in Bairstow's method .

The real variant of Jenkins–Traub algorithm is an improvement of this method.

Polynomials with rational coefficients [ edit ] Main article: Square-free factorization For polynomials whose coefficients are exactly given as integers or rational numbers , there is an efficient method to factorize them into factors that have only simple roots and whose coefficients are also given in precise terms. This method, called square-free factorization , is based on the multiple roots of a polynomial being the roots of the greatest common divisor of the polynomial and its derivative.

The square-free factorization of a polynomial p is a factorization p = p 1 p 2 2 ⋯ ⋯ p k k {\displaystyle p=p_{1}p_{2}^{2}\cdots p_{k}^{k}} where each p i {\displaystyle p_{i}} is either 1 or a polynomial without multiple roots, and two different p i {\displaystyle p_{i}} do not have any common root.

An efficient method to compute this factorization is Yun's algorithm .

See also [ edit ] Rational root theorem References [ edit ] ^ Pan, Victor Y. (January 1997).

"Solving a Polynomial Equation: Some History and Recent Progress" .

SIAM Review .

39 (2): 187– 220.

doi : 10.1137/S0036144595288554 .

ISSN 0036-1445 .

^ Berriman, A. E. (1956).

"The Babylonian Quadratic Equation" .

The Mathematical Gazette .

40 (333): 185– 192.

doi : 10.2307/3608807 .

ISSN 0025-5572 .

JSTOR 3608807 .

^ Brown, Jim (2000).

"Abel and the insolvability of the quintic" (PDF) .

^ Wildberger, N. J.; Rubine, D. (8 April 2025).

"A Hyper-Catalan Series Solution to Polynomial Equations, and the Geode" .

The American Mathematical Monthly .

132 (5): 383– 402.

doi : 10.1080/00029890.2025.2460966 .

^ Fowler, David; Robson, Eleanor (November 1998).

"Square Root Approximations in Old Babylonian Mathematics: YBC 7289 in Context" .

Historia Mathematica .

25 (4): 366– 378.

doi : 10.1006/hmat.1998.2209 .

^ Cajori, Florian (1911-02-01).

"Historical Note on the Newton-Raphson Method of Approximation" .

The American Mathematical Monthly .

18 (2): 29– 32.

doi : 10.1080/00029890.1911.11997596 .

ISSN 0002-9890 .

^ Akritas, Alkiviadis G.; Danielopoulos, Stylianos D. (1978-11-01).

"On the forgotten theorem of Mr. Vincent" .

Historia Mathematica .

5 (4): 427– 435.

doi : 10.1016/0315-0860(78)90211-2 .

ISSN 0315-0860 .

^ Uspensky, J. V. (James Victor) (1948).

Theory of equations. -- . Internet Archive. New York : McGraw-Hill Book Co.

^ Rouillier, Fabrice; Zimmermann, Paul (January 2004).

"Efficient isolation of polynomial's real roots" .

Journal of Computational and Applied Mathematics .

162 (1): 33– 50.

doi : 10.1016/j.cam.2003.08.015 .

^ Frame, J. S. (1945).

"Machines for solving algebraic equations" .

Mathematics of Computation .

1 (9): 337– 353.

doi : 10.1090/S0025-5718-1945-0011196-2 .

ISSN 0025-5718 .

^ Bashforth, Francis; British Association, Cambridge, 1845 (1892).

Reprint of "A description of a machine for finding the numerical roots of equations and tracing a variety of useful curves." Communicated to the British Association, 1845. With an appendix containing extracts from papers relating to the invention of the tide predicter . Cambridge.

{{ cite book }} :  CS1 maint: location missing publisher ( link ) CS1 maint: multiple names: authors list ( link ) CS1 maint: numeric names: authors list ( link ) ^ Parker (2011) , p. 37.

^ Thomas, Federico (2008-08-01).

"A short account on Leonardo Torres' endless spindle" .

Mechanism and Machine Theory .

43 (8): 1055– 1063.

doi : 10.1016/j.mechmachtheory.2007.07.003 .

hdl : 10261/30460 .

ISSN 0094-114X .

^ "Polynomial roots - MATLAB roots" .

MathWorks . 2021-03-01 . Retrieved 2021-09-20 .

^ Jared L. Aurentz, Thomas Mach, Raf Vandebril and David S. Watkins: "Fast and Backward Stable Computation of Roots of Polynomials", SIAM Journal on Matrix Analysis and Applications, Vol.36, No.3 (2015).

^ Jared L. Aurentz, Thomas Mach, Leonardo Robol, Raf Vandebril and David S. Watkins: "Fast and Backward Stable Computation of Roots of Polynomials, Part II: Backward Error Analysis; Companion Matrix and Companion Pencil", SIAM Journal on Matrix Analysis and Applications, Vol.39, No.3 (2018).

^ Nguyen, Hoi; Nguyen, Oanh; Vu, Van (2016).

"On the number of real roots of random polynomials" .

Communications in Contemporary Mathematics .

18 (4): 1550052.

arXiv : 1402.4628 .

doi : 10.1142/S0219199715500522 .

ISSN 0219-1997 .

v t e Root-finding algorithms Bracketing (no derivative) Bisection method Regula falsi ITP method Householder Newton's method Halley's method Quasi-Newton Broyden's method Secant method Newton–Krylov method Steffensen's method Hybrid methods Brent's method Ridders' method Polynomial methods Aberth method Bairstow's method Bernoulli's method Durand–Kerner method Graeffe's method Jenkins–Traub algorithm Lehmer–Schur algorithm Laguerre's method Splitting circle method Other methods Fixed-point iteration Inverse quadratic interpolation Muller's method Sidi's generalized secant method NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐xwdw8
Cached time: 20250812030923
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.398 seconds
Real time usage: 0.535 seconds
Preprocessor visited node count: 1731/1000000
Revision size: 29236/2097152 bytes
Post‐expand include size: 45016/2097152 bytes
Template argument size: 1060/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 65252/5000000 bytes
Lua time usage: 0.244/10.000 seconds
Lua memory usage: 6200381/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  386.625      1 -total
 47.17%  182.371      1 Template:Reflist
 35.66%  137.876     10 Template:Cite_journal
 18.26%   70.602      1 Template:Root-finding_algorithms
 17.79%   68.767      1 Template:Navbox
 15.99%   61.829      1 Template:Short_description
  9.49%   36.692      2 Template:Pagetype
  8.14%   31.484      1 Template:Citation_needed
  7.01%   27.111      1 Template:Fix
  5.10%   19.699      1 Template:Broader Saved in parser cache with key enwiki:pcache:72719782:|#|:idhash:canonical and timestamp 20250812030923 and revision id 1304542218. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Polynomial_root-finding&oldid=1304542218 " Categories : Polynomials Polynomial factorization algorithms Hidden categories: CS1 maint: location missing publisher CS1 maint: multiple names: authors list CS1 maint: numeric names: authors list Articles with short description Short description is different from Wikidata All articles with unsourced statements Articles with unsourced statements from November 2018 This page was last edited on 6 August 2025, at 17:21 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Polynomial root-finding Add languages Add topic

