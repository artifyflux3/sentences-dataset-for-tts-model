Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 History 2 Definition Toggle Definition subsection 2.1 Binary classification confusion matrix 3 Examples Toggle Examples subsection 3.1 Simple example 3.2 Same percentages but different numbers 4 Properties Toggle Properties subsection 4.1 Hypothesis testing and confidence interval 4.2 Interpreting magnitude 4.3 Kappa maximum 4.4 Limitations 5 Related statistics Toggle Related statistics subsection 5.1 Scott's Pi 5.2 Fleiss' kappa 5.3 Weighted kappa 6 See also 7 Further reading 8 External links 9 References Toggle the table of contents Cohen's kappa 17 languages العربية Català Deutsch Español Euskara فارسی Français 한국어 Italiano Magyar Polski Português සිංහල Suomi ไทย Türkçe 粵語 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Statistic measuring inter-rater agreement for categorical items Cohen's kappa coefficient ('κ', lowercase Greek kappa ) is a statistic that is used to measure inter-rater reliability for qualitative (categorical) items.

[ 1 ] It is generally thought to be a more robust measure than simple percent agreement calculation, as κ incorporates the possibility of the agreement occurring by chance. There is controversy surrounding Cohen's kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.

[ 2 ] History [ edit ] The first mention of a kappa-like statistic is attributed to Galton in 1892.

[ 3 ] [ 4 ] The seminal paper introducing kappa as a new technique was published by Jacob Cohen in the journal Educational and Psychological Measurement in 1960.

[ 5 ] Definition [ edit ] Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories. The definition of κ κ {\textstyle \kappa } is κ κ ≡ ≡ p o − − p e 1 − − p e = 1 − − 1 − − p o 1 − − p e , {\displaystyle \kappa \equiv {\frac {p_{o}-p_{e}}{1-p_{e}}}=1-{\frac {1-p_{o}}{1-p_{e}}},} where p o is the relative observed agreement among raters, and p e is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly selecting each category. If the raters are in complete agreement then κ κ = 1 {\textstyle \kappa =1} . If there is no agreement among the raters other than what would be expected by chance (as given by p e ), κ κ = 0 {\textstyle \kappa =0} . It is possible for the statistic to be negative, [ 6 ] which can occur by chance if there is no relationship between the ratings of the two raters, or it may reflect a real tendency of the raters to give differing ratings.

For k categories, N observations to categorize and n k i {\displaystyle n_{ki}} the number of times rater i predicted category k : p e = 1 N 2 ∑ ∑ k n k 1 n k 2 {\displaystyle p_{e}={\frac {1}{N^{2}}}\sum _{k}n_{k1}n_{k2}} This is derived from the following construction: p e = ∑ ∑ k p k 12 ^ ^ = ind.

∑ ∑ k p k 1 ^ ^ p k 2 ^ ^ = ∑ ∑ k n k 1 N n k 2 N = 1 N 2 ∑ ∑ k n k 1 n k 2 {\displaystyle p_{e}=\sum _{k}{\widehat {p_{k12}}}{\overset {\text{ind.}}{=}}\sum _{k}{\widehat {p_{k1}}}{\widehat {p_{k2}}}=\sum _{k}{\frac {n_{k1}}{N}}{\frac {n_{k2}}{N}}={\frac {1}{N^{2}}}\sum _{k}n_{k1}n_{k2}} Where p k 12 ^ ^ {\displaystyle {\widehat {p_{k12}}}} is the estimated probability that both rater 1 and rater 2 will classify the same item as k, while p k 1 ^ ^ {\displaystyle {\widehat {p_{k1}}}} is the estimated probability that rater 1 will classify an item as k (and similarly for rater 2).
The relation p k ^ ^ = ∑ ∑ k p k 1 ^ ^ p k 2 ^ ^ {\textstyle {\widehat {p_{k}}}=\sum _{k}{\widehat {p_{k1}}}{\widehat {p_{k2}}}} is based on using the assumption that the rating of the two raters are independent . The term p k 1 ^ ^ {\displaystyle {\widehat {p_{k1}}}} is estimated by using the number of items classified as k by rater 1 ( n k 1 {\displaystyle n_{k1}} ) divided by the total items to classify ( N {\displaystyle N} ): p k 1 ^ ^ = n k 1 N {\displaystyle {\widehat {p_{k1}}}={n_{k1} \over N}} (and similarly for rater 2).

Binary classification confusion matrix [ edit ] In the traditional 2 × 2 confusion matrix employed in machine learning and statistics to evaluate binary classifications , the Cohen's Kappa formula can be written as: [ 7 ] κ κ = 2 × × ( T P × × T N − − F N × × F P ) ( T P + F P ) × × ( F P + T N ) + ( T P + F N ) × × ( F N + T N ) {\displaystyle \kappa ={\frac {2\times (TP\times TN-FN\times FP)}{(TP+FP)\times (FP+TN)+(TP+FN)\times (FN+TN)}}} where TP are the true positives, FP are the false positives, TN are the true negatives, and FN are the false negatives. In this case, Cohen's Kappa is equivalent to the Heidke skill score known in Meteorology .

[ 8 ] The measure was first introduced by Myrick Haskell Doolittle in 1888.

[ 9 ] Examples [ edit ] Simple example [ edit ] Suppose that you were analyzing data related to a group of 50 people applying for a grant. Each grant proposal was read by two readers and each reader either said "Yes" or "No" to the proposal. Suppose the disagreement count data were as follows, where A and B are readers, data on the main diagonal of the matrix (a and d) count the number of agreements and off-diagonal data (b and c) count the number of disagreements: B A Yes No Yes a b No c d e.g.

B A Yes No Yes 20 5 No 10 15 The observed proportionate agreement is: p o = a + d a + b + c + d = 20 + 15 50 = 0.7 {\displaystyle p_{o}={\frac {a+d}{a+b+c+d}}={\frac {20+15}{50}}=0.7} To calculate p e (the probability of random agreement) we note that: Reader A said "Yes" to 25 applicants and "No" to 25 applicants. Thus reader A said "Yes" 50% of the time.

Reader B said "Yes" to 30 applicants and "No" to 20 applicants. Thus reader B said "Yes" 60% of the time.

So the expected probability that both would say yes at random is: p Yes = a + b a + b + c + d ⋅ ⋅ a + c a + b + c + d = 0.5 × × 0.6 = 0.3 {\displaystyle p_{\text{Yes}}={\frac {a+b}{a+b+c+d}}\cdot {\frac {a+c}{a+b+c+d}}=0.5\times 0.6=0.3} Similarly: p No = c + d a + b + c + d ⋅ ⋅ b + d a + b + c + d = 0.5 × × 0.4 = 0.2 {\displaystyle p_{\text{No}}={\frac {c+d}{a+b+c+d}}\cdot {\frac {b+d}{a+b+c+d}}=0.5\times 0.4=0.2} Overall random agreement probability is the probability that they agreed on either Yes or No, i.e.: p e = p Yes + p No = 0.3 + 0.2 = 0.5 {\displaystyle p_{e}=p_{\text{Yes}}+p_{\text{No}}=0.3+0.2=0.5} So now applying our formula for Cohen's Kappa we get: κ κ = p o − − p e 1 − − p e = 0.7 − − 0.5 1 − − 0.5 = 0.4 {\displaystyle \kappa ={\frac {p_{o}-p_{e}}{1-p_{e}}}={\frac {0.7-0.5}{1-0.5}}=0.4} Same percentages but different numbers [ edit ] A case sometimes considered to be a problem with Cohen's Kappa occurs when comparing the Kappa calculated for two pairs of raters with the two raters in each pair having the same percentage agreement but one pair give a similar number of ratings in each class while the other pair give a very different number of ratings in each class.

[ 10 ] (In the cases below, notice B has 70 yeses and 30 nos, in the first case, but those numbers are reversed in the second.) For instance, in the following two cases there is equal agreement between A and B (60 out of 100 in both cases) in terms of agreement in each class, so we would expect the relative values of Cohen's Kappa to reflect this. However, calculating Cohen's Kappa for each: B A Yes No Yes 45 15 No 25 15 κ κ = 0.60 − − 0.54 1 − − 0.54 = 0.1304 {\displaystyle \kappa ={\frac {0.60-0.54}{1-0.54}}=0.1304} B A Yes No Yes 25 35 No 5 35 κ κ = 0.60 − − 0.46 1 − − 0.46 = 0.2593 {\displaystyle \kappa ={\frac {0.60-0.46}{1-0.46}}=0.2593} we find that it shows greater similarity between A and B in the second case, compared to the first. This is because while the percentage agreement is the same, the percentage agreement that would occur 'by chance' is significantly higher in the first case (0.54 compared to 0.46).

Properties [ edit ] Hypothesis testing and confidence interval [ edit ] P-value for kappa is rarely reported, probably because even relatively low values of kappa can nonetheless be significantly different from zero but not of sufficient magnitude to satisfy investigators.

[ 11 ] : 66 Still, its standard error has been described [ 12 ] and is computed by various computer programs.

[ 13 ] Confidence intervals for Kappa may be constructed, for the expected Kappa values if we had infinite number of items checked, using the following formula: [ 1 ] C I : κ κ ± ± Z 1 − − α α / 2 S E κ κ {\displaystyle CI:\kappa \pm Z_{1-\alpha /2}SE_{\kappa }} Where Z 1 − − α α / 2 = 1.960 {\displaystyle Z_{1-\alpha /2}=1.960} is the standard normal percentile when α α = 5 % % {\displaystyle \alpha =5\%} , and S E κ κ {\displaystyle SE_{\kappa }} is calculated by jackknife , bootstrap or the asymptotic formula described by Fleiss & Cohen.

[ 12 ] Interpreting magnitude [ edit ] Kappa (vertical axis) and Accuracy (horizontal axis) calculated from the same simulated binary data. Each point on the graph is calculated from a pairs of judges randomly rating 10 subjects for having a diagnosis of X or not. Note in this example a Kappa=0 is approximately equivalent to an accuracy=0.5 If statistical significance is not a useful guide, what magnitude of kappa reflects adequate agreement? Guidelines would be helpful, but factors other than agreement can influence its magnitude, which makes interpretation of a given magnitude problematic. As Sim and Wright noted, two important factors are prevalence (are the codes equiprobable or do their probabilities vary) and bias (are the marginal probabilities for the two observers similar or different). Other things being equal, kappas are higher when codes are equiprobable. On the other hand, Kappas are higher when codes are distributed asymmetrically by the two observers. In contrast to probability variations, the effect of bias is greater when Kappa is small than when it is large.

[ 14 ] : 261–262 Another factor is the number of codes. As number of codes increases, kappas become higher. Based on a simulation study, Bakeman and colleagues concluded that for fallible observers, values for kappa were lower when codes were fewer. And, in agreement with Sim & Wrights's statement concerning prevalence, kappas were higher when codes were roughly equiprobable. Thus Bakeman et al. concluded that "no one value of kappa can be regarded as universally acceptable." [ 15 ] : 357 They also provide a computer program that lets users compute values for kappa specifying number of codes, their probability, and observer accuracy. For example, given equiprobable codes and observers who are 85% accurate, value of kappa are 0.49, 0.60, 0.66, and 0.69 when number of codes is 2, 3, 5, and 10, respectively.

Nonetheless, magnitude guidelines have appeared in the literature. Perhaps the first was Landis and Koch, [ 16 ] who characterized values < 0 as indicating no agreement and 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement. This set of guidelines is however by no means universally accepted; Landis and Koch supplied no evidence to support it, basing it instead on personal opinion. It has been noted that these guidelines may be more harmful than helpful.

[ 17 ] Fleiss's [ 18 ] : 218 equally arbitrary guidelines characterize kappas over 0.75 as excellent, 0.40 to 0.75 as fair to good, and below 0.40 as poor.

Kappa maximum [ edit ] Kappa assumes its theoretical maximum value of 1 only when both observers distribute codes the same, that is, when corresponding row and column sums are identical. Anything less is less than perfect agreement. Still, the maximum value kappa could achieve given unequal distributions helps interpret the value of kappa actually obtained. The equation for κ maximum is: [ 19 ] κ κ max = P max − − P exp 1 − − P exp {\displaystyle \kappa _{\max }={\frac {P_{\max }-P_{\exp }}{1-P_{\exp }}}} where P exp = ∑ ∑ i = 1 k P i + P + i {\displaystyle P_{\exp }=\sum _{i=1}^{k}P_{i+}P_{+i}} , as usual, P max = ∑ ∑ i = 1 k min ( P i + , P + i ) {\displaystyle P_{\max }=\sum _{i=1}^{k}\min(P_{i+},P_{+i})} , k = number of codes, P i + {\displaystyle P_{i+}} are the row probabilities, and P + i {\displaystyle P_{+i}} are the column probabilities.

Limitations [ edit ] Kappa is an index that considers observed agreement with respect to a baseline agreement. However, investigators must consider carefully whether Kappa's baseline agreement is relevant for the particular research question. Kappa's baseline is frequently described as the agreement due to chance, which is only partially correct. Kappa's baseline agreement is the agreement that would be expected due to random allocation, given the quantities specified by the marginal totals of square contingency table. Thus, κ = 0 when the observed allocation is apparently random, regardless of the quantity disagreement as constrained by the marginal totals. However, for many applications, investigators should be more interested in the quantity disagreement in the marginal totals than in the allocation disagreement as described by the additional information on the diagonal of the square contingency table. Thus for many applications, Kappa's baseline is more distracting than enlightening. Consider the following example: Kappa example Comparison 1 Reference G R Comparison G 1 14 R 0 1 The disagreement proportion is 14/16 or 0.875. The disagreement is due to quantity because allocation is optimal. κ is 0.01.

Comparison 2 Reference G R Comparison G 0 1 R 1 14 The disagreement proportion is 2/16 or 0.125. The disagreement is due to allocation because quantities are identical. Kappa is −0.07.

Here, reporting quantity and allocation disagreement is informative while Kappa obscures information. Furthermore, Kappa introduces some challenges in calculation and interpretation because Kappa is a ratio. It is possible for Kappa's ratio to return an undefined value due to zero in the denominator. Furthermore, a ratio does not reveal its numerator nor its denominator. It is more informative for researchers to report disagreement in two components, quantity and allocation. These two components describe the relationship between the categories more clearly than a single summary statistic. When predictive accuracy is the goal, researchers can more easily begin to think about ways to improve a prediction by using two components of quantity and allocation, rather than one ratio of Kappa.

[ 2 ] For a measure of difference between two continuous variables, see coefficient of determination .

Some researchers have expressed concern over κ's tendency to take the observed categories' frequencies as givens, which can make it unreliable for measuring agreement in situations such as the diagnosis of rare diseases. In these situations, κ tends to underestimate the agreement on the rare category.

[ 20 ] For this reason, κ is considered an overly conservative measure of agreement.

[ 21 ] Others [ 22 ] [ citation needed ] contest the assertion that kappa "takes into account" chance agreement. To do this effectively would require an explicit model of how chance affects rater decisions. The so-called chance adjustment of kappa statistics supposes that, when not completely certain, raters simply guess—a very unrealistic scenario. Moreover, some works [ 23 ] have shown how kappa statistics can lead to a wrong conclusion for unbalanced data.

Related statistics [ edit ] Scott's Pi [ edit ] A similar statistic, called pi , was proposed by Scott (1955). Cohen's kappa and Scott's pi differ in terms of how p e is calculated.

Fleiss' kappa [ edit ] Note that Cohen's kappa measures agreement between two raters only. For a similar measure of agreement ( Fleiss' kappa ) used when there are more than two raters, see Fleiss (1971). The Fleiss kappa, however, is a multi-rater generalization of Scott's pi statistic, not Cohen's kappa. Kappa is also used to compare performance in machine learning , but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.

[ 24 ] Weighted kappa [ edit ] The weighted kappa allows disagreements to be weighted differently [ 25 ] and is especially useful when codes are ordered.

[ 11 ] : 66 Three matrices are involved, the matrix of observed scores, the matrix of expected scores based on chance agreement, and the weight matrix. Weight matrix cells located on the diagonal (upper-left to bottom-right) represent agreement and thus contain zeros. Off-diagonal cells contain weights indicating the seriousness of that disagreement. Often, cells one off the diagonal are weighted 1, those two off 2, etc.

The equation for weighted κ is: κ κ = 1 − − ∑ ∑ i = 1 k ∑ ∑ j = 1 k w i j x i j ∑ ∑ i = 1 k ∑ ∑ j = 1 k w i j m i j {\displaystyle \kappa =1-{\frac {\sum _{i=1}^{k}\sum _{j=1}^{k}w_{ij}x_{ij}}{\sum _{i=1}^{k}\sum _{j=1}^{k}w_{ij}m_{ij}}}} where k =number of codes and w i j {\displaystyle w_{ij}} , x i j {\displaystyle x_{ij}} , and m i j {\displaystyle m_{ij}} are elements in the weight, observed, and expected matrices, respectively. When diagonal cells contain weights of 0 and all off-diagonal cells weights of 1, this formula produces the same value of kappa as the calculation given above.

See also [ edit ] Bangdiwala's B Intraclass correlation Krippendorff's alpha Statistical classification Further reading [ edit ] Banerjee, M.; Capozzoli, Michelle; McSweeney, Laura; Sinha, Debajyoti (1999).

"Beyond Kappa: A Review of Interrater Agreement Measures" .

The Canadian Journal of Statistics .

27 (1): 3– 23.

doi : 10.2307/3315487 .

JSTOR 3315487 .

S2CID 37082712 .

Chicco, D.; Warrens, M.J.; Jurman, G. (2021).

"The Matthews correlation coefficient (MCC) is more informative than Cohen's Kappa and Brier score in binary classification assessment" .

IEEE Access .

9 : 78368–81.

Bibcode : 2021IEEEA...978368C .

doi : 10.1109/access.2021.3084050 .

hdl : 10281/430460 .

S2CID 235308708 .

Cohen, Jacob (1960). "A coefficient of agreement for nominal scales".

Educational and Psychological Measurement .

20 (1): 37– 46.

doi : 10.1177/001316446002000104 .

hdl : 1942/28116 .

S2CID 15926286 .

Cohen, J. (1968). "Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit".

Psychological Bulletin .

70 (4): 213– 220.

doi : 10.1037/h0026256 .

PMID 19673146 .

Fleiss, J.L.; Cohen, J. (1973). "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability".

Educational and Psychological Measurement .

33 (3): 613– 9.

doi : 10.1177/001316447303300309 .

S2CID 145183399 .

Sim, J.; Wright, C.C. (2005). "The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements".

Physical Therapy .

85 (3): 257– 268.

doi : 10.1093/ptj/85.3.257 .

PMID 15733050 .

Warrens, J. (2011). "Cohen's kappa is a weighted average".

Statistical Methodology .

8 (6): 473– 484.

doi : 10.1016/j.stamet.2011.06.002 .

hdl : 1887/18062 .

External links [ edit ] Kappa, its meaning, problems, and several alternatives (Link is dead as of 2022-12-16) Kappa Statistics: Pros and Cons Software implementations Windows program "ComKappa" for kappa, weighted kappa, and kappa maximum (Error "Access Denied (Error Code 1020)" as of 2022-12-16) References [ edit ] ^ a b McHugh, Mary L. (2012).

"Interrater reliability: The kappa statistic" .

Biochemia Medica .

22 (3): 276– 282.

doi : 10.11613/bm.2012.031 .

PMC 3900052 .

PMID 23092060 .

^ a b Pontius, Robert; Millones, Marco (2011).

"Death to Kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment" .

International Journal of Remote Sensing .

32 (15): 4407– 4429.

Bibcode : 2011IJRS...32.4407P .

doi : 10.1080/01431161.2011.552923 .

S2CID 62883674 .

^ Galton, F. (1892) Finger Prints Macmillan, London.

^ Smeeton, N.C. (1985). "Early History of the Kappa Statistic".

Biometrics .

41 (3): 795.

JSTOR 2531300 .

^ Cohen, Jacob (1960). "A coefficient of agreement for nominal scales".

Educational and Psychological Measurement .

20 (1): 37– 46.

doi : 10.1177/001316446002000104 .

hdl : 1942/28116 .

S2CID 15926286 .

^ Sim, Julius; Wright, Chris C. (2005).

"The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements" .

Physical Therapy .

85 (3): 257– 268.

doi : 10.1093/ptj/85.3.257 .

ISSN 1538-6724 .

PMID 15733050 .

^ Chicco D.; Warrens M.J.; Jurman G. (June 2021).

"The Matthews correlation coefficient (MCC) is more informative than Cohen's Kappa and Brier score in binary classification assessment" .

IEEE Access .

9 : 78368 - 78381.

Bibcode : 2021IEEEA...978368C .

doi : 10.1109/ACCESS.2021.3084050 .

hdl : 10281/430460 .

^ Heidke, P. (1926-12-01). "Berechnung Des Erfolges Und Der Güte Der Windstärkevorhersagen Im Sturmwarnungsdienst".

Geografiska Annaler .

8 (4): 301– 349.

doi : 10.1080/20014422.1926.11881138 .

ISSN 2001-4422 .

^ Philosophical Society of Washington (Washington, D.C.) (1887).

Bulletin of the Philosophical Society of Washington . Vol. 10. Washington, D.C.: Published by the co-operation of the Smithsonian Institution. p. 83.

^ Kilem Gwet (May 2002).

"Inter-Rater Reliability: Dependency on Trait Prevalence and Marginal Homogeneity" (PDF) .

Statistical Methods for Inter-Rater Reliability Assessment .

2 : 1– 10. Archived from the original (PDF) on 2011-07-07 . Retrieved 2011-02-02 .

^ a b Bakeman, R.; Gottman, J.M. (1997).

Observing interaction: An introduction to sequential analysis (2nd ed.). Cambridge, UK: Cambridge University Press.

ISBN 978-0-521-27593-4 .

^ a b Fleiss, J.L.; Cohen, J.; Everitt, B.S. (1969). "Large sample standard errors of kappa and weighted kappa".

Psychological Bulletin .

72 (5): 323– 327.

doi : 10.1037/h0028106 .

^ Robinson, B.F; Bakeman, R. (1998).

"ComKappa: A Windows 95 program for calculating kappa and related statistics" .

Behavior Research Methods, Instruments, and Computers .

30 (4): 731– 732.

doi : 10.3758/BF03209495 .

^ Sim, J; Wright, C. C (2005).

"The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements" .

Physical Therapy .

85 (3): 257– 268.

doi : 10.1093/ptj/85.3.257 .

PMID 15733050 .

^ Bakeman, R.; Quera, V.; McArthur, D.; Robinson, B. F. (1997). "Detecting sequential patterns and determining their reliability with fallible observers".

Psychological Methods .

2 (4): 357– 370.

doi : 10.1037/1082-989X.2.4.357 .

^ Landis, J.R.; Koch, G.G. (1977).

"The measurement of observer agreement for categorical data" .

Biometrics .

33 (1): 159– 174.

doi : 10.2307/2529310 .

JSTOR 2529310 .

PMID 843571 .

S2CID 11077516 .

^ Gwet, K. (2010). " Handbook of Inter-Rater Reliability (Second Edition) " ISBN 978-0-9708062-2-2 [ page needed ] ^ Fleiss, J.L. (1981).

Statistical methods for rates and proportions (2nd ed.). New York: John Wiley.

ISBN 978-0-471-26370-8 .

^ Umesh, U. N.; Peterson, R.A.; Sauber M. H. (1989). "Interjudge agreement and the maximum value of kappa".

Educational and Psychological Measurement .

49 (4): 835– 850.

doi : 10.1177/001316448904900407 .

S2CID 123306239 .

^ Viera, Anthony J.; Garrett, Joanne M. (2005). "Understanding interobserver agreement: the kappa statistic".

Family Medicine .

37 (5): 360– 363.

PMID 15883903 .

^ Strijbos, J.; Martens, R.; Prins, F.; Jochems, W. (2006). "Content analysis: What are they talking about?".

Computers & Education .

46 : 29– 48.

CiteSeerX 10.1.1.397.5780 .

doi : 10.1016/j.compedu.2005.04.002 .

S2CID 14183447 .

^ Uebersax, JS. (1987).

"Diversity of decision-making models and the measurement of interrater agreement" (PDF) .

Psychological Bulletin .

101 : 140– 146.

CiteSeerX 10.1.1.498.4965 .

doi : 10.1037/0033-2909.101.1.140 .

S2CID 39240770 . Archived from the original (PDF) on 2016-03-03 . Retrieved 2010-10-16 .

^ Delgado, Rosario; Tibau, Xavier-Andoni (2019-09-26).

"Why Cohen's Kappa should be avoided as performance measure in classification" .

PLOS ONE .

14 (9): e0222916.

Bibcode : 2019PLoSO..1422916D .

doi : 10.1371/journal.pone.0222916 .

ISSN 1932-6203 .

PMC 6762152 .

PMID 31557204 .

^ Powers, David M. W. (2012).

"The Problem with Kappa" (PDF) .

Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop . Archived from the original (PDF) on 2016-05-18 . Retrieved 2012-07-20 .

^ Cohen, J. (1968). "Weighed kappa: Nominal scale agreement with provision for scaled disagreement or partial credit".

Psychological Bulletin .

70 (4): 213– 220.

doi : 10.1037/h0026256 .

PMID 19673146 .

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Machine learning evaluation metrics Regression MSE MAE sMAPE MAPE MASE MSPE RMS RMSE/RMSD R 2 MDA MAD Classification F-score P4 Accuracy Precision Recall Kappa MCC AUC ROC Sensitivity and specificity Logarithmic loss Clustering Silhouette Calinski–Harabasz index Davies–Bouldin index Dunn index Hopkins statistic Jaccard index Rand index Similarity measure SMC DBCV index Ranking MRR NDCG AP Computer vision PSNR SSIM IoU NLP Perplexity BLEU Deep learning Inception score FID Recommender system Coverage Intra-list similarity Similarity Cosine similarity Euclidean distance Pearson correlation coefficient Confusion matrix Authority control databases : National Germany Retrieved from " https://en.wikipedia.org/w/index.php?title=Cohen%27s_kappa&oldid=1302489981 " Categories : Categorical variable interactions Nonparametric statistics Inter-rater reliability Summary statistics for contingency tables Hidden categories: Wikipedia articles needing page number citations from April 2012 Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from April 2012 Pages that use a deprecated format of the math tags This page was last edited on 25 July 2025, at 18:54 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Cohen's kappa 17 languages Add topic

