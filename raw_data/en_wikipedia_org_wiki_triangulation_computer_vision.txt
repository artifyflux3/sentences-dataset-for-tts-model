Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Introduction 2 Properties Toggle Properties subsection 2.1 Singularities 2.2 Invariance 2.3 Computational complexity 3 Methods Toggle Methods subsection 3.1 Mid-point method 3.2 Direct linear transformation 3.3 Via the essential matrix 4 See also 5 References 6 External links Toggle the table of contents Triangulation (computer vision) 5 languages Español 한국어 Polski Português Українська Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Method of determining a point in 3D space For broader coverage of this topic, see Computer stereo vision .

For other uses, see Triangulation (disambiguation) .

In computer vision , triangulation refers to the process of determining a point in 3D space given its projections onto two, or more, images.  In order to solve this problem it is necessary to know the parameters of the camera projection function from 3D to 2D for the cameras involved, in the simplest case represented by the camera matrices .  Triangulation is sometimes also referred to as reconstruction or intersection .

The triangulation problem is in principle trivial. Since each point in an image corresponds to a line in 3D space, all points on the line in 3D are projected to the point in the image.  If a pair of corresponding points in two, or more images, can be found it must be the case that they are the projection of a common 3D point x .  The set of lines generated by the image points must intersect at x (3D point) and the algebraic formulation of the coordinates of x (3D point) can be computed in a variety of ways, as is presented below.

In practice, however, the coordinates of image points cannot be measured with arbitrary accuracy.  Instead, various types of noise, such as geometric noise from lens distortion or interest point detection error, lead to inaccuracies in the measured image coordinates.  As a consequence, the lines generated by the corresponding image points do not always intersect in 3D space.  The problem, then, is to find a 3D point which optimally fits the measured image points.  In the literature there are multiple proposals for how to define optimality and how to find the optimal 3D point.  Since they are based on different optimality criteria, the various methods produce different estimates of the 3D point x when noise is involved.

Introduction [ edit ] In the following, it is assumed that triangulation is made on corresponding image points from two views generated by pinhole cameras .

The ideal case of epipolar geometry. A 3D point x is projected onto two camera images through lines (green) which intersect with each camera's focal point, O 1 and O 2 . The resulting image points are y 1 and y 2 . The green lines intersect at x .

In practice, the image points y 1 and y 2 cannot be measured with arbitrary accuracy. Instead points y' 1 and y' 2 are detected and used for the triangulation. The corresponding projection lines (blue) do not, in general, intersect in 3D space and may also not intersect with point x .

The image to the left illustrates the epipolar geometry of a pair of stereo cameras of pinhole model .  A point x (3D point) in 3D space is projected onto the respective image plane along a line (green) which goes through the camera's focal point , O 1 {\displaystyle \mathbf {O} _{1}} and O 2 {\displaystyle \mathbf {O} _{2}} , resulting in the two corresponding image points y 1 {\displaystyle \mathbf {y} _{1}} and y 2 {\displaystyle \mathbf {y} _{2}} .  If y 1 {\displaystyle \mathbf {y} _{1}} and y 2 {\displaystyle \mathbf {y} _{2}} are given and the geometry of the two cameras are known, the two projection lines (green lines) can be determined and it must be the case that they intersect at point x (3D point). Using basic linear algebra that intersection point can be determined in a straightforward way.

The image to the right shows the real case.  The position of the image points y 1 {\displaystyle \mathbf {y} _{1}} and y 2 {\displaystyle \mathbf {y} _{2}} cannot be measured exactly.  The reason is a combination of factors such as Geometric distortion, for example lens distortion , which means that the 3D to 2D mapping of the camera deviates from the pinhole camera model .  To some extent these errors can be compensated for, leaving a residual geometric error.

A single ray of light from x (3D point) is dispersed in the lens system of the cameras according to a point spread function .  The recovery of the corresponding image point from measurements of the dispersed intensity function in the images gives errors.

In a digital camera, the image intensity function is only measured in discrete sensor elements.  Inexact interpolation of the discrete intensity function have to be used to recover the true one.

The image points y 1 ' and y 2 ' used for triangulation are often found using various types of feature extractors, for example of corners or interest points in general.  There is an inherent localization error for any type of feature extraction based on neighborhood operations .

As a consequence, the measured image points are y 1 ′ {\displaystyle \mathbf {y} '_{1}} and y 2 ′ {\displaystyle \mathbf {y} '_{2}} instead of y 1 {\displaystyle \mathbf {y} _{1}} and y 2 {\displaystyle \mathbf {y} _{2}} .  However, their projection lines (blue) do not have to intersect in 3D space or come close to x .  In fact, these lines intersect if and only if y 1 ′ {\displaystyle \mathbf {y} '_{1}} and y 2 ′ {\displaystyle \mathbf {y} '_{2}} satisfy the epipolar constraint defined by the fundamental matrix .  Given the measurement noise in y 1 ′ {\displaystyle \mathbf {y} '_{1}} and y 2 ′ {\displaystyle \mathbf {y} '_{2}} it is rather likely that the epipolar constraint is not satisfied and the projection lines do not intersect.

This observation leads to the problem which is solved in triangulation.  Which 3D point x est is the best estimate of x given y 1 ′ {\displaystyle \mathbf {y} '_{1}} and y 2 ′ {\displaystyle \mathbf {y} '_{2}} and the geometry of the cameras?  The answer is often found by defining an error measure which depends on x est and then minimizing this error.  In the following sections, some of the various methods for computing x est presented in the literature are briefly described.

All triangulation methods produce x est = x in the case that y 1 = y 1 ′ {\displaystyle \mathbf {y} _{1}=\mathbf {y} '_{1}} and y 2 = y 2 ′ {\displaystyle \mathbf {y} _{2}=\mathbf {y} '_{2}} , that is, when the epipolar constraint is satisfied (except for singular points, see below).  It is what happens when the constraint is not satisfied which differs between the methods.

Properties [ edit ] A triangulation method can be described in terms of a function τ τ {\displaystyle \tau \,} such that x ∼ ∼ τ τ ( y 1 ′ , y 2 ′ , C 1 , C 2 ) {\displaystyle \mathbf {x} \sim \tau (\mathbf {y} '_{1},\mathbf {y} '_{2},\mathbf {C} _{1},\mathbf {C} _{2})} where y 1 ′ , y 2 ′ {\displaystyle \mathbf {y} '_{1},\mathbf {y} '_{2}} are the homogeneous coordinates of the detected image points and C 1 , C 2 {\displaystyle \mathbf {C} _{1},\mathbf {C} _{2}} are the camera matrices.

x (3D point) is the homogeneous representation of the resulting 3D point.  The ∼ ∼ {\displaystyle \sim \,} sign implies that τ τ {\displaystyle \tau \,} is only required to produce a vector which is equal to x up to a multiplication by a non-zero scalar since homogeneous vectors are involved.

Before looking at the specific methods, that is, specific functions τ τ {\displaystyle \tau \,} , there are some general concepts related to the methods that need to be explained.  Which triangulation method is chosen for a particular problem depends to some extent on these characteristics.

Singularities [ edit ] Some of the methods fail to correctly compute an estimate of x (3D point) if it lies in a certain subset of the 3D space, corresponding to some combination of y 1 ′ , y 2 ′ , C 1 , C 2 {\displaystyle \mathbf {y} '_{1},\mathbf {y} '_{2},\mathbf {C} _{1},\mathbf {C} _{2}} .  A point in this subset is then a singularity of the triangulation method.  The reason for the failure can be that some equation system to be solved is under-determined or that the projective representation of x est becomes the zero vector for the singular points.

Invariance [ edit ] In some applications, it is desirable that the triangulation is independent of the coordinate system used to represent 3D points; if the triangulation problem is formulated in one coordinate system and then transformed into another the resulting estimate x est should transform in the same way.  This property is commonly referred to as invariance .  Not every triangulation method assures invariance, at least not for general types of coordinate transformations.

For a homogeneous representation of 3D coordinates, the most general transformation is a projective transformation, represented by a 4 × × 4 {\displaystyle 4\times 4} matrix T {\displaystyle \mathbf {T} } .  If the homogeneous coordinates are transformed according to x ¯ ¯ ∼ ∼ T x {\displaystyle \mathbf {\bar {x}} \sim \mathbf {T} \,\mathbf {x} } then the camera matrices must transform as ( C k ) C ¯ ¯ k ∼ ∼ C k T − − 1 {\displaystyle \mathbf {\bar {C}} _{k}\sim \mathbf {C} _{k}\,\mathbf {T} ^{-1}} to produce the same homogeneous image coordinates ( y k ) y k ∼ ∼ C ¯ ¯ k x ¯ ¯ = C k x {\displaystyle \mathbf {y} _{k}\sim \mathbf {\bar {C}} _{k}\,\mathbf {\bar {x}} =\mathbf {C} _{k}\,\mathbf {x} } If the triangulation function τ τ {\displaystyle \tau } is invariant to T {\displaystyle \mathbf {T} } then the following relation must be valid x ¯ ¯ e s t ∼ ∼ T x e s t {\displaystyle \mathbf {\bar {x}} _{\rm {est}}\sim \mathbf {T} \,\mathbf {x} _{\rm {est}}} from which follows that τ τ ( y 1 ′ , y 2 ′ , C 1 , C 2 ) ∼ ∼ T − − 1 τ τ ( y 1 ′ , y 2 ′ , C 1 T − − 1 , C 2 T − − 1 ) , {\displaystyle \tau (\mathbf {y} '_{1},\mathbf {y} '_{2},\mathbf {C} _{1},\mathbf {C} _{2})\sim \mathbf {T} ^{-1}\,\tau (\mathbf {y} '_{1},\mathbf {y} '_{2},\mathbf {C} _{1}\,\mathbf {T} ^{-1},\mathbf {C} _{2}\,\mathbf {T} ^{-1}),} for all y 1 ′ , y 2 ′ {\displaystyle \mathbf {y} '_{1},\mathbf {y} '_{2}} For each triangulation method, it can be determined if this last relation is valid.  If it is, it may be satisfied only for a subset of the projective transformations, for example, rigid or affine transformations.

Computational complexity [ edit ] The function τ τ {\displaystyle \tau } is only an abstract representation of a computation which, in practice, may be relatively complex.  Some methods result in a τ τ {\displaystyle \tau } which is a closed-form continuous function while others need to be decomposed into a series of computational steps involving, for example, SVD or finding the roots of a polynomial.  Yet another class of methods results in τ τ {\displaystyle \tau } which must rely on iterative estimation of some parameters.  This means that both the computation time and the complexity of the operations involved may vary between the different methods.

Methods [ edit ] Mid-point method [ edit ] Further information: Skew lines § Nearest points Each of the two image points y 1 ′ {\displaystyle \mathbf {y} '_{1}} and y 2 ′ {\displaystyle \mathbf {y} '_{2}} has a corresponding projection line (blue in the right image above), here denoted as L 1 ′ {\displaystyle \mathbf {L} '_{1}} and L 2 ′ {\displaystyle \mathbf {L} '_{2}} , which can be determined given the camera matrices C 1 , C 2 {\displaystyle \mathbf {C} _{1},\mathbf {C} _{2}} .  Let d {\displaystyle d\,} be a distance function between a (3D line) L and a x (3D point) such that d ( L , x ) {\displaystyle d(\mathbf {L} ,\mathbf {x} )} is the Euclidean distance between L {\displaystyle \mathbf {L} } and x {\displaystyle \mathbf {x} } .
The midpoint method finds the point x est which minimizes d ( L 1 ′ , x ) 2 + d ( L 2 ′ , x ) 2 {\displaystyle d(\mathbf {L} '_{1},\mathbf {x} )^{2}+d(\mathbf {L} '_{2},\mathbf {x} )^{2}} It turns out that x est lies exactly at the middle of the shortest line segment which joins the two projection lines.

Direct linear transformation [ edit ] Main article: Direct linear transformation Via the essential matrix [ edit ] The problem to be solved there is how to compute ( x 1 , x 2 , x 3 ) {\displaystyle (x_{1},x_{2},x_{3})} given corresponding normalized image coordinates ( y 1 , y 2 ) {\displaystyle (y_{1},y_{2})} and ( y 1 ′ , y 2 ′ ) {\displaystyle (y'_{1},y'_{2})} . If the essential matrix is known and the corresponding rotation and translation transformations have been determined, this algorithm (described in Longuet-Higgins' paper) provides a solution.

Let r k {\displaystyle \mathbf {r} _{k}} denote row k of the rotation matrix R {\displaystyle \mathbf {R} } : R = ( − − r 1 − − − − r 2 − − − − r 3 − − ) {\displaystyle \mathbf {R} ={\begin{pmatrix}-\mathbf {r} _{1}-\\-\mathbf {r} _{2}-\\-\mathbf {r} _{3}-\end{pmatrix}}} Combining the above relations between 3D coordinates in the two coordinate systems and the mapping between 3D and 2D points described earlier gives y 1 ′ = x 1 ′ x 3 ′ = r 1 ⋅ ⋅ ( x ~ ~ − − t ) r 3 ⋅ ⋅ ( x ~ ~ − − t ) = r 1 ⋅ ⋅ ( y − − t / x 3 ) r 3 ⋅ ⋅ ( y − − t / x 3 ) {\displaystyle y'_{1}={\frac {x'_{1}}{x'_{3}}}={\frac {\mathbf {r} _{1}\cdot ({\tilde {\mathbf {x} }}-\mathbf {t} )}{\mathbf {r} _{3}\cdot ({\tilde {\mathbf {x} }}-\mathbf {t} )}}={\frac {\mathbf {r} _{1}\cdot (\mathbf {y} -\mathbf {t} /x_{3})}{\mathbf {r} _{3}\cdot (\mathbf {y} -\mathbf {t} /x_{3})}}} or x 3 = ( r 1 − − y 1 ′ r 3 ) ⋅ ⋅ t ( r 1 − − y 1 ′ r 3 ) ⋅ ⋅ y {\displaystyle x_{3}={\frac {(\mathbf {r} _{1}-y'_{1}\,\mathbf {r} _{3})\cdot \mathbf {t} }{(\mathbf {r} _{1}-y'_{1}\,\mathbf {r} _{3})\cdot \mathbf {y} }}} Once x 3 {\displaystyle x_{3}} is determined, the other two coordinates can be computed as ( x 1 x 2 ) = x 3 ( y 1 y 2 ) {\displaystyle {\begin{pmatrix}x_{1}\\x_{2}\end{pmatrix}}=x_{3}{\begin{pmatrix}y_{1}\\y_{2}\end{pmatrix}}} The above derivation is not unique. It is also possible to start with an expression for y 2 ′ {\displaystyle y'_{2}} and derive an expression for x 3 {\displaystyle x_{3}} according to x 3 = ( r 2 − − y 2 ′ r 3 ) ⋅ ⋅ t ( r 2 − − y 2 ′ r 3 ) ⋅ ⋅ y {\displaystyle x_{3}={\frac {(\mathbf {r} _{2}-y'_{2}\,\mathbf {r} _{3})\cdot \mathbf {t} }{(\mathbf {r} _{2}-y'_{2}\,\mathbf {r} _{3})\cdot \mathbf {y} }}} In the ideal case, when the camera maps the 3D points according to a perfect pinhole camera and the resulting 2D points can be detected without any noise, the two expressions for x 3 {\displaystyle x_{3}} are equal. In practice, however, they are not and it may be advantageous to combine the two estimates of x 3 {\displaystyle x_{3}} , for example, in terms of some sort of average.

There are also other types of extensions of the above computations which are possible.  They started with an expression of the primed image coordinates and derived 3D coordinates in the unprimed system.  It is also possible to start with unprimed image coordinates and obtain primed 3D coordinates, which finally can be transformed into unprimed 3D coordinates.  Again, in the ideal case the result should be equal to the above expressions, but in practice they may deviate.

A final remark relates to the fact that if the essential matrix is determined from corresponding image coordinate, which often is the case when 3D points are determined in this way, the translation vector t {\displaystyle \mathbf {t} } is known only up to an unknown positive scaling. As a consequence, the reconstructed 3D points, too, are undetermined with respect to a positive scaling.

See also [ edit ] 3D reconstruction from multiple images Bundle adjustment Line–line intersection References [ edit ] Richard Hartley and Andrew Zisserman (2003).

Multiple View Geometry in computer vision . Cambridge University Press.

ISBN 978-0-521-54051-3 .

External links [ edit ] Two view and multi-view triangulation in Matlab NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐z6lxp
Cached time: 20250812024534
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.191 seconds
Real time usage: 0.321 seconds
Preprocessor visited node count: 607/1000000
Revision size: 15404/2097152 bytes
Post‐expand include size: 3828/2097152 bytes
Template argument size: 481/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 5/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 6212/5000000 bytes
Lua time usage: 0.099/10.000 seconds
Lua memory usage: 3441232/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  174.556      1 -total
 46.64%   81.412      1 Template:Short_description
 34.24%   59.774      1 Template:Cite_book
 29.27%   51.100      2 Template:Pagetype
 11.01%   19.227      1 Template:Broader
 10.95%   19.115      2 Template:Main_other
  9.78%   17.080      1 Template:SDcat
  2.92%    5.094      1 Template:Other_uses
  2.73%    4.759      1 Template:Further
  1.45%    2.535      1 Template:Main Saved in parser cache with key enwiki:pcache:12891058:|#|:idhash:canonical and timestamp 20250812024534 and revision id 1241190657. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Triangulation_(computer_vision)&oldid=1241190657 " Categories : Geometry in computer vision Stereophotogrammetry Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 19 August 2024, at 20:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Triangulation (computer vision) 5 languages Add topic

