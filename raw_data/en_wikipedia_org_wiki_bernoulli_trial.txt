Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Preliminary 2 Examples Toggle Examples subsection 2.1 Tossing coins 2.1.1 Solution 2.2 Rolling dice 2.2.1 Solution 3 See also 4 References 5 External links Toggle the table of contents Bernoulli trial 24 languages বাংলা Català Čeština Español Esperanto Euskara فارسی Français 한국어 Magyar Македонски Монгол Nederlands 日本語 Norsk bokmål Piemontèis Polski Português Slovenščina Suomi Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Any experiment with two possible random outcomes Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e Graphs of probability P of not observing independent events each of probability p after n Bernoulli trials vs np for various p . Three examples are shown: Blue curve : Throwing a 6-sided die 6 times gives a 33.5% chance that 6 (or any other given number) never turns up; it can be observed that as n increases, the probability of a 1/ n -chance event never appearing after n tries rapidly converges to 1/e .

Grey curve : To get 50-50 chance of throwing a Yahtzee (5 cubic dice all showing the same number) requires 0.69 × 1296 ~ 898 throws.

Green curve : Drawing a card from a deck of playing cards without jokers 100 (1.92 × 52) times with replacement gives 85.7% chance of drawing the ace of spades at least once.

In the theory of probability and statistics , a Bernoulli trial (or binomial trial ) is a random experiment with exactly two possible outcomes , "success" and "failure", in which the probability of success is the same every time the experiment is conducted.

[ 1 ] It is named after Jacob Bernoulli , a 17th-century Swiss mathematician, who analyzed them in his Ars Conjectandi (1713).

[ 2 ] The mathematical formalization and advanced formulation of the Bernoulli trial is known as the Bernoulli process .

Since a Bernoulli trial has only two possible outcomes, it can be framed as a "yes or no" question. For example: Is the top card of a shuffled deck an ace?

Was the newborn child a girl? (See human sex ratio .) Success and failure are in this context labels for the two outcomes, and should not be construed literally or as value judgments. More generally, given any probability space , for any event (set of outcomes), one can define a Bernoulli trial according to whether the event occurred or not (event or complementary event ). Examples of Bernoulli trials include: Flipping a coin . In this context, obverse ("heads") conventionally denotes success and reverse ("tails") denotes failure. A fair coin has the probability of success 0.5 by definition. In this case, there are exactly two possible outcomes.

Rolling a die, where a six is "success" and everything else a "failure". In this case, there are six possible outcomes, and the event is a six; the complementary event "not a six" corresponds to the other five possible outcomes.

In conducting a political opinion poll , choosing a voter at random to ascertain whether that voter will vote "yes" in an upcoming referendum.

Preliminary [ edit ] Suppose there exists an experiment consisting of independently repeated trials, each of which has only two possible outcomes; called experimental Bernoulli trials. The collection of n {\displaystyle n} experimental realizations of success (1) and failure (0) will be defined by a Bernoulli random variable: b X r | ==> x : b X r == f ( b X r = x ) :: [ x = 1 , x = 0 ; ; ( p , p − − 1 ) ] {\displaystyle bX_{r}|==>{x:bX_{r}==f(bX_{r}=x)::[x=1,x=0;;(p,p-1)]}} | p = t o t a l 1 / n {\displaystyle p=total_{1}/n} Let p {\displaystyle p} be the probability of success in a Bernoulli trial, and q {\displaystyle q} be the probability of failure. Then the probability of success and the probability of failure sum to one, since these are complementary events: "success" and "failure" are mutually exclusive and exhaustive . Thus, one has the following relations: p = 1 − − q , q = 1 − − p , p + q = 1.

{\displaystyle p=1-q,\quad \quad q=1-p,\quad \quad p+q=1.} Alternatively, these can be stated in terms of odds : given probability p {\displaystyle p} of success and q {\displaystyle q} of failure, the odds for are p : q {\displaystyle p:q} and the odds against are q : p .

{\displaystyle q:p.} These can also be expressed as numbers, by dividing, yielding the odds for, o f {\displaystyle o_{f}} , and the odds against, o a {\displaystyle o_{a}} : o f = p / q = p / ( 1 − − p ) = ( 1 − − q ) / q o a = q / p = ( 1 − − p ) / p = q / ( 1 − − q ) .

{\displaystyle {\begin{aligned}o_{f}&=p/q=p/(1-p)=(1-q)/q\\o_{a}&=q/p=(1-p)/p=q/(1-q).\end{aligned}}} These are multiplicative inverses , so they multiply to 1, with the following relations: o f = 1 / o a , o a = 1 / o f , o f ⋅ ⋅ o a = 1.

{\displaystyle o_{f}=1/o_{a},\quad o_{a}=1/o_{f},\quad o_{f}\cdot o_{a}=1.} In the case that a Bernoulli trial is representing an event from finitely many equally likely outcomes , where S {\displaystyle S} of the outcomes are success and F {\displaystyle F} of the outcomes are failure, the odds for are S : F {\displaystyle S:F} and the odds against are F : S .

{\displaystyle F:S.} This yields the following formulas for probability and odds: p = S / ( S + F ) q = F / ( S + F ) o f = S / F o a = F / S .

{\displaystyle {\begin{aligned}p&=S/(S+F)\\q&=F/(S+F)\\o_{f}&=S/F\\o_{a}&=F/S.\end{aligned}}} Here the odds are computed by dividing the number of outcomes, not the probabilities, but the proportion is the same, since these ratios only differ by multiplying both terms by the same constant factor.

Random variables describing Bernoulli trials are often encoded using the convention that 1 = "success", 0 = "failure".

Closely related to a Bernoulli trial is a binomial experiment, which consists of a fixed number n {\displaystyle n} of statistically independent Bernoulli trials, each with a probability of success p {\displaystyle p} , and counts the number of successes.  A random variable corresponding to a binomial experiment is denoted by B ( n , p ) {\displaystyle B(n,p)} , and is said to have a binomial distribution .
The probability of exactly k {\displaystyle k} successes in the experiment B ( n , p ) {\displaystyle B(n,p)} is given by: P ( k ) = ( n k ) p k q n − − k {\displaystyle P(k)={n \choose k}p^{k}q^{n-k}} where ( n k ) {\displaystyle {n \choose k}} is a binomial coefficient .

Bernoulli trials may also lead to negative binomial distributions (which count the number of successes in a series of repeated Bernoulli trials until a specified number of failures are seen), as well as various other distributions.

When multiple Bernoulli trials are performed, each with its own probability of success, these are sometimes referred to as Poisson trials .

[ 3 ] Examples [ edit ] Tossing coins [ edit ] Consider the simple experiment where a fair coin is tossed four times. Find the probability that exactly two of the tosses result in heads.

Solution [ edit ] A representation of the possible outcomes of flipping a fair coin four times in terms of the number of heads. As can be seen, the probability of getting exactly two heads in four flips is 6/16 = 3/8, which matches the calculations.

For this experiment, let a heads be defined as a success and a tails as a failure.

Because the coin is assumed to be fair, the probability of success is p = 1 2 {\displaystyle p={\tfrac {1}{2}}} . Thus, the probability of failure, q {\displaystyle q} , is given by q = 1 − − p = 1 − − 1 2 = 1 2 {\displaystyle q=1-p=1-{\tfrac {1}{2}}={\tfrac {1}{2}}} .

Using the equation above, the probability of exactly two tosses out of four total tosses resulting in a heads is given by: P ( 2 ) = ( 4 2 ) p 2 q 4 − − 2 = 6 × × ( 1 2 ) 2 × × ( 1 2 ) 2 = 3 8 .

{\displaystyle {\begin{aligned}P(2)&={4 \choose 2}p^{2}q^{4-2}\\&=6\times \left({\tfrac {1}{2}}\right)^{2}\times \left({\tfrac {1}{2}}\right)^{2}\\&={\dfrac {3}{8}}.\end{aligned}}} Rolling dice [ edit ] What is probability that when three independent fair six-sided dice are rolled, exactly two yield sixes?

Solution [ edit ] Probabilities of rolling k sixes from n independent fair dice, with crossed out dice denoting non-six rolls – 2 sixes out of 3 dice is circled On one die, the probability of rolling a six, p = 1 6 {\displaystyle p={\tfrac {1}{6}}} . Thus, the probability of not rolling a six, q = 1 − − p = 5 6 {\displaystyle q=1-p={\tfrac {5}{6}}} .

As above, the probability of exactly two sixes out of three, P ( 2 ) = ( 3 2 ) p 2 q 3 − − 2 = 3 × × ( 1 6 ) 2 × × ( 5 6 ) 1 = 5 72 ≈ ≈ 0.069.

{\displaystyle {\begin{aligned}P(2)&={3 \choose 2}p^{2}q^{3-2}\\&=3\times \left({\tfrac {1}{6}}\right)^{2}\times \left({\tfrac {5}{6}}\right)^{1}\\&={\dfrac {5}{72}}\approx 0.069.\end{aligned}}} See also [ edit ] Bernoulli scheme Bernoulli sampling Bernoulli distribution Binomial distribution Binomial coefficient Binomial proportion confidence interval Poisson sampling Sampling design Coin flipping Jacob Bernoulli Fisher's exact test Boschloo's test References [ edit ] ^ Papoulis, A. (1984). "Bernoulli Trials".

Probability, Random Variables, and Stochastic Processes (2nd ed.). New York: McGraw-Hill . pp.

57– 63.

^ James Victor Uspensky: Introduction to Mathematical Probability , McGraw-Hill, New York 1937, page 45 ^ Rajeev Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press, New York (NY), 1995, p.67-68 External links [ edit ] Wikimedia Commons has media related to Bernoulli trial .

"Bernoulli trials" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] "Simulation of n Bernoulli trials" . math.uah.edu . Retrieved 2025-03-16 .

Retrieved from " https://en.wikipedia.org/w/index.php?title=Bernoulli_trial&oldid=1280820336 " Categories : Discrete distributions Coin flipping Experiment (probability theory) Hidden categories: Articles with short description Short description matches Wikidata Articles containing Latin-language text Commons category link from Wikidata This page was last edited on 16 March 2025, at 17:20 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Bernoulli trial 24 languages Add topic

