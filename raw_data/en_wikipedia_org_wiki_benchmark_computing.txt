Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Purpose 2 Functionality 3 Challenges 4 Benchmarking principles 5 Types of benchmark 6 Common benchmarks Toggle Common benchmarks subsection 6.1 Industry standard (audited and verifiable) 6.2 Open source benchmarks 6.3 Microsoft Windows benchmarks 6.4 Unusual benchmark 6.5 Others 7 See also 8 References 9 Further reading 10 External links Toggle the table of contents Benchmark (computing) 32 languages العربية Azərbaycanca Bosanski Català Dansk Deutsch Eesti Español Euskara فارسی Français Galego 한국어 Bahasa Indonesia Íslenska Italiano עברית Қазақша Nederlands 日本語 Polski Português Русский Simple English Српски / srpski Srpskohrvatski / српскохрватски ไทย Türkçe Українська اردو Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Standardized performance evaluation This article is about the use of benchmarks in computing. For other uses, see Benchmark .

This article has multiple issues.

Please help improve it or discuss these issues on the talk page .

( Learn how and when to remove these messages ) This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Benchmark" computing – news · newspapers · books · scholar · JSTOR ( July 2015 ) ( Learn how and when to remove this message ) This article needs attention from an expert in computer science . The specific problem is: Outdated or deprecated sources.

See the talk page for details.

WikiProject Computer science may be able to help recruit an expert.

( October 2022 ) ( Learn how and when to remove this message ) A graphical demo running as a benchmark of the OGRE engine In computing , a benchmark is the act of running a computer program , a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it.

[ 1 ] The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.

Benchmarking is usually associated with assessing performance characteristics of computer hardware , for example, the floating point operation performance of a CPU , but there are circumstances when the technique is also applicable to software . Software benchmarks are, for example, run against compilers or database management systems (DBMS).

Benchmarks provide a method of comparing the performance of various subsystems across different chip/system architectures . Benchmarking as a part of continuous integration is called Continuous Benchmarking.

[ 2 ] Purpose [ edit ] As computer architecture advanced, it became more difficult to compare the performance of various computer systems simply by looking at their specifications. Therefore, tests were developed that allowed comparison of different architectures. For example, Pentium 4 processors generally operated at a higher clock frequency than Athlon XP or PowerPC processors, which did not necessarily translate to more computational power; a processor with a slower clock frequency might perform as well as or even better than a processor operating at a higher frequency. See BogoMips and the megahertz myth .

Benchmarks are designed to mimic a particular type of workload on a component or system. Synthetic benchmarks do this by specially created programs that impose the workload on the component. Application benchmarks run real-world programs on the system. While application benchmarks usually give a much better measure of real-world performance on a given system, synthetic benchmarks are useful for testing individual components, like a hard disk or networking device.

Benchmarks are particularly important in CPU design , giving processor architects the ability to measure and make tradeoffs in microarchitectural decisions. For example, if a benchmark extracts the key algorithms of an application, it will contain the performance-sensitive aspects of that application. Running this much smaller snippet on a cycle-accurate simulator can give clues on how to improve performance.

Prior to 2000, computer and microprocessor architects used SPEC to do this, although SPEC's Unix-based benchmarks were quite lengthy and thus unwieldy to use intact.

Computer companies are known to configure their systems to give unrealistically high performance on benchmark tests that are not replicated in real usage. For instance, during the 1980s some compilers could detect a specific mathematical operation used in a well-known floating-point benchmark and replace the operation with a faster mathematically equivalent operation. However, such a transformation was rarely useful outside the benchmark until the mid-1990s, when RISC and VLIW architectures emphasized the importance of compiler technology as it related to performance. Benchmarks are now regularly used by compiler companies to improve not only their own benchmark scores, but real application performance.

CPUs that have many execution units — such as a superscalar CPU, a VLIW CPU, or a reconfigurable computing CPU — typically have slower clock rates than a sequential CPU with one or two execution units when built from transistors that are just as fast. Nevertheless, CPUs with many execution units often complete real-world and benchmark tasks in less time than the supposedly faster high-clock-rate CPU.

Given the large number of benchmarks available, a vendor can usually find at least one benchmark that shows its system will outperform another system; the other systems can be shown to excel with a different benchmark.

Software vendors also use benchmarks in their marketing, such as the "benchmark wars" between rival relational database makers in the 1980s and 1990s. Companies commonly report only those benchmarks (or aspects of benchmarks) that show their products in the best light. They also have been known to mis-represent the significance of benchmarks, again to show their products in the best possible light.

[ 3 ] [ 4 ] Ideally benchmarks should only substitute for real applications if the application is unavailable, or too difficult or costly to port to a specific processor or computer system. If performance is critical, the only benchmark that matters is the target environment's application suite.

Functionality [ edit ] Features of benchmarking software may include recording/ exporting the course of performance to a spreadsheet file, visualization such as drawing line graphs or color-coded tiles, and pausing the process to be able to resume without having to start over. Software can have additional features specific to its purpose, for example, disk benchmarking software may be able to optionally start measuring the disk speed within a specified range of the disk rather than the full disk, measure random access reading speed and latency , have a "quick scan" feature which measures the speed through samples of specified intervals and sizes, and allow specifying a data block size, meaning the number of requested bytes per read request.

[ 5 ] Challenges [ edit ] Benchmarking is not easy and often involves several iterative rounds in order to arrive at predictable, useful conclusions. Interpretation of benchmarking data is also extraordinarily difficult. Here is a partial list of common challenges: Vendors tend to tune their products specifically for industry-standard benchmarks. Norton SysInfo (SI) is particularly easy to tune for, since it mainly biased toward the speed of multiple operations. Use extreme caution in interpreting such results.

Some vendors have been accused of "cheating" at benchmarks — designing their systems such that they give much higher benchmark numbers, but are not as effective at the actual likely workload.

[ 6 ] Many benchmarks focus entirely on the speed of computational performance , neglecting other important features of a computer system, such as: Qualities of service, aside from raw performance. Examples of unmeasured qualities of service include security, availability, reliability, execution integrity, serviceability, scalability (especially the ability to quickly and nondisruptively add or reallocate capacity), etc. There are often real trade-offs between and among these qualities of service, and all are important in business computing.

Transaction Processing Performance Council Benchmark specifications partially address these concerns by specifying ACID property tests, database scalability rules, and service level requirements.

In general, benchmarks do not measure Total cost of ownership . Transaction Processing Performance Council Benchmark specifications partially address this concern by specifying that a price/performance metric must be reported in addition to a raw performance metric, using a simplified TCO formula. However, the costs are necessarily only partial, and vendors have been known to price specifically (and only) for the benchmark, designing a highly specific "benchmark special" configuration with an artificially low price. Even a tiny deviation from the benchmark package results in a much higher price in real world experience.

Facilities burden (space, power, and cooling). When more power is used, a portable system will have a shorter battery life and require recharging more often. A server that consumes more power and/or space may not be able to fit within existing data center resource constraints, including cooling limitations. There are real trade-offs as most semiconductors require more power to switch faster. See also performance per watt .

In some embedded systems, where memory is a significant cost, better code density can significantly reduce costs.

Vendor benchmarks tend to ignore requirements for development, test, and disaster recovery computing capacity. Vendors only like to report what might be narrowly required for production capacity in order to make their initial acquisition price seem as low as possible.

Benchmarks are having trouble adapting to widely distributed servers, particularly those with extra sensitivity to network topologies. The emergence of grid computing , in particular, complicates benchmarking since some workloads are "grid friendly", while others are not.

Users can have very different perceptions of performance than benchmarks may suggest. In particular, users appreciate predictability — servers that always meet or exceed service level agreements . Benchmarks tend to emphasize mean scores (IT perspective), rather than maximum worst-case response times ( real-time computing perspective), or low standard deviations (user perspective).

Many server architectures degrade dramatically at high (near 100%) levels of usage — "fall off a cliff" — and benchmarks should (but often do not) take that factor into account. Vendors, in particular, tend to publish server benchmarks at continuous at about 80% usage — an unrealistic situation — and do not document what happens to the overall system when demand spikes beyond that level.

Many benchmarks focus on one application, or even one application tier, to the exclusion of other applications. Most data centers are now implementing virtualization extensively for a variety of reasons, and benchmarking is still catching up to that reality where multiple applications and application tiers are concurrently running on consolidated servers.

There are few (if any) high quality benchmarks that help measure the performance of batch computing, especially high volume concurrent batch and online computing.

Batch computing tends to be much more focused on the predictability of completing long-running tasks correctly before deadlines, such as end of month or end of fiscal year. Many important core business processes are batch-oriented and probably always will be, such as billing.

Benchmarking institutions often disregard or do not follow basic scientific method. This includes, but is not limited to: small sample size, lack of variable control, and the limited repeatability of results.

[ 7 ] Benchmarking principles [ edit ] There are seven vital characteristics for benchmarks.

[ 8 ] These key properties are: Relevance: Benchmarks should measure relatively vital features.

Representativeness: Benchmark performance metrics should be broadly accepted by industry and academia.

Equity: All systems should be fairly compared.

Repeatability: Benchmark results can be verified.

Cost-effectiveness: Benchmark tests are economical.

Scalability: Benchmark tests should work across systems possessing a range of resources from low to high.

Transparency: Benchmark metrics should be easy to understand.

Types of benchmark [ edit ] Real program word processing software tool software of CAD user's application software (i.e.: MIS) Video games Compilers building a large project, for example Chromium browser or Linux kernel Component Benchmark / Microbenchmark core routine consists of a relatively small and specific piece of code.

measure performance of a computer's basic components [ 9 ] may be used for automatic detection of computer's hardware parameters like number of registers, cache size, memory latency , etc.

Kernel contains key codes normally abstracted from actual program popular kernel: Livermore loop linpack benchmark (contains basic linear algebra subroutine written in FORTRAN language) results are represented in Mflop/s.

Synthetic Benchmark Procedure for programming synthetic benchmark: take statistics of all types of operations from many application programs get proportion of each operation write program based on the proportion above Types of Synthetic Benchmark are: Whetstone Dhrystone These were the first general purpose industry standard computer benchmarks. They do not necessarily obtain high scores on modern pipelined computers.

I/O benchmarks Database benchmarks measure the throughput and response times of database management systems (DBMS) Parallel benchmarks used on machines with multiple cores and/or processors, or systems consisting of multiple machines Common benchmarks [ edit ] Industry standard (audited and verifiable) [ edit ] Embedded Microprocessor Benchmark Consortium (EEMBC) Standard Performance Evaluation Corporation (SPEC), in particular their SPECint and SPECfp Transaction Processing Performance Council (TPC): DBMS benchmarks [ 10 ] Open source benchmarks [ edit ] AIM Multiuser Benchmark – composed of a list of tests that could be mixed to create a 'load mix' that would simulate a specific computer function on any UNIX-type OS.

Bonnie++ – filesystem and hard drive benchmark BRL-CAD – cross-platform architecture-agnostic benchmark suite based on multithreaded ray tracing performance; baselined against a VAX-11/780; and used since 1984 for evaluating relative CPU performance, compiler differences, optimization levels, coherency, architecture differences, and operating system differences.

Collective Knowledge – customizable, cross-platform framework to crowdsource benchmarking and optimization of user workloads (such as deep learning ) across hardware provided by volunteers Coremark – Embedded computing benchmark DEISA Benchmark Suite – scientific HPC applications benchmark Dhrystone – integer arithmetic performance, often reported in DMIPS (Dhrystone millions of instructions per second) DiskSpd – Command-line tool for storage benchmarking that generates a variety of requests against computer files , partitions or storage devices Fhourstones – an integer benchmark HINT – designed to measure overall CPU and memory performance Iometer – I/O subsystem measurement and characterization tool for single and clustered systems.

IOzone – Filesystem benchmark LINPACK benchmarks – traditionally used to measure FLOPS Livermore loops NAS parallel benchmarks NBench – synthetic benchmark suite measuring performance of integer arithmetic, memory operations, and floating-point arithmetic PAL – a benchmark for realtime physics engines PerfKitBenchmarker – A set of benchmarks to measure and compare cloud offerings.

Phoronix Test Suite – open-source cross-platform benchmarking suite for Linux, OpenSolaris, FreeBSD, OSX and Windows. It includes a number of other benchmarks included on this page to simplify execution.

POV-Ray – 3D render Tak (function) – a simple benchmark used to test recursion performance TATP Benchmark – Telecommunication Application Transaction Processing Benchmark TPoX – An XML transaction processing benchmark for XML databases VUP (VAX unit of performance) – also called VAX MIPS Whetstone – floating-point arithmetic performance, often reported in millions of Whetstone instructions per second (MWIPS) Microsoft Windows benchmarks [ edit ] CrystalDiskMark Underwriters Laboratories (UL) : 3DMark , PCMark Heaven Benchmark PiFast Superposition Benchmark Super PI SuperPrime Whetstone Windows System Assessment Tool , included with Windows Vista and later releases, providing an index for consumers to rate their systems easily Worldbench (discontinued) Unusual benchmark [ edit ] Will Smith Eating Spaghetti test - an informal test to determine the capabilities of text-to-video models.

Others [ edit ] AnTuTu – commonly used on phones and ARM-based devices.

Byte Sieve - originally tested language performance, but widely used as a machine benchmark as well.

Creative Computing Benchmark – Compares the BASIC programming language on various platforms. Introduced in 1983.

Geekbench – A cross-platform benchmark for Windows, Linux, macOS, iOS and Android.

iCOMP – the Intel comparative microprocessor performance, published by Intel Khornerstone Novabench - a computer benchmarking utility for Microsoft Windows, macOS, and Linux Performance Rating – modeling scheme used by AMD and Cyrix to reflect the relative performance usually compared to competing products.

Rugg/Feldman benchmarks - one of the earliest microcomputer benchmarks, from 1977.

SunSpider – a browser speed test UserBenchmark - PC benchmark utility VMmark – a virtualization benchmark suite.

See also [ edit ] Benchmarking (business perspective) Figure of merit Lossless compression benchmarks Performance Counter Monitor Test suite –  a collection of test cases intended to show that a software program has some specified set of behaviors References [ edit ] ^ Fleming, Philip J.; Wallace, John J. (1986-03-01).

"How not to lie with statistics: the correct way to summarize benchmark results" .

Communications of the ACM .

29 (3): 218– 221.

doi : 10.1145/5666.5673 .

ISSN 0001-0782 .

S2CID 1047380 .

^ Grambow, Martin; Lehmann, Fabian; Bermbach, David (2019).

"Continuous Benchmarking: Using System Benchmarking in Build Pipelines" .

2019 IEEE International Conference on Cloud Engineering (IC2E) . pp.

241– 246.

doi : 10.1109/IC2E.2019.00039 .

ISBN 978-1-7281-0218-4 . Retrieved 2023-12-03 .

^ "RDBMS Workshop: Informix" (PDF) (Interview). Interviewed by Luanne Johnson. Computer History Museum. 2007-06-12 . Retrieved 2025-05-30 .

^ "RDBMS Workshop: Ingres and Sybase" (PDF) (Interview). Interviewed by Doug Jerger. Computer History Museum. 2007-06-13 . Retrieved 2025-05-30 .

^ Software: HDDScan, GNOME Disks ^ Krazit, Tom (2003).

"NVidia's Benchmark Tactics Reassessed" .

IDG News . Archived from the original on 2011-06-06 . Retrieved 2009-08-08 .

^ Castor, Kevin (2006).

"Hardware Testing and Benchmarking Methodology" . Archived from the original on 2008-02-05 . Retrieved 2008-02-24 .

^ Dai, Wei; Berleant, Daniel (December 12–14, 2019).

"Benchmarking Contemporary Deep Learning Hardware and Frameworks: a Survey of Qualitative Metrics" (PDF) .

2019 IEEE First International Conference on Cognitive Machine Intelligence (CogMI) . Los Angeles, CA, USA: IEEE. pp.

148– 155.

arXiv : 1907.03626 .

doi : 10.1109/CogMI48466.2019.00029 .

^ Ehliar, Andreas; Liu, Dake.

"Benchmarking network processors" (PDF) .

{{ cite journal }} : Cite journal requires |journal= ( help ) ^ Transaction Processing Performance Council (February 1998).

"History and Overview of the TPC" .

TPC .

Transaction Processing Performance Council . Retrieved 2018-07-02 .

Further reading [ edit ] Gray, Jim, ed. (1993).

The Benchmark Handbook for Database and Transaction Systems . Morgan Kaufmann Series in Data Management Systems (2nd ed.). Morgan Kaufmann Publishers, Inc.

ISBN 1-55860-292-5 .

Scalzo, Bert; Kline, Kevin; Fernandez, Claudia; Burleson, Donald K.; Ault, Mike (2007).

Database Benchmarking Practical Methods for Oracle & SQL Server . Rampant TechPress.

ISBN 978-0-9776715-3-3 .

Nambiar, Raghunath; Poess, Meikel, eds. (2009).

Performance Evaluation and Benchmarking . Springer.

ISBN 978-3-642-10423-7 .

External links [ edit ] Lewis, Byron C.; Crews, Albert E. (1985).

"The Evolution of Benchmarking as a Computer Performance Evaluation Technique" .

MIS Quarterly .

9 (1): 7– 16.

doi : 10.2307/249270 .

ISSN 0276-7783 .

JSTOR 249270 .

The dates: 1962-1976 Wikimedia Commons has media related to Benchmarks (computing) .

Authority control databases : National Czech Republic v t e Software testing Test levels Acceptance testing System integration testing System testing Integration testing Unit testing Test types, techniques, tactics A/B testing Benchmark Compatibility testing Concolic testing Concurrent testing Conformance testing Continuous testing Destructive testing Development testing Differential testing Dynamic program analysis Installation testing Negative testing Random testing Regression testing Security testing Smoke testing (software) Software performance testing Stress testing Symbolic execution Test automation Usability testing [x]-box style Black-box testing All-pairs testing Exploratory testing Fuzz testing Model-based testing Scenario testing Grey-box testing White-box testing API testing Mutation testing Static testing See also Graphical user interface testing Manual testing Orthogonal array testing Pair testing Soak testing Software reliability testing Stress testing Web testing v t e Processing benchmarks Concepts Free software Proprietary software Performance per watt Data center infrastructure efficiency Giga-updates per second (memory) CPU power dissipation Organizations EEMBC Futuremark Standard Performance Evaluation Corporation (SPEC) Transaction Processing Performance Council (TPC) Processor Floating-point unit ( FLOPS ) SPECfp LINPACK LAPACK Prime95 Super PI SuperPrime Whetstone IBM iSeries benchmarks (Computational Intensive Workload) Integer ( ALU ) Dhrystone Fhourstones SPECint CoreMark Digital signal processor (DSP) BTDi Graphics processing unit (GPU) BRL-CAD Parallel computing DEISA Benchmark Suite Livermore loops NAS Parallel Benchmarks HPC Challenge Benchmark Princeton Application Repository for Shared-Memory Computers (PARSEC) Peripherals Network BreakingPoint Systems SUPS Filesystems and storage Bonnie++ HD Tach IOzone Diskspd Computer memory BSS Random Access benchmark HPC Challenge Random Memory Access Input/output Iometer Ioblazer IBM iSeries benchmarks (Commercial Processing Workload) Computer system (entire) Hierarchical INTegration (HINT) NBench (CPU, memory) Energy consumption Average CPU power ( x86 ) EEMBC ( embedded systems ) Data center infrastructure efficiency SPECpower ( Java software) Server Efficiency Rating Tool (SERT) Software JavaScript engine Browser speed test Cryptography Cycles per byte Multiuser system SDET AIM Multiuser Benchmark Virtual machine VMmark SPECvirt Recursion performance Tak (function) Database transactions TATP Benchmark Transaction Processing over XML (TPoX) YCSB ( NoSQL ) Web server benchmarking Apache JMeter Curl-loader httperf OpenSTA TPC-W Tsung Platform specific Adjusted Peak Performance ( Nuclear weapon simulation) AnTuTu ( ARM ) BogoMips ( Linux ) Coremark ( embedded systems ) iCOMP (index) (Intel) Novabench (Windows and macOS ) Phoronix Test Suite (Linux) Performance Rating (AMD) Sysinfo & SysSpeed ( Motorola 68k ) WorldBench (Windows) Retrieved from " https://en.wikipedia.org/w/index.php?title=Benchmark_(computing)&oldid=1303527907 " Categories : Benchmarks (computing) Hardware testing Hidden categories: CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Articles needing additional references from July 2015 All articles needing additional references Articles needing expert attention from October 2022 All articles needing expert attention Computer science articles needing expert attention Articles with multiple maintenance issues Commons category link is on Wikidata This page was last edited on 31 July 2025, at 14:20 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Benchmark (computing) 32 languages Add topic

