Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Properties Toggle Properties subsection 2.1 Memorylessness 2.2 Moments and cumulants 2.2.1 Proof of expected value 2.3 Summary statistics 3 Entropy and Fisher's Information Toggle Entropy and Fisher's Information subsection 3.1 Entropy (Geometric Distribution, Failures Before Success) 3.2 Fisher's Information (Geometric Distribution, Failures Before Success) 3.3 Entropy (Geometric Distribution, Trials Until Success) 3.4 Fisher's Information (Geometric Distribution, Trials Until Success) 3.5 General properties 4 Related distributions 5 Statistical inference Toggle Statistical inference subsection 5.1 Method of moments 5.2 Maximum likelihood estimation 5.3 Bayesian inference 6 Random variate generation 7 Applications 8 See also 9 References Toggle the table of contents Geometric distribution 33 languages العربية Беларуская Català Čeština Deutsch Ελληνικά Español Euskara فارسی Français Galego 한국어 Italiano עברית Magyar Nederlands 日本語 Novial Polski Português Русский Shqip Simple English Slovenčina Slovenščina Suomi Svenska ไทย Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution Not to be confused with Hypergeometric distribution .

Geometric Probability mass function Cumulative distribution function Parameters 0 < p ≤ ≤ 1 {\displaystyle 0<p\leq 1} success probability ( real ) 0 < p ≤ ≤ 1 {\displaystyle 0<p\leq 1} success probability ( real ) Support k trials where k ∈ ∈ N = { 1 , 2 , 3 , … … } {\displaystyle k\in \mathbb {N} =\{1,2,3,\dotsc \}} k failures where k ∈ ∈ N 0 = { 0 , 1 , 2 , … … } {\displaystyle k\in \mathbb {N} _{0}=\{0,1,2,\dotsc \}} PMF ( 1 − − p ) k − − 1 p {\displaystyle (1-p)^{k-1}p} ( 1 − − p ) k p {\displaystyle (1-p)^{k}p} CDF 1 − − ( 1 − − p ) ⌊ ⌊ x ⌋ ⌋ {\displaystyle 1-(1-p)^{\lfloor x\rfloor }} for x ≥ ≥ 1 {\displaystyle x\geq 1} , 0 {\displaystyle 0} for x < 1 {\displaystyle x<1} 1 − − ( 1 − − p ) ⌊ ⌊ x ⌋ ⌋ + 1 {\displaystyle 1-(1-p)^{\lfloor x\rfloor +1}} for x ≥ ≥ 0 {\displaystyle x\geq 0} , 0 {\displaystyle 0} for x < 0 {\displaystyle x<0} Mean 1 p {\displaystyle {\frac {1}{p}}} 1 − − p p {\displaystyle {\frac {1-p}{p}}} Median ⌈ − − 1 log 2 ⁡ ⁡ ( 1 − − p ) ⌉ {\displaystyle \left\lceil {\frac {-1}{\log _{2}(1-p)}}\right\rceil } (not unique if − − 1 / log 2 ⁡ ⁡ ( 1 − − p ) {\displaystyle -1/\log _{2}(1-p)} is an integer) ⌈ − − 1 log 2 ⁡ ⁡ ( 1 − − p ) ⌉ − − 1 {\displaystyle \left\lceil {\frac {-1}{\log _{2}(1-p)}}\right\rceil -1} (not unique if − − 1 / log 2 ⁡ ⁡ ( 1 − − p ) {\displaystyle -1/\log _{2}(1-p)} is an integer) Mode 1 {\displaystyle 1} 0 {\displaystyle 0} Variance 1 − − p p 2 {\displaystyle {\frac {1-p}{p^{2}}}} 1 − − p p 2 {\displaystyle {\frac {1-p}{p^{2}}}} Skewness 2 − − p 1 − − p {\displaystyle {\frac {2-p}{\sqrt {1-p}}}} 2 − − p 1 − − p {\displaystyle {\frac {2-p}{\sqrt {1-p}}}} Excess kurtosis 6 + p 2 1 − − p {\displaystyle 6+{\frac {p^{2}}{1-p}}} 6 + p 2 1 − − p {\displaystyle 6+{\frac {p^{2}}{1-p}}} Entropy − − ( 1 − − p ) log ⁡ ⁡ ( 1 − − p ) − − p log ⁡ ⁡ p p {\displaystyle {\tfrac {-(1-p)\log(1-p)-p\log p}{p}}} − − ( 1 − − p ) log ⁡ ⁡ ( 1 − − p ) − − p log ⁡ ⁡ p p {\displaystyle {\tfrac {-(1-p)\log(1-p)-p\log p}{p}}} MGF p e t 1 − − ( 1 − − p ) e t , {\displaystyle {\frac {pe^{t}}{1-(1-p)e^{t}}},} for t < − − ln ⁡ ⁡ ( 1 − − p ) {\displaystyle t<-\ln(1-p)} p 1 − − ( 1 − − p ) e t , {\displaystyle {\frac {p}{1-(1-p)e^{t}}},} for t < − − ln ⁡ ⁡ ( 1 − − p ) {\displaystyle t<-\ln(1-p)} CF p e i t 1 − − ( 1 − − p ) e i t {\displaystyle {\frac {pe^{it}}{1-(1-p)e^{it}}}} p 1 − − ( 1 − − p ) e i t {\displaystyle {\frac {p}{1-(1-p)e^{it}}}} PGF p z 1 − − ( 1 − − p ) z {\displaystyle {\frac {pz}{1-(1-p)z}}} p 1 − − ( 1 − − p ) z {\displaystyle {\frac {p}{1-(1-p)z}}} Fisher information 1 p 2 ⋅ ⋅ ( 1 − − p ) {\displaystyle {\tfrac {1}{p^{2}\cdot (1-p)}}} 1 p 2 ⋅ ⋅ ( 1 − − p ) {\displaystyle {\tfrac {1}{p^{2}\cdot (1-p)}}} In probability theory and statistics , the geometric distribution is either one of two discrete probability distributions : The probability distribution of the number X {\displaystyle X} of Bernoulli trials needed to get one success, supported on N = { 1 , 2 , 3 , … … } {\displaystyle \mathbb {N} =\{1,2,3,\ldots \}} ; The probability distribution of the number Y = X − − 1 {\displaystyle Y=X-1} of failures before the first success, supported on N 0 = { 0 , 1 , 2 , … … } {\displaystyle \mathbb {N} _{0}=\{0,1,2,\ldots \}} .

These two different geometric distributions should not be confused with each other. Often, the name shifted geometric distribution is adopted for the former one (distribution of X {\displaystyle X} ); however, to avoid ambiguity, it is considered wise to indicate which is intended, by mentioning the support explicitly.

The geometric distribution gives the probability that the first occurrence of success requires k {\displaystyle k} independent trials, each with success probability p {\displaystyle p} . If the probability of success on each trial is p {\displaystyle p} , then the probability that the k {\displaystyle k} -th trial is the first success is Pr ( X = k ) = ( 1 − − p ) k − − 1 p {\displaystyle \Pr(X=k)=(1-p)^{k-1}p} for k = 1 , 2 , 3 , 4 , … … {\displaystyle k=1,2,3,4,\dots } The above form of the geometric distribution is used for modeling the number of trials up to and including the first success. By contrast, the following form of the geometric distribution is used for modeling the number of failures until the first success: Pr ( Y = k ) = Pr ( X = k + 1 ) = ( 1 − − p ) k p {\displaystyle \Pr(Y=k)=\Pr(X=k+1)=(1-p)^{k}p} for k = 0 , 1 , 2 , 3 , … … {\displaystyle k=0,1,2,3,\dots } The geometric distribution gets its name because its probabilities follow a geometric sequence . It is sometimes called the Furry distribution after Wendell H. Furry .

[ 1 ] : 210 Definition [ edit ] The geometric distribution is the discrete probability distribution that describes when the first success in an infinite sequence of independent and identically distributed Bernoulli trials occurs. Its probability mass function depends on its parameterization and support . When supported on N {\displaystyle \mathbb {N} } , the probability mass function is P ( X = k ) = ( 1 − − p ) k − − 1 p {\displaystyle P(X=k)=(1-p)^{k-1}p} where k = 1 , 2 , 3 , … … {\displaystyle k=1,2,3,\dotsc } is the number of trials and p {\displaystyle p} is the probability of success in each trial.

[ 2 ] : 260–261 The support may also be N 0 {\displaystyle \mathbb {N} _{0}} , defining Y = X − − 1 {\displaystyle Y=X-1} . This alters the probability mass function into P ( Y = k ) = ( 1 − − p ) k p {\displaystyle P(Y=k)=(1-p)^{k}p} where k = 0 , 1 , 2 , … … {\displaystyle k=0,1,2,\dotsc } is the number of failures before the first success.

[ 3 ] : 66 An alternative parameterization of the distribution gives the probability mass function P ( Y = k ) = ( P Q ) k ( 1 − − P Q ) {\displaystyle P(Y=k)=\left({\frac {P}{Q}}\right)^{k}\left(1-{\frac {P}{Q}}\right)} where P = 1 − − p p {\displaystyle P={\frac {1-p}{p}}} and Q = 1 p {\displaystyle Q={\frac {1}{p}}} .

[ 1 ] : 208–209 An example of a geometric distribution arises from rolling a six-sided die until a "1" appears. Each roll is independent with a 1 / 6 {\displaystyle 1/6} chance of success. The number of rolls needed follows a geometric distribution with p = 1 / 6 {\displaystyle p=1/6} .

Properties [ edit ] Memorylessness [ edit ] Main article: Memorylessness The geometric distribution is the only memoryless discrete probability distribution.

[ 4 ] It is the discrete version of the same property found in the exponential distribution .

[ 1 ] : 228 The property asserts that the number of previously failed trials does not affect the number of future trials needed for a success.

Because there are two definitions of the geometric distribution, there are also two definitions of memorylessness for discrete random variables.

[ 5 ] Expressed in terms of conditional probability , the two definitions are Pr ( X > m + n ∣ ∣ X > n ) = Pr ( X > m ) , {\displaystyle \Pr(X>m+n\mid X>n)=\Pr(X>m),} and Pr ( Y > m + n ∣ ∣ Y ≥ ≥ n ) = Pr ( Y > m ) , {\displaystyle \Pr(Y>m+n\mid Y\geq n)=\Pr(Y>m),} where m {\displaystyle m} and n {\displaystyle n} are natural numbers , X {\displaystyle X} is a geometrically distributed random variable defined over N {\displaystyle \mathbb {N} } , and Y {\displaystyle Y} is a geometrically distributed random variable defined over N 0 {\displaystyle \mathbb {N} _{0}} . Note that these definitions are not equivalent for discrete random variables; Y {\displaystyle Y} does not satisfy the first equation and X {\displaystyle X} does not satisfy the second.

Moments and cumulants [ edit ] The expected value and variance of a geometrically distributed random variable X {\displaystyle X} defined over N {\displaystyle \mathbb {N} } is [ 2 ] : 261 E ⁡ ⁡ ( X ) = 1 p , var ⁡ ⁡ ( X ) = 1 − − p p 2 .

{\displaystyle \operatorname {E} (X)={\frac {1}{p}},\qquad \operatorname {var} (X)={\frac {1-p}{p^{2}}}.} With a geometrically distributed random variable Y {\displaystyle Y} defined over N 0 {\displaystyle \mathbb {N} _{0}} , the expected value changes into E ⁡ ⁡ ( Y ) = 1 − − p p , {\displaystyle \operatorname {E} (Y)={\frac {1-p}{p}},} while the variance stays the same.

[ 6 ] : 114–115 For example, when rolling a six-sided die until landing on a "1", the average number of rolls needed is 1 1 / 6 = 6 {\displaystyle {\frac {1}{1/6}}=6} and the average number of failures is 1 − − 1 / 6 1 / 6 = 5 {\displaystyle {\frac {1-1/6}{1/6}}=5} .

The moment generating function of the geometric distribution when defined over N {\displaystyle \mathbb {N} } and N 0 {\displaystyle \mathbb {N} _{0}} respectively is [ 7 ] [ 6 ] : 114 M X ( t ) = p e t 1 − − ( 1 − − p ) e t M Y ( t ) = p 1 − − ( 1 − − p ) e t , t < − − ln ⁡ ⁡ ( 1 − − p ) {\displaystyle {\begin{aligned}M_{X}(t)&={\frac {pe^{t}}{1-(1-p)e^{t}}}\\M_{Y}(t)&={\frac {p}{1-(1-p)e^{t}}},t<-\ln(1-p)\end{aligned}}} The moments for the number of failures before the first success are given by E ( Y n ) = ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k p ⋅ ⋅ k n = p Li − − n ⁡ ⁡ ( 1 − − p ) ( for n ≠ ≠ 0 ) {\displaystyle {\begin{aligned}\mathrm {E} (Y^{n})&{}=\sum _{k=0}^{\infty }(1-p)^{k}p\cdot k^{n}\\&{}=p\operatorname {Li} _{-n}(1-p)&({\text{for }}n\neq 0)\end{aligned}}} where Li − − n ⁡ ⁡ ( 1 − − p ) {\displaystyle \operatorname {Li} _{-n}(1-p)} is the polylogarithm function .

[ 8 ] The cumulant generating function of the geometric distribution defined over N 0 {\displaystyle \mathbb {N} _{0}} is [ 1 ] : 216 K ( t ) = ln ⁡ ⁡ p − − ln ⁡ ⁡ ( 1 − − ( 1 − − p ) e t ) {\displaystyle K(t)=\ln p-\ln(1-(1-p)e^{t})} The cumulants κ κ r {\displaystyle \kappa _{r}} satisfy the recursion κ κ r + 1 = q δ δ κ κ r δ δ q , r = 1 , 2 , … … {\displaystyle \kappa _{r+1}=q{\frac {\delta \kappa _{r}}{\delta q}},r=1,2,\dotsc } where q = 1 − − p {\displaystyle q=1-p} , when defined over N 0 {\displaystyle \mathbb {N} _{0}} .

[ 1 ] : 216 Proof of expected value [ edit ] Consider the expected value E ( X ) {\displaystyle \mathrm {E} (X)} of X as above, i.e. the average number of trials until a success. 
The first trial either succeeds with probability p {\displaystyle p} , or fails with probability 1 − − p {\displaystyle 1-p} . 
If it fails, the remaining mean number of trials until a success is identical to the original mean - 
this follows from the fact that all trials are independent.

From this we get the formula: E ⁡ ⁡ ( X ) = p + ( 1 − − p ) ( 1 + E [ X ] ) , {\displaystyle \operatorname {\mathrm {E} } (X)=p+(1-p)(1+\mathrm {E} [X]),} which, when solved for E ( X ) {\displaystyle \mathrm {E} (X)} , gives: E ⁡ ⁡ ( X ) = 1 p .

{\displaystyle \operatorname {E} (X)={\frac {1}{p}}.} The expected number of failures Y {\displaystyle Y} can be found from the linearity of expectation , E ( Y ) = E ( X − − 1 ) = E ( X ) − − 1 = 1 p − − 1 = 1 − − p p {\displaystyle \mathrm {E} (Y)=\mathrm {E} (X-1)=\mathrm {E} (X)-1={\frac {1}{p}}-1={\frac {1-p}{p}}} . It can also be shown in the following way: E ⁡ ⁡ ( Y ) = p ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k k = p ( 1 − − p ) ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k − − 1 k = p ( 1 − − p ) ( − − ∑ ∑ k = 0 ∞ ∞ d d p [ ( 1 − − p ) k ] ) = p ( 1 − − p ) [ d d p ( − − ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k ) ] = p ( 1 − − p ) d d p ( − − 1 p ) = 1 − − p p .

{\displaystyle {\begin{aligned}\operatorname {E} (Y)&=p\sum _{k=0}^{\infty }(1-p)^{k}k\\&=p(1-p)\sum _{k=0}^{\infty }(1-p)^{k-1}k\\&=p(1-p)\left(-\sum _{k=0}^{\infty }{\frac {d}{dp}}\left[(1-p)^{k}\right]\right)\\&=p(1-p)\left[{\frac {d}{dp}}\left(-\sum _{k=0}^{\infty }(1-p)^{k}\right)\right]\\&=p(1-p){\frac {d}{dp}}\left(-{\frac {1}{p}}\right)\\&={\frac {1-p}{p}}.\end{aligned}}} The interchange of summation and differentiation is justified by the fact that convergent power series converge uniformly on compact subsets of the set of points where they converge.

Summary statistics [ edit ] The mean of the geometric distribution is its expected value which is, as previously discussed in § Moments and cumulants , 1 p {\displaystyle {\frac {1}{p}}} or 1 − − p p {\displaystyle {\frac {1-p}{p}}} when defined over N {\displaystyle \mathbb {N} } or N 0 {\displaystyle \mathbb {N} _{0}} respectively.

The median of the geometric distribution is ⌈ − − log ⁡ ⁡ 2 log ⁡ ⁡ ( 1 − − p ) ⌉ {\displaystyle \left\lceil -{\frac {\log 2}{\log(1-p)}}\right\rceil } when defined over N {\displaystyle \mathbb {N} } [ 9 ] and ⌊ − − log ⁡ ⁡ 2 log ⁡ ⁡ ( 1 − − p ) ⌋ {\displaystyle \left\lfloor -{\frac {\log 2}{\log(1-p)}}\right\rfloor } when defined over N 0 {\displaystyle \mathbb {N} _{0}} .

[ 3 ] : 69 The mode of the geometric distribution is the first value in the support set. This is 1 when defined over N {\displaystyle \mathbb {N} } and 0 when defined over N 0 {\displaystyle \mathbb {N} _{0}} .

[ 3 ] : 69 The skewness of the geometric distribution is 2 − − p 1 − − p {\displaystyle {\frac {2-p}{\sqrt {1-p}}}} .

[ 6 ] : 115 The kurtosis of the geometric distribution is 9 + p 2 1 − − p {\displaystyle 9+{\frac {p^{2}}{1-p}}} .

[ 6 ] : 115 The excess kurtosis of a distribution is the difference between its kurtosis and the kurtosis of a normal distribution , 3 {\displaystyle 3} .

[ 10 ] : 217 Therefore, the excess kurtosis of the geometric distribution is 6 + p 2 1 − − p {\displaystyle 6+{\frac {p^{2}}{1-p}}} . Since p 2 1 − − p ≥ ≥ 0 {\displaystyle {\frac {p^{2}}{1-p}}\geq 0} , the excess kurtosis is always positive so the distribution is leptokurtic .

[ 3 ] : 69 In other words, the tail of a geometric distribution decays faster than a Gaussian.

[ 10 ] : 217 Entropy and Fisher's Information [ edit ] Entropy (Geometric Distribution, Failures Before Success) [ edit ] Entropy is a measure of uncertainty in a probability distribution. For the geometric distribution that models the number of failures before the first success, the probability mass function is: P ( X = k ) = ( 1 − − p ) k p , k = 0 , 1 , 2 , … … {\displaystyle P(X=k)=(1-p)^{k}p,\quad k=0,1,2,\dots } The entropy H ( X ) {\displaystyle H(X)} for this distribution is defined as: H ( X ) = − − ∑ ∑ k = 0 ∞ ∞ P ( X = k ) ln ⁡ ⁡ P ( X = k ) = − − ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k p ln ⁡ ⁡ ( ( 1 − − p ) k p ) = − − ∑ ∑ k = 0 ∞ ∞ ( 1 − − p ) k p [ k ln ⁡ ⁡ ( 1 − − p ) + ln ⁡ ⁡ p ] = − − log ⁡ ⁡ p − − 1 − − p p log ⁡ ⁡ ( 1 − − p ) {\displaystyle {\begin{aligned}H(X)&=-\sum _{k=0}^{\infty }P(X=k)\ln P(X=k)\\&=-\sum _{k=0}^{\infty }(1-p)^{k}p\ln \left((1-p)^{k}p\right)\\&=-\sum _{k=0}^{\infty }(1-p)^{k}p\left[k\ln(1-p)+\ln p\right]\\&=-\log p-{\frac {1-p}{p}}\log(1-p)\end{aligned}}} The entropy increases as the probability p {\displaystyle p} decreases, reflecting greater uncertainty as success becomes rarer.

Fisher's Information (Geometric Distribution, Failures Before Success) [ edit ] Fisher information measures the amount of information that an observable random variable X {\displaystyle X} carries about an unknown parameter p {\displaystyle p} . For the geometric distribution (failures before the first success), the Fisher information with respect to p {\displaystyle p} is given by: I ( p ) = 1 p 2 ( 1 − − p ) {\displaystyle I(p)={\frac {1}{p^{2}(1-p)}}} Proof: The Likelihood Function for a geometric random variable X {\displaystyle X} is: L ( p ; X ) = ( 1 − − p ) X p {\displaystyle L(p;X)=(1-p)^{X}p} The Log-Likelihood Function is: ln ⁡ ⁡ L ( p ; X ) = X ln ⁡ ⁡ ( 1 − − p ) + ln ⁡ ⁡ p {\displaystyle \ln L(p;X)=X\ln(1-p)+\ln p} The Score Function (first derivative of the log-likelihood w.r.t.

p {\displaystyle p} ) is: ∂ ∂ ∂ ∂ p ln ⁡ ⁡ L ( p ; X ) = 1 p − − X 1 − − p {\displaystyle {\frac {\partial }{\partial p}}\ln L(p;X)={\frac {1}{p}}-{\frac {X}{1-p}}} The second derivative of the log-likelihood function is: ∂ ∂ 2 ∂ ∂ p 2 ln ⁡ ⁡ L ( p ; X ) = − − 1 p 2 − − X ( 1 − − p ) 2 {\displaystyle {\frac {\partial ^{2}}{\partial p^{2}}}\ln L(p;X)=-{\frac {1}{p^{2}}}-{\frac {X}{(1-p)^{2}}}} Fisher Information is calculated as the negative expected value of the second derivative: I ( p ) = − − E [ ∂ ∂ 2 ∂ ∂ p 2 ln ⁡ ⁡ L ( p ; X ) ] = − − ( − − 1 p 2 − − 1 − − p p ( 1 − − p ) 2 ) = 1 p 2 ( 1 − − p ) {\displaystyle {\begin{aligned}I(p)&=-E\left[{\frac {\partial ^{2}}{\partial p^{2}}}\ln L(p;X)\right]\\&=-\left(-{\frac {1}{p^{2}}}-{\frac {1-p}{p(1-p)^{2}}}\right)\\&={\frac {1}{p^{2}(1-p)}}\end{aligned}}} Fisher information increases as p {\displaystyle p} decreases, indicating that rarer successes provide more information about the parameter p {\displaystyle p} .

Entropy (Geometric Distribution, Trials Until Success) [ edit ] For the geometric distribution modeling the number of trials until the first success, the probability mass function is: P ( X = k ) = ( 1 − − p ) k − − 1 p , k = 1 , 2 , 3 , … … {\displaystyle P(X=k)=(1-p)^{k-1}p,\quad k=1,2,3,\dots } The entropy H ( X ) {\displaystyle H(X)} for this distribution is given by: H ( X ) = − − ∑ ∑ k = 1 ∞ ∞ P ( X = k ) ln ⁡ ⁡ P ( X = k ) = − − ∑ ∑ k = 1 ∞ ∞ ( 1 − − p ) k − − 1 p ln ⁡ ⁡ ( ( 1 − − p ) k − − 1 p ) = − − ∑ ∑ k = 1 ∞ ∞ ( 1 − − p ) k − − 1 p [ ( k − − 1 ) ln ⁡ ⁡ ( 1 − − p ) + ln ⁡ ⁡ p ] = − − log ⁡ ⁡ p + 1 − − p p log ⁡ ⁡ ( 1 − − p ) {\displaystyle {\begin{aligned}H(X)&=-\sum _{k=1}^{\infty }P(X=k)\ln P(X=k)\\&=-\sum _{k=1}^{\infty }(1-p)^{k-1}p\ln \left((1-p)^{k-1}p\right)\\&=-\sum _{k=1}^{\infty }(1-p)^{k-1}p\left[(k-1)\ln(1-p)+\ln p\right]\\&=-\log p+{\frac {1-p}{p}}\log(1-p)\end{aligned}}} Entropy increases as p {\displaystyle p} decreases, reflecting greater uncertainty as the probability of success in each trial becomes smaller.

Fisher's Information (Geometric Distribution, Trials Until Success) [ edit ] Fisher information for the geometric distribution modeling the number of trials until the first success is given by: I ( p ) = 1 p 2 ( 1 − − p ) {\displaystyle I(p)={\frac {1}{p^{2}(1-p)}}} Proof: The Likelihood Function for a geometric random variable X {\displaystyle X} is: L ( p ; X ) = ( 1 − − p ) X − − 1 p {\displaystyle L(p;X)=(1-p)^{X-1}p} The Log-Likelihood Function is: ln ⁡ ⁡ L ( p ; X ) = ( X − − 1 ) ln ⁡ ⁡ ( 1 − − p ) + ln ⁡ ⁡ p {\displaystyle \ln L(p;X)=(X-1)\ln(1-p)+\ln p} The Score Function (first derivative of the log-likelihood w.r.t.

p {\displaystyle p} ) is: ∂ ∂ ∂ ∂ p ln ⁡ ⁡ L ( p ; X ) = 1 p − − X − − 1 1 − − p {\displaystyle {\frac {\partial }{\partial p}}\ln L(p;X)={\frac {1}{p}}-{\frac {X-1}{1-p}}} The second derivative of the log-likelihood function is: ∂ ∂ 2 ∂ ∂ p 2 ln ⁡ ⁡ L ( p ; X ) = − − 1 p 2 − − X − − 1 ( 1 − − p ) 2 {\displaystyle {\frac {\partial ^{2}}{\partial p^{2}}}\ln L(p;X)=-{\frac {1}{p^{2}}}-{\frac {X-1}{(1-p)^{2}}}} Fisher Information is calculated as the negative expected value of the second derivative: I ( p ) = − − E [ ∂ ∂ 2 ∂ ∂ p 2 ln ⁡ ⁡ L ( p ; X ) ] = − − ( − − 1 p 2 − − 1 − − p p ( 1 − − p ) 2 ) = 1 p 2 ( 1 − − p ) {\displaystyle {\begin{aligned}I(p)&=-E\left[{\frac {\partial ^{2}}{\partial p^{2}}}\ln L(p;X)\right]\\&=-\left(-{\frac {1}{p^{2}}}-{\frac {1-p}{p(1-p)^{2}}}\right)\\&={\frac {1}{p^{2}(1-p)}}\end{aligned}}} General properties [ edit ] The probability generating functions of geometric random variables X {\displaystyle X} and Y {\displaystyle Y} defined over N {\displaystyle \mathbb {N} } and N 0 {\displaystyle \mathbb {N} _{0}} are, respectively, [ 6 ] : 114–115 G X ( s ) = s p 1 − − s ( 1 − − p ) , G Y ( s ) = p 1 − − s ( 1 − − p ) , | s | < ( 1 − − p ) − − 1 .

{\displaystyle {\begin{aligned}G_{X}(s)&={\frac {s\,p}{1-s\,(1-p)}},\\[10pt]G_{Y}(s)&={\frac {p}{1-s\,(1-p)}},\quad |s|<(1-p)^{-1}.\end{aligned}}} The characteristic function φ φ ( t ) {\displaystyle \varphi (t)} is equal to G ( e i t ) {\displaystyle G(e^{it})} so the geometric distribution's characteristic function, when defined over N {\displaystyle \mathbb {N} } and N 0 {\displaystyle \mathbb {N} _{0}} respectively, is [ 11 ] : 1630 φ φ X ( t ) = p e i t 1 − − ( 1 − − p ) e i t , φ φ Y ( t ) = p 1 − − ( 1 − − p ) e i t .

{\displaystyle {\begin{aligned}\varphi _{X}(t)&={\frac {pe^{it}}{1-(1-p)e^{it}}},\\[10pt]\varphi _{Y}(t)&={\frac {p}{1-(1-p)e^{it}}}.\end{aligned}}} The entropy of a geometric distribution with parameter p {\displaystyle p} is [ 12 ] − − p log 2 ⁡ ⁡ p + ( 1 − − p ) log 2 ⁡ ⁡ ( 1 − − p ) p {\displaystyle -{\frac {p\log _{2}p+(1-p)\log _{2}(1-p)}{p}}} Given a mean , the geometric distribution is the maximum entropy probability distribution of all discrete probability distributions. The corresponding continuous distribution is the exponential distribution .

[ 13 ] The geometric distribution defined on N 0 {\displaystyle \mathbb {N} _{0}} is infinitely divisible , that is, for any positive integer n {\displaystyle n} , there exist n {\displaystyle n} independent identically distributed random variables whose sum is also geometrically distributed. This is because the negative binomial distribution can be derived from a Poisson-stopped sum of logarithmic random variables .

[ 11 ] : 606–607 The decimal digits of the geometrically distributed random variable Y are a sequence of independent (and not identically distributed) random variables.

[ citation needed ] For example, the hundreds digit D has this probability distribution: Pr ( D = d ) = q 100 d 1 + q 100 + q 200 + ⋯ ⋯ + q 900 , {\displaystyle \Pr(D=d)={q^{100d} \over 1+q^{100}+q^{200}+\cdots +q^{900}},} where q = 1 − p , and similarly for the other digits, and, more generally, similarly for numeral systems with other bases than 10.  When the base is 2, this shows that a geometrically distributed random variable can be written as a sum of independent random variables whose probability distributions are indecomposable .

Golomb coding is the optimal prefix code [ clarification needed ] for the geometric discrete distribution.

[ 12 ] Related distributions [ edit ] The sum of r {\displaystyle r} independent geometric random variables with parameter p {\displaystyle p} is a negative binomial random variable with parameters r {\displaystyle r} and p {\displaystyle p} .

[ 14 ] The geometric distribution is a special case of the negative binomial distribution, with r = 1 {\displaystyle r=1} .

The geometric distribution is a special case of discrete compound Poisson distribution .

[ 11 ] : 606 The minimum of n {\displaystyle n} geometric random variables with parameters p 1 , … … , p n {\displaystyle p_{1},\dotsc ,p_{n}} is also geometrically distributed with parameter 1 − − ∏ ∏ i = 1 n ( 1 − − p i ) {\displaystyle 1-\prod _{i=1}^{n}(1-p_{i})} .

[ 15 ] Suppose 0 < r < 1, and for k = 1, 2, 3, ... the random variable X k has a Poisson distribution with expected value r k / k .  Then ∑ ∑ k = 1 ∞ ∞ k X k {\displaystyle \sum _{k=1}^{\infty }k\,X_{k}} has a geometric distribution taking values in N 0 {\displaystyle \mathbb {N} _{0}} , with expected value r /(1 − r ).

[ citation needed ] The exponential distribution is the continuous analogue of the geometric distribution. Applying the floor function to the exponential distribution with parameter λ λ {\displaystyle \lambda } creates a geometric distribution with parameter p = 1 − − e − − λ λ {\displaystyle p=1-e^{-\lambda }} defined over N 0 {\displaystyle \mathbb {N} _{0}} .

[ 3 ] : 74 This can be used to generate geometrically distributed random numbers as detailed in § Random variate generation .

If p = 1/ n and X is geometrically distributed with parameter p , then the distribution of X / n approaches an exponential distribution with expected value 1 as n → ∞, since Pr ( X / n > a ) = Pr ( X > n a ) = ( 1 − − p ) n a = ( 1 − − 1 n ) n a = [ ( 1 − − 1 n ) n ] a → → [ e − − 1 ] a = e − − a as n → → ∞ ∞ .

{\displaystyle {\begin{aligned}\Pr(X/n>a)=\Pr(X>na)&=(1-p)^{na}=\left(1-{\frac {1}{n}}\right)^{na}=\left[\left(1-{\frac {1}{n}}\right)^{n}\right]^{a}\\&\to [e^{-1}]^{a}=e^{-a}{\text{ as }}n\to \infty .\end{aligned}}} More generally, if p = λ / n , where λ is a parameter, then as n → ∞ the distribution of X / n approaches an exponential distribution with rate λ : Pr ( X > n x ) = lim n → → ∞ ∞ ( 1 − − λ λ / n ) n x = e − − λ λ x {\displaystyle \Pr(X>nx)=\lim _{n\to \infty }(1-\lambda /n)^{nx}=e^{-\lambda x}} therefore the distribution function of X / n converges to 1 − − e − − λ λ x {\displaystyle 1-e^{-\lambda x}} , which is that of an exponential random variable.

[ citation needed ] The index of dispersion of the geometric distribution is 1 p {\displaystyle {\frac {1}{p}}} and its coefficient of variation is 1 1 − − p {\displaystyle {\frac {1}{\sqrt {1-p}}}} . The distribution is overdispersed .

[ 1 ] : 216 Statistical inference [ edit ] The true parameter p {\displaystyle p} of an unknown geometric distribution can be inferred through estimators and conjugate distributions.

Method of moments [ edit ] Provided they exist, the first l {\displaystyle l} moments of a probability distribution can be estimated from a sample x 1 , … … , x n {\displaystyle x_{1},\dotsc ,x_{n}} using the formula m i = 1 n ∑ ∑ j = 1 n x j i {\displaystyle m_{i}={\frac {1}{n}}\sum _{j=1}^{n}x_{j}^{i}} where m i {\displaystyle m_{i}} is the i {\displaystyle i} th sample moment and 1 ≤ ≤ i ≤ ≤ l {\displaystyle 1\leq i\leq l} .

[ 16 ] : 349–350 Estimating E ( X ) {\displaystyle \mathrm {E} (X)} with m 1 {\displaystyle m_{1}} gives the sample mean , denoted x ¯ ¯ {\displaystyle {\bar {x}}} . Substituting this estimate in the formula for the expected value of a geometric distribution and solving for p {\displaystyle p} gives the estimators p ^ ^ = 1 x ¯ ¯ {\displaystyle {\hat {p}}={\frac {1}{\bar {x}}}} and p ^ ^ = 1 x ¯ ¯ + 1 {\displaystyle {\hat {p}}={\frac {1}{{\bar {x}}+1}}} when supported on N {\displaystyle \mathbb {N} } and N 0 {\displaystyle \mathbb {N} _{0}} respectively. These estimators are biased since E ( 1 x ¯ ¯ ) > 1 E ( x ¯ ¯ ) = p {\displaystyle \mathrm {E} \left({\frac {1}{\bar {x}}}\right)>{\frac {1}{\mathrm {E} ({\bar {x}})}}=p} as a result of Jensen's inequality .

[ 17 ] : 53–54 Maximum likelihood estimation [ edit ] The maximum likelihood estimator of p {\displaystyle p} is the value that maximizes the likelihood function given a sample.

[ 16 ] : 308 By finding the zero of the derivative of the log-likelihood function when the distribution is defined over N {\displaystyle \mathbb {N} } , the maximum likelihood estimator can be found to be p ^ ^ = 1 x ¯ ¯ {\displaystyle {\hat {p}}={\frac {1}{\bar {x}}}} , where x ¯ ¯ {\displaystyle {\bar {x}}} is the sample mean.

[ 18 ] If the domain is N 0 {\displaystyle \mathbb {N} _{0}} , then the estimator shifts to p ^ ^ = 1 x ¯ ¯ + 1 {\displaystyle {\hat {p}}={\frac {1}{{\bar {x}}+1}}} . As previously discussed in § Method of moments , these estimators are biased.

Regardless of the domain, the bias is equal to b ≡ ≡ E ⁡ ⁡ [ ( p ^ ^ m l e − − p ) ] = p ( 1 − − p ) n {\displaystyle b\equiv \operatorname {E} {\bigg [}\;({\hat {p}}_{\mathrm {mle} }-p)\;{\bigg ]}={\frac {p\,(1-p)}{n}}} which yields the bias-corrected maximum likelihood estimator , [ citation needed ] p ^ ^ mle ∗ ∗ = p ^ ^ mle − − b ^ ^ {\displaystyle {\hat {p\,}}_{\text{mle}}^{*}={\hat {p\,}}_{\text{mle}}-{\hat {b\,}}} Bayesian inference [ edit ] In Bayesian inference , the parameter p {\displaystyle p} is a random variable from a prior distribution with a posterior distribution calculated using Bayes' theorem after observing samples.

[ 17 ] : 167 If a beta distribution is chosen as the prior distribution, then the posterior will also be a beta distribution and it is called the conjugate distribution . In particular, if a B e t a ( α α , β β ) {\displaystyle \mathrm {Beta} (\alpha ,\beta )} prior is selected, then the posterior, after observing samples k 1 , … … , k n ∈ ∈ N {\displaystyle k_{1},\dotsc ,k_{n}\in \mathbb {N} } , is [ 19 ] p ∼ ∼ B e t a ( α α + n , β β + ∑ ∑ i = 1 n ( k i − − 1 ) ) .

{\displaystyle p\sim \mathrm {Beta} \left(\alpha +n,\ \beta +\sum _{i=1}^{n}(k_{i}-1)\right).\!} Alternatively, if the samples are in N 0 {\displaystyle \mathbb {N} _{0}} , the posterior distribution is [ 20 ] p ∼ ∼ B e t a ( α α + n , β β + ∑ ∑ i = 1 n k i ) .

{\displaystyle p\sim \mathrm {Beta} \left(\alpha +n,\beta +\sum _{i=1}^{n}k_{i}\right).} Since the expected value of a B e t a ( α α , β β ) {\displaystyle \mathrm {Beta} (\alpha ,\beta )} distribution is α α α α + β β {\displaystyle {\frac {\alpha }{\alpha +\beta }}} , [ 11 ] : 145 as α α {\displaystyle \alpha } and β β {\displaystyle \beta } approach zero, the posterior mean approaches its maximum likelihood estimate.

Random variate generation [ edit ] Further information: Non-uniform random variate generation The geometric distribution can be generated experimentally from i.i.d.

standard uniform random variables by finding the first such random variable to be less than or equal to p {\displaystyle p} . However, the number of random variables needed is also geometrically distributed and the algorithm slows as p {\displaystyle p} decreases.

[ 21 ] : 498 Random generation can be done in constant time by truncating exponential random numbers . An exponential random variable E {\displaystyle E} can become geometrically distributed with parameter p {\displaystyle p} through ⌈ ⌈ − − E / log ⁡ ⁡ ( 1 − − p ) ⌉ ⌉ {\displaystyle \lceil -E/\log(1-p)\rceil } . In turn, E {\displaystyle E} can be generated from a standard uniform random variable U {\displaystyle U} altering the formula into ⌈ ⌈ log ⁡ ⁡ ( U ) / log ⁡ ⁡ ( 1 − − p ) ⌉ ⌉ {\displaystyle \lceil \log(U)/\log(1-p)\rceil } .

[ 21 ] : 499–500 [ 22 ] Applications [ edit ] The geometric distribution is used in many disciplines. In queueing theory , the M/M/1 queue has a steady state following a geometric distribution.

[ 23 ] In stochastic processes , the Yule Furry process is geometrically distributed.

[ 24 ] The distribution also arises when modeling the lifetime of a device in discrete contexts.

[ 25 ] It has also been used to fit data including modeling patients spreading COVID-19 .

[ 26 ] See also [ edit ] Hypergeometric distribution Coupon collector's problem Compound Poisson distribution Negative binomial distribution References [ edit ] ^ a b c d e f Johnson, Norman L.; Kemp, Adrienne W.

; Kotz, Samuel (2005-08-19).

Univariate Discrete Distributions . Wiley Series in Probability and Statistics (1 ed.). Wiley.

doi : 10.1002/0471715816 .

ISBN 978-0-471-27246-5 .

^ a b Nagel, Werner; Steyer, Rolf (2017-04-04).

Probability and Conditional Expectation: Fundamentals for the Empirical Sciences . Wiley Series in Probability and Statistics (1st ed.). Wiley.

doi : 10.1002/9781119243496 .

ISBN 978-1-119-24352-6 .

^ a b c d e Chattamvelli, Rajan; Shanmugam, Ramalingam (2020).

Discrete Distributions in Engineering and the Applied Sciences . Synthesis Lectures on Mathematics & Statistics. Cham: Springer International Publishing.

doi : 10.1007/978-3-031-02425-2 .

ISBN 978-3-031-01297-6 .

^ Dekking, Frederik Michel; Kraaikamp, Cornelis; Lopuhaä, Hendrik Paul; Meester, Ludolf Erwin (2005).

A Modern Introduction to Probability and Statistics . Springer Texts in Statistics. London: Springer London. p. 50.

doi : 10.1007/1-84628-168-7 .

ISBN 978-1-85233-896-1 .

^ Weisstein, Eric W.

"Memoryless" .

mathworld.wolfram.com . Retrieved 2024-07-25 .

^ a b c d e Forbes, Catherine; Evans, Merran; Hastings, Nicholas; Peacock, Brian (2010-11-29).

Statistical Distributions (1st ed.). Wiley.

doi : 10.1002/9780470627242 .

ISBN 978-0-470-39063-4 .

^ Bertsekas, Dimitri P.; Tsitsiklis, John N. (2008).

Introduction to probability . Optimization and computation series (2nd ed.). Belmont: Athena Scientific. p. 235.

ISBN 978-1-886529-23-6 .

^ Weisstein, Eric W.

"Geometric Distribution" .

MathWorld . Retrieved 2024-07-13 .

^ Aggarwal, Charu C. (2024).

Probability and Statistics for Machine Learning: A Textbook . Cham: Springer Nature Switzerland. p. 138.

doi : 10.1007/978-3-031-53282-5 .

ISBN 978-3-031-53281-8 .

^ a b Chan, Stanley (2021).

Introduction to Probability for Data Science (1st ed.).

Michigan Publishing .

ISBN 978-1-60785-747-1 .

^ a b c d Lovric, Miodrag, ed. (2011).

International Encyclopedia of Statistical Science (1st ed.). Berlin, Heidelberg: Springer Berlin Heidelberg.

doi : 10.1007/978-3-642-04898-2 .

ISBN 978-3-642-04897-5 .

^ a b Gallager, R.; van Voorhis, D. (March 1975). "Optimal source codes for geometrically distributed integer alphabets (Corresp.)".

IEEE Transactions on Information Theory .

21 (2): 228– 230.

doi : 10.1109/TIT.1975.1055357 .

ISSN 0018-9448 .

^ Lisman, J. H. C.; Zuylen, M. C. A. van (March 1972).

"Note on the generation of most probable frequency distributions" .

Statistica Neerlandica .

26 (1): 19– 23.

doi : 10.1111/j.1467-9574.1972.tb00152.x .

ISSN 0039-0402 .

^ Pitman, Jim (1993).

Probability . New York, NY: Springer New York. p. 372.

doi : 10.1007/978-1-4612-4374-8 .

ISBN 978-0-387-94594-1 .

^ Ciardo, Gianfranco; Leemis, Lawrence M.; Nicol, David (1 June 1995).

"On the minimum of independent geometrically distributed random variables" .

Statistics & Probability Letters .

23 (4): 313– 326.

doi : 10.1016/0167-7152(94)00130-Z .

hdl : 2060/19940028569 .

S2CID 1505801 .

^ a b Evans, Michael; Rosenthal, Jeffrey (2023).

Probability and Statistics: The Science of Uncertainty (2nd ed.). Macmillan Learning.

ISBN 978-1429224628 .

^ a b Held, Leonhard; Sabanés Bové, Daniel (2020).

Likelihood and Bayesian Inference: With Applications in Biology and Medicine . Statistics for Biology and Health. Berlin, Heidelberg: Springer Berlin Heidelberg.

doi : 10.1007/978-3-662-60792-3 .

ISBN 978-3-662-60791-6 .

^ Siegrist, Kyle (2020-05-05).

"7.3: Maximum Likelihood" .

Statistics LibreTexts . Retrieved 2024-06-20 .

^ Fink, Daniel. "A Compendium of Conjugate Priors".

CiteSeerX 10.1.1.157.5540 .

^ "3. Conjugate families of distributions" (PDF) .

Archived (PDF) from the original on 2010-04-08.

^ a b Devroye, Luc (1986).

Non-Uniform Random Variate Generation . New York, NY: Springer New York.

doi : 10.1007/978-1-4613-8643-8 .

ISBN 978-1-4613-8645-2 .

^ Knuth, Donald Ervin (1997).

The Art of Computer Programming . Vol. 2 (3rd ed.). Reading, Mass: Addison-Wesley . p. 136.

ISBN 978-0-201-89683-1 .

^ Daskin, Mark S. (2021).

Bite-Sized Operations Management . Synthesis Lectures on Operations Research and Applications. Cham: Springer International Publishing. p. 127.

doi : 10.1007/978-3-031-02493-1 .

ISBN 978-3-031-01365-2 .

^ Madhira, Sivaprasad; Deshmukh, Shailaja (2023).

Introduction to Stochastic Processes Using R . Singapore: Springer Nature Singapore. p. 449.

doi : 10.1007/978-981-99-5601-2 .

ISBN 978-981-99-5600-5 .

^ Gupta, Rakesh; Gupta, Shubham; Ali, Irfan (2023), Garg, Harish (ed.), "Some Discrete Parametric Markov–Chain System Models to Analyze Reliability" , Advances in Reliability, Failure and Risk Analysis , Singapore: Springer Nature Singapore, pp.

305– 306, doi : 10.1007/978-981-19-9909-3_14 , ISBN 978-981-19-9908-6 , retrieved 2024-07-13 ^ Polymenis, Athanase (2021-10-01).

"An application of the geometric distribution for assessing the risk of infection with SARS-CoV-2 by location" .

Asian Journal of Medical Sciences .

12 (10): 8– 11.

doi : 10.3126/ajms.v12i10.38783 .

ISSN 2091-0576 .

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐7c956d68b4‐n9j9v
Cached time: 20250817043925
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.884 seconds
Real time usage: 1.170 seconds
Preprocessor visited node count: 11549/1000000
Revision size: 36086/2097152 bytes
Post‐expand include size: 154571/2097152 bytes
Template argument size: 7831/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 7/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 122522/5000000 bytes
Lua time usage: 0.390/10.000 seconds
Lua memory usage: 7574541/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  655.856      1 -total
 35.82%  234.921      1 Template:Reflist
 22.22%  145.714     16 Template:Cite_book
 19.64%  128.790     30 Template:Rp
 18.46%  121.046     30 Template:R/superscript
 14.15%   92.790      4 Template:Navbox
 13.44%   88.126      1 Template:ProbDistributions
 10.32%   67.703      1 Template:Short_description
  9.35%   61.295     90 Template:R/where
  7.39%   48.456      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:45922:|#|:idhash:canonical and timestamp 20250817043925 and revision id 1299235250. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Geometric_distribution&oldid=1299235250 " Categories : Discrete distributions Exponential family distributions Infinitely divisible probability distributions Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from May 2012 Wikipedia articles needing clarification from May 2012 Articles with unsourced statements from July 2024 Articles with example R code This page was last edited on 7 July 2025, at 06:38 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Geometric distribution 33 languages Add topic

