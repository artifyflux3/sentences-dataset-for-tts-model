Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Background and definition 2 Types of vulnerabilities 3 Mitigation 4 Footnotes 5 References 6 External links Toggle the table of contents Vulnerable world hypothesis Add languages Add links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Existential risk concept The vulnerable world hypothesis [ 1 ] or the "black ball" hypothesis [ 2 ] refers to the idea that civilizations may likely be destroyed by some disruptive technologies (a black ball) unless extraordinary measures are taken against the scenario from happening. The philosopher Nick Bostrom introduced the hypothesis in an initial publication in 2019 in the journal Global Policy [ 3 ] [ 1 ] and later further discussed in a 2022 essay published in Aeon along with co-author Matthew van der Merwe.

[ 4 ] The hypothesis is quoted in discussions about the safety of advanced technologies.

[ 5 ] [ 6 ] Background and definition [ edit ] Bostrom illustrated the hypothesis using an urn analogy. He likened the process of technological invention to drawing balls from an urn where the color of balls represents their impact. White balls are beneficial and constitute most of the balls drawn from the urn. Some balls are gray, which represent technologies with mixed or moderate effects. Black balls represent hypothetical technologies that tend to destroy by default the civilization that invents it. According to Bostrom, it is largely due to luck that humanity hasn't encountered a black ball yet, rather than carefulness or wisdom.

[ 5 ] Bostrom defined the vulnerable world hypothesis as the possibility that "If technological development continues then a set of capabilities will at some point be attained that make the devastation of civilization extremely likely, unless civilization sufficiently exits the semi-anarchic default condition." [ 3 ] except in some specific cases.

[ a ] The "semi-anarchic default condition" refers here to having: [ 3 ] [ 7 ] Limited capacity for preventive policing.

Limited capacity for global governance.

Actors with diverse motivations [ b ] Types of vulnerabilities [ edit ] To exemplify the vulnerabilities, Bostrom proposed a classification system and gave examples of how technology could have gone wrong, and policy recommendations such as differential technological development .

[ 5 ] [ 3 ] If a technology that entails such a vulnerability is developed, the solutions supposed to be needed to survive (i.e. effective global governance or preventive policing depending on the type of vulnerability) are controversial.

[ 5 ] [ 6 ] [ 8 ] The classification includes: [ 3 ] [ 1 ] Type 0 ("surprising strangelets") : a technology carries a hidden risk and inadvertently devastates the civilization.

A proposed hypothetical example of this is if nuclear bombs had been able to ignite the atmosphere. Nuclear ignition was predicted not to occur for the Trinity nuclear test in a report commissioned by Robert Oppenheimer . But the report has been deemed shaky given the potential consequences : "One may conclude that the arguments of this paper make it unreasonable to expect that the N + N reaction could propagate. An unlimited propagation is even less likely. However, the complexity of the argument and the absence of satisfactory experimental foundation makes further work on the subject highly desirable." [ 5 ] Type 1 ("easy nukes") : a technology gives small groups of people the ability to cause mass destruction.

The "easy nukes" thought experiment proposed by Nick Bostrom opens the question of what would have happened if nuclear chain reactions had been easier to produce, for example by "sending an electric current through a metal object placed between two sheets of glass." [ 5 ] Type 2a ("safe first strike") : a technology has the potential to devastate the civilization, and powerful actors are incentivized to use it, potentially because using it first seems to bring an advantage, or because of some tragedy of the commons scenario.

Type 2b ("worse global warming") : a great many actors face incentives to take some slightly damaging action such that the combined effect of those actions is civilizational devastation.

Mitigation [ edit ] According to Bostrom, pausing the technological progress may not be possible or desirable. An alternative would be to prioritize the technologies that are expected to have a positive impact, and to delay those that may be catastrophic, a principle called differential technological development .

[ 5 ] The potential solutions varies depending on the type of vulnerability. Dealing with type-2 vulnerabilities may require a very effective governance and international cooperation. For type-1 vulnerabilities, if mass-destruction ever gets accessible to individuals, there may be at least some small fraction of the population that would use it.

[ 5 ] In extreme cases, mass surveillance might be required to avoid the destruction of civilization, a controversial prospect that received significant media coverage.

[ 9 ] [ 10 ] [ 11 ] [ 12 ] [ 13 ] Technologies that have been proposed as potential vulnerabilities are advanced artificial intelligence , nanotechnology and synthetic biology (synthetic biology may give the ability to easily create enhanced pandemics).

[ 14 ] [ 2 ] [ 15 ] [ 16 ] Footnotes [ edit ] ^ It depends, according to Nick Bostrom, on whether society is in a "semi-anarchic default condition" (see § Definitions).

^ And in particular, the motivation of at least some small fraction of the population to destroy the civilization even at a personal cost. According to Bostrom : “Given the diversity of human character and circumstance, for any ever so imprudent, immoral, or self-defeating action, there is some residual fraction of humans who would choose to take that action.” [ 5 ] References [ edit ] ^ a b c Bostrom, Nick (November 2019).

"The Vulnerable World Hypothesis" .

Global Policy .

10 (4): 455– 476.

doi : 10.1111/1758-5899.12718 .

^ a b Bilton, Nick (2018-11-28).

"The "Black Ball" Hypothesis: Is Gene Editing More Dangerous Than Nuclear Weapons?" .

Vanity Fair . Retrieved 2023-11-07 .

^ a b c d e Katte, Abhijeet (2018-12-25).

"AI Doomsday Can Be Avoided If We Establish 'World Government': Nick Bostrom" .

Analytics India Magazine . Retrieved 2023-05-28 .

^ Bostrom, Nick; van der Merwe, Matthew (2022).

"None of our technologies has managed to destroy humanity – yet" .

Aeon .

^ a b c d e f g h i Piper, Kelsey (2018-11-19).

"How technological progress is making it likelier than ever that humans will destroy ourselves" .

Vox . Retrieved 2023-05-28 .

^ a b Finley, Klint.

"Technology That Could End Humanity—and How to Stop It" .

Wired .

ISSN 1059-1028 . Retrieved 2023-11-07 .

^ "Notes on the Vulnerable World Hypothesis" .

michaelnotebook.com .

^ "How to Protect Humanity From the Invention That Inadvertently Kills Us All" .

Inverse . 2019-04-18 . Retrieved 2023-11-07 .

^ Houser, Kristin (19 April 2019).

"Professor: Total surveillance is the only way to save humanity" .

Futurism . Retrieved 2023-05-28 .

^ Bendix, Aria.

"An Oxford philosopher who's inspired Elon Musk thinks mass surveillance might be the only way to save humanity from doom" .

Business Insider . Retrieved 2023-05-28 .

^ Taggart, Dagny (2019-04-24).

"Global Government and Surveillance May Be Needed to Save Humanity" .

The Organic Prepper . Retrieved 2023-10-16 .

^ Gheorghe, Ana (2019-04-27).

"Mass surveillance could save us from extinction, claims Professor" .

Cherwell . Retrieved 2023-10-16 .

^ "None of our technologies has managed to destroy humanity – yet" .

Aeon . 12 February 2021 . Retrieved 2023-05-28 .

^ Walsh, Bryan (July 15, 2020).

"The dire lessons of the first nuclear bomb test" .

Axios .

^ Torres, Phil (2019-10-21).

"Omniviolence Is Coming and the World Isn't Ready" .

Nautilus . Retrieved 2023-05-29 .

^ "AI-Powered Malware Holds Potential For Extreme Consequences - Could Artificial Intelligence Be a Black Ball From the Urn of Creativity?" .

Zvelo . 2023-04-26 . Retrieved 2023-11-07 .

External links [ edit ] The Vulnerable World Hypothesis How civilization could destroy itself -- and 4 ways we could prevent it - TED interview of Nick Bostrom by Chris Anderson v t e Global catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Human extinction risk estimates Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Ransomware Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeover Existential risk from artificial intelligence Technological singularity Transhumanism Sociological Anthropogenic hazard Collapsology Doomsday argument Self-indication assumption doomsday argument rebuttal Self-referencing doomsday argument rebuttal Economic collapse Malthusian catastrophe New World Order (conspiracy theory) Nuclear holocaust cobalt famine winter Riots Social crisis Societal collapse State collapse World War III Ecological Climate change Anoxic event Biodiversity loss Mass mortality event Cascade effect Cataclysmic pole shift hypothesis Deforestation Desertification Plant or animal species extinctions Civilizational collapse Tipping points Climate sensitivity Flood basalt Global dimming Global terrestrial stilling Global warming Hypercane Ice age Ecocide Ecological collapse Environmental degradation Habitat destruction Human impact on the environment coral reefs on marine life Land degradation Land consumption Land surface effects on climate Ocean acidification Ozone depletion Resource depletion Sea level rise Supervolcano winter Verneshot Water pollution Water scarcity Earth Overshoot Day Overexploitation Overpopulation Human overpopulation Biological Extinction Extinction event Holocene extinction Human extinction List of extinction events Genetic erosion Genetic pollution Others Biodiversity loss Decline in amphibian populations Decline in insect populations Biotechnology risk Biological agent Biological warfare Bioterrorism Colony collapse disorder Defaunation Dysgenics Interplanetary contamination Pandemic Pollinator decline Overfishing Astronomical Big Crunch Big Rip Coronal mass ejection Cosmological phase transition Geomagnetic storm False vacuum decay Gamma-ray burst Heat death of the universe Proton decay Virtual black hole Impact event Asteroid impact avoidance Asteroid impact prediction Potentially hazardous object Near-Earth object winter Rogue planet Rogue star Near-Earth supernova Hypernova Micronova Solar flare Stellar collision Eschatological Buddhist Maitreya Three Ages Hindu Kalki Kali Yuga Last Judgement Second Coming 1 Enoch Daniel Abomination of desolation Prophecy of Seventy Weeks Messiah Christian Futurism Historicism Interpretations of Revelation Idealism Preterism 2 Esdras 2 Thessalonians Man of sin Katechon Antichrist Book of Revelation Events Four Horsemen of the Apocalypse Seven bowls Seven seals The Beast Two witnesses War in Heaven Whore of Babylon Great Apostasy New Earth New Jerusalem Olivet Discourse Great Tribulation Son of perdition Sheep and Goats Islamic Al-Qa'im Beast of the Earth Dhu al-Qarnayn Dhul-Suwayqatayn Dajjal Israfil Mahdi Sufyani Jewish Messiah War of Gog and Magog Third Temple Norse Zoroastrian Saoshyant Others 2011 end times prediction 2012 phenomenon Apocalypse Apocalyptic literature Apocalypticism Armageddon Blood moon prophecy Earth Changes End time Gog and Magog List of dates predicted for apocalyptic events Messianism Messianic Age Millenarianism Millennialism Premillennialism Amillennialism Postmillennialism Nemesis (hypothetical star) Nibiru cataclysm Rapture Prewrath Posttribulation rapture Resurrection of the dead Vulnerable world hypothesis World to come Fictional Alien invasion Apocalyptic and post-apocalyptic fiction List of apocalyptic and post-apocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films Zombie apocalypse Zombie Organizations Centre for the Study of Existential Risk Future of Humanity Institute Future of Life Institute Nuclear Threat Initiative General Disaster Depression Financial crisis Survivalism World portal Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios v t e Existential risk from artificial intelligence Concepts AGI AI alignment AI boom AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial intelligence Friendly artificial intelligence Instrumental convergence Vulnerable world hypothesis Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI Safe Superintelligence People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Ilya Sutskever Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Artificial Intelligence Act Do You Trust This Computer?

Human Compatible Open letter on artificial intelligence (2015) Our Final Invention Roko's basilisk Statement on AI risk of extinction Superintelligence: Paths, Dangers, Strategies The Precipice Category NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐b6qh5
Cached time: 20250812050708
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.403 seconds
Real time usage: 0.550 seconds
Preprocessor visited node count: 1431/1000000
Revision size: 10888/2097152 bytes
Post‐expand include size: 82332/2097152 bytes
Template argument size: 1164/2097152 bytes
Highest expansion depth: 9/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 78137/5000000 bytes
Lua time usage: 0.248/10.000 seconds
Lua memory usage: 5116156/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  454.772      1 -total
 43.95%  199.866      2 Template:Reflist
 36.01%  163.747      4 Template:Navbox
 34.73%  157.958      1 Template:Doomsday
 17.26%   78.502     12 Template:Cite_web
 16.55%   75.252      1 Template:Cite_journal
 15.69%   71.365      1 Template:Short_description
  9.03%   41.070      2 Template:Pagetype
  4.33%   19.700      5 Template:Main_other
  3.81%   17.334      1 Template:SDcat Saved in parser cache with key enwiki:pcache:73908187:|#|:idhash:canonical and timestamp 20250812050708 and revision id 1284386261. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Vulnerable_world_hypothesis&oldid=1284386261 " Category : Existential risk Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 7 April 2025, at 07:21 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Vulnerable world hypothesis Add languages Add topic

