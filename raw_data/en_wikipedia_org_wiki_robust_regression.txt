Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Applications Toggle Applications subsection 1.1 Heteroscedastic errors 1.2 Presence of outliers 2 History and unpopularity of robust regression 3 Methods for robust regression Toggle Methods for robust regression subsection 3.1 Least squares alternatives 3.2 Parametric alternatives 3.3 Unit weights 4 Example: BUPA liver data Toggle Example: BUPA liver data subsection 4.1 Outlier detection 5 See also 6 References 7 External links Toggle the table of contents Robust regression 5 languages Español فارسی Nederlands Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Specialized form of regression analysis, in statistics Part of a series on Regression analysis Models Linear regression Simple regression Polynomial regression General linear model Generalized linear model Vector generalized linear model Discrete choice Binomial regression Binary regression Logistic regression Multinomial logistic regression Mixed logit Probit Multinomial probit Ordered logit Ordered probit Poisson Multilevel model Fixed effects Random effects Linear mixed-effects model Nonlinear mixed-effects model Nonlinear regression Nonparametric Semiparametric Robust Quantile Isotonic Principal components Least angle Local Segmented Errors-in-variables Estimation Least squares Linear Non-linear Ordinary Weighted Generalized Generalized estimating equation Partial Total Non-negative Ridge regression Regularized Least absolute deviations Iteratively reweighted Bayesian Bayesian multivariate Least-squares spectral analysis Background Regression validation Mean and predicted response Errors and residuals Goodness of fit Studentized residual Gauss–Markov theorem Mathematics portal v t e In robust statistics , robust regression seeks to overcome some limitations of traditional regression analysis . A regression analysis models the relationship between one or more independent variables and a dependent variable . Standard types of regression, such as ordinary least squares , have favourable properties if their underlying assumptions are true, but can give misleading results otherwise (i.e. are not robust to assumption violations). Robust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates.

For example, least squares estimates for regression models are highly sensitive to outliers : an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss , and therefore has more leverage over the regression estimates. The Huber loss function is a robust alternative to standard square error loss that reduces outliers' contributions to the squared error loss, thereby limiting their impact on regression estimates.

Applications [ edit ] Heteroscedastic errors [ edit ] One instance in which robust estimation should be considered is when there is a strong suspicion of heteroscedasticity . In the homoscedastic model, it is assumed that the variance of the error term is constant for all values of x . Heteroscedasticity allows the variance to be dependent on x , which is more accurate for many real scenarios. For example, the variance of expenditure is often larger for individuals with higher income than for individuals with lower incomes. Software packages usually default to a homoscedastic model, even though such a model may be less accurate than a heteroscedastic model. One simple approach ( Tofallis, 2008 ) is to apply least squares to percentage errors, as this reduces the influence of the larger values of the dependent variable compared to ordinary least squares.

Presence of outliers [ edit ] Another common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.) Although it is sometimes claimed that least squares (or classical statistical methods in general) are robust, they are only robust in the sense that the type I error rate does not increase under violations of the model. In fact, the type I error rate tends to be lower than the nominal level when outliers are present, and there is often a dramatic increase in the type II error rate . The reduction of the type I error rate has been labelled as the conservatism of classical methods.

History and unpopularity of robust regression [ edit ] Despite their superior performance over least squares estimation in many situations, robust methods for regression are still not widely used.  Several reasons may help explain their unpopularity ( Hampel et al. 1986, 2005 ).  One possible reason is that there are several competing methods [ citation needed ] and the field got off to many false starts. Also, robust estimates are much more computationally intensive than least squares estimation [ citation needed ] ; in recent years, however, this objection has become less relevant, as computing power has increased greatly. Another reason may be that some popular statistical software packages failed to implement the methods ( Stromberg, 2004 ). Perhaps the most important reason for the unpopularity of robust regression methods is that when the error variance is quite large or does not exist, for any given dataset, any estimate of the regression coefficients, robust or otherwise, will likely be practically worthless unless the sample is quite large.

Although uptake of robust methods has been slow, modern mainstream statistics text books often include discussion of these methods (for example, the books by Seber and Lee, and by Faraway [ vague ] ; for a good general description of how the various robust regression methods developed from one another see Andersen's book [ vague ] ). Also, modern statistical software packages such as R , SAS , Statsmodels, Stata and S-PLUS include considerable functionality for robust estimation (see, for example, the books by Venables and Ripley, and by Maronna et al.

[ vague ] ).

Methods for robust regression [ edit ] Least squares alternatives [ edit ] The simplest methods of estimating parameters in a regression model that are less sensitive to outliers than the least squares estimates, is to use least absolute deviations . Even then, gross outliers can still have a considerable impact on the model, motivating research into even more robust approaches.

In 1964, Huber introduced M-estimation for regression. The M in M-estimation stands for "maximum likelihood type". The method is robust to outliers in the response variable, but turned out not to be resistant to outliers in the explanatory variables ( leverage points). In fact, when there are outliers in the explanatory variables, the method has no advantage over least squares.

In the 1980s, several alternatives to M-estimation were proposed as attempts to overcome the lack of resistance. See the book by Rousseeuw and Leroy [ vague ] for a very practical review.

Least trimmed squares (LTS) is a viable alternative and is currently (2007) the preferred choice of Rousseeuw and Ryan (1997, 2008). The Theil–Sen estimator has a lower breakdown point than LTS but is statistically efficient and popular. Another proposed solution was S-estimation. This method finds a line (plane or hyperplane) that minimizes a robust estimate of the scale (from which the method gets the S in its name) of the residuals. This method is highly resistant to leverage points and is robust to outliers in the response. However, this method was also found to be inefficient.

MM-estimation attempts to retain the robustness and resistance of S-estimation, whilst gaining the efficiency of M-estimation. The method proceeds by finding a highly robust and resistant S-estimate that minimizes an M-estimate of the scale of the residuals (the first M in the method's name). The estimated scale is then held constant whilst a close by M-estimate of the parameters is located (the second M).

Parametric alternatives [ edit ] Another approach to robust estimation of regression models is to replace the normal distribution with a heavy-tailed distribution. A t -distribution with 4–6 degrees of freedom has been reported to be a good choice in various practical situations. Bayesian robust regression, being fully parametric, relies heavily on such distributions.

Under the assumption of t -distributed residuals, the distribution is a location-scale family. That is, x ← ← ( x − − μ μ ) / σ σ {\displaystyle x\leftarrow (x-\mu )/\sigma } . The degrees of freedom of the t -distribution is sometimes called the kurtosis parameter . Lange, Little and Taylor (1989) discuss this model in some depth from a non-Bayesian point of view. A Bayesian account appears in Gelman et al. (2003).

An alternative parametric approach is to assume that the residuals follow a mixture of normal distributions ( Daemi et al. 2019 ); in particular, a contaminated normal distribution in which the majority of observations are from a specified normal distribution, but a small proportion are from a normal distribution with much higher variance. That is, residuals have probability 1 − − ε ε {\displaystyle 1-\varepsilon } of coming from a normal distribution with variance σ σ 2 {\displaystyle \sigma ^{2}} , where ε ε {\displaystyle \varepsilon } is small, and probability ε ε {\displaystyle \varepsilon } of coming from a normal distribution with variance c σ σ 2 {\displaystyle c\sigma ^{2}} for some c > 1 {\displaystyle c>1} : e i ∼ ∼ ( 1 − − ε ε ) N ( 0 , σ σ 2 ) + ε ε N ( 0 , c σ σ 2 ) .

{\displaystyle e_{i}\sim (1-\varepsilon )N(0,\sigma ^{2})+\varepsilon N(0,c\sigma ^{2}).} Typically, ε ε < 0.1 {\displaystyle \varepsilon <0.1} . This is sometimes called the ε ε {\displaystyle \varepsilon } -contamination model.

Parametric approaches have the advantage that likelihood theory provides an "off-the-shelf" approach to inference (although for mixture models such as the ε ε {\displaystyle \varepsilon } -contamination model, the usual regularity conditions might not apply), and it is possible to build simulation models from the fit. However, such parametric models still assume that the underlying model is literally true. As such, they do not account for skewed residual distributions or finite observation precisions.

Unit weights [ edit ] Another robust method is the use of unit weights ( Wainer & Thissen, 1976), a method that can be applied when there are multiple predictors of a single outcome.

Ernest Burgess (1928) used unit weights to predict success on parole.  He scored 21 positive factors as present (e.g., "no prior arrest" = 1) or absent ("prior arrest" = 0), then summed to yield a predictor score, which was shown to be a useful predictor of parole success.

Samuel S. Wilks (1938) showed that nearly all sets of regression weights sum to composites that are very highly correlated with one another, including unit weights, a result referred to as Wilks' theorem (Ree, Carretta, & Earles, 1998).

Robyn Dawes (1979) examined decision making in applied settings, showing that simple models with unit weights often outperformed human experts.  Bobko, Roth, and Buster (2007) reviewed the literature on unit weights and concluded that decades of empirical studies show that unit weights perform similar to ordinary regression weights on cross validation.

Example: BUPA liver data [ edit ] The BUPA liver data have been studied by various authors, including Breiman (2001). The data can be found at the classic data sets page, and there is some discussion in the article on the Box–Cox transformation . A plot of the logs of ALT versus the logs of γGT appears below. The two regression lines are those estimated by ordinary least squares (OLS) and by robust MM-estimation. The analysis was performed in R using software made available by Venables and Ripley (2002).

The two regression lines appear to be very similar (and this is not unusual in a data set of this size). However, the advantage of the robust approach comes to light when the estimates of residual scale are considered. For ordinary least squares, the estimate of scale is 0.420, compared to 0.373 for the robust method. Thus, the relative efficiency of ordinary least squares to MM-estimation in this example is 1.266. This inefficiency leads to loss of power in hypothesis tests and to unnecessarily wide confidence intervals on estimated parameters.

Outlier detection [ edit ] Another consequence of the inefficiency of the ordinary least squares fit is that several outliers are masked because the estimate of residual scale is inflated; the scaled residuals are pushed closer to zero than when a more appropriate estimate of scale is used. The plots of the scaled residuals from the two models appear below. The variable on the x axis is just the observation number as it appeared in the data set. Rousseeuw and Leroy (1986) contains many such plots.

The horizontal reference lines are at 2 and −2, so that any observed scaled residual beyond these boundaries can be considered to be an outlier. Clearly, the least squares method leads to many interesting observations being masked.

Whilst in one or two dimensions outlier detection using classical methods can be performed manually, with large data sets and in high dimensions the problem of masking can make identification of many outliers impossible. Robust methods automatically detect these observations, offering a serious advantage over classical methods when outliers are present.

See also [ edit ] Regression Iteratively reweighted least squares M-estimator Relaxed intersection RANSAC Repeated median regression Theil–Sen estimator , a method for robust simple linear regression References [ edit ] Liu, J.; Cosman, P. C.; Rao, B. D. (2018).

"Robust Linear Regression via L0 Regularization" .

IEEE Transactions on Signal Processing .

66 (3): 698– 713.

doi : 10.1109/TSP.2017.2771720 .

Andersen, R. (2008).

Modern Methods for Robust Regression . Sage University Paper Series on Quantitative Applications in the Social Sciences, 07-152.

Ben-Gal I., Outlier detection , In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers," Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2 .

Bobko, P., Roth, P. L., & Buster, M. A. (2007). "The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and meta-analysis".

Organizational Research Methods , volume 10, pages 689-709.

doi : 10.1177/1094428106294734 Daemi, Atefeh, Hariprasad Kodamana, and Biao Huang. "Gaussian process modelling with Gaussian mixture likelihood." Journal of Process Control 81 (2019): 209-220.

doi : 10.1016/j.jprocont.2019.06.007 Breiman, L. (2001).

"Statistical Modeling: the Two Cultures" .

Statistical Science .

16 (3): 199– 231.

doi : 10.1214/ss/1009213725 .

JSTOR 2676681 .

Burgess, E. W. (1928). "Factors determining success or failure on parole". In A. A. Bruce (Ed.), The Workings of the Indeterminate Sentence Law and Parole in Illinois (pp. 205–249). Springfield, Illinois: Illinois State Parole Board.

Google books Dawes, Robyn M. (1979). "The robust beauty of improper linear models in decision making".

American Psychologist , volume 34, pages 571-582.

doi : 10.1037/0003-066X.34.7.571 .

archived pdf Draper, David (1988).

"Rank-Based Robust Analysis of Linear Models. I. Exposition and Review" .

Statistical Science .

3 (2): 239– 257.

doi : 10.1214/ss/1177012915 .

JSTOR 2245578 .

Faraway, J. J. (2004).

Linear Models with R . Chapman & Hall/CRC.

Fornalski, K. W. (2015). "Applications of the robust Bayesian regression analysis".

International Journal of Society Systems Science .

7 (4): 314– 333.

doi : 10.1504/IJSSS.2015.073223 .

Gelman, A.; J. B. Carlin; H. S. Stern; D. B. Rubin (2003).

Bayesian Data Analysis (Second ed.). Chapman & Hall/CRC.

Hampel, F. R.; E. M. Ronchetti; P. J. Rousseeuw; W. A. Stahel (2005) [1986].

Robust Statistics: The Approach Based on Influence Functions . Wiley.

Lange, K. L.; R. J. A. Little; J. M. G. Taylor (1989).

"Robust statistical modeling using the t -distribution" .

Journal of the American Statistical Association .

84 (408): 881– 896.

doi : 10.2307/2290063 .

JSTOR 2290063 .

Lerman, G.; McCoy, M.; Tropp, J. A.; Zhang T. (2012).

"Robust computation of linear models, or how to find a needle in a haystack" , arXiv : 1202.4044 .

Maronna, R.; D. Martin; V. Yohai (2006).

Robust Statistics: Theory and Methods . Wiley.

McKean, Joseph W. (2004).

"Robust Analysis of Linear Models" .

Statistical Science .

19 (4): 562– 570.

doi : 10.1214/088342304000000549 .

JSTOR 4144426 .

Radchenko S.G. (2005).

Robust methods for statistical models estimation: Monograph. (on Russian language) . Kiev: РР «Sanspariel». p. 504.

ISBN 978-966-96574-0-4 .

Ree, M. J., Carretta, T. R., & Earles, J. A. (1998). "In top-down decisions, weighting variables does not matter: A consequence of Wilk's theorem.

Organizational Research Methods , volume 1(4), pages 407-420.

doi : 10.1177/109442819814003 Rousseeuw, P. J.

; A. M. Leroy (2003) [1986].

Robust Regression and Outlier Detection . Wiley.

Ryan, T. P. (2008) [1997].

Modern Regression Methods . Wiley.

Seber, G. A. F.; A. J. Lee (2003).

Linear Regression Analysis (Second ed.). Wiley.

Stromberg, A. J. (2004).

"Why write statistical software? The case of robust statistical methods" .

Journal of Statistical Software .

10 (5).

doi : 10.18637/jss.v010.i05 .

Strutz, T. (2016).

Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond) . Springer Vieweg.

ISBN 978-3-658-11455-8 .

Tofallis, Chris (2008).

"Least Squares Percentage Regression" .

Journal of Modern Applied Statistical Methods .

7 : 526– 534.

doi : 10.2139/ssrn.1406472 .

hdl : 2299/965 .

SSRN 1406472 .

Venables, W. N.; B. D. Ripley (2002).

Modern Applied Statistics with S . Springer.

Wainer, H.

, & Thissen, D. (1976). "Three steps toward robust regression." Psychometrika , volume 41(1), pages 9–34.

doi : 10.1007/BF02291695 Wilks, S. S. (1938). "Weighting systems for linear functions of correlated variables when there is no dependent variable".

Psychometrika , volume 3, pages 23–40.

doi : 10.1007/BF02287917 External links [ edit ] R programming wikibooks Brian Ripley's robust statistics course notes.

Nick Fieller's course notes on Statistical Modelling and Computation contain material on robust regression.

Olfa Nasraoui's Overview of Robust Statistics Olfa Nasraoui's Overview of Robust Clustering Why write statistical software? The case of robust statistical methods, A. J. Stromberg Free software (Fortran 95) L1-norm regression. Minimization of absolute deviations instead of least squares.

Free open-source python implementation for robust nonlinear regression.

v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean Arithmetic Arithmetic-Geometric Contraharmonic Cubic Generalized/power Geometric Harmonic Heronian Heinz Lehmer Median Mode Dispersion Average absolute deviation Coefficient of variation Interquartile range Percentile Range Standard deviation Variance Shape Central limit theorem Moments Kurtosis L-moments Skewness Count data Index of dispersion Summary tables Contingency table Frequency distribution Grouped data Dependence Partial correlation Pearson product-moment correlation Rank correlation Kendall's τ Spearman's ρ Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Radar chart Run chart Scatter plot Stem-and-leaf display Violin plot Data collection Study design Effect size Missing data Optimal design Population Replication Sample size determination Statistic Statistical power Survey methodology Sampling Cluster Stratified Opinion poll Questionnaire Standard error Controlled experiments Blocking Factorial experiment Interaction Random assignment Randomized controlled trial Randomized experiment Scientific control Adaptive designs Adaptive clinical trial Stochastic approximation Up-and-down designs Observational studies Cohort study Cross-sectional study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Model specification L p space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Score/Lagrange multiplier Wald Specific tests Z -test (normal) Student's t -test F -test Goodness of fit Chi-squared G -test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Van der Waerden test Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Homoscedasticity and Heteroscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / multivariate / time-series / survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Cochran–Mantel–Haenszel statistics Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Least-squares spectral analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics Jurimetrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Mathematics portal Commons WikiProject v t e Least squares and regression analysis Computational statistics Least squares Linear least squares Non-linear least squares Iteratively reweighted least squares Correlation and dependence Pearson product-moment correlation Rank correlation ( Spearman's rho Kendall's tau ) Partial correlation Confounding variable Regression analysis Ordinary least squares Partial least squares Total least squares Ridge regression Regression as a statistical model Linear regression Simple linear regression Ordinary least squares Generalized least squares Weighted least squares General linear model Predictor structure Polynomial regression Growth curve (statistics) Segmented regression Local regression Non-standard Nonlinear regression Nonparametric Semiparametric Robust Quantile Isotonic Non-normal errors Generalized linear model Binomial Poisson Logistic Decomposition of variance Analysis of variance Analysis of covariance Multivariate AOV Model exploration Stepwise regression Model selection Mallows's C p AIC BIC Model specification Regression validation Background Mean and predicted response Gauss–Markov theorem Errors and residuals Goodness of fit Studentized residual Minimum mean-square error Frisch–Waugh–Lovell theorem Design of experiments Response surface methodology Optimal design Bayesian design Numerical approximation Numerical analysis Approximation theory Numerical integration Gaussian quadrature Orthogonal polynomials Chebyshev polynomials Chebyshev nodes Applications Curve fitting Calibration curve Numerical smoothing and differentiation System identification Moving least squares Regression analysis category Statistics category Mathematics portal Statistics outline Statistics topics Retrieved from " https://en.wikipedia.org/w/index.php?title=Robust_regression&oldid=1305275133 " Category : Robust regression Hidden categories: Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from June 2017 Articles with unsourced statements from March 2025 All Wikipedia articles needing clarification Wikipedia articles needing clarification from November 2023 This page was last edited on 11 August 2025, at 03:08 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Robust regression 5 languages Add topic

