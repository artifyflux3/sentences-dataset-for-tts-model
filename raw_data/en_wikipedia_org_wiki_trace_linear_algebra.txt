Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Example 3 Properties Toggle Properties subsection 3.1 Basic properties 3.2 Trace of a product 3.3 Cyclic property 3.4 Trace of a Kronecker product 3.5 Characterization of the trace 3.6 Trace as the sum of eigenvalues 3.7 Trace of commutator 3.8 Traces of special kinds of matrices 3.9 Relationship to the characteristic polynomial 4 Relationship to eigenvalues Toggle Relationship to eigenvalues subsection 4.1 Derivative relationships 5 Trace of a linear operator 6 Numerical algorithms Toggle Numerical algorithms subsection 6.1 Stochastic estimator 7 Applications 8 Lie algebra Toggle Lie algebra subsection 8.1 Bilinear forms 9 Generalizations 10 Traces in the language of tensor products 11 See also 12 Notes 13 References 14 External links Toggle the table of contents Trace (linear algebra) 40 languages العربية 閩南語 / Bân-lâm-gí Català Чӑвашла Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Bahasa Indonesia Italiano עברית Magyar Nederlands 日本語 Norsk nynorsk Polski Português Română Русский Shqip Slovenčina Slovenščina Српски / srpski Suomi Svenska தமிழ் ไทย Тоҷикӣ Türkçe Українська Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Sum of elements on the main diagonal This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Trace" linear algebra – news · newspapers · books · scholar · JSTOR ( November 2023 ) ( Learn how and when to remove this message ) This article needs editing to comply with Wikipedia's Manual of Style .

In particular, it has problems with MOS:FORMULA - avoid mixing <math>...</math> and {{ math }} in the same expression.

Please help improve the content .

( July 2025 ) ( Learn how and when to remove this message ) In linear algebra , the trace of a square matrix A , denoted tr( A ) , [ 1 ] is the sum of the elements on its main diagonal , a 11 + a 22 + ⋯ ⋯ + a n n {\displaystyle a_{11}+a_{22}+\dots +a_{nn}} . It is only defined for a square matrix ( n × n ).

The trace of a matrix is the sum of its eigenvalues (counted with multiplicities). Also, tr( AB ) = tr( BA ) for any matrices A and B of the same size. Thus, similar matrices have the same trace. As a consequence, one can define the trace of a linear operator mapping a finite-dimensional vector space into itself, since all matrices describing such an operator with respect to a basis are similar.

The trace is related to the derivative of the determinant (see Jacobi's formula ).

Definition [ edit ] The trace of an n × n square matrix A is defined as [ 1 ] [ 2 ] [ 3 ] : 34 tr ⁡ ⁡ ( A ) = ∑ ∑ i = 1 n a i i = a 11 + a 22 + ⋯ ⋯ + a n n {\displaystyle \operatorname {tr} (\mathbf {A} )=\sum _{i=1}^{n}a_{ii}=a_{11}+a_{22}+\dots +a_{nn}} where a ii denotes the entry on the i th row and i th column of A . The entries of A can be real numbers , complex numbers , or more generally elements of a field F . The trace is not defined for non-square matrices.

Example [ edit ] Let A be a matrix, with A = ( a 11 a 12 a 13 a 21 a 22 a 23 a 31 a 32 a 33 ) = ( 1 0 3 11 5 2 6 12 − − 5 ) {\displaystyle \mathbf {A} ={\begin{pmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{pmatrix}}={\begin{pmatrix}1&0&3\\11&5&2\\6&12&-5\end{pmatrix}}} Then tr ⁡ ⁡ ( A ) = ∑ ∑ i = 1 3 a i i = a 11 + a 22 + a 33 = 1 + 5 + ( − − 5 ) = 1 {\displaystyle \operatorname {tr} (\mathbf {A} )=\sum _{i=1}^{3}a_{ii}=a_{11}+a_{22}+a_{33}=1+5+(-5)=1} Properties [ edit ] Basic properties [ edit ] The trace is a linear mapping . That is, [ 1 ] [ 2 ] tr ⁡ ⁡ ( A + B ) = tr ⁡ ⁡ ( A ) + tr ⁡ ⁡ ( B ) tr ⁡ ⁡ ( c A ) = c tr ⁡ ⁡ ( A ) {\displaystyle {\begin{aligned}\operatorname {tr} (\mathbf {A} +\mathbf {B} )&=\operatorname {tr} (\mathbf {A} )+\operatorname {tr} (\mathbf {B} )\\\operatorname {tr} (c\mathbf {A} )&=c\operatorname {tr} (\mathbf {A} )\end{aligned}}} for all square matrices A and B , and all scalars c .

[ 3 ] : 34 A matrix and its transpose have the same trace: [ 1 ] [ 2 ] [ 3 ] : 34 tr ⁡ ⁡ ( A ) = tr ⁡ ⁡ ( A T ) .

{\displaystyle \operatorname {tr} (\mathbf {A} )=\operatorname {tr} \left(\mathbf {A} ^{\mathsf {T}}\right).} This follows immediately from the fact that transposing a square matrix does not affect elements along the main diagonal.

Trace of a product [ edit ] The trace of a square matrix which is the product of two matrices can be rewritten as the sum of entry-wise products of their elements, i.e. as the sum of all elements of their Hadamard product . Phrased directly, if A and B are two m × n matrices, then: tr ⁡ ⁡ ( A T B ) = tr ⁡ ⁡ ( A B T ) = tr ⁡ ⁡ ( B T A ) = tr ⁡ ⁡ ( B A T ) = ∑ ∑ i = 1 m ∑ ∑ j = 1 n a i j b i j .

{\displaystyle \operatorname {tr} \left(\mathbf {A} ^{\mathsf {T}}\mathbf {B} \right)=\operatorname {tr} \left(\mathbf {A} \mathbf {B} ^{\mathsf {T}}\right)=\operatorname {tr} \left(\mathbf {B} ^{\mathsf {T}}\mathbf {A} \right)=\operatorname {tr} \left(\mathbf {B} \mathbf {A} ^{\mathsf {T}}\right)=\sum _{i=1}^{m}\sum _{j=1}^{n}a_{ij}b_{ij}\;.} If one views any real m × n matrix as a vector of length mn (an operation called vectorization ) then the above operation on A and B coincides with the standard dot product . According to the above expression, tr( A ⊤ A ) is a sum of squares and hence is nonnegative, equal to zero if and only if A is zero.

[ 4 ] : 7 Furthermore, as noted in the above formula, tr( A ⊤ B ) = tr( B ⊤ A ) . These demonstrate the positive-definiteness and symmetry required of an inner product ; it is common to call tr( A ⊤ B ) the Frobenius inner product of A and B . This is a natural inner product on the vector space of all real matrices of fixed dimensions. The norm derived from this inner product is called the Frobenius norm , and it satisfies a submultiplicative property, as can be proven with the Cauchy–Schwarz inequality : 0 ≤ ≤ [ tr ⁡ ⁡ ( A B ) ] 2 ≤ ≤ tr ⁡ ⁡ ( A T A ) tr ⁡ ⁡ ( B T B ) , {\displaystyle 0\leq \left[\operatorname {tr} (\mathbf {A} \mathbf {B} )\right]^{2}\leq \operatorname {tr} \left(\mathbf {A} ^{\mathsf {T}}\mathbf {A} \right)\operatorname {tr} \left(\mathbf {B} ^{\mathsf {T}}\mathbf {B} \right),} if A and B are real matrices such that A B is a square matrix. The Frobenius inner product and norm arise frequently in matrix calculus and statistics .

The Frobenius inner product may be extended to a hermitian inner product on the complex vector space of all complex matrices of a fixed size, by replacing B by its complex conjugate .

The symmetry of the Frobenius inner product may be phrased more directly as follows: the matrices in the trace of a product can be switched without changing the result. If A and B are m × n and n × m real or complex matrices, respectively, then [ 1 ] [ 2 ] [ 3 ] : 34 [ note 1 ] tr ⁡ ⁡ ( A B ) = tr ⁡ ⁡ ( B A ) {\displaystyle \operatorname {tr} (\mathbf {A} \mathbf {B} )=\operatorname {tr} (\mathbf {B} \mathbf {A} )} This is notable both for the fact that AB does not usually equal BA , and also since the trace of either does not usually equal tr( A )tr( B ) .

[ note 2 ] The similarity-invariance of the trace, meaning that tr( A ) = tr( P −1 AP ) for any square matrix A and any invertible matrix P of the same dimensions, is a fundamental consequence. This is proved by tr ⁡ ⁡ ( P − − 1 ( A P ) ) = tr ⁡ ⁡ ( ( A P ) P − − 1 ) = tr ⁡ ⁡ ( A ) .

{\displaystyle \operatorname {tr} \left(\mathbf {P} ^{-1}(\mathbf {A} \mathbf {P} )\right)=\operatorname {tr} \left((\mathbf {A} \mathbf {P} )\mathbf {P} ^{-1}\right)=\operatorname {tr} (\mathbf {A} ).} Similarity invariance is the crucial property of the trace in order to discuss traces of linear transformations as below.

Additionally, for real column vectors a ∈ ∈ R n {\displaystyle \mathbf {a} \in \mathbb {R} ^{n}} and b ∈ ∈ R n {\displaystyle \mathbf {b} \in \mathbb {R} ^{n}} , the trace of the outer product is equivalent to the inner product: tr ⁡ ⁡ ( b a T ) = a T b {\displaystyle \operatorname {tr} \left(\mathbf {b} \mathbf {a} ^{\textsf {T}}\right)=\mathbf {a} ^{\textsf {T}}\mathbf {b} } Cyclic property [ edit ] More generally, the trace is invariant under circular shifts , that is, tr ⁡ ⁡ ( A B C D ) = tr ⁡ ⁡ ( B C D A ) = tr ⁡ ⁡ ( C D A B ) = tr ⁡ ⁡ ( D A B C ) .

{\displaystyle \operatorname {tr} (\mathbf {A} \mathbf {B} \mathbf {C} \mathbf {D} )=\operatorname {tr} (\mathbf {B} \mathbf {C} \mathbf {D} \mathbf {A} )=\operatorname {tr} (\mathbf {C} \mathbf {D} \mathbf {A} \mathbf {B} )=\operatorname {tr} (\mathbf {D} \mathbf {A} \mathbf {B} \mathbf {C} ).} This is known as the cyclic property .

Arbitrary permutations are not allowed: in general, tr ⁡ ⁡ ( A B C D ) ≠ ≠ tr ⁡ ⁡ ( A C B D ) .

{\displaystyle \operatorname {tr} (\mathbf {A} \mathbf {B} \mathbf {C} \mathbf {D} )\neq \operatorname {tr} (\mathbf {A} \mathbf {C} \mathbf {B} \mathbf {D} )~.} However, if products of three symmetric matrices are considered, any permutation is allowed, since: tr ⁡ ⁡ ( A B C ) = tr ⁡ ⁡ ( ( A B C ) T ) = tr ⁡ ⁡ ( C B A ) = tr ⁡ ⁡ ( A C B ) , {\displaystyle \operatorname {tr} (\mathbf {A} \mathbf {B} \mathbf {C} )=\operatorname {tr} \left(\left(\mathbf {A} \mathbf {B} \mathbf {C} \right)^{\mathsf {T}}\right)=\operatorname {tr} (\mathbf {C} \mathbf {B} \mathbf {A} )=\operatorname {tr} (\mathbf {A} \mathbf {C} \mathbf {B} ),} where the first equality is because the traces of a matrix and its transpose are equal. Note that this is not true in general for more than three factors.

Trace of a Kronecker product [ edit ] The trace of the Kronecker product of two matrices is the product of their traces: tr ⁡ ⁡ ( A ⊗ ⊗ B ) = tr ⁡ ⁡ ( A ) tr ⁡ ⁡ ( B ) .

{\displaystyle \operatorname {tr} (\mathbf {A} \otimes \mathbf {B} )=\operatorname {tr} (\mathbf {A} )\operatorname {tr} (\mathbf {B} ).} Characterization of the trace [ edit ] The following three properties: tr ⁡ ⁡ ( A + B ) = tr ⁡ ⁡ ( A ) + tr ⁡ ⁡ ( B ) , tr ⁡ ⁡ ( c A ) = c tr ⁡ ⁡ ( A ) , tr ⁡ ⁡ ( A B ) = tr ⁡ ⁡ ( B A ) , {\displaystyle {\begin{aligned}\operatorname {tr} (\mathbf {A} +\mathbf {B} )&=\operatorname {tr} (\mathbf {A} )+\operatorname {tr} (\mathbf {B} ),\\\operatorname {tr} (c\mathbf {A} )&=c\operatorname {tr} (\mathbf {A} ),\\\operatorname {tr} (\mathbf {A} \mathbf {B} )&=\operatorname {tr} (\mathbf {B} \mathbf {A} ),\end{aligned}}} characterize the trace up to a scalar multiple in the following sense: If f {\displaystyle f} is a linear functional on the space of square matrices that satisfies f ( x y ) = f ( y x ) , {\displaystyle f(xy)=f(yx),} then f {\displaystyle f} and tr {\displaystyle \operatorname {tr} } are proportional.

[ note 3 ] For n × × n {\displaystyle n\times n} matrices, imposing the normalization f ( I ) = n {\displaystyle f(\mathbf {I} )=n} makes f {\displaystyle f} equal to the trace.

Trace as the sum of eigenvalues [ edit ] Given any n × n matrix A , there is tr ⁡ ⁡ ( A ) = ∑ ∑ i = 1 n λ λ i {\displaystyle \operatorname {tr} (\mathbf {A} )=\sum _{i=1}^{n}\lambda _{i}} where λ 1 , ..., λ n are the eigenvalues of A counted with multiplicity. This holds true even if A is a real matrix and some (or all) of the eigenvalues are complex numbers. This may be regarded as a consequence of the existence of the Jordan canonical form , together with the similarity-invariance of the trace discussed above.

Trace of commutator [ edit ] When both A and B are n × n matrices, the trace of the (ring-theoretic) commutator of A and B vanishes: tr([ A , B ]) = 0 , because tr( AB ) = tr( BA ) and tr is linear. One can state this as "the trace is a map of Lie algebras gl n → k from operators to scalars", as the commutator of scalars is trivial (it is an Abelian Lie algebra ). In particular, using similarity invariance, it follows that the identity matrix is never similar to the commutator of any pair of matrices.

Conversely, any square matrix with zero trace is a linear combination of the commutators of pairs of matrices.

[ note 4 ] Moreover, any square matrix with zero trace is unitarily equivalent to a square matrix with diagonal consisting of all zeros.

Traces of special kinds of matrices [ edit ] The trace of the n × n identity matrix is the dimension of the space, namely n .

tr ⁡ ⁡ ( I n ) = n {\displaystyle \operatorname {tr} \left(\mathbf {I} _{n}\right)=n} This leads to generalizations of dimension using trace .

The trace of a Hermitian matrix is real, because the elements on the diagonal are real.

The trace of a permutation matrix is the number of fixed points of the corresponding permutation, because the diagonal term a ii is 1 if the i th point is fixed and 0 otherwise.

The trace of a projection matrix is the dimension of the target space.

P X = X ( X T X ) − − 1 X T ⟹ ⟹ tr ⁡ ⁡ ( P X ) = rank ⁡ ⁡ ( X ) .

{\displaystyle {\begin{aligned}\mathbf {P} _{\mathbf {X} }&=\mathbf {X} \left(\mathbf {X} ^{\mathsf {T}}\mathbf {X} \right)^{-1}\mathbf {X} ^{\mathsf {T}}\\[3pt]\Longrightarrow \operatorname {tr} \left(\mathbf {P} _{\mathbf {X} }\right)&=\operatorname {rank} (\mathbf {X} ).\end{aligned}}} The matrix P X is idempotent.

More generally, the trace of any idempotent matrix , i.e. one with A 2 = A , equals its own rank .

The trace of a nilpotent matrix is zero.

When the characteristic of the base field is zero, the converse also holds: if tr( A k ) = 0 for all k , then A is nilpotent.

When the characteristic n > 0 is positive, the identity in n dimensions is a counterexample, as tr ⁡ ⁡ ( I n k ) = tr ⁡ ⁡ ( I n ) = n ≡ ≡ 0 {\displaystyle \operatorname {tr} \left(\mathbf {I} _{n}^{k}\right)=\operatorname {tr} \left(\mathbf {I} _{n}\right)=n\equiv 0} , but the identity is not nilpotent.

Relationship to the characteristic polynomial [ edit ] The trace of an n × × n {\displaystyle n\times n} matrix A {\displaystyle A} is the coefficient of t n − − 1 {\displaystyle t^{n-1}} in the characteristic polynomial , possibly changed of sign, according to the convention in the definition of the characteristic polynomial.

Relationship to eigenvalues [ edit ] If A is a linear operator represented by a square matrix with real or complex entries and if λ 1 , ..., λ n are the eigenvalues of A (listed according to their algebraic multiplicities ), then tr ⁡ ⁡ ( A ) = ∑ ∑ i λ λ i {\displaystyle \operatorname {tr} (\mathbf {A} )=\sum _{i}\lambda _{i}} This follows from the fact that A is always similar to its Jordan form , an upper triangular matrix having λ 1 , ..., λ n on the main diagonal. In contrast, the determinant of A is the product of its eigenvalues; that is, det ( A ) = ∏ ∏ i λ λ i .

{\displaystyle \det(\mathbf {A} )=\prod _{i}\lambda _{i}.} Everything in the present section applies as well to any square matrix with coefficients in an algebraically closed field .

Derivative relationships [ edit ] If ΔA is a square matrix with small entries and I denotes the identity matrix , then we have approximately det ( I + Δ Δ A ) ≈ ≈ 1 + tr ⁡ ⁡ ( Δ Δ A ) .

{\displaystyle \det(\mathbf {I} +\mathbf {\Delta A} )\approx 1+\operatorname {tr} (\mathbf {\Delta A} ).} Precisely this means that the trace is the derivative of the determinant function at the identity matrix.

Jacobi's formula d det ( A ) = tr ⁡ ⁡ ( adj ⁡ ⁡ ( A ) ⋅ ⋅ d A ) {\displaystyle d\det(\mathbf {A} )=\operatorname {tr} {\big (}\operatorname {adj} (\mathbf {A} )\cdot d\mathbf {A} {\big )}} is more general and describes the differential of the determinant at an arbitrary square matrix, in terms of the trace and the adjugate of the matrix.

From this (or from the connection between the trace and the eigenvalues), one can derive a relation between the trace function, the matrix exponential function, and the determinant: det ( exp ⁡ ⁡ ( A ) ) = exp ⁡ ⁡ ( tr ⁡ ⁡ ( A ) ) .

{\displaystyle \det(\exp(\mathbf {A} ))=\exp(\operatorname {tr} (\mathbf {A} )).} A related characterization of the trace applies to linear vector fields . Given a matrix A , define a vector field F on R n by F ( x ) = Ax . The components of this vector field are linear functions (given by the rows of A ). Its divergence div F is a constant function, whose value is equal to tr( A ) .

By the divergence theorem , one can interpret this in terms of flows: if F ( x ) represents the velocity of a fluid at location x and U is a region in R n , the net flow of the fluid out of U is given by tr( A ) · vol( U ) , where vol( U ) is the volume of U .

The trace is a linear operator, hence it commutes with the derivative: d tr ⁡ ⁡ ( X ) = tr ⁡ ⁡ ( d X ) .

{\displaystyle d\operatorname {tr} (\mathbf {X} )=\operatorname {tr} (d\mathbf {X} ).} Trace of a linear operator [ edit ] In general, given some linear map f : V → V (where V is a finite- dimensional vector space ), we can define the trace of this map by considering the trace of a matrix representation of f , that is, choosing a basis for V and describing f as a matrix relative to this basis, and taking the trace of this square matrix. The result will not depend on the basis chosen, since different bases will give rise to similar matrices , allowing for the possibility of a basis-independent definition for the trace of a linear map.

Such a definition can be given using the canonical isomorphism between the space End( V ) of linear maps on V and V ⊗ V * , where V * is the dual space of V . Let v be in V and let g be in V * . Then the trace of the indecomposable element v ⊗ g is defined to be g ( v ) ; the trace of a general element is defined by linearity. The trace of  a linear map f : V → V can then be defined as the trace, in the above sense, of the element of V ⊗ V * corresponding to f under the above mentioned canonical isomorphism. Using an explicit basis for V and the corresponding dual basis for V * , one can show that this gives the same definition of the trace as given above.

Numerical algorithms [ edit ] Stochastic estimator [ edit ] The trace can be estimated unbiasedly by "Hutchinson's trick": [ 5 ] Given any matrix W ∈ ∈ R n × × n {\displaystyle {\boldsymbol {W}}\in \mathbb {R} ^{n\times n}} , and any random u ∈ ∈ R n {\displaystyle {\boldsymbol {u}}\in \mathbb {R} ^{n}} with E [ u u ⊺ ⊺ ] = I {\displaystyle \mathbb {E} [{\boldsymbol {u}}{\boldsymbol {u}}^{\intercal }]=\mathbf {I} } , we have E [ u ⊺ ⊺ W u ] = tr ⁡ ⁡ W {\displaystyle \mathbb {E} [{\boldsymbol {u}}^{\intercal }{\boldsymbol {W}}{\boldsymbol {u}}]=\operatorname {tr} {\boldsymbol {W}}} .

For a proof expand the expectation directly.

Usually, the random vector is sampled from N ⁡ ⁡ ( 0 , I ) {\displaystyle \operatorname {N} (\mathbf {0} ,\mathbf {I} )} (normal distribution) or { ± ± n − − 1 / 2 } n {\displaystyle \{\pm n^{-1/2}\}^{n}} ( Rademacher distribution ).

More sophisticated stochastic estimators of trace have been developed.

[ 6 ] Applications [ edit ] If a 2 x 2 real matrix has zero trace, its square is a diagonal matrix .

The trace of a 2 × 2 complex matrix is used to classify Möbius transformations . First, the matrix is normalized to make its determinant equal to one. Then, if the square of the trace is 4, the corresponding transformation is parabolic . If the square is in the interval [0,4) , it is elliptic . Finally, if the square is greater than 4, the transformation is loxodromic . See classification of Möbius transformations .

The trace is used to define characters of group representations . Two representations A , B : G → GL ( V ) of a group G are equivalent (up to change of basis on V ) if tr( A ( g )) = tr( B ( g )) for all g ∈ G .

The trace also plays a central role in the distribution of quadratic forms .

Lie algebra [ edit ] The trace is a map of Lie algebras tr : g l n → → K {\displaystyle \operatorname {tr} :{\mathfrak {gl}}_{n}\to K} from the Lie algebra g l n {\displaystyle {\mathfrak {gl}}_{n}} of linear operators on an n -dimensional space ( n × n matrices with entries in K {\displaystyle K} ) to the Lie algebra K of scalars; as K is Abelian (the Lie bracket vanishes), the fact that this is a map of Lie algebras is exactly the statement that the trace of a bracket vanishes: tr ⁡ ⁡ ( [ A , B ] ) = 0 for each A , B ∈ ∈ g l n .

{\displaystyle \operatorname {tr} ([\mathbf {A} ,\mathbf {B} ])=0{\text{ for each }}\mathbf {A} ,\mathbf {B} \in {\mathfrak {gl}}_{n}.} The kernel of this map, a matrix whose trace is zero , is often said to be traceless or trace free , and these matrices form the simple Lie algebra s l n {\displaystyle {\mathfrak {sl}}_{n}} , which is the Lie algebra of the special linear group of matrices with determinant 1. The special linear group consists of the matrices which do not change volume, while the special linear Lie algebra is the matrices which do not alter volume of infinitesimal sets.

In fact, there is an internal direct sum decomposition g l n = s l n ⊕ ⊕ K {\displaystyle {\mathfrak {gl}}_{n}={\mathfrak {sl}}_{n}\oplus K} of operators/matrices into traceless operators/matrices and scalars operators/matrices. The projection map onto scalar operators can be expressed in terms of the trace, concretely as: A ↦ ↦ 1 n tr ⁡ ⁡ ( A ) I .

{\displaystyle \mathbf {A} \mapsto {\frac {1}{n}}\operatorname {tr} (\mathbf {A} )\mathbf {I} .} Formally, one can compose the trace (the counit map) with the unit map K → → g l n {\displaystyle K\to {\mathfrak {gl}}_{n}} of "inclusion of scalars " to obtain a map g l n → → g l n {\displaystyle {\mathfrak {gl}}_{n}\to {\mathfrak {gl}}_{n}} mapping onto scalars, and multiplying by n . Dividing by n makes this a projection, yielding the formula above.

In terms of short exact sequences , one has 0 → → s l n → → g l n → → tr K → → 0 {\displaystyle 0\to {\mathfrak {sl}}_{n}\to {\mathfrak {gl}}_{n}{\overset {\operatorname {tr} }{\to }}K\to 0} which is analogous to 1 → → SL n → → GL n → → det K ∗ ∗ → → 1 {\displaystyle 1\to \operatorname {SL} _{n}\to \operatorname {GL} _{n}{\overset {\det }{\to }}K^{*}\to 1} (where K ∗ ∗ = K ∖ ∖ { 0 } {\displaystyle K^{*}=K\setminus \{0\}} ) for Lie groups . However, the trace splits naturally (via 1 / n {\displaystyle 1/n} times scalars) so g l n = s l n ⊕ ⊕ K {\displaystyle {\mathfrak {gl}}_{n}={\mathfrak {sl}}_{n}\oplus K} , but the splitting of the determinant would be as the n th root times scalars, and this does not in general define a function, so the determinant does not split and the general linear group does not decompose: GL n ≠ ≠ SL n × × K ∗ ∗ .

{\displaystyle \operatorname {GL} _{n}\neq \operatorname {SL} _{n}\times K^{*}.} Bilinear forms [ edit ] The bilinear form (where X , Y are square matrices) B ( X , Y ) = tr ⁡ ⁡ ( ad ⁡ ⁡ ( X ) ad ⁡ ⁡ ( Y ) ) {\displaystyle B(\mathbf {X} ,\mathbf {Y} )=\operatorname {tr} (\operatorname {ad} (\mathbf {X} )\operatorname {ad} (\mathbf {Y} ))} where ad ⁡ ⁡ ( X ) Y = [ X , Y ] = X Y − − Y X {\displaystyle \operatorname {ad} (\mathbf {X} )\mathbf {Y} =[\mathbf {X} ,\mathbf {Y} ]=\mathbf {X} \mathbf {Y} -\mathbf {Y} \mathbf {X} } and for orientation, if det ⁡ ⁡ Y ≠ ≠ 0 {\displaystyle \operatorname {det} \mathbf {Y} \neq 0} then ad ⁡ ⁡ ( X ) = X − − Y X Y − − 1 .

{\displaystyle \operatorname {ad} (\mathbf {X} )=\mathbf {X} -\mathbf {Y} \mathbf {X} \mathbf {Y} ^{-1}~.} B ( X , Y ) {\displaystyle B(\mathbf {X} ,\mathbf {Y} )} is called the Killing form ; it is used to classify Lie algebras .

The trace defines a bilinear form: ( X , Y ) ↦ ↦ tr ⁡ ⁡ ( X Y ) .

{\displaystyle (\mathbf {X} ,\mathbf {Y} )\mapsto \operatorname {tr} (\mathbf {X} \mathbf {Y} )~.} The form is symmetric, non-degenerate [ note 5 ] and associative in the sense that: tr ⁡ ⁡ ( X [ Y , Z ] ) = tr ⁡ ⁡ ( [ X , Y ] Z ) .

{\displaystyle \operatorname {tr} (\mathbf {X} [\mathbf {Y} ,\mathbf {Z} ])=\operatorname {tr} ([\mathbf {X} ,\mathbf {Y} ]\mathbf {Z} ).} For a complex simple Lie algebra (such as s l {\displaystyle {\mathfrak {sl}}} n ), every such bilinear form is proportional to each other; in particular, to the Killing form [ citation needed ] .

Two matrices X and Y are said to be trace orthogonal if tr ⁡ ⁡ ( X Y ) = 0.

{\displaystyle \operatorname {tr} (\mathbf {X} \mathbf {Y} )=0.} There is a generalization to a general representation ( ρ ρ , g , V ) {\displaystyle (\rho ,{\mathfrak {g}},V)} of a Lie algebra g {\displaystyle {\mathfrak {g}}} , such that ρ ρ {\displaystyle \rho } is a homomorphism of Lie algebras ρ ρ : g → → End ( V ) .

{\displaystyle \rho :{\mathfrak {g}}\rightarrow {\text{End}}(V).} The trace form tr V {\displaystyle {\text{tr}}_{V}} on End ( V ) {\displaystyle {\text{End}}(V)} is defined as above. The bilinear form ϕ ϕ ( X , Y ) = tr V ( ρ ρ ( X ) ρ ρ ( Y ) ) {\displaystyle \phi (\mathbf {X} ,\mathbf {Y} )={\text{tr}}_{V}(\rho (\mathbf {X} )\rho (\mathbf {Y} ))} is symmetric and invariant due to cyclicity.

Generalizations [ edit ] The concept of trace of a matrix is generalized to the trace class of compact operators on Hilbert spaces , and the analog of the Frobenius norm is called the Hilbert–Schmidt norm.

If K {\displaystyle K} is a trace-class operator, then for any orthonormal basis { e n } n = 1 {\displaystyle \{e_{n}\}_{n=1}} , the trace is given by tr ⁡ ⁡ ( K ) = ∑ ∑ n ⟨ e n , K e n ⟩ , {\displaystyle \operatorname {tr} (K)=\sum _{n}\left\langle e_{n},Ke_{n}\right\rangle ,} and is finite and independent of the orthonormal basis.

[ 7 ] The partial trace is another generalization of the trace that is operator-valued. The trace of a linear operator Z {\displaystyle Z} which lives on a product space A ⊗ ⊗ B {\displaystyle A\otimes B} is equal to the partial traces over A {\displaystyle A} and B {\displaystyle B} : tr ⁡ ⁡ ( Z ) = tr A ⁡ ⁡ ( tr B ⁡ ⁡ ( Z ) ) = tr B ⁡ ⁡ ( tr A ⁡ ⁡ ( Z ) ) .

{\displaystyle \operatorname {tr} (Z)=\operatorname {tr} _{A}\left(\operatorname {tr} _{B}(Z)\right)=\operatorname {tr} _{B}\left(\operatorname {tr} _{A}(Z)\right).} For more properties and a generalization of the partial trace, see traced monoidal categories .

If A {\displaystyle A} is a general associative algebra over a field k {\displaystyle k} , then a trace on A {\displaystyle A} is often defined to be any functional tr : A → → k {\displaystyle \operatorname {tr} :A\to k} which vanishes on commutators; tr ⁡ ⁡ ( [ a , b ] ) = 0 {\displaystyle \operatorname {tr} ([a,b])=0} for all a , b ∈ ∈ A {\displaystyle a,b\in A} . Such a trace is not uniquely defined; it can always at least be modified by multiplication by a nonzero scalar.

A supertrace is the generalization of a trace to the setting of superalgebras .

The operation of tensor contraction generalizes the trace to arbitrary tensors.

Gomme and Klein (2011) define a matrix trace operator trm {\displaystyle \operatorname {trm} } that operates on block matrices and use it to compute second-order perturbation solutions to dynamic economic models without the need for tensor notation .

[ 8 ] Traces in the language of tensor products [ edit ] Given a vector space V , there is a natural bilinear map V × V ∗ → F given by sending ( v , φ) to the scalar φ( v ) . The universal property of the tensor product V ⊗ V ∗ automatically implies that this bilinear map is induced by a linear functional on V ⊗ V ∗ .

[ 9 ] Similarly, there is a natural bilinear map V × V ∗ → Hom( V , V ) given by sending ( v , φ) to the linear map w ↦ φ( w ) v . The universal property of the tensor product, just as used previously, says that this bilinear map is induced by a linear map V ⊗ V ∗ → Hom( V , V ) . If V is finite-dimensional, then this linear map is a linear isomorphism .

[ 9 ] This fundamental fact is a straightforward consequence of the existence of a (finite) basis of V , and can also be phrased as saying that any linear map V → V can be written as the sum of (finitely many) rank-one linear maps. Composing the inverse of the isomorphism with the linear functional obtained above results in a linear functional on Hom( V , V ) . This linear functional is exactly the same as the trace.

Using the definition of trace as the sum of diagonal elements, the matrix formula tr( AB ) = tr( BA ) is straightforward to prove, and was given above. In the present perspective, one is considering linear maps S and T , and viewing them as sums of rank-one maps, so that there are linear functionals φ i and ψ j and nonzero vectors v i and w j such that S ( u ) = Σ φ i ( u ) v i and T ( u ) = Σ ψ j ( u ) w j for any u in V . Then ( S ∘ ∘ T ) ( u ) = ∑ ∑ i φ φ i ( ∑ ∑ j ψ ψ j ( u ) w j ) v i = ∑ ∑ i ∑ ∑ j ψ ψ j ( u ) φ φ i ( w j ) v i {\displaystyle (S\circ T)(u)=\sum _{i}\varphi _{i}\left(\sum _{j}\psi _{j}(u)w_{j}\right)v_{i}=\sum _{i}\sum _{j}\psi _{j}(u)\varphi _{i}(w_{j})v_{i}} for any u in V . The rank-one linear map u ↦ ψ j ( u ) φ i ( w j ) v i has trace ψ j ( v i ) φ i ( w j ) and so tr ⁡ ⁡ ( S ∘ ∘ T ) = ∑ ∑ i ∑ ∑ j ψ ψ j ( v i ) φ φ i ( w j ) = ∑ ∑ j ∑ ∑ i φ φ i ( w j ) ψ ψ j ( v i ) .

{\displaystyle \operatorname {tr} (S\circ T)=\sum _{i}\sum _{j}\psi _{j}(v_{i})\varphi _{i}(w_{j})=\sum _{j}\sum _{i}\varphi _{i}(w_{j})\psi _{j}(v_{i}).} Following the same procedure with S and T reversed, one finds exactly the same formula, proving that tr( S ∘ T ) equals tr( T ∘ S ) .

The above proof can be regarded as being based upon tensor products, given that the fundamental identity of End( V ) with V ⊗ V ∗ is equivalent to the expressibility of any linear map as the sum of rank-one linear maps. As such, the proof may be written in the notation of tensor products. Then one may consider the multilinear map V × V ∗ × V × V ∗ → V ⊗ V ∗ given by sending ( v , φ , w , ψ ) to φ ( w ) v ⊗ ψ . Further composition with the trace map then results in φ ( w ) ψ ( v ) , and this is unchanged if one were to have started with ( w , ψ , v , φ ) instead. One may also consider the bilinear map End( V ) × End( V ) → End( V ) given by sending ( f , g ) to the composition f ∘ g , which is then induced by a linear map End( V ) ⊗ End( V ) → End( V ) . It can be seen that this coincides with the linear map V ⊗ V ∗ ⊗ V ⊗ V ∗ → V ⊗ V ∗ . The established symmetry upon composition with the trace map then establishes the equality of the two traces.

[ 9 ] For any finite dimensional vector space V , there is a natural linear map F → V ⊗ V ' ; in the language of linear maps, it assigns to a scalar c the linear map c ⋅id V . Sometimes this is called coevaluation map , and the trace V ⊗ V ' → F is called evaluation map .

[ 9 ] These structures can be axiomatized to define categorical traces in the abstract setting of category theory .

See also [ edit ] Trace of a tensor with respect to a metric tensor Characteristic function Field trace Golden–Thompson inequality Singular trace Specht's theorem Trace class Trace identity Trace inequalities von Neumann's trace inequality Notes [ edit ] ^ This is immediate from the definition of the matrix product : tr ⁡ ⁡ ( A B ) = ∑ ∑ i = 1 m ( A B ) i i = ∑ ∑ i = 1 m ∑ ∑ j = 1 n a i j b j i = ∑ ∑ j = 1 n ∑ ∑ i = 1 m b j i a i j = ∑ ∑ j = 1 n ( B A ) j j = tr ⁡ ⁡ ( B A ) .

{\displaystyle \operatorname {tr} (\mathbf {A} \mathbf {B} )=\sum _{i=1}^{m}\left(\mathbf {A} \mathbf {B} \right)_{ii}=\sum _{i=1}^{m}\sum _{j=1}^{n}a_{ij}b_{ji}=\sum _{j=1}^{n}\sum _{i=1}^{m}b_{ji}a_{ij}=\sum _{j=1}^{n}\left(\mathbf {B} \mathbf {A} \right)_{jj}=\operatorname {tr} (\mathbf {B} \mathbf {A} ).} ^ For example, if A = ( 0 1 0 0 ) , B = ( 0 0 1 0 ) , {\displaystyle \mathbf {A} ={\begin{pmatrix}0&1\\0&0\end{pmatrix}},\quad \mathbf {B} ={\begin{pmatrix}0&0\\1&0\end{pmatrix}},} then the product is A B = ( 1 0 0 0 ) , {\displaystyle \mathbf {AB} ={\begin{pmatrix}1&0\\0&0\end{pmatrix}},} and the traces are tr( AB ) = 1 ≠ 0 ⋅ 0 = tr( A )tr( B ) .

^ Proof: Let e i j {\displaystyle e_{ij}} the standard basis and note that f ( e i j ) = f ( e i e j ⊤ ⊤ ) = f ( e i e 1 ⊤ ⊤ e 1 e j ⊤ ⊤ ) = f ( e 1 e j ⊤ ⊤ e i e 1 ⊤ ⊤ ) = f ( 0 ) = 0 {\displaystyle f\left(e_{ij}\right)=f\left(e_{i}e_{j}^{\top }\right)=f\left(e_{i}e_{1}^{\top }e_{1}e_{j}^{\top }\right)=f\left(e_{1}e_{j}^{\top }e_{i}e_{1}^{\top }\right)=f\left(0\right)=0} if i ≠ ≠ j {\displaystyle i\neq j} and f ( e j j ) = f ( e 11 ) {\displaystyle f\left(e_{jj}\right)=f\left(e_{11}\right)} f ( A ) = ∑ ∑ i , j [ A ] i j f ( e i j ) = ∑ ∑ i [ A ] i i f ( e 11 ) = f ( e 11 ) tr ⁡ ⁡ ( A ) .

{\displaystyle f(\mathbf {A} )=\sum _{i,j}[\mathbf {A} ]_{ij}f\left(e_{ij}\right)=\sum _{i}[\mathbf {A} ]_{ii}f\left(e_{11}\right)=f\left(e_{11}\right)\operatorname {tr} (\mathbf {A} ).} More abstractly, this corresponds to the decomposition g l n = s l n ⊕ ⊕ k , {\displaystyle {\mathfrak {gl}}_{n}={\mathfrak {sl}}_{n}\oplus k,} as tr ⁡ ⁡ ( A B ) = tr ⁡ ⁡ ( B A ) {\displaystyle \operatorname {tr} (AB)=\operatorname {tr} (BA)} (equivalently, tr ⁡ ⁡ ( [ A , B ] ) = 0 {\displaystyle \operatorname {tr} ([A,B])=0} ) defines the trace on s l n , {\displaystyle {\mathfrak {sl}}_{n},} which has complement the scalar matrices, and leaves one degree of freedom: any such map is determined by its value on scalars, which is one scalar parameter and hence all are multiple of the trace, a nonzero such map.

^ Proof: s l n {\displaystyle {\mathfrak {sl}}_{n}} is a semisimple Lie algebra and thus every element in it is a linear combination of commutators of some pairs of elements, otherwise the derived algebra would be a proper ideal.

^ This follows from the fact that tr( A * A ) = 0 if and only if A = 0 .

References [ edit ] ^ a b c d e "Rank, trace, determinant, transpose, and inverse of matrices" .

fourier.eng.hmc.edu . Retrieved 2020-09-09 .

^ a b c d Weisstein, Eric W.

(2003) [1999].

"Trace (matrix)" . In Weisstein, Eric W. (ed.).

CRC Concise Encyclopedia of Mathematics (2nd ed.). Boca Raton, FL: Chapman & Hall .

doi : 10.1201/9781420035223 .

ISBN 1-58488-347-2 .

MR 1944431 .

Zbl 1079.00009 . Retrieved 2020-09-09 .

^ a b c d Lipschutz, Seymour; Lipson, Marc (September 2005).

Theory and Problems of Linear Algebra . Schaum's Outline. McGraw-Hill.

ISBN 9780070605022 .

^ Horn, Roger A.; Johnson, Charles R. (2013).

Matrix Analysis (2nd ed.). Cambridge University Press.

ISBN 9780521839402 .

^ Hutchinson, M.F. (January 1989).

"A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian Smoothing Splines" .

Communications in Statistics - Simulation and Computation .

18 (3): 1059– 1076.

doi : 10.1080/03610918908812806 .

ISSN 0361-0918 .

^ Avron, Haim; Toledo, Sivan (2011-04-11).

"Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix" .

Journal of the ACM .

58 (2): 8:1–8:34.

doi : 10.1145/1944345.1944349 .

ISSN 0004-5411 .

S2CID 5827717 .

^ Teschl, G. (30 October 2014).

Mathematical Methods in Quantum Mechanics . Graduate Studies in Mathematics. Vol. 157 (2nd ed.). American Mathematical Society.

ISBN 978-1470417048 .

^ P. Gomme, P. Klein (2011). "Second-order approximation of dynamic models without the use of tensors".

Journal of Economic Dynamics & Control .

35 (4): 604– 615.

doi : 10.1016/j.jedc.2010.10.006 .

^ a b c d Kassel, Christian (1995).

Quantum groups .

Graduate Texts in Mathematics . Vol. 155. New York: Springer-Verlag .

doi : 10.1007/978-1-4612-0783-2 .

ISBN 0-387-94370-6 .

MR 1321145 .

Zbl 0808.17003 .

Gantmacher, F.R.

(1959).

The Theory of Matrices . Translated by Hirsch, K.A.

New York, NY: Chelsea Publishing Company .

MR 0107649 .

Horn, R.A.

; Johnson, C.R.

(2013) [1985].

Matrix Analysis (2nd ed.). Cambridge, UK: Cambridge University Press .

ISBN 978-0-521-54823-6 .

MR 2978290 .

Strang, G.

(2004) [1976].

Linear Algebra and its Applications (4th ed.).

Cengage Learning .

ISBN 978-003010567-8 .

External links [ edit ] "Trace of a square matrix" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐lwtfr
Cached time: 20250812014227
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.610 seconds
Real time usage: 1.013 seconds
Preprocessor visited node count: 10522/1000000
Revision size: 37954/2097152 bytes
Post‐expand include size: 82826/2097152 bytes
Template argument size: 14591/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 58651/5000000 bytes
Lua time usage: 0.256/10.000 seconds
Lua memory usage: 7578514/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  550.664      1 -total
 24.79%  136.508      2 Template:Reflist
 21.60%  118.958    140 Template:Math
 15.97%   87.939      1 Template:Short_description
 12.61%   69.438      1 Template:Cite_web
 11.57%   63.727      2 Template:Pagetype
 10.17%   55.990      2 Template:Ambox
  9.74%   53.633      1 Template:More_citations_needed
  7.40%   40.769      5 Template:Rp
  6.62%   36.437      5 Template:R/superscript Saved in parser cache with key enwiki:pcache:43270:|#|:idhash:canonical and timestamp 20250812014227 and revision id 1303394342. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Trace_(linear_algebra)&oldid=1303394342 " Categories : Linear algebra Matrix theory Trace theory Hidden categories: Articles with short description Short description is different from Wikidata Articles needing additional references from November 2023 All articles needing additional references Wikipedia articles with style issues from July 2025 All articles with style issues All articles with unsourced statements Articles with unsourced statements from June 2022 This page was last edited on 30 July 2025, at 18:57 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Trace (linear algebra) 40 languages Add topic

