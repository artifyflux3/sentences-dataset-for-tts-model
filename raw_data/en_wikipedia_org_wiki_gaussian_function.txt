Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Properties 2 Integral of a Gaussian function Toggle Integral of a Gaussian function subsection 2.1 Relation to standard Gaussian integral 3 Two-dimensional Gaussian function Toggle Two-dimensional Gaussian function subsection 3.1 Meaning of parameters for the general equation 3.2 Higher-order Gaussian or super-Gaussian function or generalized Gaussian function 4 Multi-dimensional Gaussian function 5 Estimation of parameters Toggle Estimation of parameters subsection 5.1 Parameter precision 6 Discrete Gaussian 7 Applications 8 See also 9 References 10 Further reading 11 External links Toggle the table of contents Gaussian function 27 languages العربية Català Чӑвашла Čeština Deutsch Español Esperanto فارسی Français 한국어 Bahasa Indonesia Italiano עברית Lietuvių Nederlands 日本語 Português Русский Shqip Slovenčina کوردی ไทย Türkçe Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Mathematical function "Gaussian curve" redirects here. For the band, see Gaussian Curve (band) .

This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Gaussian function" – news · newspapers · books · scholar · JSTOR ( August 2009 ) ( Learn how and when to remove this message ) In mathematics , a Gaussian function , often simply referred to as a Gaussian , is a function of the base form f ( x ) = exp ⁡ ⁡ ( − − x 2 ) {\displaystyle f(x)=\exp(-x^{2})} and with parametric extension f ( x ) = a exp ⁡ ⁡ ( − − ( x − − b ) 2 2 c 2 ) {\displaystyle f(x)=a\exp \left(-{\frac {(x-b)^{2}}{2c^{2}}}\right)} for arbitrary real constants a , b and non-zero c . It is named after the mathematician Carl Friedrich Gauss . The graph of a Gaussian is a characteristic symmetric " bell curve " shape. The parameter a is the height of the curve's peak, b is the position of the center of the peak, and c (the standard deviation , sometimes called the Gaussian RMS width) controls the width of the "bell".

Gaussian functions are often used to represent the probability density function of a normally distributed random variable with expected value μ = b and variance σ 2 = c 2 . In this case, the Gaussian is of the form [ 1 ] g ( x ) = 1 σ σ 2 π π exp ⁡ ⁡ ( − − 1 2 ( x − − μ μ ) 2 σ σ 2 ) .

{\displaystyle g(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}\exp \left(-{\frac {1}{2}}{\frac {(x-\mu )^{2}}{\sigma ^{2}}}\right).} Gaussian functions are widely used in statistics to describe the normal distributions , in signal processing to define Gaussian filters , in image processing where two-dimensional Gaussians are used for Gaussian blurs , and in mathematics to solve heat equations and diffusion equations and to define the Weierstrass transform . They are also abundantly used in quantum chemistry to form basis sets .

Properties [ edit ] Gaussian functions arise by composing the exponential function with a concave quadratic function : f ( x ) = exp ⁡ ⁡ ( α α x 2 + β β x + γ γ ) , {\displaystyle f(x)=\exp(\alpha x^{2}+\beta x+\gamma ),} where α α = − − 1 / 2 c 2 , {\displaystyle \alpha =-1/2c^{2},} β β = b / c 2 , {\displaystyle \beta =b/c^{2},} γ γ = ln ⁡ ⁡ a − − ( b 2 / 2 c 2 ) .

{\displaystyle \gamma =\ln a-(b^{2}/2c^{2}).} (Note: a = 1 / ( σ σ 2 π π ) {\displaystyle a=1/(\sigma {\sqrt {2\pi }})} in ln ⁡ ⁡ a {\displaystyle \ln a} , not to be confused with α α = − − 1 / 2 c 2 {\displaystyle \alpha =-1/2c^{2}} ) The Gaussian functions are thus those functions whose logarithm is a concave quadratic function.

The parameter c is related to the full width at half maximum (FWHM) of the peak according to FWHM = 2 2 ln ⁡ ⁡ 2 c ≈ ≈ 2.35482 c .

{\displaystyle {\text{FWHM}}=2{\sqrt {2\ln 2}}\,c\approx 2.35482\,c.} The function may then be expressed in terms of the FWHM, represented by w : f ( x ) = a e − − 4 ( ln ⁡ ⁡ 2 ) ( x − − b ) 2 / w 2 .

{\displaystyle f(x)=ae^{-4(\ln 2)(x-b)^{2}/w^{2}}.} Alternatively, the parameter c can be interpreted by saying that the two inflection points of the function occur at x = b ± c .

The full width at tenth of maximum (FWTM) for a Gaussian could be of interest and is FWTM = 2 2 ln ⁡ ⁡ 10 c ≈ ≈ 4.29193 c .

{\displaystyle {\text{FWTM}}=2{\sqrt {2\ln 10}}\,c\approx 4.29193\,c.} Gaussian functions are analytic , and their limit as x → ∞ is 0 (for the above case of b = 0 ).

Gaussian functions are among those functions that are elementary but lack elementary antiderivatives ; the integral of the Gaussian function is the error function : ∫ ∫ e − − x 2 d x = π π 2 erf ⁡ ⁡ x + C .

{\displaystyle \int e^{-x^{2}}\,dx={\frac {\sqrt {\pi }}{2}}\operatorname {erf} x+C.} Nonetheless, their improper integrals over the whole real line can be evaluated exactly, using the Gaussian integral ∫ ∫ − − ∞ ∞ ∞ ∞ e − − x 2 d x = π π , {\displaystyle \int _{-\infty }^{\infty }e^{-x^{2}}\,dx={\sqrt {\pi }},} and one obtains ∫ ∫ − − ∞ ∞ ∞ ∞ a e − − ( x − − b ) 2 / ( 2 c 2 ) d x = a c ⋅ ⋅ 2 π π .

{\displaystyle \int _{-\infty }^{\infty }ae^{-(x-b)^{2}/(2c^{2})}\,dx=ac\cdot {\sqrt {2\pi }}.} Normalized Gaussian curves with expected value μ and variance σ 2 . The corresponding parameters are a = 1 σ σ 2 π π {\textstyle a={\tfrac {1}{\sigma {\sqrt {2\pi }}}}} , b = μ and c = σ .

This integral is 1 if and only if a = 1 c 2 π π {\textstyle a={\tfrac {1}{c{\sqrt {2\pi }}}}} (the normalizing constant ), and in this case the Gaussian is the probability density function of a normally distributed random variable with expected value μ = b and variance σ 2 = c 2 : g ( x ) = 1 σ σ 2 π π exp ⁡ ⁡ ( − − ( x − − μ μ ) 2 2 σ σ 2 ) .

{\displaystyle g(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}\exp \left({\frac {-(x-\mu )^{2}}{2\sigma ^{2}}}\right).} These Gaussians are plotted in the accompanying figure.

The product of two Gaussian functions is a Gaussian, and the convolution of two Gaussian functions is also a Gaussian, with variance being the sum of the original variances: c 2 = c 1 2 + c 2 2 {\displaystyle c^{2}=c_{1}^{2}+c_{2}^{2}} . The product of two Gaussian probability density functions (PDFs), though, is not in general a Gaussian PDF.

The Fourier uncertainty principle becomes an equality if and only if (modulated) Gaussian functions are considered.

[ 2 ] Taking the Fourier transform (unitary, angular-frequency convention) of a Gaussian function with parameters a = 1 , b = 0 and c yields another Gaussian function, with parameters c {\displaystyle c} , b = 0 and 1 / c {\displaystyle 1/c} .

[ 3 ] So in particular the Gaussian functions with b = 0 and c = 1 {\displaystyle c=1} are kept fixed by the Fourier transform (they are eigenfunctions of the Fourier transform with eigenvalue 1).
A physical realization is that of the diffraction pattern : for example, a photographic slide whose transmittance has a Gaussian variation is also a Gaussian function.

The fact that the Gaussian function is an eigenfunction of the continuous Fourier transform allows us to derive the following interesting [ clarification needed ] identity from the Poisson summation formula : ∑ ∑ k ∈ ∈ Z exp ⁡ ⁡ ( − − π π ⋅ ⋅ ( k c ) 2 ) = c ⋅ ⋅ ∑ ∑ k ∈ ∈ Z exp ⁡ ⁡ ( − − π π ⋅ ⋅ ( k c ) 2 ) .

{\displaystyle \sum _{k\in \mathbb {Z} }\exp \left(-\pi \cdot \left({\frac {k}{c}}\right)^{2}\right)=c\cdot \sum _{k\in \mathbb {Z} }\exp \left(-\pi \cdot (kc)^{2}\right).} Integral of a Gaussian function [ edit ] The integral of an arbitrary Gaussian function is ∫ ∫ − − ∞ ∞ ∞ ∞ a e − − ( x − − b ) 2 / 2 c 2 d x = a | c | 2 π π .

{\displaystyle \int _{-\infty }^{\infty }a\,e^{-(x-b)^{2}/2c^{2}}\,dx=\ a\,|c|\,{\sqrt {2\pi }}.} An alternative form is ∫ ∫ − − ∞ ∞ ∞ ∞ k e − − f x 2 + g x + h d x = ∫ ∫ − − ∞ ∞ ∞ ∞ k e − − f ( x − − g / ( 2 f ) ) 2 + g 2 / ( 4 f ) + h d x = k π π f exp ⁡ ⁡ ( g 2 4 f + h ) , {\displaystyle \int _{-\infty }^{\infty }k\,e^{-fx^{2}+gx+h}\,dx=\int _{-\infty }^{\infty }k\,e^{-f{\big (}x-g/(2f){\big )}^{2}+g^{2}/(4f)+h}\,dx=k\,{\sqrt {\frac {\pi }{f}}}\,\exp \left({\frac {g^{2}}{4f}}+h\right),} where f must be strictly positive for the integral to converge.

Relation to standard Gaussian integral [ edit ] The integral ∫ ∫ − − ∞ ∞ ∞ ∞ a e − − ( x − − b ) 2 / 2 c 2 d x {\displaystyle \int _{-\infty }^{\infty }ae^{-(x-b)^{2}/2c^{2}}\,dx} for some real constants a , b and c > 0 can be calculated by putting it into the form of a Gaussian integral . First, the constant a can simply be factored out of the integral. Next, the variable of integration is changed from x to y = x − b : a ∫ ∫ − − ∞ ∞ ∞ ∞ e − − y 2 / 2 c 2 d y , {\displaystyle a\int _{-\infty }^{\infty }e^{-y^{2}/2c^{2}}\,dy,} and then to z = y / 2 c 2 {\displaystyle z=y/{\sqrt {2c^{2}}}} : a 2 c 2 ∫ ∫ − − ∞ ∞ ∞ ∞ e − − z 2 d z .

{\displaystyle a{\sqrt {2c^{2}}}\int _{-\infty }^{\infty }e^{-z^{2}}\,dz.} Then, using the Gaussian integral identity ∫ ∫ − − ∞ ∞ ∞ ∞ e − − z 2 d z = π π , {\displaystyle \int _{-\infty }^{\infty }e^{-z^{2}}\,dz={\sqrt {\pi }},} we have ∫ ∫ − − ∞ ∞ ∞ ∞ a e − − ( x − − b ) 2 / 2 c 2 d x = a 2 π π c 2 .

{\displaystyle \int _{-\infty }^{\infty }ae^{-(x-b)^{2}/2c^{2}}\,dx=a{\sqrt {2\pi c^{2}}}.} Two-dimensional Gaussian function [ edit ] 3d plot of a Gaussian function with a two-dimensional domain Base form: f ( x , y ) = exp ⁡ ⁡ ( − − x 2 − − y 2 ) {\displaystyle f(x,y)=\exp(-x^{2}-y^{2})} In two dimensions, the power to which e is raised in the Gaussian function is any negative-definite quadratic form.  Consequently, the level sets of the Gaussian will always be ellipses.

A particular example of a two-dimensional Gaussian function is f ( x , y ) = A exp ⁡ ⁡ ( − − ( ( x − − x 0 ) 2 2 σ σ X 2 + ( y − − y 0 ) 2 2 σ σ Y 2 ) ) .

{\displaystyle f(x,y)=A\exp \left(-\left({\frac {(x-x_{0})^{2}}{2\sigma _{X}^{2}}}+{\frac {(y-y_{0})^{2}}{2\sigma _{Y}^{2}}}\right)\right).} Here the coefficient A is the amplitude, x 0 , y 0 is the center, and σ x , σ y are the x and y spreads of the blob. The figure on the right was created using A = 1, x 0 = 0, y 0 = 0, σ x = σ y = 1.

The volume under the Gaussian function is given by V = ∫ ∫ − − ∞ ∞ ∞ ∞ ∫ ∫ − − ∞ ∞ ∞ ∞ f ( x , y ) d x d y = 2 π π A σ σ X σ σ Y .

{\displaystyle V=\int _{-\infty }^{\infty }\int _{-\infty }^{\infty }f(x,y)\,dx\,dy=2\pi A\sigma _{X}\sigma _{Y}.} In general, a two-dimensional elliptical Gaussian function is expressed as f ( x , y ) = A exp ⁡ ⁡ ( − − ( a ( x − − x 0 ) 2 + 2 b ( x − − x 0 ) ( y − − y 0 ) + c ( y − − y 0 ) 2 ) ) , {\displaystyle f(x,y)=A\exp {\Big (}-{\big (}a(x-x_{0})^{2}+2b(x-x_{0})(y-y_{0})+c(y-y_{0})^{2}{\big )}{\Big )},} where the matrix [ a b b c ] {\displaystyle {\begin{bmatrix}a&b\\b&c\end{bmatrix}}} is positive-definite .

Using this formulation, the figure on the right can be created using A = 1 , ( x 0 , y 0 ) = (0, 0) , a = c = 1/2 , b = 0 .

Meaning of parameters for the general equation [ edit ] For the general form of the equation the coefficient A is the height of the peak and ( x 0 , y 0 ) is the center of the blob.

If we set a = cos 2 ⁡ ⁡ θ θ 2 σ σ X 2 + sin 2 ⁡ ⁡ θ θ 2 σ σ Y 2 , b = − − sin ⁡ ⁡ θ θ cos ⁡ ⁡ θ θ 2 σ σ X 2 + sin ⁡ ⁡ θ θ cos ⁡ ⁡ θ θ 2 σ σ Y 2 , c = sin 2 ⁡ ⁡ θ θ 2 σ σ X 2 + cos 2 ⁡ ⁡ θ θ 2 σ σ Y 2 , {\displaystyle {\begin{aligned}a&={\frac {\cos ^{2}\theta }{2\sigma _{X}^{2}}}+{\frac {\sin ^{2}\theta }{2\sigma _{Y}^{2}}},\\b&=-{\frac {\sin \theta \cos \theta }{2\sigma _{X}^{2}}}+{\frac {\sin \theta \cos \theta }{2\sigma _{Y}^{2}}},\\c&={\frac {\sin ^{2}\theta }{2\sigma _{X}^{2}}}+{\frac {\cos ^{2}\theta }{2\sigma _{Y}^{2}}},\end{aligned}}} then we rotate the blob by a positive, counter-clockwise angle θ θ {\displaystyle \theta } (for negative, clockwise rotation, invert the signs in the b coefficient).

[ 4 ] To get back the coefficients θ θ {\displaystyle \theta } , σ σ X {\displaystyle \sigma _{X}} and σ σ Y {\displaystyle \sigma _{Y}} from a {\displaystyle a} , b {\displaystyle b} and c {\displaystyle c} use θ θ = 1 2 arctan ⁡ ⁡ ( 2 b a − − c ) , θ θ ∈ ∈ [ − − 45 , 45 ] , σ σ X 2 = 1 2 ( a ⋅ ⋅ cos 2 ⁡ ⁡ θ θ + 2 b ⋅ ⋅ cos ⁡ ⁡ θ θ sin ⁡ ⁡ θ θ + c ⋅ ⋅ sin 2 ⁡ ⁡ θ θ ) , σ σ Y 2 = 1 2 ( a ⋅ ⋅ sin 2 ⁡ ⁡ θ θ − − 2 b ⋅ ⋅ cos ⁡ ⁡ θ θ sin ⁡ ⁡ θ θ + c ⋅ ⋅ cos 2 ⁡ ⁡ θ θ ) .

{\displaystyle {\begin{aligned}\theta &={\frac {1}{2}}\arctan \left({\frac {2b}{a-c}}\right),\quad \theta \in [-45,45],\\\sigma _{X}^{2}&={\frac {1}{2(a\cdot \cos ^{2}\theta +2b\cdot \cos \theta \sin \theta +c\cdot \sin ^{2}\theta )}},\\\sigma _{Y}^{2}&={\frac {1}{2(a\cdot \sin ^{2}\theta -2b\cdot \cos \theta \sin \theta +c\cdot \cos ^{2}\theta )}}.\end{aligned}}} Example rotations of Gaussian blobs can be seen in the following examples: θ θ = 0 {\displaystyle \theta =0} θ θ = − − π π / 6 {\displaystyle \theta =-\pi /6} θ θ = − − π π / 3 {\displaystyle \theta =-\pi /3} Using the following Octave code, one can easily see the effect of changing the parameters: A = 1 ; x0 = 0 ; y0 = 0 ; sigma_X = 1 ; sigma_Y = 2 ; [ X , Y ] = meshgrid ( - 5 :.

1 : 5 , - 5 :.

1 : 5 ); for theta = 0 : pi / 100 : pi a = cos ( theta ) ^ 2 / ( 2 * sigma_X ^ 2 ) + sin ( theta ) ^ 2 / ( 2 * sigma_Y ^ 2 ); b = sin ( 2 * theta ) / ( 4 * sigma_X ^ 2 ) - sin ( 2 * theta ) / ( 4 * sigma_Y ^ 2 ); c = sin ( theta ) ^ 2 / ( 2 * sigma_X ^ 2 ) + cos ( theta ) ^ 2 / ( 2 * sigma_Y ^ 2 ); Z = A * exp ( - ( a * ( X - x0 ) .^ 2 + 2 * b * ( X - x0 ) .* ( Y - y0 ) + c * ( Y - y0 ) .^ 2 )); surf ( X , Y , Z ); shading interp ; view ( - 36 , 36 ) waitforbuttonpress end Such functions are often used in image processing and in computational models of visual system function—see the articles on scale space and affine shape adaptation .

Also see multivariate normal distribution .

Higher-order Gaussian or super-Gaussian function or generalized Gaussian function [ edit ] A more general formulation of a Gaussian function with a flat-top and Gaussian fall-off can be taken by raising the content of the exponent to a power P {\displaystyle P} : f ( x ) = A exp ⁡ ⁡ ( − − ( ( x − − x 0 ) 2 2 σ σ X 2 ) P ) .

{\displaystyle f(x)=A\exp \left(-\left({\frac {(x-x_{0})^{2}}{2\sigma _{X}^{2}}}\right)^{P}\right).} This function is known as a super-Gaussian function and is often used for Gaussian beam formulation.

[ 5 ] This function may also be expressed in terms of the full width at half maximum (FWHM), represented by w : f ( x ) = A exp ⁡ ⁡ ( − − ln ⁡ ⁡ 2 ( 4 ( x − − x 0 ) 2 w 2 ) P ) .

{\displaystyle f(x)=A\exp \left(-\ln 2\left(4{\frac {(x-x_{0})^{2}}{w^{2}}}\right)^{P}\right).} In a two-dimensional formulation, a Gaussian function along x {\displaystyle x} and y {\displaystyle y} can be combined [ 6 ] with potentially different P X {\displaystyle P_{X}} and P Y {\displaystyle P_{Y}} to form a rectangular Gaussian distribution: f ( x , y ) = A exp ⁡ ⁡ ( − − ( ( x − − x 0 ) 2 2 σ σ X 2 ) P X − − ( ( y − − y 0 ) 2 2 σ σ Y 2 ) P Y ) .

{\displaystyle f(x,y)=A\exp \left(-\left({\frac {(x-x_{0})^{2}}{2\sigma _{X}^{2}}}\right)^{P_{X}}-\left({\frac {(y-y_{0})^{2}}{2\sigma _{Y}^{2}}}\right)^{P_{Y}}\right).} or an elliptical Gaussian distribution: f ( x , y ) = A exp ⁡ ⁡ ( − − ( ( x − − x 0 ) 2 2 σ σ X 2 + ( y − − y 0 ) 2 2 σ σ Y 2 ) P ) {\displaystyle f(x,y)=A\exp \left(-\left({\frac {(x-x_{0})^{2}}{2\sigma _{X}^{2}}}+{\frac {(y-y_{0})^{2}}{2\sigma _{Y}^{2}}}\right)^{P}\right)} Multi-dimensional Gaussian function [ edit ] Main article: Multivariate normal distribution In an n {\displaystyle n} -dimensional space a Gaussian function can be defined as f ( x ) = exp ⁡ ⁡ ( − − x T C x ) , {\displaystyle f(x)=\exp(-x^{\mathsf {T}}Cx),} where x = [ x 1 ⋯ ⋯ x n ] {\displaystyle x={\begin{bmatrix}x_{1}&\cdots &x_{n}\end{bmatrix}}} is a column of n {\displaystyle n} coordinates, C {\displaystyle C} is a positive-definite n × × n {\displaystyle n\times n} matrix, and T {\displaystyle {}^{\mathsf {T}}} denotes matrix transposition .

The integral of this Gaussian function over the whole n {\displaystyle n} -dimensional space is given as ∫ ∫ R n exp ⁡ ⁡ ( − − x T C x ) d x = π π n det C .

{\displaystyle \int _{\mathbb {R} ^{n}}\exp(-x^{\mathsf {T}}Cx)\,dx={\sqrt {\frac {\pi ^{n}}{\det C}}}.} It can be easily calculated by diagonalizing the matrix C {\displaystyle C} and changing the integration variables to the eigenvectors of C {\displaystyle C} .

More generally a shifted Gaussian function is defined as f ( x ) = exp ⁡ ⁡ ( − − x T C x + s T x ) , {\displaystyle f(x)=\exp(-x^{\mathsf {T}}Cx+s^{\mathsf {T}}x),} where s = [ s 1 ⋯ ⋯ s n ] {\displaystyle s={\begin{bmatrix}s_{1}&\cdots &s_{n}\end{bmatrix}}} is the shift vector and the matrix C {\displaystyle C} can be assumed to be symmetric, C T = C {\displaystyle C^{\mathsf {T}}=C} , and positive-definite. The following integrals with this function can be calculated with the same technique: ∫ ∫ R n e − − x T C x + v T x d x = π π n det C exp ⁡ ⁡ ( 1 4 v T C − − 1 v ) ≡ ≡ M .

{\displaystyle \int _{\mathbb {R} ^{n}}e^{-x^{\mathsf {T}}Cx+v^{\mathsf {T}}x}\,dx={\sqrt {\frac {\pi ^{n}}{\det {C}}}}\exp \left({\frac {1}{4}}v^{\mathsf {T}}C^{-1}v\right)\equiv {\mathcal {M}}.} ∫ ∫ R n e − − x T C x + v T x ( a T x ) d x = ( a T u ) ⋅ ⋅ M , where u = 1 2 C − − 1 v .

{\displaystyle \int _{\mathbb {R} ^{n}}e^{-x^{\mathsf {T}}Cx+v^{\mathsf {T}}x}(a^{\mathsf {T}}x)\,dx=(a^{T}u)\cdot {\mathcal {M}},{\text{ where }}u={\frac {1}{2}}C^{-1}v.} ∫ ∫ R n e − − x T C x + v T x ( x T D x ) d x = ( u T D u + 1 2 tr ⁡ ⁡ ( D C − − 1 ) ) ⋅ ⋅ M .

{\displaystyle \int _{\mathbb {R} ^{n}}e^{-x^{\mathsf {T}}Cx+v^{\mathsf {T}}x}(x^{\mathsf {T}}Dx)\,dx=\left(u^{\mathsf {T}}Du+{\frac {1}{2}}\operatorname {tr} (DC^{-1})\right)\cdot {\mathcal {M}}.} ∫ ∫ R n e − − x T C ′ x + s ′ T x ( − − ∂ ∂ ∂ ∂ x Λ Λ ∂ ∂ ∂ ∂ x ) e − − x T C x + s T x d x = ( 2 tr ⁡ ⁡ ( C ′ Λ Λ C B − − 1 ) + 4 u T C ′ Λ Λ C u − − 2 u T ( C ′ Λ Λ s + C Λ Λ s ′ ) + s ′ T Λ Λ s ) ⋅ ⋅ M , {\displaystyle {\begin{aligned}&\int _{\mathbb {R} ^{n}}e^{-x^{\mathsf {T}}C'x+s'^{\mathsf {T}}x}\left(-{\frac {\partial }{\partial x}}\Lambda {\frac {\partial }{\partial x}}\right)e^{-x^{\mathsf {T}}Cx+s^{\mathsf {T}}x}\,dx\\&\qquad =\left(2\operatorname {tr} (C'\Lambda CB^{-1})+4u^{\mathsf {T}}C'\Lambda Cu-2u^{\mathsf {T}}(C'\Lambda s+C\Lambda s')+s'^{\mathsf {T}}\Lambda s\right)\cdot {\mathcal {M}},\end{aligned}}} where u = 1 2 B − − 1 v , v = s + s ′ , B = C + C ′ .

{\textstyle u={\frac {1}{2}}B^{-1}v,\ v=s+s',\ B=C+C'.} Estimation of parameters [ edit ] See also: Normal distribution § Estimation of parameters A number of fields such as stellar photometry , Gaussian beam characterization, and emission/absorption line spectroscopy work with sampled Gaussian functions and need to accurately estimate the height, position, and width parameters of the function. There are three unknown parameters for a 1D Gaussian function ( a , b , c ) and five for a 2D Gaussian function ( A ; x 0 , y 0 ; σ σ X , σ σ Y ) {\displaystyle (A;x_{0},y_{0};\sigma _{X},\sigma _{Y})} .

The most common method for estimating the Gaussian parameters is to take the logarithm of the data and fit a parabola to the resulting data set.

[ 7 ] [ 8 ] While this provides a simple curve fitting procedure, the resulting algorithm may be biased by excessively weighting small data values, which can produce large errors in the profile estimate. One can partially compensate for this problem through weighted least squares estimation, reducing the weight of small data values, but this too can be biased by allowing the tail of the Gaussian to dominate the fit. In order to remove the bias, one can instead use an iteratively reweighted least squares procedure, in which the weights are updated at each iteration.

[ 8 ] It is also possible to perform non-linear regression directly on the data, without involving the logarithmic data transformation ; for more options, see probability distribution fitting .

Parameter precision [ edit ] Once one has an algorithm for estimating the Gaussian function parameters, it is also important to know how precise those estimates are. Any least squares estimation algorithm can provide numerical estimates for the variance of each parameter (i.e., the variance of the estimated height, position, and width of the function). One can also use Cramér–Rao bound theory to obtain an analytical expression for the lower bound on the parameter variances, given certain assumptions about the data.

[ 9 ] [ 10 ] The noise in the measured profile is either i.i.d.

Gaussian, or the noise is Poisson-distributed .

The spacing between each sampling (i.e. the distance between pixels measuring the data) is uniform.

The peak is "well-sampled", so that less than 10% of the area or volume under the peak (area if a 1D Gaussian, volume if a 2D Gaussian) lies outside the measurement region.

The width of the peak is much larger than the distance between sample locations (i.e. the detector pixels must be at least 5 times smaller than the Gaussian FWHM).

When these assumptions are satisfied, the following covariance matrix K applies for the 1D profile parameters a {\displaystyle a} , b {\displaystyle b} , and c {\displaystyle c} under i.i.d. Gaussian noise and under Poisson noise: [ 9 ] K Gauss = σ σ 2 π π δ δ X Q 2 ( 3 2 c 0 − − 1 a 0 2 c a 2 0 − − 1 a 0 2 c a 2 ) , K Poiss = 1 2 π π ( 3 a 2 c 0 − − 1 2 0 c a 0 − − 1 2 0 c 2 a ) , {\displaystyle \mathbf {K} _{\text{Gauss}}={\frac {\sigma ^{2}}{{\sqrt {\pi }}\delta _{X}Q^{2}}}{\begin{pmatrix}{\frac {3}{2c}}&0&{\frac {-1}{a}}\\0&{\frac {2c}{a^{2}}}&0\\{\frac {-1}{a}}&0&{\frac {2c}{a^{2}}}\end{pmatrix}}\ ,\qquad \mathbf {K} _{\text{Poiss}}={\frac {1}{\sqrt {2\pi }}}{\begin{pmatrix}{\frac {3a}{2c}}&0&-{\frac {1}{2}}\\0&{\frac {c}{a}}&0\\-{\frac {1}{2}}&0&{\frac {c}{2a}}\end{pmatrix}}\ ,} where δ δ X {\displaystyle \delta _{X}} is the width of the pixels used to sample the function, Q {\displaystyle Q} is the quantum efficiency of the detector, and σ σ {\displaystyle \sigma } indicates the standard deviation of the measurement noise. Thus, the individual variances for the parameters are, in the Gaussian noise case, var ⁡ ⁡ ( a ) = 3 σ σ 2 2 π π δ δ X Q 2 c var ⁡ ⁡ ( b ) = 2 σ σ 2 c δ δ X π π Q 2 a 2 var ⁡ ⁡ ( c ) = 2 σ σ 2 c δ δ X π π Q 2 a 2 {\displaystyle {\begin{aligned}\operatorname {var} (a)&={\frac {3\sigma ^{2}}{2{\sqrt {\pi }}\,\delta _{X}Q^{2}c}}\\\operatorname {var} (b)&={\frac {2\sigma ^{2}c}{\delta _{X}{\sqrt {\pi }}\,Q^{2}a^{2}}}\\\operatorname {var} (c)&={\frac {2\sigma ^{2}c}{\delta _{X}{\sqrt {\pi }}\,Q^{2}a^{2}}}\end{aligned}}} and in the Poisson noise case, var ⁡ ⁡ ( a ) = 3 a 2 2 π π c var ⁡ ⁡ ( b ) = c 2 π π a var ⁡ ⁡ ( c ) = c 2 2 π π a .

{\displaystyle {\begin{aligned}\operatorname {var} (a)&={\frac {3a}{2{\sqrt {2\pi }}\,c}}\\\operatorname {var} (b)&={\frac {c}{{\sqrt {2\pi }}\,a}}\\\operatorname {var} (c)&={\frac {c}{2{\sqrt {2\pi }}\,a}}.\end{aligned}}} For the 2D profile parameters giving the amplitude A {\displaystyle A} , position ( x 0 , y 0 ) {\displaystyle (x_{0},y_{0})} , and width ( σ σ X , σ σ Y ) {\displaystyle (\sigma _{X},\sigma _{Y})} of the profile, the following covariance matrices apply: [ 10 ] K Gauss = σ σ 2 π π δ δ X δ δ Y Q 2 ( 2 σ σ X σ σ Y 0 0 − − 1 A σ σ Y − − 1 A σ σ X 0 2 σ σ X A 2 σ σ Y 0 0 0 0 0 2 σ σ Y A 2 σ σ X 0 0 − − 1 A σ σ y 0 0 2 σ σ X A 2 σ σ y 0 − − 1 A σ σ X 0 0 0 2 σ σ Y A 2 σ σ X ) K Poisson = 1 2 π π ( 3 A σ σ X σ σ Y 0 0 − − 1 σ σ Y − − 1 σ σ X 0 σ σ X A σ σ Y 0 0 0 0 0 σ σ Y A σ σ X 0 0 − − 1 σ σ Y 0 0 2 σ σ X 3 A σ σ Y 1 3 A − − 1 σ σ X 0 0 1 3 A 2 σ σ Y 3 A σ σ X ) .

{\displaystyle {\begin{aligned}\mathbf {K} _{\text{Gauss}}={\frac {\sigma ^{2}}{\pi \delta _{X}\delta _{Y}Q^{2}}}&{\begin{pmatrix}{\frac {2}{\sigma _{X}\sigma _{Y}}}&0&0&{\frac {-1}{A\sigma _{Y}}}&{\frac {-1}{A\sigma _{X}}}\\0&{\frac {2\sigma _{X}}{A^{2}\sigma _{Y}}}&0&0&0\\0&0&{\frac {2\sigma _{Y}}{A^{2}\sigma _{X}}}&0&0\\{\frac {-1}{A\sigma _{y}}}&0&0&{\frac {2\sigma _{X}}{A^{2}\sigma _{y}}}&0\\{\frac {-1}{A\sigma _{X}}}&0&0&0&{\frac {2\sigma _{Y}}{A^{2}\sigma _{X}}}\end{pmatrix}}\\[6pt]\mathbf {K} _{\operatorname {Poisson} }={\frac {1}{2\pi }}&{\begin{pmatrix}{\frac {3A}{\sigma _{X}\sigma _{Y}}}&0&0&{\frac {-1}{\sigma _{Y}}}&{\frac {-1}{\sigma _{X}}}\\0&{\frac {\sigma _{X}}{A\sigma _{Y}}}&0&0&0\\0&0&{\frac {\sigma _{Y}}{A\sigma _{X}}}&0&0\\{\frac {-1}{\sigma _{Y}}}&0&0&{\frac {2\sigma _{X}}{3A\sigma _{Y}}}&{\frac {1}{3A}}\\{\frac {-1}{\sigma _{X}}}&0&0&{\frac {1}{3A}}&{\frac {2\sigma _{Y}}{3A\sigma _{X}}}\end{pmatrix}}.\end{aligned}}} where the individual parameter variances are given by the diagonal elements of the covariance matrix.

Discrete Gaussian [ edit ] Main article: Discrete Gaussian kernel The discrete Gaussian kernel (solid), compared with the sampled Gaussian kernel (dashed) for scales t = 0.5 , 1 , 2 , 4.

{\displaystyle t=0.5,1,2,4.} One may ask for a discrete analog to the Gaussian;
this is necessary in discrete applications, particularly digital signal processing . A simple answer is to sample the continuous Gaussian, yielding the sampled Gaussian kernel . However, this discrete function does not have the discrete analogs of the properties of the continuous function, and can lead to undesired effects, as described in the article scale space implementation .

An alternative approach is to use the discrete Gaussian kernel : [ 11 ] T ( n , t ) = e − − t I n ( t ) {\displaystyle T(n,t)=e^{-t}I_{n}(t)} where I n ( t ) {\displaystyle I_{n}(t)} denotes the modified Bessel functions of integer order.

This is the discrete analog of the continuous Gaussian in that it is the solution to the discrete diffusion equation (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.

[ 11 ] [ 12 ] Applications [ edit ] Gaussian functions appear in many contexts in the natural sciences , the social sciences , mathematics , and engineering .  Some examples include: In statistics and probability theory , Gaussian functions appear as the density function of the normal distribution , which is a limiting probability distribution of complicated sums, according to the central limit theorem .

Gaussian functions are the Green's function for the (homogeneous and isotropic) diffusion equation (and to the heat equation , which is the same thing), a partial differential equation that describes the time evolution of a mass-density under diffusion . Specifically, if the mass-density at time t =0 is given by a Dirac delta , which essentially means that the mass is initially concentrated in a single point, then the mass-distribution at time t will be given by a Gaussian function, with the parameter a being linearly related to 1/ √ t and c being linearly related to √ t ; this time-varying Gaussian is described by the heat kernel . More generally, if the initial mass-density is φ( x ), then the mass-density at later times is obtained by taking the convolution of φ with a Gaussian function. The convolution of a function with a Gaussian is also known as a Weierstrass transform .

A Gaussian function is the wave function of the ground state of the quantum harmonic oscillator .

The molecular orbitals used in computational chemistry can be linear combinations of Gaussian functions called Gaussian orbitals (see also basis set (chemistry) ).

Mathematically, the derivatives of the Gaussian function can be represented using Hermite functions . For unit variance, the n -th derivative of the Gaussian is the Gaussian function itself multiplied by the n -th Hermite polynomial , up to scale.

Consequently, Gaussian functions are also associated with the vacuum state in quantum field theory .

Gaussian beams are used in optical systems, microwave systems and lasers.

In scale space representation, Gaussian functions are used as smoothing kernels for generating multi-scale representations in computer vision and image processing . Specifically, derivatives of Gaussians ( Hermite functions ) are used as a basis for defining a large number of types of visual operations.

Gaussian functions are used to define some types of artificial neural networks .

In fluorescence microscopy a 2D Gaussian function is used to approximate the Airy disk , describing the intensity distribution produced by a point source .

In signal processing they serve to define Gaussian filters , such as in image processing where 2D Gaussians are used for Gaussian blurs . In digital signal processing , one uses a discrete Gaussian kernel , which may be approximated by the Binomial coefficient [ 13 ] or sampling a Gaussian.

In geostatistics they have been used for understanding the variability between the patterns of a complex training image . They are used with kernel methods to cluster the patterns in the feature space.

[ 14 ] See also [ edit ] Bell-shaped function Cauchy distribution Normal distribution Radial basis function kernel References [ edit ] ^ Squires, G. L. (2001-08-30).

Practical Physics (4 ed.). Cambridge University Press.

doi : 10.1017/cbo9781139164498 .

ISBN 978-0-521-77940-1 .

^ Folland, Gerald B.; Sitaram, Alladi (1997). "The uncertainty principle: A mathematical survey".

The Journal of Fourier Analysis and Applications .

3 (3): 207– 238.

Bibcode : 1997JFAA....3..207F .

doi : 10.1007/BF02649110 .

ISSN 1069-5869 .

^ Weisstein, Eric W.

"Fourier Transform – Gaussian" .

MathWorld . Retrieved 19 December 2013 .

^ Nawri, Nikolai.

"Berechnung von Kovarianzellipsen" (PDF) . Archived from the original (PDF) on 2019-08-14 . Retrieved 14 August 2019 .

^ Parent, A., M. Morin, and P. Lavigne. "Propagation of super-Gaussian field distributions".

Optical and Quantum Electronics 24.9 (1992): S1071–S1079.

^ "GLAD optical software commands manual, Entry on GAUSSIAN command" (PDF) .

Applied Optics Research . 2016-12-15.

^ Caruana, Richard A.; Searle, Roger B.; Heller, Thomas.; Shupack, Saul I. (1986). "Fast algorithm for the resolution of spectra".

Analytical Chemistry .

58 (6). American Chemical Society (ACS): 1162– 1167.

doi : 10.1021/ac00297a041 .

ISSN 0003-2700 .

^ a b Hongwei Guo, "A simple algorithm for fitting a Gaussian function," IEEE Sign. Proc. Mag. 28(9): 134-137 (2011).

^ a b N. Hagen, M. Kupinski, and E. L. Dereniak, "Gaussian profile estimation in one dimension," Appl. Opt. 46:5374–5383 (2007) ^ a b N. Hagen and E. L. Dereniak, "Gaussian profile estimation in two dimensions," Appl. Opt. 47:6842–6851 (2008) ^ a b Lindeberg, T., "Scale-space for discrete signals," PAMI(12), No. 3, March 1990, pp. 234–254.

^ Campbell, J, 2007, The SMM model as a boundary value problem using the discrete diffusion equation , Theor Popul Biol. 2007 Dec;72(4):539–46.

^ Haddad, R.A. and Akansu, A.N., 1991, A Class of Fast Gaussian Binomial Filters for Speech and Image processing , IEEE Trans. on Signal Processing, 39-3: 723–727 ^ Honarkhah, M and Caers, J, 2010, Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling , Mathematical Geosciences, 42: 487–517 Further reading [ edit ] Haberman, Richard (2013). "10.3.3 Inverse Fourier transform of a Gaussian".

Applied Partial Differential Equations . Boston: PEARSON.

ISBN 978-0-321-79705-6 .

External links [ edit ] Mathworld, includes a proof for the relations between c and FWHM "Integrating The Bell Curve" .

MathPages.com .

Haskell, Erlang and Perl implementation of Gaussian distribution Bensimhoun Michael, N -Dimensional Cumulative Function, And Other Useful Facts About Gaussians and Normal Densities (2009) Code for fitting Gaussians in ImageJ and Fiji.

NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐fvgbc
Cached time: 20250812013822
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.411 seconds
Real time usage: 0.602 seconds
Preprocessor visited node count: 2707/1000000
Revision size: 30922/2097152 bytes
Post‐expand include size: 32404/2097152 bytes
Template argument size: 2705/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 9/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 44120/5000000 bytes
Lua time usage: 0.200/10.000 seconds
Lua memory usage: 6193867/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  372.729      1 -total
 33.91%  126.400      1 Template:Reflist
 23.02%   85.802      2 Template:Cite_book
 21.74%   81.044      1 Template:Short_description
 15.24%   56.802      2 Template:Pagetype
 15.00%   55.910      1 Template:More_citations_needed
 13.65%   50.861      1 Template:Ambox
  8.89%   33.143      1 Template:Redirect
  5.43%   20.243     21 Template:Math
  4.87%   18.142      1 Template:Clarify Saved in parser cache with key enwiki:pcache:245552:|#|:idhash:canonical and timestamp 20250812013822 and revision id 1283951381. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Gaussian_function&oldid=1283951381 " Categories : Gaussian function Exponentials Hidden categories: Articles with short description Short description matches Wikidata Articles needing additional references from August 2009 All articles needing additional references Wikipedia articles needing clarification from August 2016 Articles containing proofs Articles with example MATLAB/Octave code This page was last edited on 4 April 2025, at 17:40 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Gaussian function 27 languages Add topic

