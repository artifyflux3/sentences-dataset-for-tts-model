Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Conditional discrete distributions Toggle Conditional discrete distributions subsection 1.1 Example 2 Conditional continuous distributions Toggle Conditional continuous distributions subsection 2.1 Example 3 Relation to independence 4 Properties 5 Measure-theoretic formulation Toggle Measure-theoretic formulation subsection 5.1 Relation to conditional expectation 5.2 Interpretation of conditioning on a Sigma Field 6 See also 7 References Toggle References subsection 7.1 Citations 7.2 Sources Toggle the table of contents Conditional probability distribution 20 languages Беларуская Català Deutsch Esperanto فارسی Italiano עברית ქართული Magyar Nederlands 日本語 Polski Português Русский Slovenščina Sunda ไทย Українська 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability theory and statistics concept This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Conditional probability distribution" – news · newspapers · books · scholar · JSTOR ( April 2013 ) ( Learn how and when to remove this message ) In probability theory and statistics , the conditional probability distribution is a probability distribution that describes the probability of an outcome given the occurrence of a particular event. Given two jointly distributed random variables X {\displaystyle X} and Y {\displaystyle Y} , the conditional probability distribution of Y {\displaystyle Y} given X {\displaystyle X} is the probability distribution of Y {\displaystyle Y} when X {\displaystyle X} is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value x {\displaystyle x} of X {\displaystyle X} as a parameter. When both X {\displaystyle X} and Y {\displaystyle Y} are categorical variables , a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable.

If the conditional distribution of Y {\displaystyle Y} given X {\displaystyle X} is a continuous distribution , then its probability density function is known as the conditional density function .

[ 1 ] The properties of a conditional distribution, such as the moments , are often referred to by corresponding names such as the conditional mean and conditional variance .

More generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.

Conditional discrete distributions [ edit ] For discrete random variables , the conditional probability mass function of Y {\displaystyle Y} given X = x {\displaystyle X=x} can be written according to its definition as: p Y | X ( y ∣ ∣ x ) ≜ ≜ P ( Y = y ∣ ∣ X = x ) = P ( { X = x } ∩ ∩ { Y = y } ) P ( X = x ) {\displaystyle p_{Y|X}(y\mid x)\triangleq P(Y=y\mid X=x)={\frac {P(\{X=x\}\cap \{Y=y\})}{P(X=x)}}\qquad } Due to the occurrence of P ( X = x ) {\displaystyle P(X=x)} in the denominator, this is defined only for non-zero (hence strictly positive) P ( X = x ) .

{\displaystyle P(X=x).} The relation with the probability distribution of X {\displaystyle X} given Y {\displaystyle Y} is: P ( Y = y ∣ ∣ X = x ) P ( X = x ) = P ( { X = x } ∩ ∩ { Y = y } ) = P ( X = x ∣ ∣ Y = y ) P ( Y = y ) .

{\displaystyle P(Y=y\mid X=x)P(X=x)=P(\{X=x\}\cap \{Y=y\})=P(X=x\mid Y=y)P(Y=y).} Example [ edit ] Consider the roll of a fair die and let X = 1 {\displaystyle X=1} if the number is even (i.e., 2, 4, or 6) and X = 0 {\displaystyle X=0} otherwise. Furthermore, let Y = 1 {\displaystyle Y=1} if the number is prime (i.e., 2, 3, or 5) and Y = 0 {\displaystyle Y=0} otherwise.

D 1 2 3 4 5 6 X 0 1 0 1 0 1 Y 0 1 1 0 1 0 Then the unconditional probability that X = 1 {\displaystyle X=1} is 3/6 = 1/2 (since there are six possible rolls of the dice, of which three are even), whereas the probability that X = 1 {\displaystyle X=1} conditional on Y = 1 {\displaystyle Y=1} is 1/3 (since there are three possible prime number rolls—2, 3, and 5—of which one is even).

Conditional continuous distributions [ edit ] Similarly for continuous random variables , the conditional probability density function of Y {\displaystyle Y} given the occurrence of the value x {\displaystyle x} of X {\displaystyle X} can be written as [ 2 ] f Y ∣ ∣ X ( y ∣ ∣ x ) = f X , Y ( x , y ) f X ( x ) {\displaystyle f_{Y\mid X}(y\mid x)={\frac {f_{X,Y}(x,y)}{f_{X}(x)}}\qquad } where f X , Y ( x , y ) {\displaystyle f_{X,Y}(x,y)} gives the joint density of X {\displaystyle X} and Y {\displaystyle Y} , while f X ( x ) {\displaystyle f_{X}(x)} gives the marginal density for X {\displaystyle X} . Also in this case it is necessary that f X ( x ) > 0 {\displaystyle f_{X}(x)>0} .

The relation with the probability distribution of X {\displaystyle X} given Y {\displaystyle Y} is given by: f Y ∣ ∣ X ( y ∣ ∣ x ) f X ( x ) = f X , Y ( x , y ) = f X | Y ( x ∣ ∣ y ) f Y ( y ) .

{\displaystyle f_{Y\mid X}(y\mid x)f_{X}(x)=f_{X,Y}(x,y)=f_{X|Y}(x\mid y)f_{Y}(y).} The concept of the conditional distribution of a continuous random variable is not as intuitive as it might seem: Borel's paradox shows that conditional probability density functions need not be invariant under coordinate transformations.

Example [ edit ] Bivariate normal joint density The graph shows a bivariate normal joint density for random variables X {\displaystyle X} and Y {\displaystyle Y} . To see the distribution of Y {\displaystyle Y} conditional on X = 70 {\displaystyle X=70} , one can first visualize the line X = 70 {\displaystyle X=70} in the X , Y {\displaystyle X,Y} plane , and then visualize the plane containing that line and perpendicular to the X , Y {\displaystyle X,Y} plane. The intersection of that plane with the joint normal density, once rescaled to give unit area under the intersection, is the relevant conditional density of Y {\displaystyle Y} .

Y ∣ ∣ X = 70 ∼ ∼ N ( μ μ Y + σ σ Y σ σ X ρ ρ ( 70 − − μ μ X ) , ( 1 − − ρ ρ 2 ) σ σ Y 2 ) .

{\displaystyle Y\mid X=70\ \sim \ {\mathcal {N}}\left(\mu _{Y}+{\frac {\sigma _{Y}}{\sigma _{X}}}\rho (70-\mu _{X}),\,(1-\rho ^{2})\sigma _{Y}^{2}\right).} Relation to independence [ edit ] Random variables X {\displaystyle X} , Y {\displaystyle Y} are independent if and only if the conditional distribution of Y {\displaystyle Y} given X {\displaystyle X} is, for all possible realizations of X {\displaystyle X} , equal to the unconditional distribution of Y {\displaystyle Y} . For discrete random variables this means P ( Y = y | X = x ) = P ( Y = y ) {\displaystyle P(Y=y|X=x)=P(Y=y)} for all possible y {\displaystyle y} and x {\displaystyle x} with P ( X = x ) > 0 {\displaystyle P(X=x)>0} . For continuous random variables X {\displaystyle X} and Y {\displaystyle Y} , having a joint density function , it means f Y ( y | X = x ) = f Y ( y ) {\displaystyle f_{Y}(y|X=x)=f_{Y}(y)} for all possible y {\displaystyle y} and x {\displaystyle x} with f X ( x ) > 0 {\displaystyle f_{X}(x)>0} .

Properties [ edit ] Seen as a function of y {\displaystyle y} for given x {\displaystyle x} , P ( Y = y | X = x ) {\displaystyle P(Y=y|X=x)} is a probability mass function and so the sum over all y {\displaystyle y} (or integral if it is a conditional probability density) is 1.  Seen as a function of x {\displaystyle x} for given y {\displaystyle y} , it is a likelihood function , so that the sum (or integral) over all x {\displaystyle x} need not be 1.

Additionally, a marginal of a joint distribution can be expressed as the expectation of the corresponding conditional distribution. For instance, p X ( x ) = E Y [ p X | Y ( x | Y ) ] {\displaystyle p_{X}(x)=E_{Y}[p_{X|Y}(x\ |\ Y)]} .

Measure-theoretic formulation [ edit ] Let ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},P)} be a probability space , G ⊆ ⊆ F {\displaystyle {\mathcal {G}}\subseteq {\mathcal {F}}} a σ σ {\displaystyle \sigma } -field in F {\displaystyle {\mathcal {F}}} . Given A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} , the Radon–Nikodym theorem implies that there is [ 3 ] a G {\displaystyle {\mathcal {G}}} -measurable random variable P ( A ∣ ∣ G ) : Ω Ω → → R {\displaystyle P(A\mid {\mathcal {G}}):\Omega \to \mathbb {R} } , called the conditional probability , such that ∫ ∫ G P ( A ∣ ∣ G ) ( ω ω ) d P ( ω ω ) = P ( A ∩ ∩ G ) {\displaystyle \int _{G}P(A\mid {\mathcal {G}})(\omega )dP(\omega )=P(A\cap G)} for every G ∈ ∈ G {\displaystyle G\in {\mathcal {G}}} , and such a random variable is uniquely defined up to sets of probability zero. A conditional probability is called regular if P ⁡ ⁡ ( ⋅ ⋅ ∣ ∣ G ) ( ω ω ) {\displaystyle \operatorname {P} (\cdot \mid {\mathcal {G}})(\omega )} is a probability measure on ( Ω Ω , F ) {\displaystyle (\Omega ,{\mathcal {F}})} for all ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } a.e.

Special cases: For the trivial sigma algebra G = { ∅ ∅ , Ω Ω } {\displaystyle {\mathcal {G}}=\{\emptyset ,\Omega \}} , the conditional probability is the constant function P ( A ∣ ∣ { ∅ ∅ , Ω Ω } ) = P ⁡ ⁡ ( A ) .

{\displaystyle \operatorname {P} \!\left(A\mid \{\emptyset ,\Omega \}\right)=\operatorname {P} (A).} If A ∈ ∈ G {\displaystyle A\in {\mathcal {G}}} ,  then P ⁡ ⁡ ( A ∣ ∣ G ) = 1 A {\displaystyle \operatorname {P} (A\mid {\mathcal {G}})=1_{A}} , the indicator function (defined below ).

Let X : Ω Ω → → E {\displaystyle X:\Omega \to E} be a ( E , E ) {\displaystyle (E,{\mathcal {E}})} -valued random variable. For each B ∈ ∈ E {\displaystyle B\in {\mathcal {E}}} , define μ μ X | G ( B | G ) = P ( X − − 1 ( B ) | G ) .

{\displaystyle \mu _{X\,|\,{\mathcal {G}}}(B\,|\,{\mathcal {G}})=\mathrm {P} (X^{-1}(B)\,|\,{\mathcal {G}}).} For any ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } , the function μ μ X | G ( ⋅ ⋅ | G ) ( ω ω ) : E → → R {\displaystyle \mu _{X\,|{\mathcal {G}}}(\cdot \,|{\mathcal {G}})(\omega ):{\mathcal {E}}\to \mathbb {R} } is called the conditional probability distribution of X {\displaystyle X} given G {\displaystyle {\mathcal {G}}} . If it is a probability measure on ( E , E ) {\displaystyle (E,{\mathcal {E}})} , then it is called regular .

For a real-valued random variable (with respect to the Borel σ σ {\displaystyle \sigma } -field R 1 {\displaystyle {\mathcal {R}}^{1}} on R {\displaystyle \mathbb {R} } ), every conditional probability distribution is regular.

[ 4 ] In this case, E [ X ∣ ∣ G ] = ∫ ∫ − − ∞ ∞ ∞ ∞ x μ μ X ∣ ∣ G ( d x , ⋅ ⋅ ) {\displaystyle E[X\mid {\mathcal {G}}]=\int _{-\infty }^{\infty }x\,\mu _{X\mid {\mathcal {G}}}(dx,\cdot )} almost surely.

Relation to conditional expectation [ edit ] For any event A ∈ ∈ F {\displaystyle A\in {\mathcal {F}}} , define the indicator function : 1 A ( ω ω ) = { 1 if ω ω ∈ ∈ A , 0 if ω ω ∉ ∉ A , {\displaystyle \mathbf {1} _{A}(\omega )={\begin{cases}1\;&{\text{if }}\omega \in A,\\0\;&{\text{if }}\omega \notin A,\end{cases}}} which is a random variable. Note that the expectation of this random variable is equal to the probability of A itself: E ⁡ ⁡ ( 1 A ) = P ⁡ ⁡ ( A ) .

{\displaystyle \operatorname {E} (\mathbf {1} _{A})=\operatorname {P} (A).\;} Given a σ σ {\displaystyle \sigma } -field G ⊆ ⊆ F {\displaystyle {\mathcal {G}}\subseteq {\mathcal {F}}} , the conditional probability P ⁡ ⁡ ( A ∣ ∣ G ) {\displaystyle \operatorname {P} (A\mid {\mathcal {G}})} is a version of the conditional expectation of the indicator function for A {\displaystyle A} : P ⁡ ⁡ ( A ∣ ∣ G ) = E ⁡ ⁡ ( 1 A ∣ ∣ G ) {\displaystyle \operatorname {P} (A\mid {\mathcal {G}})=\operatorname {E} (\mathbf {1} _{A}\mid {\mathcal {G}})\;} An expectation of a random variable with respect to a regular conditional probability is equal to its conditional expectation.

Interpretation of conditioning on a Sigma Field [ edit ] Consider the probability space ( Ω Ω , F , P ) {\displaystyle (\Omega ,{\mathcal {F}},\mathbb {P} )} and a sub-sigma field A ⊂ ⊂ F {\displaystyle {\mathcal {A}}\subset {\mathcal {F}}} .
The sub-sigma field A {\displaystyle {\mathcal {A}}} can be loosely interpreted as containing a subset of the information in F {\displaystyle {\mathcal {F}}} . For example, we might think of P ( B | A ) {\displaystyle \mathbb {P} (B|{\mathcal {A}})} as the probability of the event B {\displaystyle B} given the information in A {\displaystyle {\mathcal {A}}} .

Also recall that an event B {\displaystyle B} is independent of a sub-sigma field A {\displaystyle {\mathcal {A}}} if P ( B | A ) = P ( B ) {\displaystyle \mathbb {P} (B|A)=\mathbb {P} (B)} for all A ∈ ∈ A {\displaystyle A\in {\mathcal {A}}} . It is incorrect to conclude in general that the information in A {\displaystyle {\mathcal {A}}} does not tell us anything about the probability of event B {\displaystyle B} occurring. This can be shown with a counter-example: Consider a probability space on the unit interval , Ω Ω = [ 0 , 1 ] {\displaystyle \Omega =[0,1]} . Let G {\displaystyle {\mathcal {G}}} be the sigma-field of all countable sets and sets whose complement is countable. So each set in G {\displaystyle {\mathcal {G}}} has measure 0 {\displaystyle 0} or 1 {\displaystyle 1} and so is independent of each event in F {\displaystyle {\mathcal {F}}} . However, notice that G {\displaystyle {\mathcal {G}}} also contains all the singleton events in F {\displaystyle {\mathcal {F}}} (those sets which contain only a single ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } ). So knowing which of the events in G {\displaystyle {\mathcal {G}}} occurred is equivalent to knowing exactly which ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } occurred! So in one sense, G {\displaystyle {\mathcal {G}}} contains no information about F {\displaystyle {\mathcal {F}}} (it is independent of it), and in another sense it contains all the information in F {\displaystyle {\mathcal {F}}} .

[ 5 ] [ page needed ] See also [ edit ] Conditioning (probability) Conditional probability Regular conditional probability Bayes' theorem References [ edit ] Citations [ edit ] ^ Ross (1993) , pp. 88–91.

^ Park (2018) , p. 99.

^ Billingsley (1995) , p. 430.

^ Billingsley (1995) , p. 439.

^ Billingsley (2012) .

Sources [ edit ] Billingsley, Patrick (1995).

Probability and Measure (3rd ed.). New York: John Wiley and Sons.

ISBN 0-471-00710-2 .

Billingsley, Patrick (2012).

Probability and Measure (Anniversary ed.). Hoboken, New Jersey: Wiley.

ISBN 978-1-118-12237-2 .

Park, Kun Il (2018).

Fundamentals of Probability and Stochastic Processes with Applications to Communications . Springer.

ISBN 978-3-319-68074-3 .

Ross, Sheldon M.

(1993).

Introduction to Probability Models (5th ed.). San Diego: Academic Press.

ISBN 0-12-598455-3 .

Authority control databases : National Germany Retrieved from " https://en.wikipedia.org/w/index.php?title=Conditional_probability_distribution&oldid=1304034175 " Categories : Theory of probability distributions Conditional probability Hidden categories: Articles with short description Short description is different from Wikidata Articles needing additional references from April 2013 All articles needing additional references Wikipedia articles needing page number citations from May 2025 This page was last edited on 3 August 2025, at 15:16 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Conditional probability distribution 20 languages Add topic

