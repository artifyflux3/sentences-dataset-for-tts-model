Title: Orthogonal matrix

URL Source: https://en.wikipedia.org/wiki/Orthogonal_matrix

Published Time: 2002-10-16T00:35:27Z

Markdown Content:
In [linear algebra](https://en.wikipedia.org/wiki/Linear_algebra "Linear algebra"), an **orthogonal matrix**, or **orthonormal matrix**, is a real [square matrix](https://en.wikipedia.org/wiki/Square_matrix "Square matrix") whose columns and rows are [orthonormal](https://en.wikipedia.org/wiki/Orthonormality "Orthonormality")[vectors](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics) "Vector (mathematics and physics)").

One way to express this is ![Image 1: {\displaystyle Q^{\mathrm {T} }Q=QQ^{\mathrm {T} }=I,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bace68ad0f492ab10e5f60fdb496159d49daf2ff) where _Q_ T is the [transpose](https://en.wikipedia.org/wiki/Transpose "Transpose") of Q and I is the [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix "Identity matrix").

This leads to the equivalent characterization: a matrix Q is orthogonal if its transpose is equal to its [inverse](https://en.wikipedia.org/wiki/Invertible_matrix "Invertible matrix"): ![Image 2: {\displaystyle Q^{\mathrm {T} }=Q^{-1},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/124eea429cef65f94d69c98f933c2c174ebe3aaf) where _Q_−1 is the inverse of Q.

An orthogonal matrix Q is necessarily invertible (with inverse _Q_−1 = _Q_ T), [unitary](https://en.wikipedia.org/wiki/Unitary_matrix "Unitary matrix") (_Q_−1 = _Q_∗), where _Q_∗ is the [Hermitian adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint "Hermitian adjoint") ([conjugate transpose](https://en.wikipedia.org/wiki/Conjugate_transpose "Conjugate transpose")) of Q, and therefore [normal](https://en.wikipedia.org/wiki/Normal_matrix "Normal matrix") (_Q_∗_Q_ = _QQ_∗) over the [real numbers](https://en.wikipedia.org/wiki/Real_number "Real number"). The [determinant](https://en.wikipedia.org/wiki/Determinant "Determinant") of any orthogonal matrix is either +1 or −1. As a [linear transformation](https://en.wikipedia.org/wiki/Linear_map "Linear map"), an orthogonal matrix preserves the [inner product](https://en.wikipedia.org/wiki/Inner_product "Inner product") of vectors, and therefore acts as an [isometry](https://en.wikipedia.org/wiki/Isometry "Isometry") of [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space"), such as a [rotation](https://en.wikipedia.org/wiki/Rotation_(mathematics) "Rotation (mathematics)"), [reflection](https://en.wikipedia.org/wiki/Reflection_(mathematics) "Reflection (mathematics)") or [rotoreflection](https://en.wikipedia.org/wiki/Improper_rotation "Improper rotation"). In other words, it is a [unitary transformation](https://en.wikipedia.org/wiki/Unitary_transformation "Unitary transformation").

The set of _n_ × _n_ orthogonal matrices, under multiplication, forms the [group](https://en.wikipedia.org/wiki/Group_(mathematics) "Group (mathematics)")O(_n_), known as the [orthogonal group](https://en.wikipedia.org/wiki/Orthogonal_group "Orthogonal group"). The [subgroup](https://en.wikipedia.org/wiki/Subgroup "Subgroup")SO(_n_) consisting of orthogonal matrices with determinant +1 is called the [special orthogonal group](https://en.wikipedia.org/wiki/Orthogonal_group#special_orthogonal_group "Orthogonal group"), and each of its elements is a special orthogonal matrix. As a linear transformation, every special orthogonal matrix acts as a rotation.

[![Image 3](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Matrix_multiplication_transpose.svg/330px-Matrix_multiplication_transpose.svg.png)](https://en.wikipedia.org/wiki/File:Matrix_multiplication_transpose.svg)

Visual understanding of multiplication by the transpose of a matrix. If A is an orthogonal matrix and B is its transpose, the ij-th element of the product AA T will vanish if i≠j, because the i-th row of A is orthogonal to the j-th row of A.

An orthogonal matrix is the real specialization of a unitary matrix, and thus always a [normal matrix](https://en.wikipedia.org/wiki/Normal_matrix "Normal matrix"). Although we consider only real matrices here, the definition can be used for matrices with entries from any [field](https://en.wikipedia.org/wiki/Field_(mathematics) "Field (mathematics)"). However, orthogonal matrices arise naturally from [dot products](https://en.wikipedia.org/wiki/Dot_product "Dot product"), and for matrices of complex numbers that leads instead to the unitary requirement. Orthogonal matrices preserve the dot product,[[1]](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_note-1) so, for vectors **u** and **v** in an n-dimensional real [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space")![Image 4: {\displaystyle {\mathbf {u} }\cdot {\mathbf {v} }=\left(Q{\mathbf {u} }\right)\cdot \left(Q{\mathbf {v} }\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3522dee4e02d0da3bfe6e68991795b9736481bf4) where Q is an orthogonal matrix. To see the inner product connection, consider a vector **v** in an n-dimensional real [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space"). Written with respect to an orthonormal basis, the squared length of **v** is **v**T**v**. If a linear transformation, in matrix form _Q_**v**, preserves vector lengths, then ![Image 5: {\displaystyle {\mathbf {v} }^{\mathrm {T} }{\mathbf {v} }=(Q{\mathbf {v} })^{\mathrm {T} }(Q{\mathbf {v} })={\mathbf {v} }^{\mathrm {T} }Q^{\mathrm {T} }Q{\mathbf {v} }.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2c1b5b19f6c33702f3ef9c72204b10682708bb9c)

Thus [finite-dimensional](https://en.wikipedia.org/wiki/Dimension_(vector_space) "Dimension (vector space)") linear isometries—rotations, reflections, and their combinations—produce orthogonal matrices. The converse is also true: orthogonal matrices imply orthogonal transformations. However, linear algebra includes orthogonal transformations between spaces which may be neither finite-dimensional nor of the same dimension, and these have no orthogonal matrix equivalent.

Orthogonal matrices are important for a number of reasons, both theoretical and practical. The _n_ × _n_ orthogonal matrices form a [group](https://en.wikipedia.org/wiki/Group_(mathematics) "Group (mathematics)") under matrix multiplication, the [orthogonal group](https://en.wikipedia.org/wiki/Orthogonal_group "Orthogonal group") denoted by O(_n_), which—with its subgroups—is widely used in mathematics and the physical sciences. For example, the [point group](https://en.wikipedia.org/wiki/Point_group "Point group") of a molecule is a subgroup of O(3). Because floating point versions of orthogonal matrices have advantageous properties, they are key to many algorithms in numerical linear algebra, such as [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition "QR decomposition"). As another example, with appropriate normalization the [discrete cosine transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform "Discrete cosine transform") (used in [MP3](https://en.wikipedia.org/wiki/MP3 "MP3") compression) is represented by an orthogonal matrix.

Below are a few examples of small orthogonal matrices and possible interpretations.

Elementary constructions
------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Orthogonal_matrix&action=edit&section=3 "Edit section: Elementary constructions")]

The simplest orthogonal matrices are the 1 × 1 matrices [1] and [−1], which we can interpret as the identity and a reflection of the real line across the origin.

The 2 × 2 matrices have the form ![Image 6: {\displaystyle {\begin{bmatrix}p&t\\q&u\end{bmatrix}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb2be46b6c6bb30da41571349cc3ac14c10e6e43) which orthogonality demands satisfy the three equations ![Image 7: {\displaystyle {\begin{aligned}1&=p^{2}+t^{2},\\1&=q^{2}+u^{2},\\0&=pq+tu.\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/881f480a77250727cb659a92e4c2e94ec77d5c4d)

In consideration of the first equation, without loss of generality let _p_ = cos _θ_, _q_ = sin _θ_; then either _t_ = −_q_, _u_ = _p_ or _t_ = _q_, _u_ = −_p_. We can interpret the first case as a rotation by θ (where _θ_ = 0 is the identity), and the second as a reflection across a line at an angle of ⁠_θ_/2⁠.

![Image 8: {\displaystyle {\begin{bmatrix}\cos \theta &-\sin \theta \\\sin \theta &\cos \theta \\\end{bmatrix}}{\text{ (rotation), }}\qquad {\begin{bmatrix}\cos \theta &\sin \theta \\\sin \theta &-\cos \theta \\\end{bmatrix}}{\text{ (reflection)}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/de88a0eaf30551b9e7a141e377e07bf05acdc8f4)

The special case of the reflection matrix with _θ_ = 90° generates a reflection about the line at 45° given by _y_ = _x_ and therefore exchanges x and y; it is a [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix "Permutation matrix"), with a single 1 in each column and row (and otherwise 0): ![Image 9: {\displaystyle {\begin{bmatrix}0&1\\1&0\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3891f816e1034f79d9c21b63596bad0308f48ca5)

The identity is also a permutation matrix.

A reflection is [its own inverse](https://en.wikipedia.org/wiki/Involutory_matrix "Involutory matrix"), which implies that a reflection matrix is [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix") (equal to its transpose) as well as orthogonal. The product of two rotation matrices is a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix "Rotation matrix"), and the product of two reflection matrices is also a rotation matrix.

Regardless of the dimension, it is always possible to classify orthogonal matrices as purely rotational or not, but for 3 × 3 matrices and larger the non-rotational matrices can be more complicated than reflections. For example, ![Image 10: {\displaystyle {\begin{bmatrix}-1&0&0\\0&-1&0\\0&0&-1\end{bmatrix}}{\text{ and }}{\begin{bmatrix}0&-1&0\\1&0&0\\0&0&-1\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/28d7177a4a95ca41d1a61e54b7c1bfe24d171b3d)

represent an _[inversion](https://en.wikipedia.org/wiki/Inversion\_in\_a\_point "Inversion in a point")_ through the origin and a _[rotoinversion](https://en.wikipedia.org/wiki/Improper\_rotation "Improper rotation")_, respectively, about the z-axis.

Rotations become more complicated in higher dimensions; they can no longer be completely characterized by one angle, and may affect more than one planar subspace. It is common to describe a 3 × 3 rotation matrix in terms of an [axis and angle](https://en.wikipedia.org/wiki/Axis_and_angle "Axis and angle"), but this only works in three dimensions. Above three dimensions two or more angles are needed, each associated with a [plane of rotation](https://en.wikipedia.org/wiki/Plane_of_rotation "Plane of rotation").

However, we have elementary building blocks for permutations, reflections, and rotations that apply in general.

The most elementary permutation is a transposition, obtained from the identity matrix by exchanging two rows. Any _n_ × _n_ permutation matrix can be constructed as a product of no more than _n_ − 1 transpositions.

A [Householder reflection](https://en.wikipedia.org/wiki/Householder_reflection "Householder reflection") is constructed from a non-null vector **v** as ![Image 11: {\displaystyle Q=I-2{\frac {{\mathbf {v} }{\mathbf {v} }^{\mathrm {T} }}{{\mathbf {v} }^{\mathrm {T} }{\mathbf {v} }}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cc9118ea4d3718e1900832ac21fb89ddcae4331b)

Here the numerator is a symmetric matrix while the denominator is a number, the squared magnitude of **v**. This is a reflection in the hyperplane perpendicular to **v** (negating any vector component parallel to **v**). If **v** is a unit vector, then _Q_ = _I_ − 2**vv**T suffices. A Householder reflection is typically used to simultaneously zero the lower part of a column. Any orthogonal matrix of size _n_ × _n_ can be constructed as a product of at most n such reflections.

A [Givens rotation](https://en.wikipedia.org/wiki/Givens_rotation "Givens rotation") acts on a two-dimensional (planar) subspace spanned by two coordinate axes, rotating by a chosen angle. It is typically used to zero a single subdiagonal entry. Any rotation matrix of size _n_ × _n_ can be constructed as a product of at most ⁠_n_(_n_ − 1)/2⁠ such rotations. In the case of 3 × 3 matrices, three such rotations suffice; and by fixing the sequence we can thus describe all 3 × 3 rotation matrices (though not uniquely) in terms of the three angles used, often called [Euler angles](https://en.wikipedia.org/wiki/Euler_angles "Euler angles").

A [Jacobi rotation](https://en.wikipedia.org/wiki/Jacobi_rotation "Jacobi rotation") has the same form as a Givens rotation, but is used to zero both off-diagonal entries of a 2 × 2 symmetric submatrix.

A real square matrix is orthogonal [if and only if](https://en.wikipedia.org/wiki/If_and_only_if "If and only if") its columns form an [orthonormal basis](https://en.wikipedia.org/wiki/Orthonormal_basis "Orthonormal basis") of the [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space "Euclidean space")**R**_n_ with the ordinary Euclidean [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product"), which is the case if and only if its rows form an orthonormal basis of **R**_n_. It might be tempting to suppose a matrix with orthogonal (not orthonormal) columns would be called an orthogonal matrix, but such matrices have no special interest and no special name; they only satisfy _M_ T _M_ = _D_, with D a [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix "Diagonal matrix").

The [determinant](https://en.wikipedia.org/wiki/Determinant "Determinant") of any orthogonal matrix is +1 or −1. This follows from basic facts about determinants, as follows: ![Image 12: {\displaystyle 1=\det(I)=\det \left(Q^{\mathrm {T} }Q\right)=\det \left(Q^{\mathrm {T} }\right)\det(Q)={\bigl (}\det(Q){\bigr )}^{2}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/821f2b797f530f042297de63ec66c018ff4bc410)

The converse is not true; having a determinant of ±1 is no guarantee of orthogonality, even with orthogonal columns, as shown by the following counterexample. ![Image 13: {\displaystyle {\begin{bmatrix}2&0\\0&{\frac {1}{2}}\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8e7ca0d151e8c172a5988b902f3e509718e4740)

With permutation matrices the determinant matches the [signature](https://en.wikipedia.org/wiki/Even_and_odd_permutations "Even and odd permutations"), being +1 or −1 as the parity of the permutation is even or odd, for the determinant is an alternating function of the rows.

Stronger than the determinant restriction is the fact that an orthogonal matrix can always be [diagonalized](https://en.wikipedia.org/wiki/Diagonalizable_matrix "Diagonalizable matrix") over the [complex numbers](https://en.wikipedia.org/wiki/Complex_number "Complex number") to exhibit a full set of [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors "Eigenvalues and eigenvectors"), all of which must have (complex) [modulus](https://en.wikipedia.org/wiki/Absolute_value "Absolute value")1.

The inverse of every orthogonal matrix is again orthogonal, as is the matrix product of two orthogonal matrices. In fact, the set of all _n_ × _n_ orthogonal matrices satisfies all the axioms of a [group](https://en.wikipedia.org/wiki/Group_(mathematics) "Group (mathematics)"). It is a [compact](https://en.wikipedia.org/wiki/Compact_space "Compact space")[Lie group](https://en.wikipedia.org/wiki/Lie_group "Lie group") of dimension ⁠_n_(_n_ − 1)/2⁠, called the [orthogonal group](https://en.wikipedia.org/wiki/Orthogonal_group "Orthogonal group") and denoted by O(_n_).

The orthogonal matrices whose determinant is +1 form a [path-connected](https://en.wikipedia.org/wiki/Connected_space "Connected space")[normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup "Normal subgroup") of O(_n_) of [index](https://en.wikipedia.org/wiki/Index_of_a_subgroup "Index of a subgroup") 2, the [special orthogonal group](https://en.wikipedia.org/wiki/Special_orthogonal_group "Special orthogonal group")SO(_n_) of rotations. The [quotient group](https://en.wikipedia.org/wiki/Quotient_group "Quotient group")O(_n_)/SO(_n_) is isomorphic to O(1), with the projection map choosing [+1] or [−1] according to the determinant. Orthogonal matrices with determinant −1 do not include the identity, and so do not form a subgroup but only a [coset](https://en.wikipedia.org/wiki/Coset "Coset"); it is also (separately) connected. Thus each orthogonal group falls into two pieces; and because the projection map [splits](https://en.wikipedia.org/wiki/Exact_sequence "Exact sequence"), O(_n_) is a [semidirect product](https://en.wikipedia.org/wiki/Semidirect_product "Semidirect product") of SO(_n_) by O(1). In practical terms, a comparable statement is that any orthogonal matrix can be produced by taking a rotation matrix and possibly negating one of its columns, as we saw with 2 × 2 matrices. If n is odd, then the semidirect product is in fact a [direct product](https://en.wikipedia.org/wiki/Direct_product_of_groups "Direct product of groups"), and any orthogonal matrix can be produced by taking a rotation matrix and possibly negating all of its columns. This follows from the property of determinants that negating a column negates the determinant, and thus negating an odd (but not even) number of columns negates the determinant.

Now consider (_n_ + 1) × (_n_ + 1) orthogonal matrices with bottom right entry equal to 1. The remainder of the last column (and last row) must be zeros, and the product of any two such matrices has the same form. The rest of the matrix is an _n_ × _n_ orthogonal matrix; thus O(_n_) is a subgroup of O(_n_ + 1) (and of all higher groups).

![Image 14: {\displaystyle {\begin{bmatrix}&&&0\\&\mathrm {O} (n)&&\vdots \\&&&0\\0&\cdots &0&1\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b56f5dee34b0b194edbefc40aff7fafff46df5f5)

Since an elementary reflection in the form of a [Householder matrix](https://en.wikipedia.org/wiki/Householder_matrix "Householder matrix") can reduce any orthogonal matrix to this constrained form, a series of such reflections can bring any orthogonal matrix to the identity; thus an orthogonal group is a [reflection group](https://en.wikipedia.org/wiki/Reflection_group "Reflection group"). The last column can be fixed to any unit vector, and each choice gives a different copy of O(_n_) in O(_n_ + 1); in this way O(_n_ + 1) is a [bundle](https://en.wikipedia.org/wiki/Fiber_bundle "Fiber bundle") over the unit sphere _S_ _n_ with fiber O(_n_).

Similarly, SO(_n_) is a subgroup of SO(_n_ + 1); and any special orthogonal matrix can be generated by [Givens plane rotations](https://en.wikipedia.org/wiki/Givens_rotation "Givens rotation") using an analogous procedure. The bundle structure persists: ![Image 15: {\displaystyle \mathrm {SO} (n)\hookrightarrow \mathrm {SO} (n+1)\to S^{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ef867448a7e42fe080b6710b731c239580357800). A single rotation can produce a zero in the first row of the last column, and series of _n_ − 1 rotations will zero all but the last row of the last column of an _n_ × _n_ rotation matrix. Since the planes are fixed, each rotation has only one degree of freedom, its angle. By induction, SO(_n_) therefore has ![Image 16: {\displaystyle (n-1)+(n-2)+\cdots +1={\frac {n(n-1)}{2}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/17bd3d5d70c0edc6bd56cd4719f11eb8c3fb1772) degrees of freedom, and so does O(_n_).

Permutation matrices are simpler still; they form, not a Lie group, but only a finite group, the order [_n_!](https://en.wikipedia.org/wiki/Factorial "Factorial")[symmetric group](https://en.wikipedia.org/wiki/Symmetric_group "Symmetric group")S _n_. By the same kind of argument, S _n_ is a subgroup of S _n_ + 1. The even permutations produce the subgroup of permutation matrices of determinant +1, the order ⁠_n_!/2⁠[alternating group](https://en.wikipedia.org/wiki/Alternating_group "Alternating group").

More broadly, the effect of any orthogonal matrix separates into independent actions on orthogonal two-dimensional subspaces. That is, if Q is special orthogonal then one can always find an orthogonal matrix P, a (rotational) [change of basis](https://en.wikipedia.org/wiki/Change_of_basis "Change of basis"), that brings Q into block diagonal form:

![Image 17: {\displaystyle P^{\mathrm {T} }QP={\begin{bmatrix}R_{1}&&\\&\ddots &\\&&R_{k}\end{bmatrix}}\ (n{\text{ even}}),\ P^{\mathrm {T} }QP={\begin{bmatrix}R_{1}&&&\\&\ddots &&\\&&R_{k}&\\&&&1\end{bmatrix}}\ (n{\text{ odd}}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/57d9ea2f8d41b5d11b059520ef0f45c90605c50e)

where the matrices _R_ 1, ..., _R_ _k_ are 2 × 2 rotation matrices, and with the remaining entries zero. Exceptionally, a rotation block may be diagonal, ±_I_. Thus, negating one column if necessary, and noting that a 2 × 2 reflection diagonalizes to a +1 and −1, any orthogonal matrix can be brought to the form ![Image 18: {\displaystyle P^{\mathrm {T} }QP={\begin{bmatrix}{\begin{matrix}R_{1}&&\\&\ddots &\\&&R_{k}\end{matrix}}&0\\0&{\begin{matrix}\pm 1&&\\&\ddots &\\&&\pm 1\end{matrix}}\\\end{bmatrix}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/48acff9a2be4fb4b25b3dc63554d943592143667)

The matrices _R_ 1, ..., _R_ _k_ give conjugate pairs of eigenvalues lying on the unit circle in the [complex plane](https://en.wikipedia.org/wiki/Complex_number "Complex number"); so this decomposition confirms that all [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors "Eigenvalues and eigenvectors") have [absolute value](https://en.wikipedia.org/wiki/Absolute_value "Absolute value") 1. If n is odd, there is at least one real eigenvalue, +1 or −1; for a 3 × 3 rotation, the eigenvector associated with +1 is the rotation axis.

Suppose the entries of Q are differentiable functions of t, and that _t_ = 0 gives _Q_ = _I_. Differentiating the orthogonality condition ![Image 19: {\displaystyle Q^{\mathrm {T} }Q=I}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2e8c59b38159f56ce9ae0bf267460a83ca4b8710) yields ![Image 20: {\displaystyle {\dot {Q}}^{\mathrm {T} }Q+Q^{\mathrm {T} }{\dot {Q}}=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/61b909bde959dc3d84ec98f8d072d7d7d7a6ebdf)

Evaluation at _t_ = 0 (_Q_ = _I_) then implies ![Image 21: {\displaystyle {\dot {Q}}^{\mathrm {T} }=-{\dot {Q}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eacaf1e00697bce8b8f457d4d09c16d885474df1)

In Lie group terms, this means that the [Lie algebra](https://en.wikipedia.org/wiki/Lie_algebra "Lie algebra") of an orthogonal matrix group consists of [skew-symmetric matrices](https://en.wikipedia.org/wiki/Skew-symmetric_matrix "Skew-symmetric matrix"). Going the other direction, the [matrix exponential](https://en.wikipedia.org/wiki/Matrix_exponential "Matrix exponential") of any skew-symmetric matrix is an orthogonal matrix (in fact, special orthogonal).

For example, the three-dimensional object physics calls [angular velocity](https://en.wikipedia.org/wiki/Angular_velocity "Angular velocity") is a differential rotation, thus a vector in the Lie algebra ![Image 22: {\displaystyle {\mathfrak {so}}(3)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fb4f1d3d3bf3da64b92af1a1018ce00545808b9d) tangent to SO(3). Given **ω** = (_xθ_, _yθ_, _zθ_), with **v** = (_x_, _y_, _z_) being a unit vector, the correct skew-symmetric matrix form of **ω** is ![Image 23: {\displaystyle \Omega ={\begin{bmatrix}0&-z\theta &y\theta \\z\theta &0&-x\theta \\-y\theta &x\theta &0\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e5330b544e12714a5dc18494b17bea5a00444a4a)

The exponential of this is the orthogonal matrix for rotation around axis **v** by angle θ; setting _c_ = cos ⁠_θ_/2⁠, _s_ = sin ⁠_θ_/2⁠, ![Image 24: {\displaystyle \exp(\Omega )={\begin{bmatrix}1-2s^{2}+2x^{2}s^{2}&2xys^{2}-2zsc&2xzs^{2}+2ysc\\2xys^{2}+2zsc&1-2s^{2}+2y^{2}s^{2}&2yzs^{2}-2xsc\\2xzs^{2}-2ysc&2yzs^{2}+2xsc&1-2s^{2}+2z^{2}s^{2}\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ddd059b06ddd6a4ff7d038a1cf38c3812572521f)

Numerical linear algebra
------------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Orthogonal_matrix&action=edit&section=12 "Edit section: Numerical linear algebra")]

[Numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis "Numerical analysis") takes advantage of many of the properties of orthogonal matrices for numerical linear algebra, and they arise naturally. For example, it is often desirable to compute an orthonormal basis for a space, or an orthogonal change of bases; both take the form of orthogonal matrices. Having determinant ±1 and all eigenvalues of magnitude 1 is of great benefit for [numeric stability](https://en.wikipedia.org/wiki/Numeric_stability "Numeric stability"). One implication is that the [condition number](https://en.wikipedia.org/wiki/Condition_number "Condition number") is 1 (which is the minimum), so errors are not magnified when multiplying with an orthogonal matrix. Many algorithms use orthogonal matrices like Householder reflections and [Givens rotations](https://en.wikipedia.org/wiki/Givens_rotation "Givens rotation") for this reason. It is also helpful that, not only is an orthogonal matrix invertible, but its inverse is available essentially free, by exchanging indices.

Permutations are essential to the success of many algorithms, including the workhorse [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") with [partial pivoting](https://en.wikipedia.org/wiki/Pivot_element#Partial_and_complete_pivoting "Pivot element") (where permutations do the pivoting). However, they rarely appear explicitly as matrices; their special form allows more efficient representation, such as a list of n indices.

Likewise, algorithms using Householder and Givens matrices typically use specialized methods of multiplication and storage. For example, a Givens rotation affects only two rows of a matrix it multiplies, changing a full [multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication") of order _n_ 3 to a much more efficient order n. When uses of these reflections and rotations introduce zeros in a matrix, the space vacated is enough to store sufficient data to reproduce the transform, and to do so robustly. (Following [Stewart (1976)](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFStewart1976), we do _not_ store a rotation angle, which is both expensive and badly behaved.)

A number of important [matrix decompositions](https://en.wikipedia.org/wiki/Matrix_decomposition "Matrix decomposition") ([Golub & Van Loan 1996](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFGolubVan_Loan1996)) involve orthogonal matrices, including especially:

[QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition "QR decomposition")_M_ = _QR_, Q orthogonal, R upper triangular[Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition "Singular value decomposition")_M_ = _U_ Σ _V_ T, U and V orthogonal, Σ diagonal matrix[Eigendecomposition of a symmetric matrix](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix "Eigendecomposition of a matrix") (decomposition according to the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem "Spectral theorem"))_S_ = _Q_ Λ _Q_ T, S symmetric, Q orthogonal, Λ diagonal[Polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition "Polar decomposition")_M_ = _QS_, Q orthogonal, S symmetric positive-semidefinite
Consider an [overdetermined system of linear equations](https://en.wikipedia.org/wiki/Overdetermined_system_of_linear_equations "Overdetermined system of linear equations"), as might occur with repeated measurements of a physical phenomenon to compensate for experimental errors. Write _A_**x** = **b**, where A is _m_ × _n_, _m_>_n_. A QR decomposition reduces A to upper triangular R. For example, if A is 5 × 3 then R has the form ![Image 25: {\displaystyle R={\begin{bmatrix}\cdot &\cdot &\cdot \\0&\cdot &\cdot \\0&0&\cdot \\0&0&0\\0&0&0\end{bmatrix}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f16b007da3153faeb457c0894f402b70ec8caaca)

The [linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics) "Linear least squares (mathematics)") problem is to find the **x** that minimizes ‖_A_**x** − **b**‖, which is equivalent to projecting **b** to the subspace spanned by the columns of A. Assuming the columns of A (and hence R) are independent, the projection solution is found from _A_ T _A_**x** = _A_ T**b**. Now _A_ T _A_ is square (_n_ × _n_) and invertible, and also equal to _R_ T _R_. But the lower rows of zeros in R are superfluous in the product, which is thus already in lower-triangular upper-triangular factored form, as in [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination "Gaussian elimination") ([Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition "Cholesky decomposition")). Here orthogonality is important not only for reducing _A_ T _A_ = (_R_ T _Q_ T)_QR_ to _R_ T _R_, but also for allowing solution without magnifying numerical problems.

In the case of a linear system which is underdetermined, or an otherwise non-[invertible matrix](https://en.wikipedia.org/wiki/Invertible_matrix "Invertible matrix"), singular value decomposition (SVD) is equally useful. With A factored as _U_ Σ _V_ T, a satisfactory solution uses the Moore-Penrose [pseudoinverse](https://en.wikipedia.org/wiki/Pseudoinverse "Pseudoinverse"), _V_ Σ+_U_ T, where Σ+ merely replaces each non-zero diagonal entry with its reciprocal. Set **x** to _V_ Σ+_U_ T**b**.

The case of a square invertible matrix also holds interest. Suppose, for example, that A is a 3 × 3 rotation matrix which has been computed as the composition of numerous twists and turns. Floating point does not match the mathematical ideal of real numbers, so A has gradually lost its true orthogonality. A [Gram–Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process "Gram–Schmidt process") could [orthogonalize](https://en.wikipedia.org/wiki/Orthogonalization "Orthogonalization") the columns, but it is not the most reliable, nor the most efficient, nor the most invariant method. The [polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition "Polar decomposition") factors a matrix into a pair, one of which is the unique _closest_ orthogonal matrix to the given matrix, or one of the closest if the given matrix is singular. (Closeness can be measured by any [matrix norm](https://en.wikipedia.org/wiki/Matrix_norm "Matrix norm") invariant under an orthogonal change of basis, such as the spectral norm or the Frobenius norm.) For a near-orthogonal matrix, rapid convergence to the orthogonal factor can be achieved by a "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method "Newton's method")" approach due to [Higham (1986)](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFHigham1986) ([1990](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFHigham1990)), repeatedly averaging the matrix with its inverse transpose. [Dubrulle (1999)](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFDubrulle1999) has published an accelerated method with a convenient convergence test.

For example, consider a non-orthogonal matrix for which the simple averaging algorithm takes seven steps ![Image 26: {\displaystyle {\begin{bmatrix}3&1\\7&5\end{bmatrix}}\rightarrow {\begin{bmatrix}1.8125&0.0625\\3.4375&2.6875\end{bmatrix}}\rightarrow \cdots \rightarrow {\begin{bmatrix}0.8&-0.6\\0.6&0.8\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3190976cdd7626ac06ec3aec4ca81de3a5f39713) and which acceleration trims to two steps (with γ = 0.353553, 0.565685).

![Image 27: {\displaystyle {\begin{bmatrix}3&1\\7&5\end{bmatrix}}\rightarrow {\begin{bmatrix}1.41421&-1.06066\\1.06066&1.41421\end{bmatrix}}\rightarrow {\begin{bmatrix}0.8&-0.6\\0.6&0.8\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3ddd7b89537d8aa4ecd2f921712dee00511608e8)

Gram-Schmidt yields an inferior solution, shown by a Frobenius distance of 8.28659 instead of the minimum 8.12404.

![Image 28: {\displaystyle {\begin{bmatrix}3&1\\7&5\end{bmatrix}}\rightarrow {\begin{bmatrix}0.393919&-0.919145\\0.919145&0.393919\end{bmatrix}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3e127c2f15c68e59e205641bba37dd96999b9d62)

Some numerical applications, such as [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method "Monte Carlo method") and exploration of high-dimensional data spaces, require generation of [uniformly distributed](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous) "Uniform distribution (continuous)") random orthogonal matrices. In this context, "uniform" is defined in terms of [Haar measure](https://en.wikipedia.org/wiki/Haar_measure "Haar measure"), which essentially requires that the distribution not change if multiplied by any freely chosen orthogonal matrix. Orthogonalizing matrices with [independent](https://en.wikipedia.org/wiki/Statistical_independence "Statistical independence") uniformly distributed random entries does not result in uniformly distributed orthogonal matrices[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\_needed "Wikipedia:Citation needed")_], but the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition "QR decomposition") of independent [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution "Normal distribution") random entries does, as long as the diagonal of R contains only positive entries ([Mezzadri 2006](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFMezzadri2006)). [Stewart (1980)](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFStewart1980) replaced this with a more efficient idea that [Diaconis & Shahshahani (1987)](https://en.wikipedia.org/wiki/Orthogonal_matrix#CITEREFDiaconisShahshahani1987) later generalized as the "subgroup algorithm" (in which form it works just as well for permutations and rotations). To generate an (_n_ + 1) × (_n_ + 1) orthogonal matrix, take an _n_ × _n_ one and a uniformly distributed unit vector of dimension _n_ + 1. Construct a Householder reflection from the vector, then apply it to the smaller matrix (embedded in the larger size with a 1 at the bottom right corner).

### Nearest orthogonal matrix

[[edit](https://en.wikipedia.org/w/index.php?title=Orthogonal_matrix&action=edit&section=17 "Edit section: Nearest orthogonal matrix")]

The problem of finding the orthogonal matrix Q nearest a given matrix M is related to the [Orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem "Orthogonal Procrustes problem"). There are several different ways to get the unique solution, the simplest of which is taking the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition "Singular value decomposition") of M and replacing the singular values with ones. Another method expresses the R explicitly but requires the use of a [matrix square root](https://en.wikipedia.org/wiki/Matrix_square_root "Matrix square root"):[[2]](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_note-2)![Image 29: {\displaystyle Q=M\left(M^{\mathrm {T} }M\right)^{-{\frac {1}{2}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8be490fed1664c021f534e44e07c706ef7feadeb)

This may be combined with the Babylonian method for extracting the square root of a matrix to give a recurrence which converges to an orthogonal matrix quadratically: ![Image 30: {\displaystyle Q_{n+1}=2M\left(Q_{n}^{-1}M+M^{\mathrm {T} }Q_{n}\right)^{-1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e4c120980140cb32a74691ea3fa5b1672aec376) where _Q_ 0 = _M_.

These iterations are stable provided the [condition number](https://en.wikipedia.org/wiki/Condition_number "Condition number") of M is less than three.[[3]](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_note-3)

Using a first-order approximation of the inverse and the same initialization results in the modified iteration:

![Image 31: {\displaystyle N_{n}=Q_{n}^{\mathrm {T} }Q_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5ca0f275829df2d5c392649508c695ec9b3bf659)![Image 32: {\displaystyle P_{n}={\frac {1}{2}}Q_{n}N_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d49d32132089c440b7031d0b4fffd27cb9ec1546)![Image 33: {\displaystyle Q_{n+1}=2Q_{n}+P_{n}N_{n}-3P_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ce061cbec85e11f4da27f6318b53e8af6deaf398)

A subtle technical problem afflicts some uses of orthogonal matrices. Not only are the group components with determinant +1 and −1 not [connected](https://en.wikipedia.org/wiki/Connected_space "Connected space") to each other, even the +1 component, SO(_n_), is not [simply connected](https://en.wikipedia.org/wiki/Simply_connected_space "Simply connected space") (except for SO(1), which is trivial). Thus it is sometimes advantageous, or even necessary, to work with a [covering group](https://en.wikipedia.org/wiki/Covering_map "Covering map") of SO(_n_), the [spin group](https://en.wikipedia.org/wiki/Spinor_group "Spinor group"), Spin(_n_). Likewise, O(_n_) has covering groups, the [pin groups](https://en.wikipedia.org/wiki/Pin_group "Pin group"), Pin(_n_). For _n_> 2, Spin(_n_) is simply connected and thus the universal covering group for SO(_n_). By far the most famous example of a spin group is Spin(3), which is nothing but SU(2), or the group of unit [quaternions](https://en.wikipedia.org/wiki/Quaternion "Quaternion").

The Pin and Spin groups are found within [Clifford algebras](https://en.wikipedia.org/wiki/Clifford_algebra "Clifford algebra"), which themselves can be built from orthogonal matrices.

Rectangular matrices
--------------------

[[edit](https://en.wikipedia.org/w/index.php?title=Orthogonal_matrix&action=edit&section=19 "Edit section: Rectangular matrices")]

If Q is not a square matrix, then the conditions _Q_ T _Q_ = _I_ and _QQ_ T = _I_ are not equivalent. The condition _Q_ T _Q_ = _I_ says that the columns of Q are orthonormal. This can only happen if Q is an _m_ × _n_ matrix with _n_ ≤ _m_ (due to linear dependence). Similarly, _QQ_ T = _I_ says that the rows of Q are orthonormal, which requires _n_ ≥ _m_.

There is no standard terminology for these matrices. They are variously called "semi-orthogonal matrices", "orthonormal matrices", "orthogonal matrices", and sometimes simply "matrices with orthonormal rows/columns".

For the case _n_ ≤ _m_, matrices with orthonormal columns may be referred to as [orthogonal k-frames](https://en.wikipedia.org/wiki/K-frame "K-frame") and they are elements of the [Stiefel manifold](https://en.wikipedia.org/wiki/Stiefel_manifold "Stiefel manifold").

*   [Biorthogonal system](https://en.wikipedia.org/wiki/Biorthogonal_system "Biorthogonal system")

1.   **[^](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_ref-1)**["Paul's online math notes"](http://tutorial.math.lamar.edu/Classes/LinAlg/OrthogonalMatrix.aspx)[_[full citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citing\_sources#What\_information\_to\_include "Wikipedia:Citing sources")_], Paul Dawkins, [Lamar University](https://en.wikipedia.org/wiki/Lamar_University "Lamar University"), 2008. Theorem 3(c)
2.   **[^](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_ref-2)**["Finding the Nearest Orthonormal Matrix"](http://people.csail.mit.edu/bkph/articles/Nearest_Orthonormal_Matrix.pdf), [Berthold K.P. Horn](https://en.wikipedia.org/wiki/Berthold_K.P._Horn "Berthold K.P. Horn"), [MIT](https://en.wikipedia.org/wiki/MIT "MIT").
3.   **[^](https://en.wikipedia.org/wiki/Orthogonal_matrix#cite_ref-3)**["Newton's Method for the Matrix Square Root"](http://www.maths.manchester.ac.uk/~nareports/narep91.pdf)[Archived](https://web.archive.org/web/20110929131330/http://www.maths.manchester.ac.uk/~nareports/narep91.pdf) 2011-09-29 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine "Wayback Machine"), Nicholas J. Higham, Mathematics of Computation, Volume 46, Number 174, 1986.

*   [Diaconis, Persi](https://en.wikipedia.org/wiki/Persi_Diaconis "Persi Diaconis"); Shahshahani, Mehrdad (1987), "The subgroup algorithm for generating uniform random variables", _Probability in the Engineering and Informational Sciences_, **1**: 15–32, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1017/S0269964800000255](https://doi.org/10.1017%2FS0269964800000255), [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0269-9648](https://search.worldcat.org/issn/0269-9648), [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[122752374](https://api.semanticscholar.org/CorpusID:122752374)
*   Dubrulle, Augustin A. (1999), ["An Optimum Iteration for the Matrix Polar Decomposition"](http://etna.mcs.kent.edu/), _Electronic Transactions on Numerical Analysis_, **8**: 21–25
*   [Golub, Gene H.](https://en.wikipedia.org/wiki/Gene_H._Golub "Gene H. Golub"); [Van Loan, Charles F.](https://en.wikipedia.org/wiki/Charles_F._Van_Loan "Charles F. Van Loan") (1996), _Matrix Computations_ (3/e ed.), Baltimore: Johns Hopkins University Press, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")[978-0-8018-5414-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8018-5414-9 "Special:BookSources/978-0-8018-5414-9")
*   [Higham, Nicholas](https://en.wikipedia.org/wiki/Nicholas_Higham "Nicholas Higham") (1986), ["Computing the Polar Decomposition—with Applications"](http://eprints.maths.manchester.ac.uk/694/1/high86p.pdf)(PDF), _SIAM Journal on Scientific and Statistical Computing_, **7** (4): 1160–1174, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1137/0907079](https://doi.org/10.1137%2F0907079), [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0196-5204](https://search.worldcat.org/issn/0196-5204)
*   [Higham, Nicholas](https://en.wikipedia.org/wiki/Nicholas_Higham "Nicholas Higham"); Schreiber, Robert (July 1990), "Fast polar decomposition of an arbitrary matrix", _SIAM Journal on Scientific and Statistical Computing_, **11** (4): 648–655, [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)")[10.1.1.230.4322](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.230.4322), [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1137/0911038](https://doi.org/10.1137%2F0911038), [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0196-5204](https://search.worldcat.org/issn/0196-5204), [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[14268409](https://api.semanticscholar.org/CorpusID:14268409)[[1]](https://web.archive.org/web/20051016153437/http://www.ma.man.ac.uk/~higham/pap-mf.html)
*   [Stewart, G. W.](https://en.wikipedia.org/w/index.php?title=G._W._Stewart&action=edit&redlink=1 "G. W. Stewart (page does not exist)") (1976), "The Economical Storage of Plane Rotations", _Numerische Mathematik_, **25** (2): 137–138, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/BF01462266](https://doi.org/10.1007%2FBF01462266), [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0029-599X](https://search.worldcat.org/issn/0029-599X), [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)")[120372682](https://api.semanticscholar.org/CorpusID:120372682)
*   [Stewart, G. W.](https://en.wikipedia.org/w/index.php?title=G._W._Stewart&action=edit&redlink=1 "G. W. Stewart (page does not exist)") (1980), "The Efficient Generation of Random Orthogonal Matrices with an Application to Condition Estimators", _SIAM Journal on Numerical Analysis_, **17** (3): 403–409, [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[1980SJNA...17..403S](https://ui.adsabs.harvard.edu/abs/1980SJNA...17..403S), [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1137/0717034](https://doi.org/10.1137%2F0717034), [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")[0036-1429](https://search.worldcat.org/issn/0036-1429)
*   Mezzadri, Francesco (2006), "How to generate random matrices from the classical compact groups", _Notices of the American Mathematical Society_, **54**, [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[math-ph/0609050](https://arxiv.org/abs/math-ph/0609050), [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2006math.ph...9050M](https://ui.adsabs.harvard.edu/abs/2006math.ph...9050M)

[![Image 34](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Wikiversity_logo_2017.svg/40px-Wikiversity_logo_2017.svg.png)](https://en.wikipedia.org/wiki/File:Wikiversity_logo_2017.svg)

*   ["Orthogonal matrix"](https://www.encyclopediaofmath.org/index.php?title=Orthogonal_matrix), _[Encyclopedia of Mathematics](https://en.wikipedia.org/wiki/Encyclopedia\_of\_Mathematics "Encyclopedia of Mathematics")_, [EMS Press](https://en.wikipedia.org/wiki/European_Mathematical_Society "European Mathematical Society"), 2001 [1994]
*   [Tutorial and Interactive Program on Orthogonal Matrix](http://people.revoledu.com/kardi/tutorial/LinearAlgebra/MatrixOrthogonal.html)
