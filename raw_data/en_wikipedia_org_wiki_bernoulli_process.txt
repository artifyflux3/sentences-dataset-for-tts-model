Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition Toggle Definition subsection 1.1 Interpretation 2 Formal definition Toggle Formal definition subsection 2.1 Borel algebra 2.2 Bernoulli measure 3 Law of large numbers, binomial distribution and central limit theorem 4 Dynamical systems Toggle Dynamical systems subsection 4.1 Bernoulli shift 4.2 The 2x mod 1 map 4.3 The Cantor set 4.4 Odometer 5 Bernoulli sequence 6 Randomness extraction Toggle Randomness extraction subsection 6.1 Basic von Neumann extractor 6.2 Iterated von Neumann extractor 7 References 8 Further reading 9 External links Toggle the table of contents Bernoulli process 15 languages Català Deutsch Español Euskara فارسی Français 한국어 Italiano עברית 日本語 Polski Português Slovenščina Türkçe 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Random process of binary (boolean) random variables This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( September 2011 ) ( Learn how and when to remove this message ) Part of a series on statistics Probability theory Probability Axioms Determinism System Indeterminism Randomness Probability space Sample space Event Collectively exhaustive events Elementary event Mutual exclusivity Outcome Singleton Experiment Bernoulli trial Probability distribution Bernoulli distribution Binomial distribution Exponential distribution Normal distribution Pareto distribution Poisson distribution Probability measure Random variable Bernoulli process Continuous or discrete Expected value Variance Markov chain Observed value Random walk Stochastic process Complementary event Joint probability Marginal probability Conditional probability Independence Conditional independence Law of total probability Law of large numbers Bayes' theorem Boole's inequality Venn diagram Tree diagram v t e In probability and statistics , a Bernoulli process (named after Jacob Bernoulli ) is a finite or infinite sequence of binary random variables , so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1. The component Bernoulli variables X i are identically distributed and independent .  Prosaically, a Bernoulli process is a repeated coin flipping , possibly with an unfair coin (but with consistent unfairness).  Every variable X i in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution . Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided die); this generalization is known as the Bernoulli scheme .

The problem of determining the process, given only a limited sample of Bernoulli trials, may be called the problem of checking whether a coin is fair .

Definition [ edit ] A Bernoulli process is a finite or infinite sequence of independent random variables X 1 , X 2 , X 3 , ..., such that for each i , the value of X i is either 0 or 1; for all values of i {\textstyle i} , the probability p that X i = 1 is the same.

In other words, a Bernoulli process is a sequence of independent identically distributed Bernoulli trials .

Independence of the trials implies that the process is memoryless , in which past event frequencies have no influence on about future event probability frequencies. In most instances the true value of p is unknown, therefore we use past frequencies to assess/forecast/estimate future events & their probabilities indirectly via applying probabilistic inference upon p .

If the process is infinite, then from any point the future trials constitute a Bernoulli process identical to the whole process, the fresh-start property.

Interpretation [ edit ] The two possible values of each X i are often called "success" and "failure". Thus, when expressed as a number 0 or 1, the outcome may be called the number of successes on the i th "trial".

Two other common interpretations of the values are true or false and yes or no. Under any interpretation of the two values, the individual variables X i may be called Bernoulli trials with parameter p.

In many applications time passes between trials, as the index i increases. In effect, the trials X 1 , X 2 , ...

X i , ... happen at "points in time" 1, 2, ..., i , .... That passage of time and the associated notions of "past" and "future" are not necessary, however. Most generally, any X i and X j in the process are simply two from a set of random variables indexed by {1, 2, ..., n }, the finite cases, or by {1, 2, 3, ...}, the infinite cases.

One experiment with only two possible outcomes, often referred to as "success" and "failure", usually encoded as 1 and 0, can be modeled as a Bernoulli distribution .

[ 1 ] Several random variables and probability distributions beside the Bernoullis may be derived from the Bernoulli process: The number of successes in the first n trials, which has a binomial distribution B( n , p ) The number of failures needed to get r successes, which has a negative binomial distribution NB( r , p ) The number of failures needed to get one success, which has a geometric distribution NB(1, p ), a special case of the negative binomial distribution The negative binomial variables may be interpreted as random waiting times .

Formal definition [ edit ] The Bernoulli process can be formalized in the language of probability spaces as a random sequence  of independent realisations of a random variable that can take values of heads or tails. The state space for an individual value is denoted by 2 = { H , T } .

{\displaystyle 2=\{H,T\}.} Borel algebra [ edit ] Consider the countably infinite direct product of copies of 2 = { H , T } {\displaystyle 2=\{H,T\}} .  It is common to examine either the one-sided set Ω Ω = 2 N = { H , T } N {\displaystyle \Omega =2^{\mathbb {N} }=\{H,T\}^{\mathbb {N} }} or the two-sided set Ω Ω = 2 Z {\displaystyle \Omega =2^{\mathbb {Z} }} . There is a natural topology on this space, called the product topology . The sets in this topology are finite sequences of coin flips, that is, finite-length strings of H and T ( H stands for heads and T stands for tails), with the rest of (infinitely long) sequence taken as "don't care".  These sets of finite sequences are referred to as cylinder sets in the product topology.  The set of all such strings forms a sigma algebra , specifically, a Borel algebra .  This algebra is then commonly written as ( Ω Ω , B ) {\displaystyle (\Omega ,{\mathcal {B}})} where the elements of B {\displaystyle {\mathcal {B}}} are the finite-length sequences of coin flips (the cylinder sets).

Bernoulli measure [ edit ] If the chances of flipping heads or tails are given by the probabilities { p , 1 − − p } {\displaystyle \{p,1-p\}} , then one can define a natural measure on the product space, given by P = { p , 1 − − p } N {\displaystyle P=\{p,1-p\}^{\mathbb {N} }} (or by P = { p , 1 − − p } Z {\displaystyle P=\{p,1-p\}^{\mathbb {Z} }} for the two-sided process).  In another word, if a discrete random variable X has a Bernoulli distribution with parameter p , where 0 ≤ p ≤ 1, and its probability mass function is given by p X ( 1 ) = P ( X = 1 ) = p {\displaystyle pX(1)=P(X=1)=p} and p X ( 0 ) = P ( X = 0 ) = 1 − − p {\displaystyle pX(0)=P(X=0)=1-p} .

We denote this distribution by Ber( p ).

[ 1 ] Given a cylinder set, that is, a specific sequence of coin flip results [ ω ω 1 , ω ω 2 , ⋯ ⋯ ω ω n ] {\displaystyle [\omega _{1},\omega _{2},\cdots \omega _{n}]} at times 1 , 2 , ⋯ ⋯ , n {\displaystyle 1,2,\cdots ,n} , the probability of observing this particular sequence is given by P ( [ ω ω 1 , ω ω 2 , ⋯ ⋯ , ω ω n ] ) = p k ( 1 − − p ) n − − k {\displaystyle P([\omega _{1},\omega _{2},\cdots ,\omega _{n}])=p^{k}(1-p)^{n-k}} where k is the number of times that H appears in the sequence, and n − k is the number of times that T appears in the sequence. There are several different kinds of notations for the above; a common one is to write P ( X 1 = x 1 , X 2 = x 2 , ⋯ ⋯ , X n = x n ) = p k ( 1 − − p ) n − − k {\displaystyle P(X_{1}=x_{1},X_{2}=x_{2},\cdots ,X_{n}=x_{n})=p^{k}(1-p)^{n-k}} where each X i {\displaystyle X_{i}} is a binary-valued random variable with x i = [ ω ω i = H ] {\displaystyle x_{i}=[\omega _{i}=H]} in Iverson bracket notation, meaning either 1 {\displaystyle 1} if ω ω i = H {\displaystyle \omega _{i}=H} or 0 {\displaystyle 0} if ω ω i = T {\displaystyle \omega _{i}=T} .  This probability P {\displaystyle P} is commonly called the Bernoulli measure .

[ 2 ] Note that the probability of any specific, infinitely long sequence of coin flips is exactly zero; this is because lim n → → ∞ ∞ p n = 0 {\displaystyle \lim _{n\to \infty }p^{n}=0} , for any 0 ≤ ≤ p < 1 {\displaystyle 0\leq p<1} . A probability equal to 1 implies that any given infinite sequence has measure zero .  Nevertheless, one can still say that some classes of infinite sequences of coin flips are far more likely than others, this is given by the asymptotic equipartition property .

To conclude the formal definition, a Bernoulli process is then given by the probability triple ( Ω Ω , B , P ) {\displaystyle (\Omega ,{\mathcal {B}},P)} , as defined above.

Law of large numbers, binomial distribution and central limit theorem [ edit ] Main articles: Law of large numbers , Central limit theorem , and Binomial distribution Let us assume the canonical process with H {\displaystyle H} represented by 1 {\displaystyle 1} and T {\displaystyle T} represented by 0 {\displaystyle 0} . The law of large numbers states that the average of the sequence, i.e., X ¯ ¯ n := 1 n ∑ ∑ i = 1 n X i {\displaystyle {\bar {X}}_{n}:={\frac {1}{n}}\sum _{i=1}^{n}X_{i}} , will approach the expected value almost certainly, that is, the events which do not satisfy this limit have zero probability. The expectation value of flipping heads , assumed to be represented by 1, is given by p {\displaystyle p} . In fact, one has E [ X i ] = P ( [ X i = 1 ] ) = p , {\displaystyle \mathbb {E} [X_{i}]=\mathbb {P} ([X_{i}=1])=p,} for any given random variable X i {\displaystyle X_{i}} out of the infinite sequence of Bernoulli trials that compose the Bernoulli process.

One is often interested in knowing how often one will observe H in a sequence of n coin flips.  This is given by simply counting:  Given n successive coin flips, that is, given the set of all possible strings of length n , the number N ( k , n ) of such strings that contain k occurrences of H is given by the binomial coefficient N ( k , n ) = ( n k ) = n !

k !

( n − − k ) !

{\displaystyle N(k,n)={n \choose k}={\frac {n!}{k!(n-k)!}}} If the probability of flipping heads is given by p , then the total probability of seeing a string of length n with k heads is P ( [ S n = k ] ) = ( n k ) p k ( 1 − − p ) n − − k , {\displaystyle \mathbb {P} ([S_{n}=k])={n \choose k}p^{k}(1-p)^{n-k},} where S n = ∑ ∑ i = 1 n X i {\displaystyle S_{n}=\sum _{i=1}^{n}X_{i}} .
The probability measure thus defined is known as the Binomial distribution .

As we can see from the above formula that, if n=1, the Binomial distribution will turn into a Bernoulli distribution . So we can know that the Bernoulli distribution is exactly a special case of Binomial distribution when n equals to 1.

Of particular interest is the question of the value of S n {\displaystyle S_{n}} for a sufficiently long sequences of coin flips, that is, for the limit n → → ∞ ∞ {\displaystyle n\to \infty } .  In this case, one may make use of Stirling's approximation to the factorial, and write n !

= 2 π π n n n e − − n ( 1 + O ( 1 n ) ) {\displaystyle n!={\sqrt {2\pi n}}\;n^{n}e^{-n}\left(1+{\mathcal {O}}\left({\frac {1}{n}}\right)\right)} Inserting this into the expression for P ( k , n ), one obtains the Normal distribution ; this is the content of the central limit theorem , and this is the simplest example thereof.

The combination of the law of large numbers, together with the central limit theorem, leads to an interesting and perhaps surprising result: the asymptotic equipartition property .  Put informally, one notes that, yes, over many coin flips, one will observe H exactly p fraction of the time, and that this corresponds exactly with the peak of the Gaussian.  The asymptotic equipartition property essentially states that this peak is infinitely sharp, with infinite fall-off on either side.  That is, given the set of all possible infinitely long strings of H and T occurring in the Bernoulli process, this set is partitioned into two: those strings that occur with probability 1, and those that occur with probability 0.  This partitioning is known as the Kolmogorov 0-1 law .

The size of this set is interesting, also, and can be explicitly determined: the logarithm of it is exactly the entropy of the Bernoulli process.  Once again, consider the set of all strings of length n . The size of this set is 2 n {\displaystyle 2^{n}} .  Of these, only a certain subset are likely; the size of this set is 2 n H {\displaystyle 2^{nH}} for H ≤ ≤ 1 {\displaystyle H\leq 1} .  By using Stirling's approximation, putting it into the expression for P ( k , n ), solving for the location and width of the peak, and finally taking n → → ∞ ∞ {\displaystyle n\to \infty } one finds that H = − − p log 2 ⁡ ⁡ p − − ( 1 − − p ) log 2 ⁡ ⁡ ( 1 − − p ) {\displaystyle H=-p\log _{2}p-(1-p)\log _{2}(1-p)} This value is the Bernoulli entropy of a Bernoulli process. Here, H stands for entropy; not to be confused with the same symbol H standing for heads .

John von Neumann posed a question about the Bernoulli process regarding the possibility of a given process being isomorphic to another, in the sense of the isomorphism of dynamical systems . The question  long defied analysis, but was finally and completely answered with the Ornstein isomorphism theorem .  This breakthrough resulted in the understanding that the Bernoulli process is unique and universal ; in a certain sense, it is the single most random process possible; nothing is 'more' random than the Bernoulli process (although one must be careful with this informal statement; certainly, systems that are mixing are, in a certain sense, "stronger" than the Bernoulli process, which is merely ergodic but not mixing. However, such processes do not consist of independent random variables: indeed, many purely deterministic, non-random systems can be mixing).

Dynamical systems [ edit ] The Bernoulli process can also be understood to be a dynamical system , as an example of an ergodic system and specifically, a measure-preserving dynamical system , in one of several different ways. One way is as a shift space , and the other is as an odometer . These are reviewed below.

Bernoulli shift [ edit ] Main articles: Bernoulli scheme and Dyadic transformation One way to create a dynamical system out of the Bernoulli process is as a shift space . There is a natural translation symmetry on the product space Ω Ω = 2 N {\displaystyle \Omega =2^{\mathbb {N} }} given by the shift operator T ( X 0 , X 1 , X 2 , ⋯ ⋯ ) = ( X 1 , X 2 , ⋯ ⋯ ) {\displaystyle T(X_{0},X_{1},X_{2},\cdots )=(X_{1},X_{2},\cdots )} The Bernoulli measure, defined above, is translation-invariant; that is, given any cylinder set σ σ ∈ ∈ B {\displaystyle \sigma \in {\mathcal {B}}} , one has P ( T − − 1 ( σ σ ) ) = P ( σ σ ) {\displaystyle P(T^{-1}(\sigma ))=P(\sigma )} and thus the Bernoulli measure is a Haar measure ; it is an invariant measure on the product space.

Instead of the probability measure P : B → → R {\displaystyle P:{\mathcal {B}}\to \mathbb {R} } , consider instead some arbitrary function f : B → → R {\displaystyle f:{\mathcal {B}}\to \mathbb {R} } . The pushforward f ∘ ∘ T − − 1 {\displaystyle f\circ T^{-1}} defined by ( f ∘ ∘ T − − 1 ) ( σ σ ) = f ( T − − 1 ( σ σ ) ) {\displaystyle \left(f\circ T^{-1}\right)(\sigma )=f(T^{-1}(\sigma ))} is again some function B → → R .

{\displaystyle {\mathcal {B}}\to \mathbb {R} .} Thus, the map T {\displaystyle T} induces another map L T {\displaystyle {\mathcal {L}}_{T}} on the space of all functions B → → R .

{\displaystyle {\mathcal {B}}\to \mathbb {R} .} That is, given some f : B → → R {\displaystyle f:{\mathcal {B}}\to \mathbb {R} } , one defines L T f = f ∘ ∘ T − − 1 {\displaystyle {\mathcal {L}}_{T}f=f\circ T^{-1}} The map L T {\displaystyle {\mathcal {L}}_{T}} is a linear operator , as (obviously) one has L T ( f + g ) = L T ( f ) + L T ( g ) {\displaystyle {\mathcal {L}}_{T}(f+g)={\mathcal {L}}_{T}(f)+{\mathcal {L}}_{T}(g)} and L T ( a f ) = a L T ( f ) {\displaystyle {\mathcal {L}}_{T}(af)=a{\mathcal {L}}_{T}(f)} for functions f , g {\displaystyle f,g} and constant a {\displaystyle a} . This linear operator is called the transfer operator or the Ruelle–Frobenius–Perron operator .  This operator has a spectrum , that is, a collection of eigenfunctions and corresponding eigenvalues. The largest eigenvalue is the Frobenius–Perron eigenvalue , and in this case, it is 1.  The associated eigenvector is the invariant measure: in this case, it is the Bernoulli measure. That is, L T ( P ) = P .

{\displaystyle {\mathcal {L}}_{T}(P)=P.} If one restricts L T {\displaystyle {\mathcal {L}}_{T}} to act on polynomials, then the eigenfunctions are (curiously) the Bernoulli polynomials !

[ 3 ] [ 4 ] This coincidence of naming was presumably not known to Bernoulli.

The 2x mod 1 map [ edit ] The map T : [0,1) → [0,1), x ↦ ↦ 2 x mod 1 {\displaystyle x\mapsto 2x{\bmod {1}}} preserves the Lebesgue measure .

The above can be made more precise. Given an infinite string of binary digits b 0 , b 1 , ⋯ ⋯ {\displaystyle b_{0},b_{1},\cdots } write y = ∑ ∑ n = 0 ∞ ∞ b n 2 n + 1 .

{\displaystyle y=\sum _{n=0}^{\infty }{\frac {b_{n}}{2^{n+1}}}.} The resulting y {\displaystyle y} is a real number in the unit interval 0 ≤ ≤ y ≤ ≤ 1.

{\displaystyle 0\leq y\leq 1.} The shift T {\displaystyle T} induces a homomorphism , also called T {\displaystyle T} , on the unit interval. Since T ( b 0 , b 1 , b 2 , ⋯ ⋯ ) = ( b 1 , b 2 , ⋯ ⋯ ) , {\displaystyle T(b_{0},b_{1},b_{2},\cdots )=(b_{1},b_{2},\cdots ),} one can see that T ( y ) = 2 y mod 1 .

{\displaystyle T(y)=2y{\bmod {1}}.} This map is called the dyadic transformation ; for the doubly-infinite sequence of bits Ω Ω = 2 Z , {\displaystyle \Omega =2^{\mathbb {Z} },} the induced homomorphism is the Baker's map .

Consider now the space of functions in y {\displaystyle y} . Given some f ( y ) {\displaystyle f(y)} one can find that [ L T f ] ( y ) = 1 2 f ( y 2 ) + 1 2 f ( y + 1 2 ) {\displaystyle \left[{\mathcal {L}}_{T}f\right](y)={\frac {1}{2}}f\left({\frac {y}{2}}\right)+{\frac {1}{2}}f\left({\frac {y+1}{2}}\right)} Restricting the action of the operator L T {\displaystyle {\mathcal {L}}_{T}} to functions that are on polynomials, one finds that it has a discrete spectrum given by L T B n = 2 − − n B n {\displaystyle {\mathcal {L}}_{T}B_{n}=2^{-n}B_{n}} where the B n {\displaystyle B_{n}} are the Bernoulli polynomials . Indeed, the Bernoulli polynomials obey the identity 1 2 B n ( y 2 ) + 1 2 B n ( y + 1 2 ) = 2 − − n B n ( y ) {\displaystyle {\frac {1}{2}}B_{n}\left({\frac {y}{2}}\right)+{\frac {1}{2}}B_{n}\left({\frac {y+1}{2}}\right)=2^{-n}B_{n}(y)} The Cantor set [ edit ] Note that the sum y = ∑ ∑ n = 0 ∞ ∞ b n 3 n + 1 {\displaystyle y=\sum _{n=0}^{\infty }{\frac {b_{n}}{3^{n+1}}}} gives the Cantor function , as conventionally defined. This is one reason why the set { H , T } N {\displaystyle \{H,T\}^{\mathbb {N} }} is sometimes called the Cantor set .

Odometer [ edit ] Main article: Markov odometer Another way to create a dynamical system is to define an odometer . Informally, this is exactly what it sounds like: just "add one" to the first position, and let the odometer "roll over" by using carry bits as the odometer rolls over. This is nothing more than base-two addition on the set of infinite strings.  Since addition forms a group , and the Bernoulli process was already given a topology, above, this provides a simple example of a topological group .

In this case, the transformation T {\displaystyle T} is given by T ( 1 , … … , 1 , 0 , X k + 1 , X k + 2 , … … ) = ( 0 , … … , 0 , 1 , X k + 1 , X k + 2 , … … ) .

{\displaystyle T\left(1,\dots ,1,0,X_{k+1},X_{k+2},\dots \right)=\left(0,\dots ,0,1,X_{k+1},X_{k+2},\dots \right).} It leaves the Bernoulli measure invariant only for the special case of p = 1 / 2 {\displaystyle p=1/2} (the "fair coin"); otherwise not. Thus, T {\displaystyle T} is a measure preserving dynamical system in this case, otherwise, it is merely a conservative system .

Bernoulli sequence [ edit ] The term Bernoulli sequence is often used informally to refer to a realization of a Bernoulli process.
However, the term has an entirely different formal definition as given below.

Suppose a Bernoulli process formally defined as a single random variable (see preceding section). For every infinite sequence x of coin flips, there is a sequence of integers Z x = { n ∈ ∈ Z : X n ( x ) = 1 } {\displaystyle \mathbb {Z} ^{x}=\{n\in \mathbb {Z} :X_{n}(x)=1\}\,} called the Bernoulli sequence [ verification needed ] associated with the Bernoulli process. For example, if x represents a sequence of coin flips, then the associated Bernoulli sequence is the list of natural numbers or time-points for which the coin toss outcome is heads .

So defined, a Bernoulli sequence Z x {\displaystyle \mathbb {Z} ^{x}} is also a random subset of the index set, the natural numbers N {\displaystyle \mathbb {N} } .

Almost all Bernoulli sequences Z x {\displaystyle \mathbb {Z} ^{x}} are ergodic sequences .

[ verification needed ] Randomness extraction [ edit ] Main article: Randomness extractor From any Bernoulli process one may derive a Bernoulli process with p = 1/2 by the von Neumann extractor , the earliest randomness extractor , which actually extracts uniform randomness.

Basic von Neumann extractor [ edit ] Represent the observed process as a sequence of zeroes and ones, or bits, and group that input stream in non-overlapping pairs of successive bits, such as (11)(00)(10)... . Then for each pair, if the bits are equal, discard; if the bits are not equal, output the first bit.

This table summarizes the computation.

input output 00 discard 01 0 10 1 11 discard For example, an input stream of eight bits 10011011 would by grouped into pairs as (10)(01)(10)(11) . Then, according to the table above, these pairs are translated into the output of the procedure: (1)(0)(1)() (= 101 ).

In the output stream 0 and 1 are equally likely, as 10 and 01 are equally likely in the original, both having probability p (1− p ) = (1− p ) p . This extraction of uniform randomness does not require the input trials to be independent, only uncorrelated . More generally, it works for any exchangeable sequence of bits: all sequences that are finite rearrangements are equally likely.

The von Neumann extractor uses two input bits to produce either zero or one output bits, so the output is shorter than the input by a factor of at least 2. On average the computation discards proportion p 2 + (1 − p ) 2 of the input pairs(00 and 11), which is near one when p is near zero or one, and is minimized at 1/4 when p = 1/2 for the original process (in which case the output stream is 1/4 the length of the input stream on average).

Von Neumann (classical) main operation pseudocode : if (Bit1 ≠ Bit2) {
   output(Bit1)
} Iterated von Neumann extractor [ edit ] This section may contain citations that do not verify the text .

Please help improve it by checking for citation inaccuracies and resourcing or removing material failing verification.

( January 2014 ) ( Learn how and when to remove this message ) This decrease in efficiency, or waste of randomness present in the input stream, can be mitigated by iterating the algorithm over the input data. This way the output can be made to be "arbitrarily close to the entropy bound".

[ 5 ] The iterated version of the von Neumann algorithm, also known as advanced multi-level strategy (AMLS), [ 6 ] was introduced by Yuval Peres in 1992.

[ 5 ] It works recursively, recycling "wasted randomness" from two sources: the sequence of discard-non-discard, and the values of discarded pairs (0 for 00, and 1 for 11). It relies on the fact that, given the sequence already generated, both of those sources are still exchangeable sequences of bits, and thus eligible for another round of extraction. While such generation of additional sequences can be iterated infinitely to extract all available entropy, an infinite amount of computational resources is required, therefore the number of iterations is typically fixed to a low value – this value either fixed in advance, or calculated at runtime.

More concretely, on an input sequence, the algorithm consumes the input bits in pairs, generating output together with two new sequences, () gives AMLS paper notation: input output new sequence 1(A) new sequence 2(1) 00 none 0 0 01 0 1 none 10 1 1 none 11 none 0 1 (If the length of the input is odd, the last bit is completely discarded.) Then the algorithm is applied recursively to each of the two new sequences, until the input is empty.

Example: The input stream from the AMLS paper, 11001011101110 using 1 for H and 0 for T, is processed this way: step number input output new sequence 1(A) new sequence 2(1) 0 (11)(00)(10)(11)(10)(11)(10) ()()(1)()(1)()(1) (1)(1)(0)(1)(0)(1)(0) (1)(0)()(1)()(1)() 1 (10)(11)(11)(01)(01)() (1)()()(0)(0) (0)(1)(1)(0)(0) ()(1)(1)()() 2 (11)(01)(10)() ()(0)(1) (0)(1)(1) (1)()() 3 (10)(11) (1) (1)(0) ()(1) 4 (11)() () (0) (1) 5 (10) (1) (1) () 6 () () () () Starting from step 1, the input is a concatenation of sequence 2 and sequence 1 from the previous step (the order is arbitrary but should be fixed). The final output is ()()(1)()(1)()(1)(1)()()(0)(0)()(0)(1)(1)()(1) (= 1111000111 ), so from 14 bits of input 10 bits of output were generated, as opposed to 3 bits through the von Neumann algorithm alone. The constant output of exactly 2 bits per round per bit pair (compared with a variable none to 1 bit in classical VN) also allows for constant-time implementations which are resistant to timing attacks .

Von Neumann–Peres (iterated) main operation pseudocode: if (Bit1 ≠ Bit2) {
   output(1, Sequence1)
   output(Bit1)
} else {
   output(0, Sequence1)
   output(Bit1, Sequence2)
} Another tweak was presented in 2016, based on the observation that the Sequence2 channel doesn't provide much throughput, and a hardware implementation with a finite number of levels can benefit from discarding it earlier in exchange for processing more levels of Sequence1.

[ 7 ] References [ edit ] ^ a b Dekking, F. M.; Kraaikamp, C.; Lopuhaä, H. P.; Meester, L. E. (2005).

A modern introduction to probability and statistics . Springer. pp.

45– 46.

ISBN 9781852338961 .

^ Klenke, Achim (2006).

Probability Theory . Springer-Verlag.

ISBN 978-1-84800-047-6 .

^ Pierre Gaspard, " r -adic one-dimensional maps and the Euler summation formula", Journal of Physics A , 25 (letter) L483-L485 (1992).

^ Dean J. Driebe, Fully Chaotic Maps and Broken Time Symmetry, (1999) Kluwer Academic Publishers, Dordrecht Netherlands ISBN 0-7923-5564-4 ^ a b Peres, Yuval (March 1992).

"Iterating Von Neumann's Procedure for Extracting Random Bits" .

The Annals of Statistics .

20 (1): 590– 597.

doi : 10.1214/aos/1176348543 .

^ "Tossing a Biased Coin" (PDF) . eecs.harvard.edu.

Archived (PDF) from the original on 2010-03-31 . Retrieved 2018-07-28 .

^ Rožić, Vladimir; Yang, Bohan; Dehaene, Wim; Verbauwhede, Ingrid (3–5 May 2016).

Iterating Von Neumann's post-processing under hardware constraints (PDF) . 2016 IEEE International Symposium on Hardware Oriented Security and Trust (HOST). Maclean, VA, USA.

doi : 10.1109/HST.2016.7495553 .

Archived (PDF) from the original on 2019-02-12.

Further reading [ edit ] Carl W. Helstrom, Probability and Stochastic Processes for Engineers , (1984) Macmillan Publishing Company, New York ISBN 0-02-353560-1 .

External links [ edit ] Using a binary tree diagram for describing a Bernoulli process v t e Stochastic processes Discrete time Bernoulli process Branching process Chinese restaurant process Galton–Watson process Independent and identically distributed random variables Markov chain Moran process Random walk Loop-erased Self-avoiding Biased Maximal entropy Continuous time Additive process Airy process Bessel process Birth–death process pure birth Brownian motion Bridge Dyson Excursion Fractional Geometric Meander Cauchy process Contact process Continuous-time random walk Cox process Diffusion process Empirical process Feller process Fleming–Viot process Gamma process Geometric process Hawkes process Hunt process Interacting particle systems Itô diffusion Itô process Jump diffusion Jump process Lévy process Local time Markov additive process McKean–Vlasov process Ornstein–Uhlenbeck process Poisson process Compound Non-homogeneous Quasimartingale Schramm–Loewner evolution Semimartingale Sigma-martingale Stable process Superprocess Telegraph process Variance gamma process Wiener process Wiener sausage Both Branching process Gaussian process Hidden Markov model (HMM) Markov process Martingale Differences Local Sub- Super- Random dynamical system Regenerative process Renewal process Stochastic chains with memory of variable length White noise Fields and other Dirichlet process Gaussian random field Gibbs measure Hopfield model Ising model Potts model Boolean network Markov random field Percolation Pitman–Yor process Point process Cox Determinantal Poisson Random field Random graph Time series models Autoregressive conditional heteroskedasticity (ARCH) model Autoregressive integrated moving average (ARIMA) model Autoregressive (AR) model Autoregressive–moving-average (ARMA) model Generalized autoregressive conditional heteroskedasticity (GARCH) model Moving-average (MA) model Financial models Binomial options pricing model Black–Derman–Toy Black–Karasinski Black–Scholes Chan–Karolyi–Longstaff–Sanders (CKLS) Chen Constant elasticity of variance (CEV) Cox–Ingersoll–Ross (CIR) Garman–Kohlhagen Heath–Jarrow–Morton (HJM) Heston Ho–Lee Hull–White Korn-Kreer-Lenssen LIBOR market Rendleman–Bartter SABR volatility Vašíček Wilkie Actuarial models Bühlmann Cramér–Lundberg Risk process Sparre–Anderson Queueing models Bulk Fluid Generalized queueing network M/G/1 M/M/1 M/M/c Properties Càdlàg paths Continuous Continuous paths Ergodic Exchangeable Feller-continuous Gauss–Markov Markov Mixing Piecewise-deterministic Predictable Progressively measurable Self-similar Stationary Time-reversible Limit theorems Central limit theorem Donsker's theorem Doob's martingale convergence theorems Ergodic theorem Fisher–Tippett–Gnedenko theorem Large deviation principle Law of large numbers (weak/strong) Law of the iterated logarithm Maximal ergodic theorem Sanov's theorem Zero–one laws ( Blumenthal , Borel–Cantelli , Engelbert–Schmidt , Hewitt–Savage , Kolmogorov , Lévy ) Inequalities Burkholder–Davis–Gundy Doob's martingale Doob's upcrossing Kunita–Watanabe Marcinkiewicz–Zygmund Tools Cameron–Martin formula Convergence of random variables Doléans-Dade exponential Doob decomposition theorem Doob–Meyer decomposition theorem Doob's optional stopping theorem Dynkin's formula Feynman–Kac formula Filtration Girsanov theorem Infinitesimal generator Itô integral Itô's lemma Karhunen–Loève theorem Kolmogorov continuity theorem Kolmogorov extension theorem Lévy–Prokhorov metric Malliavin calculus Martingale representation theorem Optional stopping theorem Prokhorov's theorem Quadratic variation Reflection principle Skorokhod integral Skorokhod's representation theorem Skorokhod space Snell envelope Stochastic differential equation Tanaka Stopping time Stratonovich integral Uniform integrability Usual hypotheses Wiener space Classical Abstract Disciplines Actuarial mathematics Control theory Econometrics Ergodic theory Extreme value theory (EVT) Large deviations theory Mathematical finance Mathematical statistics Probability theory Queueing theory Renewal theory Ruin theory Signal processing Statistics Stochastic analysis Time series analysis Machine learning List of topics Category NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐c5fpk
Cached time: 20250812020102
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.443 seconds
Real time usage: 0.681 seconds
Preprocessor visited node count: 1993/1000000
Revision size: 26860/2097152 bytes
Post‐expand include size: 69404/2097152 bytes
Template argument size: 2138/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 14/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 50963/5000000 bytes
Lua time usage: 0.225/10.000 seconds
Lua memory usage: 6337707/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  357.780      1 -total
 29.12%  104.198      1 Template:Reflist
 19.65%   70.317      2 Template:Cite_book
 18.38%   65.776      1 Template:Probability_fundamentals
 18.05%   64.582      1 Template:Sidebar
 14.22%   50.870      1 Template:Short_description
 10.76%   38.482      1 Template:More_footnotes
 10.27%   36.752      2 Template:Ambox
  8.46%   30.251      2 Template:Pagetype
  8.07%   28.855      1 Template:Stochastic_processes Saved in parser cache with key enwiki:pcache:102651:|#|:idhash:canonical and timestamp 20250812020102 and revision id 1296539230. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Bernoulli_process&oldid=1296539230 " Category : Stochastic processes Hidden categories: Use American English from February 2019 All Wikipedia articles written in American English Articles with short description Short description matches Wikidata Articles lacking in-text citations from September 2011 All articles lacking in-text citations All pages needing factual verification Wikipedia articles needing factual verification from March 2010 Articles lacking reliable references from January 2014 All articles lacking reliable references This page was last edited on 20 June 2025, at 15:54 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Bernoulli process 15 languages Add topic

