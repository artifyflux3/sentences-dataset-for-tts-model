Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition of entropy and differential entropy 2 Distributions with measured constants Toggle Distributions with measured constants subsection 2.1 Continuous case 2.2 Discrete case 2.3 Proof in the case of equality constraints 2.4 Uniqueness of the maximum 2.5 Caveats 3 Examples Toggle Examples subsection 3.1 Uniform and piecewise uniform distributions 3.2 Positive and specified mean: the exponential distribution 3.3 Specified mean and variance: the normal distribution 3.4 Discrete distributions with specified mean 3.5 Circular random variables 3.6 Maximizer for specified mean, variance and skew 3.7 Maximizer for specified mean and deviation risk measure 3.8 Other examples 4 See also 5 Notes 6 Citations 7 References Toggle the table of contents Maximum entropy probability distribution 2 languages Català Français Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Probability distribution that has the most entropy of a class This article has multiple issues.

Please help improve it or discuss these issues on the talk page .

( Learn how and when to remove these messages ) This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Maximum entropy probability distribution" – news · newspapers · books · scholar · JSTOR ( August 2009 ) ( Learn how and when to remove this message ) This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( December 2013 ) ( Learn how and when to remove this message ) ( Learn how and when to remove this message ) In statistics and information theory , a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions . According to the principle of maximum entropy , if nothing is known about a distribution except that it belongs to a certain class (usually defined in terms of specified properties or measures), then the distribution with the largest entropy should be chosen as the least-informative default. The motivation is twofold: first, maximizing entropy minimizes the amount of prior information built into the distribution; second, many physical systems tend to move towards maximal entropy configurations over time.

Definition of entropy and differential entropy [ edit ] Further information: Entropy (information theory) If X {\displaystyle X} is a continuous random variable with probability density p ( x ) {\displaystyle p(x)} , then the differential entropy of X {\displaystyle X} is defined as [ 1 ] [ 2 ] [ 3 ] H ( X ) = − − ∫ ∫ − − ∞ ∞ ∞ ∞ p ( x ) log ⁡ ⁡ p ( x ) d x .

{\displaystyle H(X)=-\int _{-\infty }^{\infty }p(x)\log p(x)\,dx~.} If X {\displaystyle X} is a discrete random variable with distribution given by Pr ( X = x k ) = p k for k = 1 , 2 , … … {\displaystyle \Pr(X{=}x_{k})=p_{k}\qquad {\text{ for }}\quad k=1,2,\ldots } then the entropy of X {\displaystyle X} is defined as H ( X ) = − − ∑ ∑ k ≥ ≥ 1 p k log ⁡ ⁡ p k .

{\displaystyle H(X)=-\sum _{k\geq 1}p_{k}\log p_{k}\,.} The seemingly divergent term p ( x ) log ⁡ ⁡ p ( x ) {\displaystyle p(x)\log p(x)} is replaced by zero, whenever p ( x ) = 0 .

{\displaystyle p(x)=0\,.} This is a special case of more general forms described in the articles Entropy (information theory) , Principle of maximum entropy , and differential entropy. In connection with maximum entropy distributions, this is the only one needed, because maximizing H ( X ) {\displaystyle H(X)} will also maximize the more general forms.

The base of the logarithm is not important, as long as the same one is used consistently: Change of base merely results in a rescaling of the entropy. Information theorists may prefer to use base 2 in order to express the entropy in bits ; mathematicians and physicists often prefer the natural logarithm , resulting in a unit of "nat"s for the entropy.

However, the chosen measure d x {\displaystyle dx} is crucial, even though the typical use of the Lebesgue measure is often defended as a "natural" choice: Which measure is chosen determines the entropy and the consequent maximum entropy distribution.

Distributions with measured constants [ edit ] Many statistical distributions of applicable interest are those for which the moments or other measurable quantities are constrained to be constants. The following theorem by Ludwig Boltzmann gives the form of the probability density under these constraints.

Continuous case [ edit ] Suppose S {\displaystyle S} is a continuous, closed subset of the real numbers R {\displaystyle \mathbb {R} } and we choose to specify n {\displaystyle n} measurable functions f 1 , … … , f n {\displaystyle f_{1},\ldots ,f_{n}} and n {\displaystyle n} numbers a 1 , … … , a n .

{\displaystyle a_{1},\ldots ,a_{n}.} We consider the class C {\displaystyle C} of all real-valued random variables which are supported on S {\displaystyle S} (i.e. whose density function is zero outside of S {\displaystyle S} ) and which satisfy the n {\displaystyle n} moment conditions: E ⁡ ⁡ [ f j ( X ) ] ≥ ≥ a j for j = 1 , … … , n {\displaystyle \operatorname {E} [f_{j}(X)]\geq a_{j}\qquad {\text{for }}\quad j=1,\ldots ,n} If there is a member in C {\displaystyle C} whose density function is positive everywhere in S , {\displaystyle S,} and if there exists a maximal entropy distribution for C , {\displaystyle C,} then its probability density p ( x ) {\displaystyle p(x)} has the following form: p ( x ) = exp ⁡ ⁡ ( ∑ ∑ j = 0 n λ λ j f j ( x ) ) for all x ∈ ∈ S {\displaystyle p(x)=\exp \left(\sum _{j=0}^{n}\lambda _{j}f_{j}(x)\right)\qquad {\text{ for all  }}~x\in S} where we assume that f 0 ( x ) = 1 .

{\displaystyle f_{0}(x)=1\,.} The constant λ λ 0 {\displaystyle \lambda _{0}} and the n {\displaystyle n} Lagrange multipliers λ λ = ( λ λ 1 , … … , λ λ n ) {\displaystyle {\boldsymbol {\lambda }}=(\lambda _{1},\ldots ,\lambda _{n})} solve the constrained optimization problem with a 0 = 1 {\displaystyle a_{0}=1} (which ensures that p {\displaystyle p} integrates to unity): [ 4 ] max λ λ 0 ; λ λ { ∑ ∑ j = 0 n λ λ j a j − − ∫ ∫ exp ⁡ ⁡ ( ∑ ∑ j = 0 n λ λ j f j ( x ) ) d x } subject to λ λ ≥ ≥ 0 {\displaystyle \max _{\lambda _{0};\,{\boldsymbol {\lambda }}}\left\{\sum _{j=0}^{n}\lambda _{j}a_{j}-\int \exp \left(\sum _{j=0}^{n}\lambda _{j}f_{j}(x)\right)dx\right\}\qquad ~{\text{ subject to }}~{\boldsymbol {\lambda }}\geq \mathbf {0} } Using the Karush–Kuhn–Tucker conditions , it can be shown that the optimization problem has a unique solution because the objective function in the optimization is concave in λ λ .

{\displaystyle {\boldsymbol {\lambda }}\,.} Note that when the moment constraints are equalities (instead of inequalities), that is, E ⁡ ⁡ [ f j ( X ) ] = a j for j = 1 , … … , n , {\displaystyle \operatorname {E} [f_{j}(X)]=a_{j}\qquad {\text{ for }}~j=1,\ldots ,n\,,} then the constraint condition λ λ ≥ ≥ 0 {\displaystyle {\boldsymbol {\lambda }}\geq \mathbf {0} } can be dropped, which makes optimization over the Lagrange multipliers unconstrained.

Discrete case [ edit ] Suppose S = { x 1 , x 2 , … … } {\displaystyle S=\{x_{1},x_{2},\ldots \}} is a (finite or infinite) discrete subset of the reals, and that we choose to specify n {\displaystyle n} functions f 1 , … … , f n {\displaystyle f_{1},\ldots ,f_{n}} and n {\displaystyle n} numbers a 1 , … … , a n .

{\displaystyle a_{1},\ldots ,a_{n}\,.} We consider the class C {\displaystyle C} of all discrete random variables X {\displaystyle X} which are supported on S {\displaystyle S} and which satisfy the n {\displaystyle n} moment conditions E ⁡ ⁡ [ f j ( X ) ] ≥ ≥ a j for j = 1 , … … , n {\displaystyle \operatorname {E} [f_{j}(X)]\geq a_{j}\qquad ~{\text{ for }}~j=1,\ldots ,n} If there exists a member of class C {\displaystyle C} which assigns positive probability to all members of S {\displaystyle S} and if there exists a  maximum entropy distribution for C , {\displaystyle C,} then this distribution has the following shape: Pr ( X = x k ) = exp ⁡ ⁡ ( ∑ ∑ j = 0 n λ λ j f j ( x k ) ) for k = 1 , 2 , … … {\displaystyle \Pr(X{=}x_{k})=\exp \left(\sum _{j=0}^{n}\lambda _{j}f_{j}(x_{k})\right)\qquad {\text{ for }}~k=1,2,\ldots } where we assume that f 0 = 1 {\displaystyle f_{0}=1} and the constants λ λ 0 , λ λ ≡ ≡ ( λ λ 1 , … … , λ λ n ) {\displaystyle \lambda _{0},\,{\boldsymbol {\lambda }}\equiv (\lambda _{1},\ldots ,\lambda _{n})} solve the constrained optimization problem with a 0 = 1 {\displaystyle a_{0}=1} : [ 5 ] max λ λ 0 ; λ λ { ∑ ∑ j = 0 n λ λ j a j − − ∑ ∑ k ≥ ≥ 1 exp ⁡ ⁡ ( ∑ ∑ j = 0 n λ λ j f j ( x k ) ) } for which λ λ ≥ ≥ 0 {\displaystyle \max _{\lambda _{0};\,{\boldsymbol {\lambda }}}\left\{\sum _{j=0}^{n}\lambda _{j}a_{j}-\sum _{k\geq 1}\exp \left(\sum _{j=0}^{n}\lambda _{j}f_{j}(x_{k})\right)\right\}\qquad {\text{ for which }}~{\boldsymbol {\lambda }}\geq \mathbf {0} } Again as above, if the moment conditions are equalities (instead of inequalities), then the constraint condition λ λ ≥ ≥ 0 {\displaystyle {\boldsymbol {\lambda }}\geq \mathbf {0} } is not present in the optimization.

Proof in the case of equality constraints [ edit ] In the case of equality constraints, this theorem is proved with the calculus of variations and Lagrange multipliers . The constraints can be written as ∫ ∫ − − ∞ ∞ ∞ ∞ f j ( x ) p ( x ) d x = a j {\displaystyle \int _{-\infty }^{\infty }f_{j}(x)p(x)\,dx=a_{j}} We consider the functional J ( p ) = ∫ ∫ − − ∞ ∞ ∞ ∞ p ( x ) ln ⁡ ⁡ p ( x ) d x − − η η 0 ( ∫ ∫ − − ∞ ∞ ∞ ∞ p ( x ) d x − − 1 ) − − ∑ ∑ j = 1 n λ λ j ( ∫ ∫ − − ∞ ∞ ∞ ∞ f j ( x ) p ( x ) d x − − a j ) {\displaystyle J(p)=\int _{-\infty }^{\infty }p(x)\ln {p(x)}\,dx-\eta _{0}\left(\int _{-\infty }^{\infty }p(x)\,dx-1\right)-\sum _{j=1}^{n}\lambda _{j}\left(\int _{-\infty }^{\infty }f_{j}(x)p(x)\,dx-a_{j}\right)} where η η 0 {\displaystyle \eta _{0}} and λ λ j , j ≥ ≥ 1 {\displaystyle \lambda _{j},j\geq 1} are the Lagrange multipliers. The zeroth constraint ensures the second axiom of probability . The other constraints are that the measurements of the function are given constants up to order n {\displaystyle n} . The entropy attains an extremum when the functional derivative is equal to zero: δ δ J ( p ) δ δ p = ln ⁡ ⁡ p ( x ) + 1 − − η η 0 − − ∑ ∑ j = 1 n λ λ j f j ( x ) = 0 {\displaystyle {\frac {\delta J(p)}{\delta p}}=\ln {p(x)}+1-\eta _{0}-\sum _{j=1}^{n}\lambda _{j}f_{j}(x)=0} Therefore, the extremal entropy probability distribution in this case must be of the form ( λ λ 0 := η η 0 − − 1 {\displaystyle \lambda _{0}:=\eta _{0}-1} ), p ( x ) = e − − 1 + η η 0 e ∑ ∑ j = 1 n λ λ j f j ( x ) = exp ⁡ ⁡ ( ∑ ∑ j = 0 n λ λ j f j ( x ) ) , {\displaystyle p(x)=e^{-1+\eta _{0}}\,e^{\sum _{j=1}^{n}\lambda _{j}f_{j}(x)}=\exp \left(\sum _{j=0}^{n}\lambda _{j}f_{j}(x)\right),} remembering that f 0 ( x ) = 1 {\displaystyle f_{0}(x)=1} . It can be verified that this is the maximal solution by checking that the variation around this solution is always negative.

Uniqueness of the maximum [ edit ] Suppose p {\displaystyle p} and p ′ {\displaystyle p'} are distributions satisfying the expectation-constraints. Letting α α ∈ ∈ ( 0 , 1 ) {\displaystyle \alpha \in (0,1)} and considering the distribution q = α α p + ( 1 − − α α ) p ′ {\displaystyle q=\alpha \,p+(1-\alpha )\,p'} it is clear that this distribution satisfies the expectation-constraints and furthermore has as support supp ⁡ ⁡ ( q ) = supp ⁡ ⁡ ( p ) ∪ ∪ supp ⁡ ⁡ ( p ′ ) .

{\displaystyle \operatorname {supp} (q)=\operatorname {supp} (p)\cup \operatorname {supp} (p')\,.} From basic facts about entropy, it holds that H ( q ) ≥ ≥ α α H ( p ) + ( 1 − − α α ) H ( p ′ ) .

{\displaystyle {\mathcal {H}}(q)\geq \alpha \,{\mathcal {H}}(p)+(1-\alpha )\,{\mathcal {H}}(p').} Taking limits α α → → 1 {\displaystyle \alpha \to 1} and α α → → 0 , {\displaystyle \alpha \to 0\,,} respectively, yields H ( q ) ≥ ≥ H ( p ) , H ( p ′ ) .

{\displaystyle {\mathcal {H}}(q)\geq {\mathcal {H}}(p),{\mathcal {H}}(p')\,.} It follows that a distribution satisfying the expectation-constraints and maximising entropy must necessarily have full support — i. e.

the distribution is almost everywhere strictly positive. It follows that the maximising distribution must be an internal point in the space of distributions satisfying the expectation-constraints, that is, it must be a local extreme. Thus it suffices to show that the local extreme is unique, in order to show both that the entropy-maximising distribution is unique (and this also shows that the local extreme is the global maximum).

Suppose p {\displaystyle p} and p ′ {\displaystyle p'} are local extremes. Reformulating the above computations these are characterised by parameters λ λ , λ λ ′ ∈ ∈ R n {\displaystyle {\boldsymbol {\lambda }},\,{\boldsymbol {\lambda }}'\in \mathbb {R} ^{n}} via p ( x ) = exp ⁡ ⁡ ⟨ λ λ , f ( x ) ⟩ / C ( λ λ ) {\displaystyle p(x)={\exp \left\langle {\boldsymbol {\lambda }},\mathbf {f} (x)\right\rangle }/{C({\boldsymbol {\lambda }})}} and similarly for p ′ , {\displaystyle p',} where C ( λ λ ) = ∫ ∫ R exp ⁡ ⁡ ⟨ λ λ , f ( x ) ⟩ d x .

{\textstyle C({\boldsymbol {\lambda }})=\int _{\mathbb {R} }\exp \left\langle {\boldsymbol {\lambda }},\mathbf {f} (x)\right\rangle \,dx\,.} We now note a series of identities: Via the satisfaction of the expectation-constraints and utilising gradients / directional derivatives, one has D log ⁡ ⁡ C ( ⋅ ⋅ ) | λ λ = D C ( ⋅ ⋅ ) C ( ⋅ ⋅ ) | λ λ = E p ⁡ ⁡ [ f ( X ) ] = a {\displaystyle {\left.D\log C(\cdot )\right|}_{\boldsymbol {\lambda }}={\left.{\tfrac {DC(\cdot )}{C(\cdot )}}\right|}_{\boldsymbol {\lambda }}=\operatorname {E} _{p}\left[\mathbf {f} (X)\right]=\mathbf {a} } and similarly for λ λ ′ .

{\displaystyle {\boldsymbol {\lambda }}'~.} Letting u = λ λ ′ − − λ λ ∈ ∈ R n {\displaystyle u={\boldsymbol {\lambda }}'-{\boldsymbol {\lambda }}\in \mathbb {R} ^{n}} one obtains: 0 = ⟨ u , a − − a ⟩ = D u log ⁡ ⁡ C ( ⋅ ⋅ ) | λ λ ′ − − D u log ⁡ ⁡ C ( ⋅ ⋅ ) | λ λ = D u 2 log ⁡ ⁡ C ( ⋅ ⋅ ) | γ γ {\displaystyle 0=\left\langle u,\mathbf {a} -\mathbf {a} \right\rangle ={\left.D_{u}\log C(\cdot )\right|}_{{\boldsymbol {\lambda }}'}-{\left.D_{u}\log C(\cdot )\right|}_{\boldsymbol {\lambda }}={\left.D_{u}^{2}\log C(\cdot )\right|}_{\boldsymbol {\gamma }}} where γ γ = θ θ λ λ + ( 1 − − θ θ ) λ λ ′ {\displaystyle {\boldsymbol {\gamma }}=\theta {\boldsymbol {\lambda }}+(1-\theta ){\boldsymbol {\lambda }}'} for some θ θ ∈ ∈ ( 0 , 1 ) .

{\displaystyle \theta \in (0,1).} Computing further, one has 0 = D u 2 log ⁡ ⁡ C ( ⋅ ⋅ ) | γ γ = D u ( D u C ( ⋅ ⋅ ) C ( ⋅ ⋅ ) ) | γ γ = D u 2 C ( ⋅ ⋅ ) C ( ⋅ ⋅ ) | γ γ − − ( D u C ( ⋅ ⋅ ) ) 2 C ( ⋅ ⋅ ) 2 | γ γ = E q ⁡ ⁡ [ ⟨ u , f ( X ) ⟩ 2 ] − − ( E q ⁡ ⁡ [ ⟨ u , f ( X ) ⟩ ] ) 2 = Var q ⁡ ⁡ [ ⟨ u , f ( X ) ⟩ ] {\displaystyle {\begin{aligned}0&={\left.D_{u}^{2}\log C(\cdot )\right|}_{\boldsymbol {\gamma }}\\[1ex]&={\left.D_{u}\left({\frac {D_{u}C(\cdot )}{C(\cdot )}}\right)\right|}_{\boldsymbol {\gamma }}={\left.{\frac {D_{u}^{2}C(\cdot )}{C(\cdot )}}\right|}_{\boldsymbol {\gamma }}-{\left.{\frac {{\left(D_{u}C(\cdot )\right)}^{2}}{C(\cdot )^{2}}}\right|}_{\boldsymbol {\gamma }}\\[1ex]&=\operatorname {E} _{q}\left[{\left\langle u,\mathbf {f} (X)\right\rangle }^{2}\right]-{\left(\operatorname {E} _{q}\left[\left\langle u,\mathbf {f} (X)\right\rangle \right]\right)}^{2}\\[2ex]&=\operatorname {Var} _{q}\left[\left\langle u,\mathbf {f} (X)\right\rangle \right]\end{aligned}}} where q {\displaystyle q} is similar to the distribution above, only parameterised by γ γ , {\displaystyle {\boldsymbol {\gamma }}~,} Assuming that no non-trivial linear combination of the observables is almost everywhere (a.e.) constant, (which e.g.

holds if the observables are independent and not a.e. constant), it holds that ⟨ ⟨ u , f ( X ) ⟩ ⟩ {\displaystyle \langle u,\mathbf {f} (X)\rangle } has non-zero variance, unless u = 0 .

{\displaystyle u=0~.} By the above equation it is thus clear, that the latter must be the case. Hence λ λ ′ − − λ λ = u = 0 , {\displaystyle {\boldsymbol {\lambda }}'-{\boldsymbol {\lambda }}=u=0\,,} so the parameters characterising the local extrema p , p ′ {\displaystyle p,\,p'} are identical, which means that the distributions themselves are identical. Thus, the local extreme is unique and by the above discussion, the maximum is unique – provided a local extreme actually exists.

Caveats [ edit ] Note that not all classes of distributions contain a maximum entropy distribution. It is possible that a class contain distributions of arbitrarily large entropy (e.g. the class of all continuous distributions on R with mean 0 but arbitrary standard deviation), or that the entropies are bounded above but there is no distribution which attains the maximal entropy.

[ a ] It is also possible that the expected value restrictions for the class C force the probability distribution to be zero in certain subsets of S . In that case our theorem doesn't apply, but one can work around this by shrinking the set S .

Examples [ edit ] Every probability distribution is trivially a maximum entropy probability distribution under the constraint that the distribution has its own entropy.  To see this, rewrite the density as p ( x ) = exp ⁡ ⁡ ( ln ⁡ ⁡ p ( x ) ) {\displaystyle p(x)=\exp {(\ln {p(x)})}} and compare to the expression of the theorem above.  By choosing ln ⁡ ⁡ p ( x ) → → f ( x ) {\displaystyle \ln {p(x)}\rightarrow f(x)} to be the measurable function and ∫ ∫ exp ⁡ ⁡ ( f ( x ) ) f ( x ) d x = − − H {\displaystyle \int \exp {(f(x))}f(x)dx=-H} to be the constant, p ( x ) {\displaystyle p(x)} is the maximum entropy probability distribution under the constraint ∫ ∫ p ( x ) f ( x ) d x = − − H .

{\displaystyle \int p(x)f(x)\,dx=-H.} Nontrivial examples are distributions that are subject to multiple constraints that are different from the assignment of the entropy.  These are often found by starting with the same procedure ln ⁡ ⁡ p ( x ) → → f ( x ) {\displaystyle \ln {p(x)}\to f(x)} and finding that f ( x ) {\displaystyle f(x)} can be separated into parts.

A table of examples of maximum entropy distributions is given in Lisman (1972) [ 6 ] and Park & Bera (2009).

[ 7 ] Uniform and piecewise uniform distributions [ edit ] The uniform distribution on the interval [ a , b ] is the maximum entropy distribution among all continuous distributions which are supported in the interval [ a , b ], and thus the probability density is 0 outside of the interval. This uniform density can be related to Laplace's principle of indifference , sometimes called the principle of insufficient reason. More generally, if we are given a subdivision a = a 0 < a 1 < ... < a k = b of the interval [ a , b ] and probabilities p 1 ,..., p k that add up to one, then we can consider the class of all continuous distributions such that Pr ( a j − − 1 ≤ ≤ X < a j ) = p j for j = 1 , … … , k {\displaystyle \Pr(a_{j-1}\leq X<a_{j})=p_{j}\quad {\text{ for }}j=1,\ldots ,k} The density of the maximum entropy distribution for this class is constant on each of the intervals [ a j −1 , a j ). The uniform distribution on the finite set { x 1 ,..., x n } (which assigns a probability of 1/ n to each of these values) is the maximum entropy distribution among all discrete distributions supported on this set.

Positive and specified mean: the exponential distribution [ edit ] The exponential distribution , for which the density function is p ( x | λ λ ) = { λ λ e − − λ λ x x ≥ ≥ 0 , 0 x < 0 , {\displaystyle p(x|\lambda )={\begin{cases}\lambda e^{-\lambda x}&x\geq 0,\\0&x<0,\end{cases}}} is the maximum entropy distribution among all continuous distributions supported in [0,∞) that have a specified mean of 1/λ.

In the case of distributions supported on [0,∞), the maximum entropy distribution depends on relationships between the first and second moments. In specific cases, it may be the exponential distribution, or may be another distribution, or may be undefinable.

[ 8 ] Specified mean and variance: the normal distribution [ edit ] The normal distribution N ( μ , σ 2 ), for which the density function is p ( x | μ μ , σ σ ) = 1 σ σ 2 π π e − − ( x − − μ μ ) 2 2 σ σ 2 , {\displaystyle p(x|\mu ,\sigma )={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}},} has maximum entropy among all real -valued distributions supported on (−∞,∞) with a specified variance σ 2 (a particular moment ). The same is true when the mean μ and the variance σ 2 is specified (the first two moments), since entropy is translation invariant on (−∞,∞). Therefore, the assumption of normality imposes the minimal prior structural constraint beyond these moments. (See the differential entropy article for a derivation.) Discrete distributions with specified mean [ edit ] Among all the discrete distributions supported on the set { x 1 ,..., x n } with a specified mean μ, the maximum entropy distribution has the following shape: Pr ( X = x k ) = C r x k for k = 1 , … … , n {\displaystyle \Pr(X{=}x_{k})=Cr^{x_{k}}\quad {\text{ for }}k=1,\ldots ,n} where the positive constants C and r can be determined by the requirements that the sum of all the probabilities must be 1 and the expected value must be μ.

For example, if a large number N of dice are thrown, and you are told that the sum of all the shown numbers is S . Based on this information alone, what would be a reasonable assumption for the number of dice showing 1, 2, ..., 6? This is an instance of the situation considered above, with { x 1 ,..., x 6 } = {1,...,6} and μ = S / N .

Finally, among all the discrete distributions supported on the infinite set { x 1 , x 2 , .

.

.

} {\displaystyle \{x_{1},x_{2},...\}} with mean μ , the maximum entropy distribution has the shape: Pr ( X = x k ) = C r x k for k = 1 , 2 , … … , {\displaystyle \Pr(X{=}x_{k})=Cr^{x_{k}}\quad {\text{ for }}k=1,2,\ldots ,} where again the constants C and r were determined by the requirements that the sum of all the probabilities must be 1 and the expected value must be μ. For example, in the case that x k = k , this gives C = 1 μ μ − − 1 , r = μ μ − − 1 μ μ , {\displaystyle C={\frac {1}{\mu -1}},\quad \quad r={\frac {\mu -1}{\mu }},} such that respective maximum entropy distribution is the geometric distribution .

Circular random variables [ edit ] For a continuous random variable θ θ i {\displaystyle \theta _{i}} distributed about the unit circle, the Von Mises distribution maximizes the entropy when the real and imaginary parts of the first circular moment are specified [ 9 ] or, equivalently, the circular mean and circular variance are specified.

When the mean and variance of the angles θ θ i {\displaystyle \theta _{i}} modulo 2 π π {\displaystyle 2\pi } are specified, the wrapped normal distribution maximizes the entropy.

[ 9 ] Maximizer for specified mean, variance and skew [ edit ] There exists an upper bound on the entropy of continuous random variables on R {\displaystyle \mathbb {R} } with a specified mean, variance, and skew.  However, there is no distribution which achieves this upper bound , because p ( x ) = c exp ⁡ ⁡ ( λ λ 1 x + λ λ 2 x 2 + λ λ 3 x 3 ) {\displaystyle p(x)=c\exp {(\lambda _{1}x+\lambda _{2}x^{2}+\lambda _{3}x^{3})}} is unbounded when λ λ 3 ≠ ≠ 0 {\displaystyle \lambda _{3}\neq 0} (see Cover & Thomas (2006: chapter 12)).

However, the maximum entropy is ε -achievable: a distribution's entropy can be arbitrarily close to the upper bound.  Start with a normal distribution of the specified mean and variance.  To introduce a positive skew, perturb the normal distribution upward by a small amount at a value many σ larger than the mean.  The skewness, being proportional to the third moment, will be affected more than the lower order moments.

This is a special case of the general case in which the exponential of any odd-order polynomial in x will be unbounded on R {\displaystyle \mathbb {R} } . For example, c e λ λ x {\displaystyle ce^{\lambda x}} will likewise be unbounded on R {\displaystyle \mathbb {R} } , but when the support is limited to a bounded or semi-bounded interval the upper entropy bound may be achieved (e.g. if x lies in the interval [0,∞] and λ< 0 , the exponential distribution will result).

Maximizer for specified mean and deviation risk measure [ edit ] Every distribution with log-concave density is a maximal entropy distribution with specified mean μ and deviation risk measure D .

[ 10 ] In particular, the maximal entropy distribution with specified mean E ( X ) ≡ ≡ μ μ {\displaystyle E(X)\equiv \mu } and deviation D ( X ) ≡ ≡ d {\displaystyle D(X)\equiv d} is: The normal distribution N ( m , d 2 ) , {\displaystyle {\mathcal {N}}(m,d^{2}),} if D ( X ) = E ⁡ ⁡ [ ( X − − μ μ ) 2 ] {\textstyle D(X)={\sqrt {\operatorname {E} \left[{\left(X-\mu \right)}^{2}\right]}}} is the standard deviation ; The Laplace distribution , if D ( X ) = E ⁡ ⁡ { | X − − μ μ | } {\displaystyle D(X)=\operatorname {E} \left\{\left|X-\mu \right|\right\}} is the average absolute deviation ; [ 6 ] The distribution with density of the form f ( x ) = c exp ⁡ ⁡ ( a x + b ⇂ ⇂ x − − μ μ ⇃ ⇃ − − 2 ) {\displaystyle f(x)=c\exp \left(ax+b{\downharpoonright x-\mu \downharpoonleft }_{-}^{2}\right)} if D ( X ) = E ⁡ ⁡ { ⇂ ⇂ X − − μ μ ⇃ ⇃ − − 2 } {\textstyle D(X)={\sqrt {\operatorname {E} \left\{{\downharpoonright X-\mu \downharpoonleft }_{-}^{2}\right\}}}} is the standard lower semi-deviation, where a , b , c {\displaystyle a,b,c} are constants and the function ⇂ ⇂ y ⇃ ⇃ − − ≡ ≡ min { 0 , y } for any y ∈ ∈ R , {\displaystyle \downharpoonright y\downharpoonleft _{-}\equiv \min \left\{0,y\right\}~{\text{ for any }}y\in \mathbb {R} \,,} returns only the negative values of its argument, otherwise zero.

[ 10 ] Other examples [ edit ] In the table below, each listed distribution maximizes the entropy for a particular set of functional constraints listed in the third column, and the constraint that x {\displaystyle x} be included in the support of the probability density, which is listed in the fourth column.

[ 6 ] [ 7 ] Several listed examples ( Bernoulli , geometric , exponential , Laplace , Pareto ) are trivially true, because their associated constraints are equivalent to the assignment of their entropy. They are included anyway because their constraint is related to a common or easily measured quantity.

For reference, Γ Γ ( x ) = ∫ ∫ 0 ∞ ∞ e − − t t x − − 1 d t {\displaystyle \Gamma (x)=\int _{0}^{\infty }e^{-t}t^{x-1}\,dt} is the gamma function , ψ ψ ( x ) = d d x ln ⁡ ⁡ Γ Γ ( x ) = Γ Γ ′ ( x ) Γ Γ ( x ) {\displaystyle \psi (x)={\frac {d}{dx}}\ln \Gamma (x)={\frac {\Gamma '(x)}{\Gamma (x)}}} is the digamma function , B ( p , q ) = Γ Γ ( p ) Γ Γ ( q ) Γ Γ ( p + q ) {\displaystyle B(p,q)={\frac {\Gamma (p)\,\Gamma (q)}{\Gamma (p+q)}}} is the beta function , and γ γ E {\displaystyle \gamma _{\mathsf {E}}} is the Euler-Mascheroni constant .

Table of probability distributions and corresponding maximum entropy constraints Distribution name Probability density / mass function Maximum entropy constraint Support Uniform (discrete) f ( k ) = 1 b − − a + 1 {\displaystyle f(k)={\frac {1}{b-a+1}}} None { a , a + 1 , .

.

.

, b − − 1 , b } {\displaystyle \{a,a+1,...,b-1,b\}} Uniform (continuous) f ( x ) = 1 b − − a {\displaystyle f(x)={\frac {1}{b-a}}} None [ a , b ] {\displaystyle [a,b]} Bernoulli f ( k ) = p k ( 1 − − p ) 1 − − k {\displaystyle f(k)=p^{k}{\left(1-p\right)}^{1-k}} E ⁡ ⁡ [ K ] = p {\displaystyle \operatorname {E} [K]=p} { 0 , 1 } {\displaystyle \{0,1\}} Geometric f ( k ) = ( 1 − − p ) k − − 1 p {\displaystyle f(k)={\left(1-p\right)}^{k-1}p} E ⁡ ⁡ [ K ] = 1 p {\displaystyle \operatorname {E} [K]={\frac {1}{p}}} N ∖ ∖ { 0 } = { 1 , 2 , 3 , .

.

.

} {\displaystyle \mathbb {N} \setminus \left\{0\right\}=\{1,2,3,...\}} Exponential f ( x ) = λ λ exp ⁡ ⁡ ( − − λ λ x ) {\displaystyle f(x)=\lambda \exp \left(-\lambda x\right)} E ⁡ ⁡ [ X ] = 1 λ λ {\displaystyle \operatorname {E} [X]={\frac {1}{\lambda }}} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Laplace f ( x ) = 1 2 b exp ⁡ ⁡ ( − − | x − − μ μ | b ) {\displaystyle f(x)={\frac {1}{2b}}\exp \left(-{\frac {|x-\mu |}{b}}\right)} E ⁡ ⁡ [ | X − − μ μ | ] = b {\displaystyle \operatorname {E} [|X-\mu |]=b} R {\displaystyle \mathbb {R} } Asymmetric Laplace f ( x ) = λ λ exp ⁡ ⁡ [ − − ( x − − m ) λ λ s κ κ s ] κ κ + 1 κ κ {\displaystyle f(x)={\frac {\lambda \exp \left[-(x-m)\,\lambda \,s\,\kappa ^{s}\right]}{\kappa +{\frac {1}{\kappa }}}}} where s ≡ ≡ sgn ⁡ ⁡ ( x − − m ) {\displaystyle ~s\equiv \operatorname {sgn}(x-m)} E ⁡ ⁡ [ ( X − − m ) s κ κ s ] = 1 λ λ {\displaystyle \operatorname {E} \left[(X-m)s\kappa ^{s}\right]={\frac {1}{\lambda }}} R {\displaystyle \mathbb {R} } Pareto f ( x ) = α α x m α α x α α + 1 {\displaystyle f(x)={\frac {\alpha x_{m}^{\alpha }}{x^{\alpha +1}}}} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = 1 α α + ln ⁡ ⁡ ( x m ) {\displaystyle \operatorname {E} [\ln X]={\frac {1}{\alpha }}+\ln(x_{m})} [ x m , ∞ ∞ ) {\displaystyle [x_{m},\infty )} Normal f ( x ) = 1 2 π π σ σ 2 exp ⁡ ⁡ ( − − ( x − − μ μ ) 2 2 σ σ 2 ) {\displaystyle f(x)={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left(-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}\right)} E ⁡ ⁡ [ X ] = μ μ , {\displaystyle \operatorname {E} [X]=\mu \,,} E ⁡ ⁡ [ X 2 ] = σ σ 2 + μ μ 2 {\displaystyle \operatorname {E} \left[X^{2}\right]=\sigma ^{2}+\mu ^{2}} R {\displaystyle \mathbb {R} } Truncated normal (see article) E ⁡ ⁡ [ X ] = μ μ T , {\displaystyle \operatorname {E} \left[X\right]=\mu _{\mathsf {T}}\,,} E ⁡ ⁡ [ X 2 ] = σ σ T 2 + μ μ T 2 {\displaystyle \operatorname {E} \left[X^{2}\right]=\sigma _{\mathsf {T}}^{2}+\mu _{\mathsf {T}}^{2}} [ a , b ] {\displaystyle [a,b]} von Mises f ( θ θ ) = 1 2 π π I 0 ( κ κ ) exp ⁡ ⁡ ( κ κ cos ⁡ ⁡ ( θ θ − − μ μ ) ) {\displaystyle f(\theta )={\frac {1}{2\pi I_{0}(\kappa )}}\exp \left(\kappa \cos {(\theta -\mu )}\right)} E ⁡ ⁡ [ cos ⁡ ⁡ Θ Θ ] = I 1 ( κ κ ) I 0 ( κ κ ) cos ⁡ ⁡ μ μ , {\displaystyle \operatorname {E} [\cos \Theta ]={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}\cos \mu \,,} E ⁡ ⁡ [ sin ⁡ ⁡ Θ Θ ] = I 1 ( κ κ ) I 0 ( κ κ ) sin ⁡ ⁡ μ μ {\displaystyle \operatorname {E} [\sin \Theta ]={\frac {I_{1}(\kappa )}{I_{0}(\kappa )}}\sin \mu } [ 0 , 2 π π ) {\displaystyle [0,2\pi )} Rayleigh f ( x ) = x σ σ 2 exp ⁡ ⁡ ( − − x 2 2 σ σ 2 ) {\displaystyle f(x)={\frac {x}{\sigma ^{2}}}\exp \left(-{\frac {x^{2}}{2\sigma ^{2}}}\right)} E ⁡ ⁡ [ X 2 ] = 2 σ σ 2 , {\displaystyle \operatorname {E} \left[X^{2}\right]=2\sigma ^{2}\,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ln ⁡ ⁡ ( 2 σ σ 2 ) − − γ γ E 2 {\displaystyle \operatorname {E} [\ln X]={\frac {\ln(2\sigma ^{2})-\gamma _{\mathrm {E} }}{2}}} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Beta f ( x ) = x α α − − 1 ( 1 − − x ) β β − − 1 B ( α α , β β ) {\displaystyle f(x)={\frac {x^{\alpha -1}(1-x)^{\beta -1}}{B(\alpha ,\beta )}}} for 0 ≤ ≤ x ≤ ≤ 1 {\displaystyle 0\leq x\leq 1} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ψ ψ ( α α ) − − ψ ψ ( α α + β β ) , {\displaystyle \operatorname {E} [\ln X]=\psi (\alpha )-\psi (\alpha +\beta )\,,} E ⁡ ⁡ [ ln ⁡ ⁡ ( 1 − − X ) ] = ψ ψ ( β β ) − − ψ ψ ( α α + β β ) {\displaystyle \operatorname {E} [\ln(1-X)]=\psi (\beta )-\psi (\alpha +\beta )} [ 0 , 1 ] {\displaystyle [0,1]} Cauchy f ( x ) = 1 π π ( 1 + x 2 ) {\displaystyle f(x)={\frac {1}{\pi \left(1+x^{2}\right)}}} E ⁡ ⁡ [ ln ⁡ ⁡ ( 1 + X 2 ) ] = 2 ln ⁡ ⁡ 2 {\displaystyle \operatorname {E} \left[\ln \left(1+X^{2}\right)\right]=2\ln 2} R {\displaystyle \mathbb {R} } Chi f ( x ) = 2 2 k / 2 Γ Γ ( k / 2 ) x k − − 1 exp ⁡ ⁡ ( − − x 2 2 ) {\displaystyle f(x)={\frac {2}{2^{k/2}\Gamma (k/2)}}x^{k-1}\exp \left(-{\frac {x^{2}}{2}}\right)} E ⁡ ⁡ [ X 2 ] = k , {\displaystyle \operatorname {E} \left[X^{2}\right]=k\,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = 1 2 [ ψ ψ ( k 2 ) + ln ⁡ ⁡ ( 2 ) ] {\displaystyle \operatorname {E} [\ln X]={\frac {1}{2}}\left[\psi {\left({\frac {k}{2}}\right)}+\ln(2)\right]} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Chi-squared f ( x ) = 1 2 k / 2 Γ Γ ( k / 2 ) x k 2 − − 1 exp ⁡ ⁡ ( − − x 2 ) {\displaystyle f(x)={\frac {1}{2^{k/2}\Gamma (k/2)}}x^{{\frac {k}{2}}\!-\!1}\exp \left(-{\frac {x}{2}}\right)} E ⁡ ⁡ [ X ] = k , {\displaystyle \operatorname {E} [X]=k\,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ψ ψ ( k 2 ) + ln ⁡ ⁡ ( 2 ) {\displaystyle \operatorname {E} [\ln X]=\psi {\left({\frac {k}{2}}\right)}+\ln(2)} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Erlang f ( x ) = λ λ k ( k − − 1 ) !

x k − − 1 exp ⁡ ⁡ ( − − λ λ x ) {\displaystyle f(x)={\frac {\lambda ^{k}}{(k-1)!}}x^{k-1}\exp(-\lambda x)} E ⁡ ⁡ [ X ] = k / λ λ , {\displaystyle \operatorname {E} [X]=k/\lambda \,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ψ ψ ( k ) − − ln ⁡ ⁡ ( λ λ ) {\displaystyle \operatorname {E} [\ln X]=\psi (k)-\ln(\lambda )} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Gamma f ( x ) = x k − − 1 e − − x / θ θ θ θ k Γ Γ ( k ) {\displaystyle f(x)={\frac {x^{k-1}e^{-x/\theta }}{\theta ^{k}\Gamma (k)}}} E ⁡ ⁡ [ X ] = k θ θ , {\displaystyle \operatorname {E} [X]=k\theta \,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ψ ψ ( k ) + ln ⁡ ⁡ θ θ {\displaystyle \operatorname {E} [\ln X]=\psi (k)+\ln \theta } [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Lognormal f ( x ) = 1 σ σ x 2 π π exp ⁡ ⁡ ( − − ( ln ⁡ ⁡ x − − μ μ ) 2 2 σ σ 2 ) {\displaystyle f(x)={\frac {1}{\sigma x{\sqrt {2\pi }}}}\exp \left(-{\frac {{\left(\ln x-\mu \right)}^{2}}{2\sigma ^{2}}}\right)} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = μ μ , {\displaystyle \operatorname {E} \left[\ln X\right]=\mu \,,} E ⁡ ⁡ [ ln ⁡ ⁡ ( X ) 2 ] = σ σ 2 + μ μ 2 {\displaystyle \operatorname {E} \left[\ln(X)^{2}\right]=\sigma ^{2}+\mu ^{2}} ( 0 , ∞ ∞ ) {\displaystyle (0,\infty )} Maxwell–Boltzmann f ( x ) = 1 a 3 2 π π x 2 exp ⁡ ⁡ ( − − x 2 2 a 2 ) {\displaystyle f(x)={\frac {1}{a^{3}}}{\sqrt {\frac {2}{\pi }}}x^{2}\exp \left(-{\frac {x^{2}}{2a^{2}}}\right)} E ⁡ ⁡ [ X 2 ] = 3 a 2 , {\displaystyle \operatorname {E} \left[X^{2}\right]=3a^{2}\,,} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = 1 + ln ⁡ ⁡ ( a 2 ) − − γ γ E 2 {\displaystyle \operatorname {E} \left[\ln X\right]=1+\ln \left({\frac {a}{\sqrt {2}}}\right)-{\frac {\gamma _{\mathrm {E} }}{2}}} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Weibull f ( x ) = k λ λ k x k − − 1 exp ⁡ ⁡ ( − − x k λ λ k ) {\displaystyle f(x)={\frac {k}{\lambda ^{k}}}x^{k-1}\exp \left(-{\frac {x^{k}}{\lambda ^{k}}}\right)} E ⁡ ⁡ [ X k ] = λ λ k , {\displaystyle \operatorname {E} \left[X^{k}\right]=\lambda ^{k},} E ⁡ ⁡ [ ln ⁡ ⁡ X ] = ln ⁡ ⁡ ( λ λ ) − − γ γ E k {\displaystyle \operatorname {E} \left[\ln X\right]=\ln(\lambda )-{\frac {\gamma _{\mathrm {E} }}{k}}} [ 0 , ∞ ∞ ) {\displaystyle [0,\infty )} Multivariate normal f X ( x ) = exp ⁡ ⁡ [ − − 1 2 ( x − − μ μ ) T Σ Σ − − 1 ( x − − μ μ ) ] ( 2 π π ) N | Σ Σ | {\displaystyle f_{X}(\mathbf {x} )={\frac {\exp \left[-{\frac {1}{2}}\left(\mathbf {x} -{\boldsymbol {\mu }}\right)^{\mathsf {T}}\Sigma ^{-1}\left(\mathbf {x} -{\boldsymbol {\mu }}\right)\right]}{\sqrt {{\left(2\pi \right)}^{N}\left|\Sigma \right|}}}} E ⁡ ⁡ [ x ] = μ μ , {\displaystyle \operatorname {E} \left[\mathbf {x} \right]={\boldsymbol {\mu }},} E ⁡ ⁡ [ ( x − − μ μ ) ( x − − μ μ ) T ] = Σ Σ {\displaystyle \operatorname {E} \left[(\mathbf {x} -{\boldsymbol {\mu }})(\mathbf {x} -{\boldsymbol {\mu }})^{\mathsf {T}}\right]=\Sigma } R n {\displaystyle \mathbb {R} ^{n}} Binomial f ( k ) = ( n k ) p k ( 1 − − p ) n − − k {\displaystyle f(k)={\binom {n}{k}}p^{k}{\left(1-p\right)}^{n-k}} E ⁡ ⁡ [ X ] = μ μ , {\displaystyle \operatorname {E} [X]=\mu \,,} f ∈ ∈ {\displaystyle f\in } n-generalized binomial distribution [ 11 ] { 0 , … … , n } {\displaystyle \left\{0,{\ldots },n\right\}} Poisson f ( k ) = λ λ k e − − λ λ k !

{\displaystyle f(k)={\frac {\lambda ^{k}e^{-\lambda }}{k!}}} E ⁡ ⁡ [ X ] = λ λ , {\displaystyle \operatorname {E} [X]=\lambda ,} f ∈ ∈ ∞ ∞ {\displaystyle f\in \infty } -generalized binomial distribution [ 11 ] N = { 0 , 1 , … … } {\displaystyle \mathbb {N} =\left\{0,1,{\ldots }\right\}} Logistic f ( x ) = e − − x ( 1 + e − − x ) 2 = e + x ( e + x + 1 ) 2 {\displaystyle f(x)={\frac {e^{-x}}{\left(1+e^{-x}\right)^{2}}}={\frac {e^{+x}}{\left(e^{+x}+1\right)^{2}}}} E ⁡ ⁡ [ X ] = 0 , {\displaystyle \operatorname {E} [X]=0,} E ⁡ ⁡ [ ln ⁡ ⁡ ( 1 + e − − X ) ] = 1 {\displaystyle \operatorname {E} \left[\ln \left(1+e^{-X}\right)\right]=1} { − − ∞ ∞ , ∞ ∞ } {\displaystyle \left\{-\infty ,\infty \right\}} The maximum entropy principle can be used to upper bound the entropy of statistical mixtures.

[ 12 ] See also [ edit ] Exponential family Gibbs measure Partition function (mathematics) Maximal entropy random walk - maximizing entropy rate for a graph Notes [ edit ] ^ For example, the class of all continuous distributions X on R with E( X ) = 0 and E( X 2 ) = E( X 3 ) = 1 (see Cover, Ch 12).

Citations [ edit ] ^ Williams, D. (2001).

Weighing the Odds .

Cambridge University Press . pp.

197– 199.

ISBN 0-521-00618-X .

^ Bernardo, J.M.; Smith, A.F.M. (2000).

Bayesian Theory . Wiley. pp. 209, 366.

ISBN 0-471-49464-X .

^ O'Hagan, A. (1994), Bayesian Inference . Kendall's Advanced Theory of Statistics. Vol. 2B.

Edward Arnold . 1994. section 5.40.

ISBN 0-340-52922-9 .

^ Botev, Z.I.; Kroese, D.P. (2011).

"The generalized cross entropy method, with applications to probability density estimation" (PDF) .

Methodology and Computing in Applied Probability .

13 (1): 1– 27.

doi : 10.1007/s11009-009-9133-7 .

S2CID 18155189 .

^ Botev, Z.I.; Kroese, D.P. (2008). "Non-asymptotic bandwidth selection for density estimation of discrete data".

Methodology and Computing in Applied Probability .

10 (3): 435.

doi : 10.1007/s11009-007-9057-zv (inactive 1 July 2025).

S2CID 122047337 .

{{ cite journal }} :  CS1 maint: DOI inactive as of July 2025 ( link ) ^ a b c Lisman, J. H. C.; van Zuylen, M. C. A. (1972). "Note on the generation of most probable frequency distributions".

Statistica Neerlandica .

26 (1): 19– 23.

doi : 10.1111/j.1467-9574.1972.tb00152.x .

^ a b Park, Sung Y.; Bera, Anil K. (2009).

"Maximum entropy autoregressive conditional heteroskedasticity model" (PDF) .

Journal of Econometrics .

150 (2): 219– 230.

CiteSeerX 10.1.1.511.9750 .

doi : 10.1016/j.jeconom.2008.12.014 . Archived from the original (PDF) on 2016-03-07 . Retrieved 2011-06-02 .

^ Dowson, D.; Wragg, A. (September 1973). "Maximum-entropy distributions having prescribed first and second moments".

IEEE Transactions on Information Theory (correspondance).

19 (5): 689– 693.

doi : 10.1109/tit.1973.1055060 .

ISSN 0018-9448 .

^ a b Jammalamadaka, S. Rao; SenGupta, A. (2001).

Topics in circular statistics . New Jersey: World Scientific.

ISBN 978-981-02-3778-3 . Retrieved 2011-05-15 .

^ a b Grechuk, Bogdan; Molyboha, Anton; Zabarankin, Michael (2009).

"Maximum entropy principle with general deviation measures" .

Mathematics of Operations Research .

34 (2): 445– 467.

doi : 10.1287/moor.1090.0377 – via researchgate.net.

^ a b Harremös, Peter (2001). "Binomial and Poisson distributions as maximum entropy distributions".

IEEE Transactions on Information Theory .

47 (5): 2039– 2041.

doi : 10.1109/18.930936 .

S2CID 16171405 .

^ Nielsen, Frank; Nock, Richard (2017). "MaxEnt upper bounds for the differential entropy of univariate continuous distributions".

IEEE Signal Processing Letters .

24 (4).

IEEE : 402– 406.

Bibcode : 2017ISPL...24..402N .

doi : 10.1109/LSP.2017.2666792 .

S2CID 14092514 .

References [ edit ] Cover, T. M.

; Thomas, J. A. (2006).

"Chapter 12, Maximum Entropy" (PDF) .

Elements of Information Theory (2 ed.). Wiley.

ISBN 978-0471241959 .

F. Nielsen, R. Nock (2017), MaxEnt upper bounds for the differential entropy of univariate continuous distributions , IEEE Signal Processing Letters , 24(4), 402–406 I. J. Taneja (2001), Generalized Information Measures and Their Applications .

Chapter 1 Nader Ebrahimi, Ehsan S. Soofi, Refik Soyer (2008), "Multivariate maximum entropy identification, transformation, and dependence", Journal of Multivariate Analysis 99: 1217–1231, doi : 10.1016/j.jmva.2007.08.004 v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐blp69
Cached time: 20250812030236
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.687 seconds
Real time usage: 1.169 seconds
Preprocessor visited node count: 2971/1000000
Revision size: 36394/2097152 bytes
Post‐expand include size: 129959/2097152 bytes
Template argument size: 10566/2097152 bytes
Highest expansion depth: 18/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 77697/5000000 bytes
Lua time usage: 0.324/10.000 seconds
Lua memory usage: 6573536/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  508.345      1 -total
 33.88%  172.210      2 Template:Reflist
 20.58%  104.618      5 Template:Cite_book
 20.20%  102.694      4 Template:Navbox
 19.74%  100.365      1 Template:ProbDistributions
 19.19%   97.562      1 Template:Short_description
 15.44%   78.469      1 Template:Multiple_issues
 13.96%   70.948      2 Template:Pagetype
 10.40%   52.889      8 Template:Cite_journal
 10.33%   52.536      2 Template:Ambox Saved in parser cache with key enwiki:pcache:1813193:|#|:idhash:canonical and timestamp 20250812030236 and revision id 1301611749. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Maximum_entropy_probability_distribution&oldid=1301611749 " Categories : Entropy and information Continuous distributions Discrete distributions Particle statistics Types of probability distributions Hidden categories: CS1 maint: DOI inactive as of July 2025 Articles with short description Short description matches Wikidata Articles needing additional references from August 2009 All articles needing additional references Articles lacking in-text citations from December 2013 All articles lacking in-text citations Articles with multiple maintenance issues This page was last edited on 20 July 2025, at 19:53 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Maximum entropy probability distribution 2 languages Add topic

