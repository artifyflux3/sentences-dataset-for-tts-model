Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 See also 2 References 3 Further reading Toggle the table of contents Cache language model Add languages Add links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia A cache language model is a type of statistical language model . These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution . Statistical language models are key components of speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems.

[ citation needed ] To understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word "elephant" because it is a very rare word in English . If the speech recognition system does not contain a cache component, the person dictating the letter may be annoyed: each time the word "elephant" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., "tell a plan"). These erroneous sequences will have to be deleted manually and replaced in the text by "elephant" each time "elephant" is spoken. If the system has a cache language model, "elephant" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that "elephant" is likely to occur again – the estimated probability of occurrence of "elephant" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once "elephant" has occurred several times, the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of "elephant" is an example of a consequence of machine learning and more specifically of pattern recognition .

There exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if "San Francisco" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability).

[ citation needed ] The cache language model was first proposed in a paper published in 1990, [ 1 ] after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated.

[ 2 ] A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: "Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes".

[ 3 ] The development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently, there has been interest in applying the cache language model in the field of statistical machine translation.

[ 4 ] The success of the cache language model in improving word prediction rests on the human tendency to use words in a "bursty" fashion: when one is discussing a certain topic in a certain context, the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this "burstiness".

[ citation needed ] Recently, the cache language model concept – originally conceived for the N-gram statistical language model paradigm – has been adapted for use in the neural paradigm. For instance, recent work on continuous cache language models in the recurrent neural network (RNN) setting has applied the cache concept to much larger contexts than before, yielding significant reductions in perplexity.

[ 5 ] Another recent line of research involves incorporating a cache component in a feed-forward neural language model (FN-LM) to achieve rapid domain adaptation.

[ 6 ] See also [ edit ] Artificial intelligence History of natural language processing History of machine translation Speech recognition Statistical machine translation References [ edit ] ^ Kuhn, R.; De Mori, R.

(June 1990).

"A Cache-Based Natural Language Model for Speech Recognition" (PDF) .

IEEE Transactions on Pattern Analysis and Machine Intelligence .

12 (6): 570– 583.

doi : 10.1109/34.56193 .

ISSN 1939-3539 . Archived from the original (PDF) on 2011-09-01 . Retrieved 2011-09-24 .

( Abstract ) ^ F. Jelinek; B. Merialdo; S. Roukos & M. Strauss (1991).

"A Dynamic Language Model for Speech Recognition" (PDF) .

The Journal of the Acoustical Society of America .

98 (2): 293– 295.

doi : 10.3115/112405.112464 .

S2CID 11601499 . Archived from the original (PDF) on June 14, 2006.

Conference: Speech and Natural Language, Proceedings of a Workshop held at Pacific Grove, California, USA, February 19–22, 1999.

^ Joshua T. Goodman (2001).

A Bit of Progress in Language Modeling: Extended Version . Redmond, WA (US): Microsoft Research.

arXiv : cs/0108005v1 .

Bibcode : 2001cs........8005G . Technical report MSR-TR-2001-72.

^ Tiedemann, Jorg (2010).

Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache (PDF) . Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010. Uppsala, Sweden: Association for Computational Linguistics. pp.

8– 15.

^ Edouard Grave; Moustapha Cisse; Armand Joulin (2017).

"Unbounded cache model for online language modeling with open vocabulary" .

NIPS'17 Proceedings of the 31st International Conference on Neural Information Processing Systems . Long Beach, California: Association for Computing Machinery. pp.

6044– 6054.

ISBN 978-1-5108-6096-4 .

^ Karel Benes; Santosh Kesiraju; Lukas Burget (2018).

i-Vectors in Language Modeling: An Efficient Way of Domain Adaptation for Feed-Forward Models . Interspeech 2018. Hyderabad, India: Interspeech. pp.

3383– 3387.

doi : 10.21437/Interspeech.2018-1070 .

S2CID 52192034 .

Further reading [ edit ] Jelinek, Frederick (1997).

Statistical Methods for Speech Recognition .

The MIT Press .

ISBN 0-262-10066-5 . Archived from the original on 2011-08-05 . Retrieved 2011-09-24 .

NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐bs6zj
Cached time: 20250812073518
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.166 seconds
Real time usage: 0.205 seconds
Preprocessor visited node count: 1010/1000000
Revision size: 8807/2097152 bytes
Post‐expand include size: 23934/2097152 bytes
Template argument size: 1920/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 26716/5000000 bytes
Lua time usage: 0.101/10.000 seconds
Lua memory usage: 4293470/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  186.247      1 -total
 62.26%  115.955      1 Template:Reflist
 39.81%   74.145      2 Template:Cite_journal
 33.30%   62.023      3 Template:Citation_needed
 24.71%   46.013      3 Template:Fix
 13.98%   26.039      6 Template:Category_handler
  9.16%   17.063      3 Template:Cite_conference
  5.73%   10.664      2 Template:Cite_book
  4.93%    9.184      3 Template:Delink
  1.43%    2.666      3 Template:Fix/category Saved in parser cache with key enwiki:pcache:31139924:|#|:idhash:canonical and timestamp 20250812073518 and revision id 1214930617. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Cache_language_model&oldid=1214930617 " Categories : Language modeling Computational linguistics Natural language processing Speech processing Speech recognition Machine translation Hidden categories: All articles with unsourced statements Articles with unsourced statements from September 2011 This page was last edited on 22 March 2024, at 02:33 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Cache language model Add languages Add topic

