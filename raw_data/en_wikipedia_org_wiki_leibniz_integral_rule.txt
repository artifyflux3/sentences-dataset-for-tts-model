Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 General form: differentiation under the integral sign 2 Three-dimensional, time-dependent case 3 Higher dimensions 4 Measure theory statement 5 Proofs Toggle Proofs subsection 5.1 Proof of basic form 5.1.1 Another proof using the bounded convergence theorem 5.2 Variable limits form 5.3 General form with variable limits 5.4 Alternative proof of the general form with variable limits, using the chain rule 5.5 Three-dimensional, time-dependent form 5.6 Alternative derivation 6 Examples Toggle Examples subsection 6.1 Example 1: Fixed limits 6.2 Example 2: Variable limits 7 Applications Toggle Applications subsection 7.1 Evaluating definite integrals 7.1.1 Example 3 7.1.2 Example 4 7.1.3 Example 5 7.1.4 Example 6 7.1.5 Other problems to solve 7.2 Infinite series 7.3 Euler-Lagrange equations 8 In popular culture 9 See also 10 References 11 Further reading 12 External links Toggle the table of contents Leibniz integral rule 16 languages العربية Bosanski Català Deutsch Esperanto हिन्दी עברית Қазақша 日本語 Polski Português Русский Svenska Українська Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Differentiation under the integral sign formula This article is about the integral rule. For the convergence test for alternating series, see Alternating series test .

This article needs additional citations for verification .

Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.

Find sources: "Leibniz integral rule" – news · newspapers · books · scholar · JSTOR ( October 2016 ) ( Learn how and when to remove this message ) Part of a series of articles about Calculus ∫ ∫ a b f ′ ( t ) d t = f ( b ) − − f ( a ) {\displaystyle \int _{a}^{b}f'(t)\,dt=f(b)-f(a)} Fundamental theorem Limits Continuity Rolle's theorem Mean value theorem Inverse function theorem Differential Definitions Derivative ( generalizations ) Differential infinitesimal of a function total Concepts Differentiation notation Second derivative Implicit differentiation Logarithmic differentiation Related rates Taylor's theorem Rules and identities Sum Product Chain Power Quotient L'Hôpital's rule Inverse General Leibniz Faà di Bruno's formula Reynolds Integral Lists of integrals Integral transform Leibniz integral rule Definitions Antiderivative Integral ( improper ) Riemann integral Lebesgue integration Contour integration Integral of inverse functions Integration by Parts Discs Cylindrical shells Substitution ( trigonometric , tangent half-angle , Euler ) Euler's formula Partial fractions ( Heaviside's method ) Changing order Reduction formulae Differentiating under the integral sign Risch algorithm Series Geometric ( arithmetico-geometric ) Harmonic Alternating Power Binomial Taylor Convergence tests Summand limit (term test) Ratio Root Integral Direct comparison Limit comparison Alternating series Cauchy condensation Dirichlet Abel Vector Gradient Divergence Curl Laplacian Directional derivative Identities Theorems Gradient Green's Stokes' Divergence Generalized Stokes Helmholtz decomposition Multivariable Formalisms Matrix Tensor Exterior Geometric Definitions Partial derivative Multiple integral Line integral Surface integral Volume integral Jacobian Hessian Advanced Calculus on Euclidean space Generalized functions Limit of distributions Specialized Fractional Malliavin Stochastic Variations Miscellanea Precalculus History Glossary List of topics Integration Bee Mathematical analysis Nonstandard analysis v t e In calculus , the Leibniz integral rule for differentiation under the integral sign, named after Gottfried Wilhelm Leibniz , states that for an integral of the form ∫ ∫ a ( x ) b ( x ) f ( x , t ) d t , {\displaystyle \int _{a(x)}^{b(x)}f(x,t)\,dt,} where − − ∞ ∞ < a ( x ) , b ( x ) < ∞ ∞ {\displaystyle -\infty <a(x),b(x)<\infty } and the integrands are functions dependent on x , {\displaystyle x,} the derivative of this integral is expressible as d d x ( ∫ ∫ a ( x ) b ( x ) f ( x , t ) d t ) = f ( x , b ( x ) ) ⋅ ⋅ d d x b ( x ) − − f ( x , a ( x ) ) ⋅ ⋅ d d x a ( x ) + ∫ ∫ a ( x ) b ( x ) ∂ ∂ ∂ ∂ x f ( x , t ) d t {\displaystyle {\begin{aligned}&{\frac {d}{dx}}\left(\int _{a(x)}^{b(x)}f(x,t)\,dt\right)\\&=f{\big (}x,b(x){\big )}\cdot {\frac {d}{dx}}b(x)-f{\big (}x,a(x){\big )}\cdot {\frac {d}{dx}}a(x)+\int _{a(x)}^{b(x)}{\frac {\partial }{\partial x}}f(x,t)\,dt\end{aligned}}} where the partial derivative ∂ ∂ ∂ ∂ x {\displaystyle {\tfrac {\partial }{\partial x}}} indicates that inside the integral, only the variation of f ( x , t ) {\displaystyle f(x,t)} with x {\displaystyle x} is considered in taking the derivative.

[ 1 ] In the special case where the functions a ( x ) {\displaystyle a(x)} and b ( x ) {\displaystyle b(x)} are constants a ( x ) = a {\displaystyle a(x)=a} and b ( x ) = b {\displaystyle b(x)=b} with values that do not depend on x , {\displaystyle x,} this simplifies to: d d x ( ∫ ∫ a b f ( x , t ) d t ) = ∫ ∫ a b ∂ ∂ ∂ ∂ x f ( x , t ) d t .

{\displaystyle {\frac {d}{dx}}\left(\int _{a}^{b}f(x,t)\,dt\right)=\int _{a}^{b}{\frac {\partial }{\partial x}}f(x,t)\,dt.} If a ( x ) = a {\displaystyle a(x)=a} is constant and b ( x ) = x {\displaystyle b(x)=x} , which is another common situation (for example, in the proof of Cauchy's repeated integration formula ), the Leibniz integral rule becomes: d d x ( ∫ ∫ a x f ( x , t ) d t ) = f ( x , x ) + ∫ ∫ a x ∂ ∂ ∂ ∂ x f ( x , t ) d t , {\displaystyle {\frac {d}{dx}}\left(\int _{a}^{x}f(x,t)\,dt\right)=f{\big (}x,x{\big )}+\int _{a}^{x}{\frac {\partial }{\partial x}}f(x,t)\,dt,} This important result may, under certain conditions, be used to interchange the integral and partial differential operators , and is particularly useful in the differentiation of integral transforms . An example of such is the moment generating function in probability theory , a variation of the Laplace transform , which can be differentiated to generate the moments of a random variable . Whether Leibniz's integral rule applies is essentially a question about the interchange of limits .

General form: differentiation under the integral sign [ edit ] Theorem — Let f ( x , t ) {\displaystyle f(x,t)} be a function such that both f ( x , t ) {\displaystyle f(x,t)} and its partial derivative f x ( x , t ) {\displaystyle f_{x}(x,t)} are continuous in t {\displaystyle t} and x {\displaystyle x} in some region of the x t {\displaystyle xt} -plane, including a ( x ) ≤ ≤ t ≤ ≤ b ( x ) , {\displaystyle a(x)\leq t\leq b(x),} x 0 ≤ ≤ x ≤ ≤ x 1 .

{\displaystyle x_{0}\leq x\leq x_{1}.} Also suppose that the functions a ( x ) {\displaystyle a(x)} and b ( x ) {\displaystyle b(x)} are both continuous and both have continuous derivatives for x 0 ≤ ≤ x ≤ ≤ x 1 .

{\displaystyle x_{0}\leq x\leq x_{1}.} Then, for x 0 ≤ ≤ x ≤ ≤ x 1 , {\displaystyle x_{0}\leq x\leq x_{1},} d d x ( ∫ ∫ a ( x ) b ( x ) f ( x , t ) d t ) = f ( x , b ( x ) ) ⋅ ⋅ d d x b ( x ) − − f ( x , a ( x ) ) ⋅ ⋅ d d x a ( x ) + ∫ ∫ a ( x ) b ( x ) ∂ ∂ ∂ ∂ x f ( x , t ) d t .

{\displaystyle {\frac {d}{dx}}\left(\int _{a(x)}^{b(x)}f(x,t)\,dt\right)=f{\big (}x,b(x){\big )}\cdot {\frac {d}{dx}}b(x)-f{\big (}x,a(x){\big )}\cdot {\frac {d}{dx}}a(x)+\int _{a(x)}^{b(x)}{\frac {\partial }{\partial x}}f(x,t)\,dt.} The right hand side may also be written using Lagrange's notation as: f ( x , b ( x ) ) b ′ ′ ( x ) − − f ( x , a ( x ) ) a ′ ′ ( x ) + ∫ ∫ a ( x ) b ( x ) f x ( x , t ) d t .

{\textstyle f(x,b(x))\,b^{\prime }(x)-f(x,a(x))\,a^{\prime }(x)+\displaystyle \int _{a(x)}^{b(x)}f_{x}(x,t)\,dt.} Stronger versions of the theorem only require that the partial derivative exist almost everywhere , and not that it be continuous.

[ 2 ] This formula is the general form of the Leibniz integral rule and can be derived using the fundamental theorem of calculus . The (first) fundamental theorem of calculus is just the particular case of the above formula where a ( x ) = a ∈ ∈ R {\displaystyle a(x)=a\in \mathbb {R} } is constant, b ( x ) = x , {\displaystyle b(x)=x,} and f ( x , t ) = f ( t ) {\displaystyle f(x,t)=f(t)} does not depend on x .

{\displaystyle x.} If both upper and lower limits are taken as constants, then the formula takes the shape of an operator equation: I t ∂ ∂ x = ∂ ∂ x I t {\displaystyle {\mathcal {I}}_{t}\partial _{x}=\partial _{x}{\mathcal {I}}_{t}} where ∂ ∂ x {\displaystyle \partial _{x}} is the partial derivative with respect to x {\displaystyle x} and I t {\displaystyle {\mathcal {I}}_{t}} is the integral operator with respect to t {\displaystyle t} over a fixed interval . That is, it is related to the symmetry of second derivatives , but involving integrals as well as derivatives. This case is also known as the Leibniz integral rule.

The following three basic theorems on the interchange of limits are essentially equivalent: the interchange of a derivative and an integral (differentiation under the integral sign; i.e., Leibniz integral rule); the change of order of partial derivatives; the change of order of integration (integration under the integral sign; i.e., Fubini's theorem ).

Three-dimensional, time-dependent case [ edit ] See also: § Higher dimensions Figure 1: A vector field F ( r , t ) defined throughout space, and a surface Σ bounded by curve ∂Σ moving with velocity v over which the field is integrated.

A Leibniz integral rule for a two dimensional surface moving in three dimensional space is [ 3 ] [ 4 ] [ 5 ] d d t ∬ ∬ Σ Σ ( t ) F ( r , t ) ⋅ ⋅ d A = ∬ ∬ Σ Σ ( t ) ( F t ( r , t ) + [ ∇ ∇ ⋅ ⋅ F ( r , t ) ] v ) ⋅ ⋅ d A − − ∮ ∮ ∂ ∂ Σ Σ ( t ) [ v × × F ( r , t ) ] ⋅ ⋅ d s , {\displaystyle {\frac {d}{dt}}\iint _{\Sigma (t)}\mathbf {F} (\mathbf {r} ,t)\cdot d\mathbf {A} =\iint _{\Sigma (t)}\left(\mathbf {F} _{t}(\mathbf {r} ,t)+\left[\nabla \cdot \mathbf {F} (\mathbf {r} ,t)\right]\mathbf {v} \right)\cdot d\mathbf {A} -\oint _{\partial \Sigma (t)}\left[\mathbf {v} \times \mathbf {F} (\mathbf {r} ,t)\right]\cdot d\mathbf {s} ,} where: F ( r , t ) is a vector field at the spatial position r at time t , F t ( r , t ) is the partial time derivative of the vector field, Σ is a surface bounded by the closed curve ∂Σ , d A is a vector element of the surface Σ , d s is a vector element of the curve ∂Σ , v is the velocity of movement of the region Σ , ∇⋅ is the vector divergence , × is the vector cross product , The double integrals are surface integrals over the surface Σ , and the line integral is over the bounding curve ∂Σ .

Higher dimensions [ edit ] The Leibniz integral rule can be extended to multidimensional integrals. In two and three dimensions, this rule is better known from the field of fluid dynamics as the Reynolds transport theorem : d d t ∫ ∫ D ( t ) F ( x , t ) d V = ∫ ∫ D ( t ) ∂ ∂ ∂ ∂ t F ( x , t ) d V + ∫ ∫ ∂ ∂ D ( t ) F ( x , t ) v b ⋅ ⋅ d Σ Σ , {\displaystyle {\frac {d}{dt}}\int _{D(t)}F(\mathbf {x} ,t)\,dV=\int _{D(t)}{\frac {\partial }{\partial t}}F(\mathbf {x} ,t)\,dV+\int _{\partial D(t)}F(\mathbf {x} ,t)\mathbf {v} _{b}\cdot d\mathbf {\Sigma } ,} where F ( x , t ) {\displaystyle F(\mathbf {x} ,t)} is a scalar function, D ( t ) and ∂ D ( t ) denote a time-varying connected region of R 3 and its boundary, respectively, v b {\displaystyle \mathbf {v} _{b}} is the Eulerian velocity of the boundary (see Lagrangian and Eulerian coordinates ) and d Σ = n dS is the unit normal component of the surface element .

The general statement of the Leibniz integral rule requires concepts from differential geometry , specifically differential forms , exterior derivatives , wedge products and interior products . With those tools, the Leibniz integral rule in n dimensions is [ 4 ] d d t ∫ ∫ Ω Ω ( t ) ω ω = ∫ ∫ Ω Ω ( t ) i v ( d x ω ω ) + ∫ ∫ ∂ ∂ Ω Ω ( t ) i v ω ω + ∫ ∫ Ω Ω ( t ) ω ω ˙ ˙ , {\displaystyle {\frac {d}{dt}}\int _{\Omega (t)}\omega =\int _{\Omega (t)}i_{\mathbf {v} }(d_{x}\omega )+\int _{\partial \Omega (t)}i_{\mathbf {v} }\omega +\int _{\Omega (t)}{\dot {\omega }},} where Ω( t ) is a time-varying domain of integration, ω is a p -form, v = ∂ ∂ x ∂ ∂ t {\displaystyle \mathbf {v} ={\frac {\partial \mathbf {x} }{\partial t}}} is the vector field of the velocity, i v {\displaystyle i_{\mathbf {v} }} denotes the interior product with v {\displaystyle \mathbf {v} } , d x ω is the exterior derivative of ω with respect to the space variables only and ω ω ˙ ˙ {\displaystyle {\dot {\omega }}} is the time derivative of ω .

The above formula can be deduced directly from the fact that the Lie derivative interacts nicely with integration of differential forms d d t ∫ ∫ Ω Ω ( t ) ω ω = ∫ ∫ Ω Ω ( t ) L Ψ Ψ ω ω , {\displaystyle {\frac {d}{dt}}\int _{\Omega (t)}\omega =\int _{\Omega (t)}{\mathcal {L}}_{\Psi }\omega ,} for the spacetime manifold M = R × × R 3 {\displaystyle M=\mathbb {R} \times \mathbb {R} ^{3}} , where the spacetime exterior derivative of ω ω {\displaystyle \omega } is d ω ω = d t ∧ ∧ ω ω ˙ ˙ + d x ω ω {\displaystyle d\omega =dt\wedge {\dot {\omega }}+d_{x}\omega } and the surface Ω Ω ( t ) {\displaystyle \Omega (t)} has spacetime velocity field Ψ Ψ = ∂ ∂ ∂ ∂ t + v {\displaystyle \Psi ={\frac {\partial }{\partial t}}+\mathbf {v} } .
Since ω ω {\displaystyle \omega } has only spatial components, the Lie derivative can be simplified using Cartan's magic formula , to L Ψ Ψ ω ω = L v ω ω + L ∂ ∂ ∂ ∂ t ω ω = i v d ω ω + d i v ω ω + i ∂ ∂ ∂ ∂ t d ω ω = i v d x ω ω + d i v ω ω + ω ω ˙ ˙ {\displaystyle {\mathcal {L}}_{\Psi }\omega ={\mathcal {L}}_{\mathbf {v} }\omega +{\mathcal {L}}_{\frac {\partial }{\partial t}}\omega =i_{\mathbf {v} }d\omega +di_{\mathbf {v} }\omega +i_{\frac {\partial }{\partial t}}d\omega =i_{\mathbf {v} }d_{x}\omega +di_{\mathbf {v} }\omega +{\dot {\omega }}} which, after integrating over Ω Ω ( t ) {\displaystyle \Omega (t)} and using generalized Stokes' theorem on the second term, reduces to the three desired terms.

Measure theory statement [ edit ] Let X {\displaystyle X} be an open subset of R {\displaystyle \mathbf {R} } , and Ω Ω {\displaystyle \Omega } be a measure space . Suppose f : : X × × Ω Ω → → R {\displaystyle f\colon X\times \Omega \to \mathbf {R} } satisfies the following conditions: [ 6 ] [ 7 ] [ 2 ] f ( x , ω ω ) {\displaystyle f(x,\omega )} is a Lebesgue-integrable function of ω ω {\displaystyle \omega } for each x ∈ ∈ X {\displaystyle x\in X} .

For almost all ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } , the partial derivative f x {\displaystyle f_{x}} exists for all x ∈ ∈ X {\displaystyle x\in X} .

There is an integrable function θ θ : : Ω Ω → → R {\displaystyle \theta \colon \Omega \to \mathbf {R} } such that | f x ( x , ω ω ) | ≤ ≤ θ θ ( ω ω ) {\displaystyle |f_{x}(x,\omega )|\leq \theta (\omega )} for all x ∈ ∈ X {\displaystyle x\in X} and almost every ω ω ∈ ∈ Ω Ω {\displaystyle \omega \in \Omega } .

Then, for all x ∈ ∈ X {\displaystyle x\in X} , d d x ∫ ∫ Ω Ω f ( x , ω ω ) d ω ω = ∫ ∫ Ω Ω f x ( x , ω ω ) d ω ω .

{\displaystyle {\frac {d}{dx}}\int _{\Omega }f(x,\omega )\,d\omega =\int _{\Omega }f_{x}(x,\omega )\,d\omega .} The proof relies on the dominated convergence theorem and the mean value theorem (details below).

Proofs [ edit ] Proof of basic form [ edit ] We first prove the case of constant limits of integration a and b .

We use Fubini's theorem to change the order of integration. For every x and h , such that h > 0 and both x and x + h are within [ x 0 , x 1 ] , we have: ∫ ∫ x x + h ∫ ∫ a b f x ( x , t ) d t d x = ∫ ∫ a b ∫ ∫ x x + h f x ( x , t ) d x d t = ∫ ∫ a b ( f ( x + h , t ) − − f ( x , t ) ) d t = ∫ ∫ a b f ( x + h , t ) d t − − ∫ ∫ a b f ( x , t ) d t {\displaystyle {\begin{aligned}\int _{x}^{x+h}\int _{a}^{b}f_{x}(x,t)\,dt\,dx&=\int _{a}^{b}\int _{x}^{x+h}f_{x}(x,t)\,dx\,dt\\[2ex]&=\int _{a}^{b}\left(f(x+h,t)-f(x,t)\right)\,dt\\[2ex]&=\int _{a}^{b}f(x+h,t)\,dt-\int _{a}^{b}f(x,t)\,dt\end{aligned}}} Note that the integrals at hand are well defined since f x ( x , t ) {\displaystyle f_{x}(x,t)} is continuous at the closed rectangle [ x 0 , x 1 ] × × [ a , b ] {\displaystyle [x_{0},x_{1}]\times [a,b]} and thus also uniformly continuous there; thus its integrals by either dt or dx are continuous in the other variable and also integrable by it (essentially this is because for uniformly continuous functions, one may pass the limit through the integration sign, as elaborated below).

Therefore: ∫ ∫ a b f ( x + h , t ) d t − − ∫ ∫ a b f ( x , t ) d t h = 1 h ∫ ∫ x x + h ∫ ∫ a b f x ( x , t ) d t d x = F ( x + h ) − − F ( x ) h {\displaystyle {\begin{aligned}{\frac {\int _{a}^{b}f(x+h,t)\,dt-\int _{a}^{b}f(x,t)\,dt}{h}}&={\frac {1}{h}}\int _{x}^{x+h}\int _{a}^{b}f_{x}(x,t)\,dt\,dx\\[2ex]&={\frac {F(x+h)-F(x)}{h}}\end{aligned}}} Where we have defined: F ( u ) := ∫ ∫ x 0 u ∫ ∫ a b f x ( x , t ) d t d x {\displaystyle F(u):=\int _{x_{0}}^{u}\int _{a}^{b}f_{x}(x,t)\,dt\,dx} (we may replace x 0 here by any other point between x 0 and x ) F is differentiable with derivative ∫ ∫ a b f x ( x , t ) d t {\textstyle \int _{a}^{b}f_{x}(x,t)\,dt} , so we can take the limit where h approaches zero. For the left hand side this limit is: d d x ∫ ∫ a b f ( x , t ) d t {\displaystyle {\frac {d}{dx}}\int _{a}^{b}f(x,t)\,dt} For the right hand side, we get: F ′ ( x ) = ∫ ∫ a b f x ( x , t ) d t {\displaystyle F'(x)=\int _{a}^{b}f_{x}(x,t)\,dt} And we thus prove the desired result: d d x ∫ ∫ a b f ( x , t ) d t = ∫ ∫ a b f x ( x , t ) d t {\displaystyle {\frac {d}{dx}}\int _{a}^{b}f(x,t)\,dt=\int _{a}^{b}f_{x}(x,t)\,dt} Another proof using the bounded convergence theorem [ edit ] If the integrals at hand are Lebesgue integrals , we may use the bounded convergence theorem (valid for these integrals, but not for Riemann integrals ) in order to show that the limit can be passed through the integral sign.

Note that this proof is weaker in the sense that it only shows that f x ( x , t ) is Lebesgue integrable, but not that it is Riemann integrable. In the former (stronger) proof, if f ( x , t ) is Riemann integrable, then so is f x ( x , t ) (and thus is obviously also Lebesgue integrable).

Let u ( x ) = ∫ ∫ a b f ( x , t ) d t .

{\displaystyle u(x)=\int _{a}^{b}f(x,t)\,dt.} 1 By the definition of the derivative, u ′ ( x ) = lim h → → 0 u ( x + h ) − − u ( x ) h .

{\displaystyle u'(x)=\lim _{h\to 0}{\frac {u(x+h)-u(x)}{h}}.} 2 Substitute equation ( 1 ) into equation ( 2 ). The difference of two integrals equals the integral of the difference, and 1/ h is a constant, so u ′ ( x ) = lim h → → 0 ∫ ∫ a b f ( x + h , t ) d t − − ∫ ∫ a b f ( x , t ) d t h = lim h → → 0 ∫ ∫ a b ( f ( x + h , t ) − − f ( x , t ) ) d t h = lim h → → 0 ∫ ∫ a b f ( x + h , t ) − − f ( x , t ) h d t .

{\displaystyle {\begin{aligned}u'(x)&=\lim _{h\to 0}{\frac {\int _{a}^{b}f(x+h,t)\,dt-\int _{a}^{b}f(x,t)\,dt}{h}}\\&=\lim _{h\to 0}{\frac {\int _{a}^{b}\left(f(x+h,t)-f(x,t)\right)\,dt}{h}}\\&=\lim _{h\to 0}\int _{a}^{b}{\frac {f(x+h,t)-f(x,t)}{h}}\,dt.\end{aligned}}} We now show that the limit can be passed through the integral sign.

We claim that the passage of the limit under the integral sign is valid by the bounded convergence theorem (a corollary of the dominated convergence theorem ).  For each δ > 0, consider the difference quotient f δ δ ( x , t ) = f ( x + δ δ , t ) − − f ( x , t ) δ δ .

{\displaystyle f_{\delta }(x,t)={\frac {f(x+\delta ,t)-f(x,t)}{\delta }}.} For t fixed, the mean value theorem implies there exists z in the interval [ x , x + δ ] such that f δ δ ( x , t ) = f x ( z , t ) .

{\displaystyle f_{\delta }(x,t)=f_{x}(z,t).} Continuity of f x ( x , t ) and compactness of the domain together imply that f x ( x , t ) is bounded.  The above application of the mean value theorem therefore gives a uniform (independent of t {\displaystyle t} ) bound on f δ δ ( x , t ) {\displaystyle f_{\delta }(x,t)} .  The difference quotients converge pointwise to the partial derivative f x by the assumption that the partial derivative exists.

The above argument shows that for every sequence { δ n } → 0, the sequence { f δ δ n ( x , t ) } {\displaystyle \{f_{\delta _{n}}(x,t)\}} is uniformly bounded and converges pointwise to f x .  The bounded convergence theorem states that if a sequence of functions on a set of finite measure is uniformly bounded and converges pointwise, then passage of the limit under the integral is valid.  In particular, the limit and integral may be exchanged for every sequence { δ n } → 0.  Therefore, the limit as δ → 0 may be passed through the integral sign.

If instead we only know that there is an integrable function θ θ : : Ω Ω → → R {\displaystyle \theta \colon \Omega \to \mathbf {R} } such that | f x ( x , ω ω ) | ≤ ≤ θ θ ( ω ω ) {\displaystyle |f_{x}(x,\omega )|\leq \theta (\omega )} , then | f δ δ ( x , t ) | = | f x ( z , t ) | ≤ ≤ θ θ ( ω ω ) {\displaystyle |f_{\delta }(x,t)|=|f_{x}(z,t)|\leq \theta (\omega )} and the dominated convergence theorem allows us to move the limit inside of the integral.

Variable limits form [ edit ] For a continuous real valued function g of one real variable , and real valued differentiable functions f 1 {\displaystyle f_{1}} and f 2 {\displaystyle f_{2}} of one real variable, d d x ( ∫ ∫ f 1 ( x ) f 2 ( x ) g ( t ) d t ) = g ( f 2 ( x ) ) f 2 ′ ( x ) − − g ( f 1 ( x ) ) f 1 ′ ( x ) .

{\displaystyle {\frac {d}{dx}}\left(\int _{f_{1}(x)}^{f_{2}(x)}g(t)\,dt\right)=g\left(f_{2}(x)\right){f_{2}'(x)}-g\left(f_{1}(x)\right){f_{1}'(x)}.} This follows from the chain rule and the First Fundamental Theorem of Calculus . Define G ( x ) = ∫ ∫ f 1 ( x ) f 2 ( x ) g ( t ) d t , {\displaystyle G(x)=\int _{f_{1}(x)}^{f_{2}(x)}g(t)\,dt,} and Γ Γ ( x ) = ∫ ∫ 0 x g ( t ) d t .

{\displaystyle \Gamma (x)=\int _{0}^{x}g(t)\,dt.} (The lower limit just has to be some number in the domain of g {\displaystyle g} ) Then, G ( x ) {\displaystyle G(x)} can be written as a composition : G ( x ) = ( Γ Γ ∘ ∘ f 2 ) ( x ) − − ( Γ Γ ∘ ∘ f 1 ) ( x ) {\displaystyle G(x)=(\Gamma \circ f_{2})(x)-(\Gamma \circ f_{1})(x)} . The Chain Rule then implies that G ′ ( x ) = Γ Γ ′ ( f 2 ( x ) ) f 2 ′ ( x ) − − Γ Γ ′ ( f 1 ( x ) ) f 1 ′ ( x ) .

{\displaystyle G'(x)=\Gamma '\left(f_{2}(x)\right)f_{2}'(x)-\Gamma '\left(f_{1}(x)\right)f_{1}'(x).} By the First Fundamental Theorem of Calculus , Γ Γ ′ ( x ) = g ( x ) {\displaystyle \Gamma '(x)=g(x)} . Therefore, substituting this result above, we get the desired equation: G ′ ( x ) = g ( f 2 ( x ) ) f 2 ′ ( x ) − − g ( f 1 ( x ) ) f 1 ′ ( x ) .

{\displaystyle G'(x)=g\left(f_{2}(x)\right){f_{2}'(x)}-g\left(f_{1}(x)\right){f_{1}'(x)}.} Note: This form can be particularly useful if the expression to be differentiated is of the form: ∫ ∫ f 1 ( x ) f 2 ( x ) h ( x ) g ( t ) d t {\displaystyle \int _{f_{1}(x)}^{f_{2}(x)}h(x)\,g(t)\,dt} Because h ( x ) {\displaystyle h(x)} does not depend on the limits of integration, it may be moved out from under the integral sign, and the above form may be used with the Product rule , i.e., d d x ( ∫ ∫ f 1 ( x ) f 2 ( x ) h ( x ) g ( t ) d t ) = d d x ( h ( x ) ∫ ∫ f 1 ( x ) f 2 ( x ) g ( t ) d t ) = h ′ ( x ) ∫ ∫ f 1 ( x ) f 2 ( x ) g ( t ) d t + h ( x ) d d x ( ∫ ∫ f 1 ( x ) f 2 ( x ) g ( t ) d t ) {\displaystyle {\begin{aligned}{\frac {d}{dx}}\left(\int _{f_{1}(x)}^{f_{2}(x)}h(x)g(t)\,dt\right)&={\frac {d}{dx}}\left(h(x)\int _{f_{1}(x)}^{f_{2}(x)}g(t)\,dt\right)\\&=h'(x)\int _{f_{1}(x)}^{f_{2}(x)}g(t)\,dt+h(x){\frac {d}{dx}}\left(\int _{f_{1}(x)}^{f_{2}(x)}g(t)\,dt\right)\end{aligned}}} General form with variable limits [ edit ] Set φ φ ( α α ) = ∫ ∫ a b f ( x , α α ) d x , {\displaystyle \varphi (\alpha )=\int _{a}^{b}f(x,\alpha )\,dx,} where a and b are functions of α that exhibit increments Δ a and Δ b , respectively, when α is increased by Δ α . Then, Δ Δ φ φ = φ φ ( α α + Δ Δ α α ) − − φ φ ( α α ) = ∫ ∫ a + Δ Δ a b + Δ Δ b f ( x , α α + Δ Δ α α ) d x − − ∫ ∫ a b f ( x , α α ) d x = ∫ ∫ a + Δ Δ a a f ( x , α α + Δ Δ α α ) d x + ∫ ∫ a b f ( x , α α + Δ Δ α α ) d x + ∫ ∫ b b + Δ Δ b f ( x , α α + Δ Δ α α ) d x − − ∫ ∫ a b f ( x , α α ) d x = − − ∫ ∫ a a + Δ Δ a f ( x , α α + Δ Δ α α ) d x + ∫ ∫ a b [ f ( x , α α + Δ Δ α α ) − − f ( x , α α ) ] d x + ∫ ∫ b b + Δ Δ b f ( x , α α + Δ Δ α α ) d x .

{\displaystyle {\begin{aligned}\Delta \varphi &=\varphi (\alpha +\Delta \alpha )-\varphi (\alpha )\\[4pt]&=\int _{a+\Delta a}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx-\int _{a}^{b}f(x,\alpha )\,dx\\[4pt]&=\int _{a+\Delta a}^{a}f(x,\alpha +\Delta \alpha )\,dx+\int _{a}^{b}f(x,\alpha +\Delta \alpha )\,dx+\int _{b}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx-\int _{a}^{b}f(x,\alpha )\,dx\\[4pt]&=-\int _{a}^{a+\Delta a}f(x,\alpha +\Delta \alpha )\,dx+\int _{a}^{b}[f(x,\alpha +\Delta \alpha )-f(x,\alpha )]\,dx+\int _{b}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx.\end{aligned}}} A form of the mean value theorem , ∫ ∫ a b f ( x ) d x = ( b − − a ) f ( ξ ξ ) {\textstyle \int _{a}^{b}f(x)\,dx=(b-a)f(\xi )} , where a < ξ < b , may be applied to the first and last integrals of the formula for Δ φ above, resulting in Δ Δ φ φ = − − Δ Δ a f ( ξ ξ 1 , α α + Δ Δ α α ) + ∫ ∫ a b [ f ( x , α α + Δ Δ α α ) − − f ( x , α α ) ] d x + Δ Δ b f ( ξ ξ 2 , α α + Δ Δ α α ) .

{\displaystyle \Delta \varphi =-\Delta af(\xi _{1},\alpha +\Delta \alpha )+\int _{a}^{b}[f(x,\alpha +\Delta \alpha )-f(x,\alpha )]\,dx+\Delta bf(\xi _{2},\alpha +\Delta \alpha ).} Divide by Δ α and let Δ α → 0.  Notice ξ 1 → a and ξ 2 → b .  We may pass the limit through the integral sign: lim Δ Δ α α → → 0 ∫ ∫ a b f ( x , α α + Δ Δ α α ) − − f ( x , α α ) Δ Δ α α d x = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x , {\displaystyle \lim _{\Delta \alpha \to 0}\int _{a}^{b}{\frac {f(x,\alpha +\Delta \alpha )-f(x,\alpha )}{\Delta \alpha }}\,dx=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx,} again by the bounded convergence theorem.  This yields the general form of the Leibniz integral rule, d φ φ d α α = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x + f ( b , α α ) d b d α α − − f ( a , α α ) d a d α α .

{\displaystyle {\frac {d\varphi }{d\alpha }}=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx+f(b,\alpha ){\frac {db}{d\alpha }}-f(a,\alpha ){\frac {da}{d\alpha }}.} Alternative proof of the general form with variable limits, using the chain rule [ edit ] The general form of Leibniz's Integral Rule with variable limits can be derived as a consequence of the basic form of Leibniz's Integral Rule, the multivariable chain rule , and the first fundamental theorem of calculus . Suppose f {\displaystyle f} is defined in a rectangle in the x − − t {\displaystyle x-t} plane, for x ∈ ∈ [ x 1 , x 2 ] {\displaystyle x\in [x_{1},x_{2}]} and t ∈ ∈ [ t 1 , t 2 ] {\displaystyle t\in [t_{1},t_{2}]} . Also, assume f {\displaystyle f} and the partial derivative ∂ ∂ f ∂ ∂ x {\textstyle {\frac {\partial f}{\partial x}}} are both continuous functions on this rectangle. Suppose a , b {\displaystyle a,b} are differentiable real valued functions defined on [ x 1 , x 2 ] {\displaystyle [x_{1},x_{2}]} , with values in [ t 1 , t 2 ] {\displaystyle [t_{1},t_{2}]} (i.e. for every x ∈ ∈ [ x 1 , x 2 ] , a ( x ) , b ( x ) ∈ ∈ [ t 1 , t 2 ] {\displaystyle x\in [x_{1},x_{2}],a(x),b(x)\in [t_{1},t_{2}]} ). Now, set F ( x , y ) = ∫ ∫ t 1 y f ( x , t ) d t , for x ∈ ∈ [ x 1 , x 2 ] and y ∈ ∈ [ t 1 , t 2 ] {\displaystyle F(x,y)=\int _{t_{1}}^{y}f(x,t)\,dt,\qquad {\text{for}}~x\in [x_{1},x_{2}]~{\text{and}}~y\in [t_{1},t_{2}]} and G ( x ) = ∫ ∫ a ( x ) b ( x ) f ( x , t ) d t , for x ∈ ∈ [ x 1 , x 2 ] {\displaystyle G(x)=\int _{a(x)}^{b(x)}f(x,t)\,dt,\quad {\text{for}}~x\in [x_{1},x_{2}]} Then, by  properties of definite Integrals , we can write G ( x ) = ∫ ∫ t 1 b ( x ) f ( x , t ) d t − − ∫ ∫ t 1 a ( x ) f ( x , t ) d t = F ( x , b ( x ) ) − − F ( x , a ( x ) ) {\displaystyle G(x)=\int _{t_{1}}^{b(x)}f(x,t)\,dt-\int _{t_{1}}^{a(x)}f(x,t)\,dt=F(x,b(x))-F(x,a(x))} Since the functions F , a , b {\displaystyle F,a,b} are all differentiable (see the remark at the end of the proof), by the multivariable chain rule , it follows that G {\displaystyle G} is differentiable, and its derivative is given by the formula: G ′ ( x ) = ( ∂ ∂ F ∂ ∂ x ( x , b ( x ) ) + ∂ ∂ F ∂ ∂ b ( x ) ( x , b ( x ) ) b ′ ( x ) ) − − ( ∂ ∂ F ∂ ∂ x ( x , a ( x ) ) + ∂ ∂ F ∂ ∂ a ( x ) ( x , a ( x ) ) a ′ ( x ) ) {\displaystyle G'(x)=\left({\frac {\partial F}{\partial x}}(x,b(x))+{\frac {\partial F}{\partial b(x)}}(x,b(x))b'(x)\right)-\left({\frac {\partial F}{\partial x}}(x,a(x))+{\frac {\partial F}{\partial a(x)}}(x,a(x))a'(x)\right)} Now, note that for every x ∈ ∈ [ x 1 , x 2 ] {\displaystyle x\in [x_{1},x_{2}]} , and for every y ∈ ∈ [ t 1 , t 2 ] {\displaystyle y\in [t_{1},t_{2}]} , we have that ∂ ∂ F ∂ ∂ x ( x , y ) = ∫ ∫ t 1 y ∂ ∂ f ∂ ∂ x ( x , t ) d t {\textstyle {\frac {\partial F}{\partial x}}(x,y)=\int _{t_{1}}^{y}{\frac {\partial f}{\partial x}}(x,t)\,dt} , because when taking the partial derivative with respect to x {\displaystyle x} of F {\displaystyle F} , we are keeping y {\displaystyle y} fixed in the expression ∫ ∫ t 1 y f ( x , t ) d t {\textstyle \int _{t_{1}}^{y}f(x,t)\,dt} ; thus the basic form of Leibniz's Integral Rule with constant limits of integration applies. Next, by the first fundamental theorem of calculus , we have that ∂ ∂ F ∂ ∂ y ( x , y ) = f ( x , y ) {\textstyle {\frac {\partial F}{\partial y}}(x,y)=f(x,y)} ; because when taking the partial derivative with respect to y {\displaystyle y} of F {\displaystyle F} , the first variable x {\displaystyle x} is fixed, so the fundamental theorem can indeed be applied.

Substituting these results into the equation for G ′ ( x ) {\displaystyle G'(x)} above gives: G ′ ( x ) = ( ∫ ∫ t 1 b ( x ) ∂ ∂ f ∂ ∂ x ( x , t ) d t + f ( x , b ( x ) ) b ′ ( x ) ) − − ( ∫ ∫ t 1 a ( x ) ∂ ∂ f ∂ ∂ x ( x , t ) d t + f ( x , a ( x ) ) a ′ ( x ) ) = f ( x , b ( x ) ) b ′ ( x ) − − f ( x , a ( x ) ) a ′ ( x ) + ∫ ∫ a ( x ) b ( x ) ∂ ∂ f ∂ ∂ x ( x , t ) d t , {\displaystyle {\begin{aligned}G'(x)&=\left(\int _{t_{1}}^{b(x)}{\frac {\partial f}{\partial x}}(x,t)\,dt+f(x,b(x))b'(x)\right)-\left(\int _{t_{1}}^{a(x)}{\dfrac {\partial f}{\partial x}}(x,t)\,dt+f(x,a(x))a'(x)\right)\\[2pt]&=f(x,b(x))b'(x)-f(x,a(x))a'(x)+\int _{a(x)}^{b(x)}{\frac {\partial f}{\partial x}}(x,t)\,dt,\end{aligned}}} as desired.

There is a technical point in the proof above which is worth noting: applying the Chain Rule to G {\displaystyle G} requires that F {\displaystyle F} already be differentiable . This is where we use our assumptions about f {\displaystyle f} . As mentioned above, the partial derivatives of F {\displaystyle F} are given by the formulas ∂ ∂ F ∂ ∂ x ( x , y ) = ∫ ∫ t 1 y ∂ ∂ f ∂ ∂ x ( x , t ) d t {\textstyle {\frac {\partial F}{\partial x}}(x,y)=\int _{t_{1}}^{y}{\frac {\partial f}{\partial x}}(x,t)\,dt} and ∂ ∂ F ∂ ∂ y ( x , y ) = f ( x , y ) {\textstyle {\frac {\partial F}{\partial y}}(x,y)=f(x,y)} . Since ∂ ∂ f ∂ ∂ x {\textstyle {\dfrac {\partial f}{\partial x}}} is continuous, its integral is also a continuous function, [ 8 ] and since f {\displaystyle f} is also continuous, these two results show that both the partial derivatives of F {\displaystyle F} are continuous. Since continuity of partial derivatives implies differentiability of the function, [ 9 ] F {\displaystyle F} is indeed differentiable.

Three-dimensional, time-dependent form [ edit ] See also: § Higher dimensions At time t the surface Σ in Figure 1 contains a set of points arranged about a centroid C ( t ) {\displaystyle \mathbf {C} (t)} .  The function F ( r , t ) {\displaystyle \mathbf {F} (\mathbf {r} ,t)} can be written as F ( C ( t ) + r − − C ( t ) , t ) = F ( C ( t ) + I , t ) , {\displaystyle \mathbf {F} (\mathbf {C} (t)+\mathbf {r} -\mathbf {C} (t),t)=\mathbf {F} (\mathbf {C} (t)+\mathbf {I} ,t),} with I {\displaystyle \mathbf {I} } independent of time. Variables are shifted to a new frame of reference attached to the moving surface, with origin at C ( t ) {\displaystyle \mathbf {C} (t)} . For a rigidly translating surface, the limits of integration are then independent of time, so: d d t ( ∬ ∬ Σ Σ ( t ) d A r ⋅ ⋅ F ( r , t ) ) = ∬ ∬ Σ Σ d A I ⋅ ⋅ d d t F ( C ( t ) + I , t ) , {\displaystyle {\frac {d}{dt}}\left(\iint _{\Sigma (t)}d\mathbf {A} _{\mathbf {r} }\cdot \mathbf {F} (\mathbf {r} ,t)\right)=\iint _{\Sigma }d\mathbf {A} _{\mathbf {I} }\cdot {\frac {d}{dt}}\mathbf {F} (\mathbf {C} (t)+\mathbf {I} ,t),} where the limits of integration confining the integral to the region Σ no longer are time dependent so differentiation passes through the integration to act on the integrand only: d d t F ( C ( t ) + I , t ) = F t ( C ( t ) + I , t ) + v ⋅ ⋅ ∇ ∇ F ( C ( t ) + I , t ) = F t ( r , t ) + v ⋅ ⋅ ∇ ∇ F ( r , t ) , {\displaystyle {\frac {d}{dt}}\mathbf {F} (\mathbf {C} (t)+\mathbf {I} ,t)=\mathbf {F} _{t}(\mathbf {C} (t)+\mathbf {I} ,t)+\mathbf {v\cdot \nabla F} (\mathbf {C} (t)+\mathbf {I} ,t)=\mathbf {F} _{t}(\mathbf {r} ,t)+\mathbf {v} \cdot \nabla \mathbf {F} (\mathbf {r} ,t),} with the velocity of motion of the surface defined by v = d d t C ( t ) .

{\displaystyle \mathbf {v} ={\frac {d}{dt}}\mathbf {C} (t).} This equation expresses the material derivative of the field, that is, the derivative with respect to a coordinate system attached to the moving surface. Having found the derivative, variables can be switched back to the original frame of reference. We notice that (see article on curl ) ∇ ∇ × × ( v × × F ) = ( ∇ ∇ ⋅ ⋅ F + F ⋅ ⋅ ∇ ∇ ) v − − ( ∇ ∇ ⋅ ⋅ v + v ⋅ ⋅ ∇ ∇ ) F , {\displaystyle \nabla \times \left(\mathbf {v} \times \mathbf {F} \right)=(\nabla \cdot \mathbf {F} +\mathbf {F} \cdot \nabla )\mathbf {v} -(\nabla \cdot \mathbf {v} +\mathbf {v} \cdot \nabla )\mathbf {F} ,} and that Stokes theorem equates the surface integral of the curl over Σ with a line integral over ∂Σ : d d t ( ∬ ∬ Σ Σ ( t ) F ( r , t ) ⋅ ⋅ d A ) = ∬ ∬ Σ Σ ( t ) ( F t ( r , t ) + ( F ⋅ ⋅ ∇ ∇ ) v + ( ∇ ∇ ⋅ ⋅ F ) v − − ( ∇ ∇ ⋅ ⋅ v ) F ) ⋅ ⋅ d A − − ∮ ∮ ∂ ∂ Σ Σ ( t ) ( v × × F ) ⋅ ⋅ d s .

{\displaystyle {\frac {d}{dt}}\left(\iint _{\Sigma (t)}\mathbf {F} (\mathbf {r} ,t)\cdot d\mathbf {A} \right)=\iint _{\Sigma (t)}{\big (}\mathbf {F} _{t}(\mathbf {r} ,t)+\left(\mathbf {F\cdot \nabla } \right)\mathbf {v} +\left(\nabla \cdot \mathbf {F} \right)\mathbf {v} -(\nabla \cdot \mathbf {v} )\mathbf {F} {\big )}\cdot d\mathbf {A} -\oint _{\partial \Sigma (t)}\left(\mathbf {v} \times \mathbf {F} \right)\cdot d\mathbf {s} .} The sign of the line integral is based on the right-hand rule for the choice of direction of line element d s . To establish this sign, for example, suppose the field F points in the positive z -direction, and the surface Σ is a portion of the xy -plane with perimeter ∂Σ. We adopt the normal to Σ to be in the positive z -direction. Positive traversal of ∂Σ is then counterclockwise (right-hand rule with thumb along z -axis). Then the integral on the left-hand side determines a positive flux of F through Σ. Suppose Σ translates in the positive x -direction at velocity v . An element of the boundary of Σ parallel to the y -axis, say d s , sweeps out an area v t × d s in time t . If we integrate around the boundary ∂Σ in a counterclockwise sense, v t × d s points in the negative z -direction on the left side of ∂Σ (where d s points downward), and in the positive z -direction on the right side of ∂Σ (where d s points upward), which makes sense because Σ is moving to the right, adding area on the right and losing it on the left. On that basis, the flux of F is increasing on the right of ∂Σ and decreasing on the left. However, the dot product v × F ⋅ d s = − F × v ⋅ d s = − F ⋅ v × d s . Consequently, the sign of the line integral is taken as negative.

If v is a constant, d d t ∬ ∬ Σ Σ ( t ) F ( r , t ) ⋅ ⋅ d A = ∬ ∬ Σ Σ ( t ) ( F t ( r , t ) + ( ∇ ∇ ⋅ ⋅ F ) v ) ⋅ ⋅ d A − − ∮ ∮ ∂ ∂ Σ Σ ( t ) ( v × × F ) ⋅ ⋅ d s , {\displaystyle {\frac {d}{dt}}\iint _{\Sigma (t)}\mathbf {F} (\mathbf {r} ,t)\cdot d\mathbf {A} =\iint _{\Sigma (t)}{\big (}\mathbf {F} _{t}(\mathbf {r} ,t)+\left(\nabla \cdot \mathbf {F} \right)\mathbf {v} {\big )}\cdot d\mathbf {A} -\oint _{\partial \Sigma (t)}\left(\mathbf {v} \times \mathbf {F} \right)\cdot \,d\mathbf {s} ,} which is the quoted result. This proof does not consider the possibility of the surface deforming as it moves.

Alternative derivation [ edit ] Lemma.

One has: ∂ ∂ ∂ ∂ b ( ∫ ∫ a b f ( x ) d x ) = f ( b ) , ∂ ∂ ∂ ∂ a ( ∫ ∫ a b f ( x ) d x ) = − − f ( a ) .

{\displaystyle {\frac {\partial }{\partial b}}\left(\int _{a}^{b}f(x)\,dx\right)=f(b),\qquad {\frac {\partial }{\partial a}}\left(\int _{a}^{b}f(x)\,dx\right)=-f(a).} Proof.

From the proof of the fundamental theorem of calculus , ∂ ∂ ∂ ∂ b ( ∫ ∫ a b f ( x ) d x ) = lim Δ Δ b → → 0 1 Δ Δ b ( ∫ ∫ a b + Δ Δ b f ( x ) d x − − ∫ ∫ a b f ( x ) d x ) = lim Δ Δ b → → 0 1 Δ Δ b ( ∫ ∫ a b f ( x ) d x + ∫ ∫ b b + Δ Δ b f ( x ) d x − − ∫ ∫ a b f ( x ) d x ) = lim Δ Δ b → → 0 1 Δ Δ b ∫ ∫ b b + Δ Δ b f ( x ) d x = lim Δ Δ b → → 0 1 Δ Δ b [ f ( b ) Δ Δ b + O ( Δ Δ b 2 ) ] = f ( b ) , {\displaystyle {\begin{aligned}{\frac {\partial }{\partial b}}\left(\int _{a}^{b}f(x)\,dx\right)&=\lim _{\Delta b\to 0}{\frac {1}{\Delta b}}\left(\int _{a}^{b+\Delta b}f(x)\,dx-\int _{a}^{b}f(x)\,dx\right)\\[1ex]&=\lim _{\Delta b\to 0}{\frac {1}{\Delta b}}\left(\int _{a}^{b}f(x)\,dx+\int _{b}^{b+\Delta b}f(x)\,dx-\int _{a}^{b}f(x)\,dx\right)\\[1ex]&=\lim _{\Delta b\to 0}{\frac {1}{\Delta b}}\int _{b}^{b+\Delta b}f(x)\,dx\\[1ex]&=\lim _{\Delta b\to 0}{\frac {1}{\Delta b}}\left[f(b)\Delta b+O\left(\Delta b^{2}\right)\right]\\[1ex]&=f(b),\end{aligned}}} and ∂ ∂ ∂ ∂ a ( ∫ ∫ a b f ( x ) d x ) = lim Δ Δ a → → 0 1 Δ Δ a [ ∫ ∫ a + Δ Δ a b f ( x ) d x − − ∫ ∫ a b f ( x ) d x ] = lim Δ Δ a → → 0 1 Δ Δ a ∫ ∫ a + Δ Δ a a f ( x ) d x = lim Δ Δ a → → 0 1 Δ Δ a [ − − f ( a ) Δ Δ a + O ( Δ Δ a 2 ) ] = − − f ( a ) .

{\displaystyle {\begin{aligned}{\frac {\partial }{\partial a}}\left(\int _{a}^{b}f(x)\,dx\right)&=\lim _{\Delta a\to 0}{\frac {1}{\Delta a}}\left[\int _{a+\Delta a}^{b}f(x)\,dx-\int _{a}^{b}f(x)\,dx\right]\\[6pt]&=\lim _{\Delta a\to 0}{\frac {1}{\Delta a}}\int _{a+\Delta a}^{a}f(x)\,dx\\[6pt]&=\lim _{\Delta a\to 0}{\frac {1}{\Delta a}}\left[-f(a)\Delta a+O\left(\Delta a^{2}\right)\right]\\[6pt]&=-f(a).\end{aligned}}} Suppose a and b are constant, and that f ( x ) involves a parameter α which is constant in the integration but may vary to form different integrals. Assume that f ( x , α ) is a continuous function of x and α in the compact set {( x , α ) : α 0 ≤ α ≤ α 1 and a ≤ x ≤ b }, and that the partial derivative f α ( x , α ) exists and is continuous.  If one defines: φ φ ( α α ) = ∫ ∫ a b f ( x , α α ) d x , {\displaystyle \varphi (\alpha )=\int _{a}^{b}f(x,\alpha )\,dx,} then φ φ {\displaystyle \varphi } may be differentiated with respect to α by differentiating under the integral sign, i.e., d φ φ d α α = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x .

{\displaystyle {\frac {d\varphi }{d\alpha }}=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx.} By the Heine–Cantor theorem it is uniformly continuous in that set. In other words, for any ε > 0 there exists Δ α such that for all values of x in [ a , b ], | f ( x , α α + Δ Δ α α ) − − f ( x , α α ) | < ε ε .

{\displaystyle |f(x,\alpha +\Delta \alpha )-f(x,\alpha )|<\varepsilon .} On the other hand, Δ Δ φ φ = φ φ ( α α + Δ Δ α α ) − − φ φ ( α α ) = ∫ ∫ a b f ( x , α α + Δ Δ α α ) d x − − ∫ ∫ a b f ( x , α α ) d x = ∫ ∫ a b ( f ( x , α α + Δ Δ α α ) − − f ( x , α α ) ) d x ≤ ≤ ε ε ( b − − a ) .

{\displaystyle {\begin{aligned}\Delta \varphi &=\varphi (\alpha +\Delta \alpha )-\varphi (\alpha )\\[6pt]&=\int _{a}^{b}f(x,\alpha +\Delta \alpha )\,dx-\int _{a}^{b}f(x,\alpha )\,dx\\[6pt]&=\int _{a}^{b}\left(f(x,\alpha +\Delta \alpha )-f(x,\alpha )\right)\,dx\\[6pt]&\leq \varepsilon (b-a).\end{aligned}}} Hence φ ( α ) is a continuous function.

Similarly if ∂ ∂ ∂ ∂ α α f ( x , α α ) {\displaystyle {\frac {\partial }{\partial \alpha }}f(x,\alpha )} exists and is continuous, then for all ε > 0 there exists Δ α such that: ∀ ∀ x ∈ ∈ [ a , b ] , | f ( x , α α + Δ Δ α α ) − − f ( x , α α ) Δ Δ α α − − ∂ ∂ f ∂ ∂ α α | < ε ε .

{\displaystyle \forall x\in [a,b],\quad \left|{\frac {f(x,\alpha +\Delta \alpha )-f(x,\alpha )}{\Delta \alpha }}-{\frac {\partial f}{\partial \alpha }}\right|<\varepsilon .} Therefore, Δ Δ φ φ Δ Δ α α = ∫ ∫ a b f ( x , α α + Δ Δ α α ) − − f ( x , α α ) Δ Δ α α d x = ∫ ∫ a b ∂ ∂ f ( x , α α ) ∂ ∂ α α d x + R , {\displaystyle {\frac {\Delta \varphi }{\Delta \alpha }}=\int _{a}^{b}{\frac {f(x,\alpha +\Delta \alpha )-f(x,\alpha )}{\Delta \alpha }}\,dx=\int _{a}^{b}{\frac {\partial f(x,\alpha )}{\partial \alpha }}\,dx+R,} where | R | < ∫ ∫ a b ε ε d x = ε ε ( b − − a ) .

{\displaystyle |R|<\int _{a}^{b}\varepsilon \,dx=\varepsilon (b-a).} Now, ε → 0 as Δ α → 0, so lim Δ Δ α α → → 0 Δ Δ φ φ Δ Δ α α = d φ φ d α α = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x .

{\displaystyle \lim _{{\Delta \alpha }\to 0}{\frac {\Delta \varphi }{\Delta \alpha }}={\frac {d\varphi }{d\alpha }}=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx.} This is the formula we set out to prove.

Now, suppose ∫ ∫ a b f ( x , α α ) d x = φ φ ( α α ) , {\displaystyle \int _{a}^{b}f(x,\alpha )\,dx=\varphi (\alpha ),} where a and b are functions of α which take increments Δ a and Δ b , respectively, when α is increased by Δ α . Then, Δ Δ φ φ = φ φ ( α α + Δ Δ α α ) − − φ φ ( α α ) = ∫ ∫ a + Δ Δ a b + Δ Δ b f ( x , α α + Δ Δ α α ) d x − − ∫ ∫ a b f ( x , α α ) d x = ∫ ∫ a + Δ Δ a a f ( x , α α + Δ Δ α α ) d x + ∫ ∫ a b f ( x , α α + Δ Δ α α ) d x + ∫ ∫ b b + Δ Δ b f ( x , α α + Δ Δ α α ) d x − − ∫ ∫ a b f ( x , α α ) d x = − − ∫ ∫ a a + Δ Δ a f ( x , α α + Δ Δ α α ) d x + ∫ ∫ a b [ f ( x , α α + Δ Δ α α ) − − f ( x , α α ) ] d x + ∫ ∫ b b + Δ Δ b f ( x , α α + Δ Δ α α ) d x .

{\displaystyle {\begin{aligned}\Delta \varphi &=\varphi (\alpha +\Delta \alpha )-\varphi (\alpha )\\[6pt]&=\int _{a+\Delta a}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx-\int _{a}^{b}f(x,\alpha )\,dx\\[6pt]&=\int _{a+\Delta a}^{a}f(x,\alpha +\Delta \alpha )\,dx+\int _{a}^{b}f(x,\alpha +\Delta \alpha )\,dx+\int _{b}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx-\int _{a}^{b}f(x,\alpha )\,dx\\[6pt]&=-\int _{a}^{a+\Delta a}f(x,\alpha +\Delta \alpha )\,dx+\int _{a}^{b}[f(x,\alpha +\Delta \alpha )-f(x,\alpha )]\,dx+\int _{b}^{b+\Delta b}f(x,\alpha +\Delta \alpha )\,dx.\end{aligned}}} A form of the mean value theorem , ∫ ∫ a b f ( x ) d x = ( b − − a ) f ( ξ ξ ) , {\textstyle \int _{a}^{b}f(x)\,dx=(b-a)f(\xi ),} where a < ξ < b , can be applied to the first and last integrals of the formula for Δ φ above, resulting in Δ Δ φ φ = − − Δ Δ a f ( ξ ξ 1 , α α + Δ Δ α α ) + ∫ ∫ a b [ f ( x , α α + Δ Δ α α ) − − f ( x , α α ) ] d x + Δ Δ b f ( ξ ξ 2 , α α + Δ Δ α α ) .

{\displaystyle \Delta \varphi =-\Delta a\,f(\xi _{1},\alpha +\Delta \alpha )+\int _{a}^{b}[f(x,\alpha +\Delta \alpha )-f(x,\alpha )]\,dx+\Delta b\,f(\xi _{2},\alpha +\Delta \alpha ).} Dividing by Δ α , letting Δ α → 0, noticing ξ 1 → a and ξ 2 → b and using the above derivation for d φ φ d α α = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x {\displaystyle {\frac {d\varphi }{d\alpha }}=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx} yields d φ φ d α α = ∫ ∫ a b ∂ ∂ ∂ ∂ α α f ( x , α α ) d x + f ( b , α α ) ∂ ∂ b ∂ ∂ α α − − f ( a , α α ) ∂ ∂ a ∂ ∂ α α .

{\displaystyle {\frac {d\varphi }{d\alpha }}=\int _{a}^{b}{\frac {\partial }{\partial \alpha }}f(x,\alpha )\,dx+f(b,\alpha ){\frac {\partial b}{\partial \alpha }}-f(a,\alpha ){\frac {\partial a}{\partial \alpha }}.} This is the general form of the Leibniz integral rule.

Examples [ edit ] Example 1: Fixed limits [ edit ] Consider the function φ φ ( α α ) = ∫ ∫ 0 1 α α x 2 + α α 2 d x .

{\displaystyle \varphi (\alpha )=\int _{0}^{1}{\frac {\alpha }{x^{2}+\alpha ^{2}}}\,dx.} The function under the integral sign is not continuous at the point ( x , α α ) = ( 0 , 0 ) {\displaystyle (x,\alpha )=(0,0)} , and the function φ φ ( α α ) {\displaystyle \varphi (\alpha )} has a discontinuity at α α = 0 {\displaystyle \alpha =0} because φ φ ( α α ) {\displaystyle \varphi (\alpha )} approaches ± ± π π / 2 {\displaystyle \pm \pi /2} as α α → → 0 ± ± {\displaystyle \alpha \to 0^{\pm }} .

If we differentiate φ φ ( α α ) {\displaystyle \varphi (\alpha )} with respect to α α {\displaystyle \alpha } under the integral sign, we get d d α α φ φ ( α α ) = ∫ ∫ 0 1 ∂ ∂ ∂ ∂ α α ( α α x 2 + α α 2 ) d x = ∫ ∫ 0 1 x 2 − − α α 2 ( x 2 + α α 2 ) 2 d x = − − x x 2 + α α 2 | 0 1 = − − 1 1 + α α 2 , {\displaystyle {\frac {d}{d\alpha }}\varphi (\alpha )=\int _{0}^{1}{\frac {\partial }{\partial \alpha }}\left({\frac {\alpha }{x^{2}+\alpha ^{2}}}\right)\,dx=\int _{0}^{1}{\frac {x^{2}-\alpha ^{2}}{(x^{2}+\alpha ^{2})^{2}}}dx=\left.-{\frac {x}{x^{2}+\alpha ^{2}}}\right|_{0}^{1}=-{\frac {1}{1+\alpha ^{2}}},} for α α ≠ ≠ 0 {\displaystyle \alpha \neq 0} .  This may be integrated (with respect to α α {\displaystyle \alpha } ) to find φ φ ( α α ) = { 0 , α α = 0 , − − arctan ⁡ ⁡ ( α α ) + π π 2 , α α ≠ ≠ 0.

{\displaystyle \varphi (\alpha )={\begin{cases}0,&\alpha =0,\\-\arctan({\alpha })+{\frac {\pi }{2}},&\alpha \neq 0.\end{cases}}} Example 2: Variable limits [ edit ] An example with variable limits: d d x ∫ ∫ sin ⁡ ⁡ x cos ⁡ ⁡ x cosh ⁡ ⁡ t 2 d t = cosh ⁡ ⁡ ( cos 2 ⁡ ⁡ x ) d d x ( cos ⁡ ⁡ x ) − − cosh ⁡ ⁡ ( sin 2 ⁡ ⁡ x ) d d x ( sin ⁡ ⁡ x ) + ∫ ∫ sin ⁡ ⁡ x cos ⁡ ⁡ x ∂ ∂ ∂ ∂ x ( cosh ⁡ ⁡ t 2 ) d t = cosh ⁡ ⁡ ( cos 2 ⁡ ⁡ x ) ( − − sin ⁡ ⁡ x ) − − cosh ⁡ ⁡ ( sin 2 ⁡ ⁡ x ) ( cos ⁡ ⁡ x ) + 0 = − − cosh ⁡ ⁡ ( cos 2 ⁡ ⁡ x ) sin ⁡ ⁡ x − − cosh ⁡ ⁡ ( sin 2 ⁡ ⁡ x ) cos ⁡ ⁡ x .

{\displaystyle {\begin{aligned}{\frac {d}{dx}}\int _{\sin x}^{\cos x}\cosh t^{2}\,dt&=\cosh \left(\cos ^{2}x\right){\frac {d}{dx}}(\cos x)-\cosh \left(\sin ^{2}x\right){\frac {d}{dx}}(\sin x)+\int _{\sin x}^{\cos x}{\frac {\partial }{\partial x}}(\cosh t^{2})\,dt\\[6pt]&=\cosh(\cos ^{2}x)(-\sin x)-\cosh(\sin ^{2}x)(\cos x)+0\\[6pt]&=-\cosh(\cos ^{2}x)\sin x-\cosh(\sin ^{2}x)\cos x.\end{aligned}}} Applications [ edit ] Evaluating definite integrals [ edit ] The formula d d x ( ∫ ∫ a ( x ) b ( x ) f ( x , t ) d t ) = f ( x , b ( x ) ) ⋅ ⋅ d d x b ( x ) − − f ( x , a ( x ) ) ⋅ ⋅ d d x a ( x ) + ∫ ∫ a ( x ) b ( x ) ∂ ∂ ∂ ∂ x f ( x , t ) d t {\displaystyle {\frac {d}{dx}}\left(\int _{a(x)}^{b(x)}f(x,t)\,dt\right)=f{\big (}x,b(x){\big )}\cdot {\frac {d}{dx}}b(x)-f{\big (}x,a(x){\big )}\cdot {\frac {d}{dx}}a(x)+\int _{a(x)}^{b(x)}{\frac {\partial }{\partial x}}f(x,t)\,dt} can be of use when evaluating certain definite integrals. When used in this context, the Leibniz integral rule for differentiating under the integral sign is also known as Feynman's trick for integration.

Example 3 [ edit ] Consider φ φ ( α α ) = ∫ ∫ 0 π π ln ⁡ ⁡ ( 1 − − 2 α α cos ⁡ ⁡ ( x ) + α α 2 ) d x , | α α | ≠ ≠ 1.

{\displaystyle \varphi (\alpha )=\int _{0}^{\pi }\ln \left(1-2\alpha \cos(x)+\alpha ^{2}\right)\,dx,\qquad |\alpha |\neq 1.} Now, d d α α φ φ ( α α ) = ∫ ∫ 0 π π − − 2 cos ⁡ ⁡ ( x ) + 2 α α 1 − − 2 α α cos ⁡ ⁡ ( x ) + α α 2 d x = 1 α α ∫ ∫ 0 π π ( 1 − − 1 − − α α 2 1 − − 2 α α cos ⁡ ⁡ ( x ) + α α 2 ) d x = π π α α − − 2 α α { arctan ⁡ ⁡ ( 1 + α α 1 − − α α tan ⁡ ⁡ ( x 2 ) ) } | 0 π π .

{\displaystyle {\begin{aligned}{\frac {d}{d\alpha }}\varphi (\alpha )&=\int _{0}^{\pi }{\frac {-2\cos(x)+2\alpha }{1-2\alpha \cos(x)+\alpha ^{2}}}dx\\[6pt]&={\frac {1}{\alpha }}\int _{0}^{\pi }\left(1-{\frac {1-\alpha ^{2}}{1-2\alpha \cos(x)+\alpha ^{2}}}\right)dx\\[6pt]&=\left.{\frac {\pi }{\alpha }}-{\frac {2}{\alpha }}\left\{\arctan \left({\frac {1+\alpha }{1-\alpha }}\tan \left({\frac {x}{2}}\right)\right)\right\}\right|_{0}^{\pi }.\end{aligned}}} As x {\displaystyle x} varies from 0 {\displaystyle 0} to π π {\displaystyle \pi } , we have { 1 + α α 1 − − α α tan ⁡ ⁡ ( x 2 ) ≥ ≥ 0 , | α α | < 1 , 1 + α α 1 − − α α tan ⁡ ⁡ ( x 2 ) ≤ ≤ 0 , | α α | > 1.

{\displaystyle {\begin{cases}{\frac {1+\alpha }{1-\alpha }}\tan \left({\frac {x}{2}}\right)\geq 0,&|\alpha |<1,\\{\frac {1+\alpha }{1-\alpha }}\tan \left({\frac {x}{2}}\right)\leq 0,&|\alpha |>1.\end{cases}}} Hence, arctan ⁡ ⁡ ( 1 + α α 1 − − α α tan ⁡ ⁡ ( x 2 ) ) | 0 π π = { π π 2 , | α α | < 1 , − − π π 2 , | α α | > 1.

{\displaystyle \left.\arctan \left({\frac {1+\alpha }{1-\alpha }}\tan \left({\frac {x}{2}}\right)\right)\right|_{0}^{\pi }={\begin{cases}{\frac {\pi }{2}},&|\alpha |<1,\\-{\frac {\pi }{2}},&|\alpha |>1.\end{cases}}} Therefore, d d α α φ φ ( α α ) = { 0 , | α α | < 1 , 2 π π α α , | α α | > 1.

{\displaystyle {\frac {d}{d\alpha }}\varphi (\alpha )={\begin{cases}0,&|\alpha |<1,\\{\frac {2\pi }{\alpha }},&|\alpha |>1.\end{cases}}} Integrating both sides with respect to α α {\displaystyle \alpha } , we get: φ φ ( α α ) = { C 1 , | α α | < 1 , 2 π π ln ⁡ ⁡ | α α | + C 2 , | α α | > 1.

{\displaystyle \varphi (\alpha )={\begin{cases}C_{1},&|\alpha |<1,\\2\pi \ln |\alpha |+C_{2},&|\alpha |>1.\end{cases}}} C 1 = 0 {\displaystyle C_{1}=0} follows from evaluating φ φ ( 0 ) {\displaystyle \varphi (0)} : φ φ ( 0 ) = ∫ ∫ 0 π π ln ⁡ ⁡ ( 1 ) d x = ∫ ∫ 0 π π 0 d x = 0.

{\displaystyle \varphi (0)=\int _{0}^{\pi }\ln(1)\,dx=\int _{0}^{\pi }0\,dx=0.} To determine C 2 {\displaystyle C_{2}} in the same manner, we should need to substitute in a value of α α {\displaystyle \alpha } greater than 1 in φ φ ( α α ) {\displaystyle \varphi (\alpha )} . This is somewhat inconvenient. Instead, we substitute α α = 1 β β {\textstyle \alpha ={\frac {1}{\beta }}} , where | β β | < 1 {\displaystyle |\beta |<1} . Then, φ φ ( α α ) = ∫ ∫ 0 π π ( ln ⁡ ⁡ ( 1 − − 2 β β cos ⁡ ⁡ ( x ) + β β 2 ) − − 2 ln ⁡ ⁡ | β β | ) d x = ∫ ∫ 0 π π ln ⁡ ⁡ ( 1 − − 2 β β cos ⁡ ⁡ ( x ) + β β 2 ) d x − − ∫ ∫ 0 π π 2 ln ⁡ ⁡ | β β | d x = 0 − − 2 π π ln ⁡ ⁡ | β β | = 2 π π ln ⁡ ⁡ | α α | .

{\displaystyle {\begin{aligned}\varphi (\alpha )&=\int _{0}^{\pi }\left(\ln \left(1-2\beta \cos(x)+\beta ^{2}\right)-2\ln |\beta |\right)dx\\[6pt]&=\int _{0}^{\pi }\ln \left(1-2\beta \cos(x)+\beta ^{2}\right)\,dx-\int _{0}^{\pi }2\ln |\beta |dx\\[6pt]&=0-2\pi \ln |\beta |\\[6pt]&=2\pi \ln |\alpha |.\end{aligned}}} Therefore, C 2 = 0 {\displaystyle C_{2}=0} The definition of φ φ ( α α ) {\displaystyle \varphi (\alpha )} is now complete: φ φ ( α α ) = { 0 , | α α | < 1 , 2 π π ln ⁡ ⁡ | α α | , | α α | > 1.

{\displaystyle \varphi (\alpha )={\begin{cases}0,&|\alpha |<1,\\2\pi \ln |\alpha |,&|\alpha |>1.\end{cases}}} The foregoing discussion, of course, does not apply when α α = ± ± 1 {\displaystyle \alpha =\pm 1} , since the conditions for differentiability are not met.

Example 4 [ edit ] I = ∫ ∫ 0 π π / 2 1 ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) 2 d x , a , b > 0.

{\displaystyle I=\int _{0}^{\pi /2}{\frac {1}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{2}}}\,dx,\qquad a,b>0.} First we calculate: J = ∫ ∫ 0 π π / 2 1 a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x d x = ∫ ∫ 0 π π / 2 1 cos 2 ⁡ ⁡ x a + b sin 2 ⁡ ⁡ x cos 2 ⁡ ⁡ x d x = ∫ ∫ 0 π π / 2 sec 2 ⁡ ⁡ x a + b tan 2 ⁡ ⁡ x d x = 1 b ∫ ∫ 0 π π / 2 1 ( a b ) 2 + tan 2 ⁡ ⁡ x d ( tan ⁡ ⁡ x ) = 1 a b arctan ⁡ ⁡ ( b a tan ⁡ ⁡ x ) | 0 π π / 2 = π π 2 a b .

{\displaystyle {\begin{aligned}J&=\int _{0}^{\pi /2}{\frac {1}{a\cos ^{2}x+b\sin ^{2}x}}dx\\[6pt]&=\int _{0}^{\pi /2}{\frac {\frac {1}{\cos ^{2}x}}{a+b{\frac {\sin ^{2}x}{\cos ^{2}x}}}}dx\\[6pt]&=\int _{0}^{\pi /2}{\frac {\sec ^{2}x}{a+b\tan ^{2}x}}dx\\[6pt]&={\frac {1}{b}}\int _{0}^{\pi /2}{\frac {1}{\left({\sqrt {\frac {a}{b}}}\right)^{2}+\tan ^{2}x}}\,d(\tan x)\\[6pt]&=\left.{\frac {1}{\sqrt {ab}}}\arctan \left({\sqrt {\frac {b}{a}}}\tan x\right)\right|_{0}^{\pi /2}\\[6pt]&={\frac {\pi }{2{\sqrt {ab}}}}.\end{aligned}}} The limits of integration being independent of a {\displaystyle a} , we have: ∂ ∂ J ∂ ∂ a = − − ∫ ∫ 0 π π / 2 cos 2 ⁡ ⁡ x ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) 2 d x {\displaystyle {\frac {\partial J}{\partial a}}=-\int _{0}^{\pi /2}{\frac {\cos ^{2}x}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{2}}}\,dx} On the other hand: ∂ ∂ J ∂ ∂ a = ∂ ∂ ∂ ∂ a ( π π 2 a b ) = − − π π 4 a 3 b .

{\displaystyle {\frac {\partial J}{\partial a}}={\frac {\partial }{\partial a}}\left({\frac {\pi }{2{\sqrt {ab}}}}\right)=-{\frac {\pi }{4{\sqrt {a^{3}b}}}}.} Equating these two relations then yields ∫ ∫ 0 π π / 2 cos 2 ⁡ ⁡ x ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) 2 d x = π π 4 a 3 b .

{\displaystyle \int _{0}^{\pi /2}{\frac {\cos ^{2}x}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{2}}}\,dx={\frac {\pi }{4{\sqrt {a^{3}b}}}}.} In a similar fashion, pursuing ∂ ∂ J ∂ ∂ b {\displaystyle {\frac {\partial J}{\partial b}}} yields ∫ ∫ 0 π π / 2 sin 2 ⁡ ⁡ x ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) 2 d x = π π 4 a b 3 .

{\displaystyle \int _{0}^{\pi /2}{\frac {\sin ^{2}x}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{2}}}\,dx={\frac {\pi }{4{\sqrt {ab^{3}}}}}.} Adding the two results then produces I = ∫ ∫ 0 π π / 2 1 ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) 2 d x = π π 4 a b ( 1 a + 1 b ) , {\displaystyle I=\int _{0}^{\pi /2}{\frac {1}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{2}}}\,dx={\frac {\pi }{4{\sqrt {ab}}}}\left({\frac {1}{a}}+{\frac {1}{b}}\right),} which computes I {\displaystyle I} as desired.

This derivation may be generalized.  Note that if we define I n = ∫ ∫ 0 π π / 2 1 ( a cos 2 ⁡ ⁡ x + b sin 2 ⁡ ⁡ x ) n d x , {\displaystyle I_{n}=\int _{0}^{\pi /2}{\frac {1}{\left(a\cos ^{2}x+b\sin ^{2}x\right)^{n}}}\,dx,} it can easily be shown that ( 1 − − n ) I n = ∂ ∂ I n − − 1 ∂ ∂ a + ∂ ∂ I n − − 1 ∂ ∂ b {\displaystyle (1-n)I_{n}={\frac {\partial I_{n-1}}{\partial a}}+{\frac {\partial I_{n-1}}{\partial b}}} Given I 1 {\displaystyle I_{1}} , this integral reduction formula can be used to compute all of the values of I n {\displaystyle I_{n}} for n > 1 {\displaystyle n>1} . Integrals like I {\displaystyle I} and J {\displaystyle J} may also be handled using the Weierstrass substitution .

Example 5 [ edit ] Here, we consider the integral I ( α α ) = ∫ ∫ 0 π π / 2 ln ⁡ ⁡ ( 1 + cos ⁡ ⁡ α α cos ⁡ ⁡ x ) cos ⁡ ⁡ x d x , 0 < α α < π π .

{\displaystyle I(\alpha )=\int _{0}^{\pi /2}{\frac {\ln(1+\cos \alpha \cos x)}{\cos x}}\,dx,\qquad 0<\alpha <\pi .} Differentiating under the integral with respect to α α {\displaystyle \alpha } , we have d d α α I ( α α ) = ∫ ∫ 0 π π / 2 ∂ ∂ ∂ ∂ α α ( ln ⁡ ⁡ ( 1 + cos ⁡ ⁡ α α cos ⁡ ⁡ x ) cos ⁡ ⁡ x ) d x = − − ∫ ∫ 0 π π / 2 sin ⁡ ⁡ α α 1 + cos ⁡ ⁡ α α cos ⁡ ⁡ x d x = − − ∫ ∫ 0 π π / 2 sin ⁡ ⁡ α α ( cos 2 ⁡ ⁡ x 2 + sin 2 ⁡ ⁡ x 2 ) + cos ⁡ ⁡ α α ( cos 2 ⁡ ⁡ x 2 − − sin 2 ⁡ ⁡ x 2 ) d x = − − sin ⁡ ⁡ α α 1 − − cos ⁡ ⁡ α α ∫ ∫ 0 π π / 2 1 cos 2 ⁡ ⁡ x 2 1 1 + cos ⁡ ⁡ α α 1 − − cos ⁡ ⁡ α α + tan 2 ⁡ ⁡ x 2 d x = − − 2 sin ⁡ ⁡ α α 1 − − cos ⁡ ⁡ α α ∫ ∫ 0 π π / 2 1 2 sec 2 ⁡ ⁡ x 2 2 cos 2 ⁡ ⁡ α α 2 2 sin 2 ⁡ ⁡ α α 2 + tan 2 ⁡ ⁡ x 2 d x = − − 2 ( 2 sin ⁡ ⁡ α α 2 cos ⁡ ⁡ α α 2 ) 2 sin 2 ⁡ ⁡ α α 2 ∫ ∫ 0 π π / 2 1 cot 2 ⁡ ⁡ α α 2 + tan 2 ⁡ ⁡ x 2 d ( tan ⁡ ⁡ x 2 ) = − − 2 cot ⁡ ⁡ α α 2 ∫ ∫ 0 π π / 2 1 cot 2 ⁡ ⁡ α α 2 + tan 2 ⁡ ⁡ x 2 d ( tan ⁡ ⁡ x 2 ) = − − 2 arctan ⁡ ⁡ ( tan ⁡ ⁡ α α 2 tan ⁡ ⁡ x 2 ) | 0 π π / 2 = − − α α .

{\displaystyle {\begin{aligned}{\frac {d}{d\alpha }}I(\alpha )&=\int _{0}^{\pi /2}{\frac {\partial }{\partial \alpha }}\left({\frac {\ln(1+\cos \alpha \cos x)}{\cos x}}\right)\,dx\\[6pt]&=-\int _{0}^{\pi /2}{\frac {\sin \alpha }{1+\cos \alpha \cos x}}\,dx\\&=-\int _{0}^{\pi /2}{\frac {\sin \alpha }{\left(\cos ^{2}{\frac {x}{2}}+\sin ^{2}{\frac {x}{2}}\right)+\cos \alpha \left(\cos ^{2}{\frac {x}{2}}-\sin ^{2}{\frac {x}{2}}\right)}}\,dx\\[6pt]&=-{\frac {\sin \alpha }{1-\cos \alpha }}\int _{0}^{\pi /2}{\frac {1}{\cos ^{2}{\frac {x}{2}}}}{\frac {1}{{\frac {1+\cos \alpha }{1-\cos \alpha }}+\tan ^{2}{\frac {x}{2}}}}\,dx\\[6pt]&=-{\frac {2\sin \alpha }{1-\cos \alpha }}\int _{0}^{\pi /2}{\frac {{\frac {1}{2}}\sec ^{2}{\frac {x}{2}}}{{\frac {2\cos ^{2}{\frac {\alpha }{2}}}{2\sin ^{2}{\frac {\alpha }{2}}}}+\tan ^{2}{\frac {x}{2}}}}\,dx\\[6pt]&=-{\frac {2\left(2\sin {\frac {\alpha }{2}}\cos {\frac {\alpha }{2}}\right)}{2\sin ^{2}{\frac {\alpha }{2}}}}\int _{0}^{\pi /2}{\frac {1}{\cot ^{2}{\frac {\alpha }{2}}+\tan ^{2}{\frac {x}{2}}}}\,d\left(\tan {\frac {x}{2}}\right)\\[6pt]&=-2\cot {\frac {\alpha }{2}}\int _{0}^{\pi /2}{\frac {1}{\cot ^{2}{\frac {\alpha }{2}}+\tan ^{2}{\frac {x}{2}}}}\,d\left(\tan {\frac {x}{2}}\right)\\[6pt]&=-2\arctan \left(\tan {\frac {\alpha }{2}}\tan {\frac {x}{2}}\right){\bigg |}_{0}^{\pi /2}\\[6pt]&=-\alpha .\end{aligned}}} Therefore: I ( α α ) = C − − α α 2 2 .

{\displaystyle I(\alpha )=C-{\frac {\alpha ^{2}}{2}}.} But I ( π π 2 ) = 0 {\textstyle I{\left({\frac {\pi }{2}}\right)}=0} by definition so C = π π 2 8 {\textstyle C={\frac {\pi ^{2}}{8}}} and I ( α α ) = π π 2 8 − − α α 2 2 .

{\displaystyle I(\alpha )={\frac {\pi ^{2}}{8}}-{\frac {\alpha ^{2}}{2}}.} Example 6 [ edit ] Here, we consider the integral ∫ ∫ 0 2 π π e cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( sin ⁡ ⁡ θ θ ) d θ θ .

{\displaystyle \int _{0}^{2\pi }e^{\cos \theta }\cos(\sin \theta )\,d\theta .} We introduce a new variable φ and rewrite the integral as f ( φ φ ) = ∫ ∫ 0 2 π π e φ φ cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) d θ θ .

{\displaystyle f(\varphi )=\int _{0}^{2\pi }e^{\varphi \cos \theta }\cos(\varphi \sin \theta )\,d\theta .} When φ = 1 this equals the original integral.  However, this more general integral may be differentiated with respect to φ φ {\displaystyle \varphi } : d f d φ φ = ∫ ∫ 0 2 π π ∂ ∂ ∂ ∂ φ φ [ e φ φ cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) ] d θ θ = ∫ ∫ 0 2 π π e φ φ cos ⁡ ⁡ θ θ [ cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) − − sin ⁡ ⁡ θ θ sin ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) ] d θ θ .

{\displaystyle {\frac {df}{d\varphi }}=\int _{0}^{2\pi }{\frac {\partial }{\partial \varphi }}\left[e^{\varphi \cos \theta }\cos(\varphi \sin \theta )\right]d\theta =\int _{0}^{2\pi }e^{\varphi \cos \theta }\left[\cos \theta \cos(\varphi \sin \theta )-\sin \theta \sin(\varphi \sin \theta )\right]d\theta .} Now, fix φ , and consider the vector field on R 2 {\displaystyle \mathbb {R} ^{2}} defined by F ( x , y ) = ( F 1 ( x , y ) , F 2 ( x , y ) ) := ( e φ φ x sin ⁡ ⁡ ( φ φ y ) , e φ φ x cos ⁡ ⁡ ( φ φ y ) ) {\displaystyle \mathbf {F} (x,y)=(F_{1}(x,y),F_{2}(x,y)):=(e^{\varphi x}\sin(\varphi y),e^{\varphi x}\cos(\varphi y))} . Further, choose the positive oriented parameterization of the unit circle S 1 {\displaystyle S^{1}} given by r : : [ 0 , 2 π π ) → → R 2 {\displaystyle \mathbf {r} \colon [0,2\pi )\to \mathbb {R} ^{2}} , r ( θ θ ) := ( cos ⁡ ⁡ θ θ , sin ⁡ ⁡ θ θ ) {\displaystyle \mathbf {r} (\theta ):=(\cos \theta ,\sin \theta )} , so that r ′ ( t ) = ( − − sin ⁡ ⁡ θ θ , cos ⁡ ⁡ θ θ ) {\displaystyle \mathbf {r} '(t)=(-\sin \theta ,\cos \theta )} . Then the final integral above is precisely ∫ ∫ 0 2 π π e φ φ cos ⁡ ⁡ θ θ [ cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) − − sin ⁡ ⁡ θ θ sin ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) ] d θ θ = ∫ ∫ 0 2 π π [ e φ φ cos ⁡ ⁡ θ θ sin ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) e φ φ cos ⁡ ⁡ θ θ cos ⁡ ⁡ ( φ φ sin ⁡ ⁡ θ θ ) ] ⋅ ⋅ [ − − sin ⁡ ⁡ θ θ − − cos ⁡ ⁡ θ θ ] d θ θ = ∫ ∫ 0 2 π π F ( r ( θ θ ) ) ⋅ ⋅ r ′ ( θ θ ) d θ θ = ∮ ∮ S 1 F ( r ) ⋅ ⋅ d r = ∮ ∮ S 1 F 1 d x + F 2 d y , {\displaystyle {\begin{aligned}&\int _{0}^{2\pi }e^{\varphi \cos \theta }\left[\cos \theta \cos(\varphi \sin \theta )-\sin \theta \sin(\varphi \sin \theta )\right]d\theta \\[6pt]={}&\int _{0}^{2\pi }{\begin{bmatrix}e^{\varphi \cos \theta }\sin(\varphi \sin \theta )\\e^{\varphi \cos \theta }\cos(\varphi \sin \theta )\end{bmatrix}}\cdot {\begin{bmatrix}-\sin \theta \\{\hphantom {-}}\cos \theta \end{bmatrix}}\,d\theta \\[6pt]={}&\int _{0}^{2\pi }\mathbf {F} (\mathbf {r} (\theta ))\cdot \mathbf {r} '(\theta )\,d\theta \\[6pt]={}&\oint _{S^{1}}\mathbf {F} (\mathbf {r} )\cdot d\mathbf {r} =\oint _{S^{1}}F_{1}\,dx+F_{2}\,dy,\end{aligned}}} the line integral of F {\displaystyle \mathbf {F} } over S 1 {\displaystyle S^{1}} . By Green's Theorem , this equals the double integral ∬ ∬ D ∂ ∂ F 2 ∂ ∂ x − − ∂ ∂ F 1 ∂ ∂ y d A , {\displaystyle \iint _{D}{\frac {\partial F_{2}}{\partial x}}-{\frac {\partial F_{1}}{\partial y}}\,dA,} where D {\displaystyle D} is the closed unit disc . Its integrand is identically 0, so d f / d φ φ {\displaystyle df/d\varphi } is likewise identically zero. This implies that f ( φ ) is constant.  The constant may be determined by evaluating f {\displaystyle f} at φ φ = 0 {\displaystyle \varphi =0} : f ( 0 ) = ∫ ∫ 0 2 π π 1 d θ θ = 2 π π .

{\displaystyle f(0)=\int _{0}^{2\pi }1\,d\theta =2\pi .} Therefore, the original integral also equals 2 π π {\displaystyle 2\pi } .

Other problems to solve [ edit ] There are innumerable other integrals that can be solved using the technique of differentiation under the integral sign. For example, in each of the following cases, the original integral may be replaced by a similar integral having a new parameter α α {\displaystyle \alpha } : ∫ ∫ 0 ∞ ∞ sin ⁡ ⁡ x x d x → → ∫ ∫ 0 ∞ ∞ e − − α α x sin ⁡ ⁡ x x d x , ∫ ∫ 0 π π / 2 x tan ⁡ ⁡ x d x → → ∫ ∫ 0 π π / 2 tan − − 1 ⁡ ⁡ ( α α tan ⁡ ⁡ x ) tan ⁡ ⁡ x d x , ∫ ∫ 0 ∞ ∞ ln ⁡ ⁡ ( 1 + x 2 ) 1 + x 2 d x → → ∫ ∫ 0 ∞ ∞ ln ⁡ ⁡ ( 1 + α α 2 x 2 ) 1 + x 2 d x ∫ ∫ 0 1 x − − 1 ln ⁡ ⁡ x d x → → ∫ ∫ 0 1 x α α − − 1 ln ⁡ ⁡ x d x .

{\displaystyle {\begin{aligned}\int _{0}^{\infty }{\frac {\sin x}{x}}\,dx&\to \int _{0}^{\infty }e^{-\alpha x}{\frac {\sin x}{x}}dx,\\[6pt]\int _{0}^{\pi /2}{\frac {x}{\tan x}}\,dx&\to \int _{0}^{\pi /2}{\frac {\tan ^{-1}(\alpha \tan x)}{\tan x}}dx,\\[6pt]\int _{0}^{\infty }{\frac {\ln(1+x^{2})}{1+x^{2}}}\,dx&\to \int _{0}^{\infty }{\frac {\ln(1+\alpha ^{2}x^{2})}{1+x^{2}}}dx\\[6pt]\int _{0}^{1}{\frac {x-1}{\ln x}}\,dx&\to \int _{0}^{1}{\frac {x^{\alpha }-1}{\ln x}}dx.\end{aligned}}} The first integral, the Dirichlet integral , is absolutely convergent for positive α but only conditionally convergent when α α = 0 {\displaystyle \alpha =0} . Therefore, differentiation under the integral sign is easy to justify when α α > 0 {\displaystyle \alpha >0} , but proving that the resulting formula remains valid when α α = 0 {\displaystyle \alpha =0} requires some careful work.

Infinite series [ edit ] The measure-theoretic version of differentiation under the integral sign also applies to summation (finite or infinite) by interpreting summation as counting measure .  An example of an application is the fact that power series are differentiable in their radius of convergence.

[ citation needed ] Euler-Lagrange equations [ edit ] The Leibniz integral rule is used in the derivation of the Euler-Lagrange equation in variational calculus .

In popular culture [ edit ] Differentiation under the integral sign is mentioned in the late physicist Richard Feynman 's best-selling memoir Surely You're Joking, Mr. Feynman!

in the chapter "A Different Box of Tools".  He describes learning it, while in high school , from an old text, Advanced Calculus (1926), by Frederick S. Woods (who was a professor of mathematics in the Massachusetts Institute of Technology ).  The technique was not often taught when Feynman later received his formal education in calculus , but using this technique, Feynman was able to solve otherwise difficult integration problems upon his arrival at graduate school at Princeton University : One thing I never did learn was contour integration . I had learned to do integrals by various methods shown in a book that my high school physics teacher Mr. Bader had given me. One day he told me to stay after class. "Feynman," he said, "you talk too much and you make too much noise. I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and study this book, and when you know everything that's in this book, you can talk again." So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were doing. I was up in the back with this book: "Advanced Calculus" , by Woods. Bader knew I had studied "Calculus for the Practical Man" a little bit, so he gave me the real works—it was for a junior or senior course in college. It had Fourier series , Bessel functions , determinants , elliptic functions —all kinds of wonderful stuff that I didn't know anything about. That book also showed how to differentiate parameters under the integral sign—it's a certain operation. It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on how to use that method, and I used that one damn tool again and again. So because I was self-taught using that book, I had peculiar methods of doing integrals. The result was, when guys at MIT or Princeton had trouble doing a certain integral, it was because they couldn't do it with the standard methods they had learned in school. If it was contour integration, they would have found it; if it was a simple series expansion, they would have found it. Then I come along and try differentiating under the integral sign, and often it worked. So I got a great reputation for doing integrals, only because my box of tools was different from everybody else's, and they had tried all their tools on it before giving the problem to me.

See also [ edit ] Mathematics portal Chain rule Differentiation of integrals Leibniz rule (generalized product rule) Reynolds transport theorem , a generalization of Leibniz rule References [ edit ] ^ Protter, Murray H.; Morrey, Charles B. Jr. (1985).

"Differentiation under the Integral Sign" .

Intermediate Calculus (Second ed.). New York: Springer. pp.

421– 426.

doi : 10.1007/978-1-4612-1086-3 .

ISBN 978-0-387-96058-6 .

^ a b Talvila, Erik (June 2001).

"Necessary and Sufficient Conditions for Differentiating under the Integral Sign" .

American Mathematical Monthly .

108 (6): 544– 548.

arXiv : math/0101012 .

doi : 10.2307/2695709 .

JSTOR 2695709 . Retrieved 16 April 2022 .

^ Abraham, Max; Becker, Richard (1950).

Classical Theory of Electricity and Magnetism (2nd ed.). London: Blackie & Sons. pp.

39– 40.

^ a b Flanders, Harly (June–July 1973).

"Differentiation under the integral sign" (PDF) .

American Mathematical Monthly .

80 (6): 615– 627.

doi : 10.2307/2319163 .

JSTOR 2319163 . Archived from the original (PDF) on 2018-09-20 . Retrieved 2017-01-28 .

^ Zangwill, Andrew (2013).

Modern Electrodynamics . Cambridge: Cambridge University Press. p. 10.

ISBN 0-521-89697-5 .

^ Folland, Gerald (1999).

Real Analysis: Modern Techniques and their Applications (2nd ed.). New York: John Wiley & Sons. p. 56.

ISBN 978-0-471-31716-6 .

^ Cheng, Steve (6 September 2010). Differentiation under the integral sign with weak derivatives (Report). CiteSeerX.

CiteSeerX 10.1.1.525.2529 .

^ Spivak, Michael (1994).

Calculus (3 ed.). Houston, Texas: Publish or Perish, Inc. pp.

267 –268.

ISBN 978-0-914098-89-8 .

^ Spivak, Michael (1965).

Calculus on Manifolds . Addison-Wesley Publishing Company. p. 31.

ISBN 978-0-8053-9021-6 .

Further reading [ edit ] Amazigo, John C.; Rubenfeld, Lester A. (1980).

"Single Integrals: Leibnitz's Rule; Numerical Integration" .

Advanced Calculus and its Applications to the Engineering and Physical Sciences . New York: Wiley. pp.

155–165 .

ISBN 0-471-04934-4 .

Kaplan, Wilfred (1973). "Integrals Depending on a Parameter—Leibnitz's Rule".

Advanced Calculus (2nd ed.). Reading: Addison-Wesley. pp.

285– 288.

External links [ edit ] Harron, Rob.

"The Leibniz Rule" (PDF) .

MAT-203 .

Retrieved from " https://en.wikipedia.org/w/index.php?title=Leibniz_integral_rule&oldid=1306162918 " Categories : Gottfried Wilhelm Leibniz Multivariable calculus Integral calculus Differential calculus Theorems in calculus Hidden categories: Articles with short description Short description matches Wikidata Articles needing additional references from October 2016 All articles needing additional references Pages using sidebar with the child parameter All articles with unsourced statements Articles with unsourced statements from January 2022 Articles containing proofs This page was last edited on 16 August 2025, at 08:10 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Leibniz integral rule 16 languages Add topic

