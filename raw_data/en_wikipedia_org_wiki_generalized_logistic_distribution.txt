Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definitions Toggle Definitions subsection 1.1 Type I 1.2 Type II 1.3 Type III 1.4 Type IV 1.5 Relationship between types 2 Type IV (logistic-beta) properties Toggle Type IV (logistic-beta) properties subsection 2.1 Relationship with Beta Distribution 2.2 Relationship with Gamma Distribution 2.3 Symmetry 2.4 Normal variance-mean mixture representation 2.5 Mean and variance 2.6 Cumulants and skewness 2.7 Mode 2.8 Tail behaviour 2.9 Exponential family properties 2.10 Relationships with other distributions 2.11 Large shape parameters 2.12 Random variate generation 3 Generalization with location and scale parameters 4 Maximum likelihood parameter estimation Toggle Maximum likelihood parameter estimation subsection 4.1 Maximum likelihood for standard Type IV 4.2 Maximum likelihood for the four-parameter family 5 See also 6 References Toggle the table of contents Generalized logistic distribution 1 language Català Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Name for several different families of probability distributions The term generalized logistic distribution is used as the name for several different families of probability distributions . For example, Johnson et al.

[ 1 ] list four forms, which are listed below.

Type I has also been called the skew-logistic distribution .

Type IV subsumes the other types and is obtained when applying the logit transform to beta random variates. Following the same convention as for the log-normal distribution , type IV may be referred to as the logistic-beta distribution , [ 2 ] with reference to the standard logistic function , which is the inverse of the logit transform.

For other families of distributions that have also been called generalized logistic distributions, see the shifted log-logistic distribution , which is a generalization of the log-logistic distribution ; and the metalog ("meta-logistic") distribution , which is highly shape-and-bounds flexible and can be fit to data with linear least squares.

Definitions [ edit ] The following definitions are for standardized versions of the families, which can be expanded to the full form as a location-scale family . Each is defined using either the cumulative distribution function ( F ) or the probability density function ( ƒ ), and is defined on (-∞,∞).

Type I [ edit ] F ( x ; α α ) = 1 ( 1 + e − − x ) α α ≡ ≡ ( 1 + e − − x ) − − α α , α α > 0.

{\displaystyle F(x;\alpha )={\frac {1}{(1+e^{-x})^{\alpha }}}\equiv (1+e^{-x})^{-\alpha },\quad \alpha >0.} The corresponding probability density function is: f ( x ; α α ) = α α e − − x ( 1 + e − − x ) α α + 1 , α α > 0.

{\displaystyle f(x;\alpha )={\frac {\alpha e^{-x}}{\left(1+e^{-x}\right)^{\alpha +1}}},\quad \alpha >0.} This type has also been called the "skew-logistic" distribution.

Type II [ edit ] F ( x ; α α ) = 1 − − e − − α α x ( 1 + e − − x ) α α , α α > 0.

{\displaystyle F(x;\alpha )=1-{\frac {e^{-\alpha x}}{(1+e^{-x})^{\alpha }}},\quad \alpha >0.} The corresponding probability density function is: f ( x ; α α ) = α α e − − α α x ( 1 + e − − x ) α α + 1 , α α > 0.

{\displaystyle f(x;\alpha )={\frac {\alpha e^{-\alpha x}}{(1+e^{-x})^{\alpha +1}}},\quad \alpha >0.} Type III [ edit ] f ( x ; α α ) = 1 B ( α α , α α ) e − − α α x ( 1 + e − − x ) 2 α α , α α > 0.

{\displaystyle f(x;\alpha )={\frac {1}{B(\alpha ,\alpha )}}{\frac {e^{-\alpha x}}{(1+e^{-x})^{2\alpha }}},\quad \alpha >0.} Here B is the beta function . The moment generating function for this type is M ( t ) = Γ Γ ( α α − − t ) Γ Γ ( α α + t ) ( Γ Γ ( α α ) ) 2 , − − α α < t < α α .

{\displaystyle M(t)={\frac {\Gamma (\alpha -t)\Gamma (\alpha +t)}{(\Gamma (\alpha ))^{2}}},\quad -\alpha <t<\alpha .} The corresponding cumulative distribution function is: F ( x ; α α ) = ( e x + 1 ) Γ Γ ( α α ) e α α ( − − x ) ( e − − x + 1 ) − − 2 α α 2 F ~ ~ 1 ( 1 , 1 − − α α ; α α + 1 ; − − e x ) B ( α α , α α ) , α α > 0.

{\displaystyle F(x;\alpha )={\frac {\left(e^{x}+1\right)\Gamma (\alpha )e^{\alpha (-x)}\left(e^{-x}+1\right)^{-2\alpha }\,_{2}{\tilde {F}}_{1}\left(1,1-\alpha ;\alpha +1;-e^{x}\right)}{B(\alpha ,\alpha )}},\quad \alpha >0.} Type IV [ edit ] f ( x ; α α , β β ) = 1 B ( α α , β β ) e − − β β x ( 1 + e − − x ) α α + β β , α α , β β > 0 = σ σ ( x ) α α σ σ ( − − x ) β β B ( α α , β β ) .

{\displaystyle {\begin{aligned}f(x;\alpha ,\beta )&={\frac {1}{B(\alpha ,\beta )}}{\frac {e^{-\beta x}}{(1+e^{-x})^{\alpha +\beta }}},\quad \alpha ,\beta >0\\[4pt]&={\frac {\sigma (x)^{\alpha }\sigma (-x)^{\beta }}{B(\alpha ,\beta )}}.\end{aligned}}} Where, B is the beta function and σ σ ( x ) = 1 / ( 1 + e − − x ) {\displaystyle \sigma (x)=1/(1+e^{-x})} is the standard logistic function . The moment generating function for this type is M ( t ) = Γ Γ ( β β − − t ) Γ Γ ( α α + t ) Γ Γ ( α α ) Γ Γ ( β β ) , − − α α < t < β β .

{\displaystyle M(t)={\frac {\Gamma (\beta -t)\Gamma (\alpha +t)}{\Gamma (\alpha )\Gamma (\beta )}},\quad -\alpha <t<\beta .} This type is also called the "exponential generalized beta of the second type".

[ 1 ] The corresponding cumulative distribution function is: F ( x ; α α , β β ) = ( e x + 1 ) Γ Γ ( α α ) e β β ( − − x ) ( e − − x + 1 ) − − α α − − β β 2 F ~ ~ 1 ( 1 , 1 − − β β ; α α + 1 ; − − e x ) B ( α α , β β ) , α α , β β > 0.

{\displaystyle F(x;\alpha ,\beta )={\frac {\left(e^{x}+1\right)\Gamma (\alpha )e^{\beta (-x)}\left(e^{-x}+1\right)^{-\alpha -\beta }\,_{2}{\tilde {F}}_{1}\left(1,1-\beta ;\alpha +1;-e^{x}\right)}{B(\alpha ,\beta )}},\quad \alpha ,\beta >0.} Relationship between types [ edit ] Type IV is the most general form of the distribution. The Type III distribution can be obtained from Type IV by fixing β β = α α {\displaystyle \beta =\alpha } . The Type II distribution can be obtained from Type IV by fixing α α = 1 {\displaystyle \alpha =1} (and renaming β β {\displaystyle \beta } to α α {\displaystyle \alpha } ). The Type I distribution can be obtained from Type IV by fixing β β = 1 {\displaystyle \beta =1} . Fixing α α = β β = 1 {\displaystyle \alpha =\beta =1} gives the standard logistic distribution .

Type IV (logistic-beta) properties [ edit ] Type IV probability density functions (means=0, variances=1) The Type IV generalized logistic , or logistic-beta [ 2 ] distribution, with support x ∈ ∈ R {\displaystyle x\in \mathbb {R} } and shape parameters α α , β β > 0 {\displaystyle \alpha ,\beta >0} , has (as shown above ) the probability density function (pdf): f ( x ; α α , β β ) = 1 B ( α α , β β ) e − − β β x ( 1 + e − − x ) α α + β β = σ σ ( x ) α α σ σ ( − − x ) β β B ( α α , β β ) , {\displaystyle f(x;\alpha ,\beta )={\frac {1}{B(\alpha ,\beta )}}{\frac {e^{-\beta x}}{(1+e^{-x})^{\alpha +\beta }}}={\frac {\sigma (x)^{\alpha }\sigma (-x)^{\beta }}{B(\alpha ,\beta )}},} where σ σ ( x ) = 1 / ( 1 + e − − x ) {\displaystyle \sigma (x)=1/(1+e^{-x})} is the standard logistic function . The probability density functions for three different sets of shape parameters are shown in the plot, where the distributions have been scaled and shifted to give zero means and unity variances, in order to facilitate comparison of the shapes.

In what follows, the notation B σ σ ( α α , β β ) {\displaystyle B_{\sigma }(\alpha ,\beta )} is used to denote the Type IV distribution.

Relationship with Beta Distribution [ edit ] As the name logistic-beta suggests, if x {\displaystyle x} follows logistic-beta with parameters α α , β β {\displaystyle \alpha ,\beta } , then σ σ ( x ) = 1 / ( 1 + e − − x ) ∼ ∼ Beta ( α α , β β ) {\displaystyle \sigma (x)=1/(1+e^{-x})\sim {\text{Beta}}(\alpha ,\beta )} Relationship with Gamma Distribution [ edit ] This distribution can be obtained in terms of the gamma distribution as follows. Let y ∼ ∼ Gamma ( α α , γ γ ) {\displaystyle y\sim {\text{Gamma}}(\alpha ,\gamma )} and independently , z ∼ ∼ Gamma ( β β , γ γ ) {\displaystyle z\sim {\text{Gamma}}(\beta ,\gamma )} and let x = ln ⁡ ⁡ y − − ln ⁡ ⁡ z {\displaystyle x=\ln y-\ln z} . Then x ∼ ∼ B σ σ ( α α , β β ) {\displaystyle x\sim B_{\sigma }(\alpha ,\beta )} .

[ 3 ] Symmetry [ edit ] If x ∼ ∼ B σ σ ( α α , β β ) {\displaystyle x\sim B_{\sigma }(\alpha ,\beta )} , then − − x ∼ ∼ B σ σ ( β β , α α ) {\displaystyle -x\sim B_{\sigma }(\beta ,\alpha )} .

Normal variance-mean mixture representation [ edit ] Logistic-beta distribution admits the following normal variance-mean mixture representation: [ 4 ] f ( x ; α α , β β ) = 1 B ( α α , β β ) e − − β β x ( 1 + e − − x ) α α + β β = ∫ ∫ 0 ∞ ∞ N ( x ; 0.5 λ λ ( α α − − β β ) , λ λ ) p Polya ( λ λ ; α α , β β ) d λ λ {\displaystyle f(x;\alpha ,\beta )={\frac {1}{B(\alpha ,\beta )}}{\frac {e^{-\beta x}}{(1+e^{-x})^{\alpha +\beta }}}=\int _{0}^{\infty }N(x;0.5\lambda (\alpha -\beta ),\lambda )p_{\text{Polya}}(\lambda ;\alpha ,\beta )d\lambda } where N ( x ; μ μ , λ λ ) {\displaystyle N(x;\mu ,\lambda )} is a normal density  with mean μ μ {\displaystyle \mu } , variance λ λ {\displaystyle \lambda } , and p Polya ( λ λ ; α α , β β ) {\displaystyle p_{\text{Polya}}(\lambda ;\alpha ,\beta )} is a density of Polya distribution with parameters α α , β β > 0 {\displaystyle \alpha ,\beta >0} , defined as λ λ = d ∑ ∑ k = 0 ∞ ∞ 2 ϵ ϵ k / { ( k + α α ) ( k + β β ) } , ϵ ϵ k ∼ ∼ i i d Exp ( 1 ) {\displaystyle \lambda {\stackrel {d}{=}}\sum _{k=0}^{\infty }2\epsilon _{k}/\{(k+\alpha )(k+\beta )\},\epsilon _{k}{\stackrel {iid}{\sim }}{\text{Exp}}(1)} .

Mean and variance [ edit ] By using the logarithmic expectations of the gamma distribution, the mean and variance can be derived as: E [ x ] = ψ ψ ( α α ) − − ψ ψ ( β β ) var [ x ] = ψ ψ ′ ( α α ) + ψ ψ ′ ( β β ) {\displaystyle {\begin{aligned}{\text{E}}[x]&=\psi (\alpha )-\psi (\beta )\\{\text{var}}[x]&=\psi '(\alpha )+\psi '(\beta )\\\end{aligned}}} where ψ ψ {\displaystyle \psi } is the digamma function , while ψ ψ ′ = ψ ψ ( 1 ) {\displaystyle \psi '=\psi ^{(1)}} is its first derivative, also known as the trigamma function , or the first polygamma function . Since ψ ψ {\displaystyle \psi } is strictly increasing , the sign of the mean is the same as the sign of α α − − β β {\displaystyle \alpha -\beta } . Since ψ ψ ′ {\displaystyle \psi '} is strictly decreasing, the shape parameters can also be interpreted as concentration parameters. Indeed, as shown below, the left and right tails respectively become thinner as α α {\displaystyle \alpha } or β β {\displaystyle \beta } are increased. The two terms of the variance represent the contributions to the variance of the left and right parts of the distribution.

Cumulants and skewness [ edit ] The cumulant generating function is K ( t ) = ln ⁡ ⁡ M ( t ) {\displaystyle K(t)=\ln M(t)} , where the moment generating function M ( t ) {\displaystyle M(t)} is given above . The cumulants , κ κ n {\displaystyle \kappa _{n}} , are the n {\displaystyle n} -th derivatives of K ( t ) {\displaystyle K(t)} , evaluated at t = 0 {\displaystyle t=0} : κ κ n = K ( n ) ( 0 ) = ψ ψ ( n − − 1 ) ( α α ) + ( − − 1 ) n ψ ψ ( n − − 1 ) ( β β ) {\displaystyle \kappa _{n}=K^{(n)}(0)=\psi ^{(n-1)}(\alpha )+(-1)^{n}\psi ^{(n-1)}(\beta )} where ψ ψ ( 0 ) = ψ ψ {\displaystyle \psi ^{(0)}=\psi } and ψ ψ ( n − − 1 ) {\displaystyle \psi ^{(n-1)}} are the digamma and polygamma functions. In agreement with the derivation above, the first cumulant, κ κ 1 {\displaystyle \kappa _{1}} , is the mean and the second, κ κ 2 {\displaystyle \kappa _{2}} , is the variance.

The third cumulant, κ κ 3 {\displaystyle \kappa _{3}} , is the third central moment E [ ( x − − E [ x ] ) 3 ] {\displaystyle E[(x-E[x])^{3}]} , which when scaled by the third power of the standard deviation gives the skewness : skew [ x ] = ψ ψ ( 2 ) ( α α ) − − ψ ψ ( 2 ) ( β β ) var [ x ] 3 {\displaystyle {\text{skew}}[x]={\frac {\psi ^{(2)}(\alpha )-\psi ^{(2)}(\beta )}{{\sqrt {{\text{var}}[x]}}^{3}}}} The sign (and therefore the handedness ) of the skewness is the same as the sign of α α − − β β {\displaystyle \alpha -\beta } .

Mode [ edit ] The mode (pdf maximum) can be derived by finding x {\displaystyle x} where the log pdf derivative is zero: d d x ln ⁡ ⁡ f ( x ; α α , β β ) = α α σ σ ( − − x ) − − β β σ σ ( x ) = 0 {\displaystyle {\frac {d}{dx}}\ln f(x;\alpha ,\beta )=\alpha \sigma (-x)-\beta \sigma (x)=0} This simplifies to α α / β β = e x {\displaystyle \alpha /\beta =e^{x}} , so that: [ 3 ] mode [ x ] = ln ⁡ ⁡ α α β β {\displaystyle {\text{mode}}[x]=\ln {\frac {\alpha }{\beta }}} Tail behaviour [ edit ] Tail comparison: Type IV (means=0, variances=1) vs standard normal , vs standard Cauchy In each of the left and right tails, one of the sigmoids in the pdf saturates to one, so that the tail is formed by the other sigmoid. For large negative x {\displaystyle x} , the left tail of the pdf is proportional to σ σ ( x ) α α ≈ ≈ e α α x {\displaystyle \sigma (x)^{\alpha }\approx e^{\alpha x}} , while the right tail (large positive x {\displaystyle x} ) is proportional to σ σ ( − − x ) β β ≈ ≈ e − − β β x {\displaystyle \sigma (-x)^{\beta }\approx e^{-\beta x}} . This means the tails are independently controlled by α α {\displaystyle \alpha } and β β {\displaystyle \beta } . Although type IV tails are heavier than those of the normal distribution ( e − − x 2 2 v {\displaystyle e^{-{\frac {x^{2}}{2v}}}} , for variance v {\displaystyle v} ), the type IV means and variances remain finite for all α α , β β > 0 {\displaystyle \alpha ,\beta >0} . This is in contrast with the Cauchy distribution for which the mean and variance do not exist. In the log pdf plots shown here, the type IV tails are linear, the normal distribution tails are quadratic and the Cauchy tails are logarithmic.

Exponential family properties [ edit ] B σ σ ( α α , β β ) {\displaystyle B_{\sigma }(\alpha ,\beta )} forms an exponential family with natural parameters α α {\displaystyle \alpha } and β β {\displaystyle \beta } and sufficient statistics log ⁡ ⁡ σ σ ( x ) {\displaystyle \log \sigma (x)} and log ⁡ ⁡ σ σ ( − − x ) {\displaystyle \log \sigma (-x)} . The expected values of the sufficient statistics can be found by differentiation of the log-normalizer: [ 5 ] E [ log ⁡ ⁡ σ σ ( x ) ] = ∂ ∂ log ⁡ ⁡ B ( α α , β β ) ∂ ∂ α α = ψ ψ ( α α ) − − ψ ψ ( α α + β β ) E [ log ⁡ ⁡ σ σ ( − − x ) ] = ∂ ∂ log ⁡ ⁡ B ( α α , β β ) ∂ ∂ β β = ψ ψ ( β β ) − − ψ ψ ( α α + β β ) {\displaystyle {\begin{aligned}E[\log \sigma (x)]&={\frac {\partial \log B(\alpha ,\beta )}{\partial \alpha }}=\psi (\alpha )-\psi (\alpha +\beta )\\E[\log \sigma (-x)]&={\frac {\partial \log B(\alpha ,\beta )}{\partial \beta }}=\psi (\beta )-\psi (\alpha +\beta )\\\end{aligned}}} Given a data set x 1 , … … , x n {\displaystyle x_{1},\ldots ,x_{n}} assumed to have been generated IID from B σ σ ( α α , β β ) {\displaystyle B_{\sigma }(\alpha ,\beta )} , the maximum-likelihood parameter estimate is: α α ^ ^ , β β ^ ^ = arg ⁡ ⁡ max α α , β β 1 n ∑ ∑ i = 1 n log ⁡ ⁡ f ( x i ; α α , β β ) = arg ⁡ ⁡ max α α , β β α α ( 1 n ∑ ∑ i log ⁡ ⁡ σ σ ( x i ) ) + β β ( 1 n ∑ ∑ i log ⁡ ⁡ σ σ ( − − x i ) ) − − log ⁡ ⁡ B ( α α , β β ) = arg ⁡ ⁡ max α α , β β α α log ⁡ ⁡ σ σ ( x ) ¯ ¯ + β β log ⁡ ⁡ σ σ ( − − x ) ¯ ¯ − − log ⁡ ⁡ B ( α α , β β ) {\displaystyle {\begin{aligned}{\hat {\alpha }},{\hat {\beta }}=\arg \max _{\alpha ,\beta }&\;{\frac {1}{n}}\sum _{i=1}^{n}\log f(x_{i};\alpha ,\beta )\\=\arg \max _{\alpha ,\beta }&\;\alpha {\Bigl (}{\frac {1}{n}}\sum _{i}\log \sigma (x_{i}){\Bigr )}+\beta {\Bigl (}{\frac {1}{n}}\sum _{i}\log \sigma (-x_{i}){\Bigr )}-\log B(\alpha ,\beta )\\=\arg \max _{\alpha ,\beta }&\;\alpha \,{\overline {\log \sigma (x)}}+\beta \,{\overline {\log \sigma (-x)}}-\log B(\alpha ,\beta )\end{aligned}}} where the overlines denote the averages of the sufficient statistics. The maximum-likelihood estimate depends on the data only via these average statistics. Indeed, at the maximum-likelihood estimate the expected values and averages agree: ψ ψ ( α α ^ ^ ) − − ψ ψ ( α α ^ ^ + β β ^ ^ ) = log ⁡ ⁡ σ σ ( x ) ¯ ¯ ψ ψ ( β β ^ ^ ) − − ψ ψ ( α α ^ ^ + β β ^ ^ ) = log ⁡ ⁡ σ σ ( − − x ) ¯ ¯ {\displaystyle {\begin{aligned}\psi ({\hat {\alpha }})-\psi ({\hat {\alpha }}+{\hat {\beta }})&={\overline {\log \sigma (x)}}\\\psi ({\hat {\beta }})-\psi ({\hat {\alpha }}+{\hat {\beta }})&={\overline {\log \sigma (-x)}}\\\end{aligned}}} which is also where the partial derivatives of the above maximand vanish.

Relationships with other distributions [ edit ] Relationships with other distributions include: The log-ratio of gamma variates is of type IV as detailed above .

If y ∼ ∼ BetaPrime ( α α , β β ) {\displaystyle y\sim {\text{BetaPrime}}(\alpha ,\beta )} , then x = ln ⁡ ⁡ y {\displaystyle x=\ln y} has a type IV distribution, with parameters α α {\displaystyle \alpha } and β β {\displaystyle \beta } . See beta prime distribution .

If z ∼ ∼ Gamma ( β β , 1 ) {\displaystyle z\sim {\text{Gamma}}(\beta ,1)} and y ∣ ∣ z ∼ ∼ Gamma ( α α , z ) {\displaystyle y\mid z\sim {\text{Gamma}}(\alpha ,z)} , where z {\displaystyle z} is used as the rate parameter of the second gamma distribution, then y {\displaystyle y} has a compound gamma distribution , which is the same as BetaPrime ( α α , β β ) {\displaystyle {\text{BetaPrime}}(\alpha ,\beta )} , so that x = ln ⁡ ⁡ y {\displaystyle x=\ln y} has a type IV distribution.

If p ∼ ∼ Beta ( α α , β β ) {\displaystyle p\sim {\text{Beta}}(\alpha ,\beta )} , then x = logit p {\displaystyle x={\text{logit}}\,p} has a type IV distribution, with parameters α α {\displaystyle \alpha } and β β {\displaystyle \beta } . See beta distribution . The logit function , l o g i t ( p ) = log ⁡ ⁡ p 1 − − p {\displaystyle \mathrm {logit} (p)=\log {\frac {p}{1-p}}} is the inverse of the logistic function . This relationship explains the name logistic-beta for this distribution: if the logistic function is applied to logistic-beta variates, the transformed distribution is beta.

Large shape parameters [ edit ] Type IV vs normal distribution with matched mean and variance. For large values of α α , β β {\displaystyle \alpha ,\beta } , the pdf's are very similar, except for very rare values of x {\displaystyle x} .

For large values of the shape parameters, α α , β β ≫ ≫ 1 {\displaystyle \alpha ,\beta \gg 1} , the distribution becomes more Gaussian , with: E [ x ] ≈ ≈ ln ⁡ ⁡ α α β β var [ x ] ≈ ≈ α α + β β α α β β {\displaystyle {\begin{aligned}E[x]&\approx \ln {\frac {\alpha }{\beta }}\\{\text{var}}[x]&\approx {\frac {\alpha +\beta }{\alpha \beta }}\end{aligned}}} This is demonstrated in the pdf and log pdf plots here.

Random variate generation [ edit ] Since random sampling from the gamma and beta distributions are readily available on many software platforms, the above relationships with those distributions can be used to generate variates from the type IV distribution.

Generalization with location and scale parameters [ edit ] A flexible, four-parameter family can be obtained by adding location and scale parameters . One way to do this is if x ∼ ∼ B σ σ ( α α , β β ) {\displaystyle x\sim B_{\sigma }(\alpha ,\beta )} , then let y = k x + δ δ {\displaystyle y=kx+\delta } , where k > 0 {\displaystyle k>0} is the scale parameter and δ δ ∈ ∈ R {\displaystyle \delta \in \mathbb {R} } is the location parameter. The four-parameter family obtained thus has the desired additional flexibility, but the new parameters may be hard to interpret because δ δ ≠ ≠ E [ y ] {\displaystyle \delta \neq E[y]} and k 2 ≠ ≠ var [ y ] {\displaystyle k^{2}\neq {\text{var}}[y]} . Moreover maximum-likelihood estimation with this parametrization is hard. These problems can be addressed as follows.

Recall that the mean and variance of x {\displaystyle x} are: μ μ ~ ~ = ψ ψ ( α α ) − − ψ ψ ( β β ) , s ~ ~ 2 = ψ ψ ′ ( α α ) + ψ ψ ′ ( β β ) {\displaystyle {\begin{aligned}{\tilde {\mu }}&=\psi (\alpha )-\psi (\beta ),&{\tilde {s}}^{2}&=\psi '(\alpha )+\psi '(\beta )\end{aligned}}} Now expand the family with location parameter μ μ ∈ ∈ R {\displaystyle \mu \in \mathbb {R} } and scale parameter s > 0 {\displaystyle s>0} , via the transformation: y = μ μ + s s ~ ~ ( x − − μ μ ~ ~ ) ⟺ ⟺ x = μ μ ~ ~ + s ~ ~ s ( y − − μ μ ) {\displaystyle {\begin{aligned}y&=\mu +{\frac {s}{\tilde {s}}}(x-{\tilde {\mu }})\iff x={\tilde {\mu }}+{\frac {\tilde {s}}{s}}(y-\mu )\end{aligned}}} so that μ μ = E [ y ] {\displaystyle \mu =E[y]} and s 2 = var [ y ] {\displaystyle s^{2}={\text{var}}[y]} are now interpretable. It may be noted that allowing s {\displaystyle s} to be either positive or negative does not generalize this family, because of the above-noted symmetry property. We adopt the notation y ∼ ∼ B ¯ ¯ σ σ ( α α , β β , μ μ , s 2 ) {\displaystyle y\sim {\bar {B}}_{\sigma }(\alpha ,\beta ,\mu ,s^{2})} for this family.

If the pdf for x ∼ ∼ B σ σ ( α α , β β ) {\displaystyle x\sim B_{\sigma }(\alpha ,\beta )} is f ( x ; α α , β β ) {\displaystyle f(x;\alpha ,\beta )} , then the pdf for y ∼ ∼ B ¯ ¯ σ σ ( α α , β β , μ μ , s 2 ) {\displaystyle y\sim {\bar {B}}_{\sigma }(\alpha ,\beta ,\mu ,s^{2})} is: f ¯ ¯ ( y ; α α , β β , μ μ , s 2 ) = s ~ ~ s f ( x ; α α , β β ) {\displaystyle {\bar {f}}(y;\alpha ,\beta ,\mu ,s^{2})={\frac {\tilde {s}}{s}}\,f(x;\alpha ,\beta )} where it is understood that x {\displaystyle x} is computed as detailed above, as a function of y , α α , β β , μ μ , s {\displaystyle y,\alpha ,\beta ,\mu ,s} . The pdf and log-pdf plots above, where the captions contain (means=0, variances=1), are for B ¯ ¯ σ σ ( α α , β β , 0 , 1 ) {\displaystyle {\bar {B}}_{\sigma }(\alpha ,\beta ,0,1)} .

Maximum likelihood parameter estimation [ edit ] In this section, maximum-likelihood estimation of the distribution parameters, given a dataset x 1 , … … , x n {\displaystyle x_{1},\ldots ,x_{n}} is discussed in turn for the families B σ σ ( α α , β β ) {\displaystyle B_{\sigma }(\alpha ,\beta )} and B ¯ ¯ σ σ ( α α , β β , μ μ , s 2 ) {\displaystyle {\bar {B}}_{\sigma }(\alpha ,\beta ,\mu ,s^{2})} .

Maximum likelihood for standard Type IV [ edit ] As noted above , B σ σ ( α α , β β ) {\displaystyle B_{\sigma }(\alpha ,\beta )} is an exponential family with natural parameters α α , β β {\displaystyle \alpha ,\beta } , the maximum-likelihood estimates of which depend only on averaged sufficient statistics: log ⁡ ⁡ σ σ ( x ) ¯ ¯ = 1 n ∑ ∑ i log ⁡ ⁡ σ σ ( x i ) and log ⁡ ⁡ σ σ ( − − x ) ¯ ¯ = 1 n ∑ ∑ i log ⁡ ⁡ σ σ ( − − x i ) {\displaystyle {\begin{aligned}{\overline {\log \sigma (x)}}&={\frac {1}{n}}\sum _{i}\log \sigma (x_{i})&&{\text{and}}&{\overline {\log \sigma (-x)}}&={\frac {1}{n}}\sum _{i}\log \sigma (-x_{i})\end{aligned}}} Once these statistics have been accumulated, the maximum-likelihood estimate is given by: α α ^ ^ , β β ^ ^ = arg ⁡ ⁡ max α α , β β > 0 α α log ⁡ ⁡ σ σ ( x ) ¯ ¯ + β β log ⁡ ⁡ σ σ ( − − x ) ¯ ¯ − − log ⁡ ⁡ B ( α α , β β ) {\displaystyle {\begin{aligned}{\hat {\alpha }},{\hat {\beta }}=\arg \max _{\alpha ,\beta >0}&\;\alpha \,{\overline {\log \sigma (x)}}+\beta \,{\overline {\log \sigma (-x)}}-\log B(\alpha ,\beta )\end{aligned}}} By using the parametrization θ θ 1 = log ⁡ ⁡ α α {\displaystyle \theta _{1}=\log \alpha } and θ θ 2 = log ⁡ ⁡ β β {\displaystyle \theta _{2}=\log \beta } an unconstrained numerical optimization algorithm like BFGS can be used. Optimization iterations are fast, because they are independent of the size of the data-set.

An alternative is to use an EM-algorithm based on the composition: x − − log ⁡ ⁡ ( γ γ δ δ ) ∼ ∼ B σ σ ( α α , β β ) {\displaystyle x-\log(\gamma \delta )\sim B_{\sigma }(\alpha ,\beta )} if z ∼ ∼ Gamma ( β β , γ γ ) {\displaystyle z\sim {\text{Gamma}}(\beta ,\gamma )} and e x ∣ ∣ z ∼ ∼ Gamma ( α α , z / δ δ ) {\displaystyle e^{x}\mid z\sim {\text{Gamma}}(\alpha ,z/\delta )} . Because of the self-conjugacy of the gamma distribution , the posterior expectations, ⟨ z ⟩ P ( z ∣ ∣ x ) {\displaystyle \left\langle z\right\rangle _{P(z\mid x)}} and ⟨ log ⁡ ⁡ z ⟩ P ( z ∣ ∣ x ) {\displaystyle \left\langle \log z\right\rangle _{P(z\mid x)}} that are required for the E-step can be computed in closed form. The M-step parameter update can be solved analogously to maximum-likelihood for the gamma distribution .

Maximum likelihood for the four-parameter family [ edit ] The maximum-likelihood problem for B ¯ ¯ σ σ ( α α , β β , μ μ , s 2 ) {\displaystyle {\bar {B}}_{\sigma }(\alpha ,\beta ,\mu ,s^{2})} , having pdf f ¯ ¯ {\displaystyle {\bar {f}}} is: α α ^ ^ , β β ^ ^ , μ μ ^ ^ , s ^ ^ = arg ⁡ ⁡ max α α , β β , μ μ , s log ⁡ ⁡ 1 n ∑ ∑ i f ¯ ¯ ( x i ; α α , β β , μ μ , s 2 ) {\displaystyle {\hat {\alpha }},{\hat {\beta }},{\hat {\mu }},{\hat {s}}=\arg \max _{\alpha ,\beta ,\mu ,s}\log {\frac {1}{n}}\sum _{i}{\bar {f}}(x_{i};\alpha ,\beta ,\mu ,s^{2})} This is no longer an exponential family, so that each optimization iteration has to traverse the whole data-set. Moreover the computation of the partial derivatives (as required for example by BFGS) is considerably more complex than for the above two-parameter case. However, all the component functions are readily available in software packages with automatic differentiation . Again, the positive parameters can be parametrized in terms of their logarithms to obtain an unconstrained numerical optimization problem.

For this problem, numerical optimization may fail unless the initial location and scale parameters are chosen appropriately. However the above-mentioned interpretability of these parameters in the parametrization of B ¯ ¯ σ σ {\displaystyle {\bar {B}}_{\sigma }} can be used to do this. Specifically, the initial values for μ μ {\displaystyle \mu } and s 2 {\displaystyle s^{2}} can be set to the empirical mean and variance of the data.

See also [ edit ] Champernowne distribution , another generalization of the logistic distribution.

References [ edit ] ^ a b Johnson, N.L., Kotz, S., Balakrishnan, N. (1995) Continuous Univariate Distributions, Volume 2 , Wiley.

ISBN 0-471-58494-0 (pages 140–142) ^ a b Lee, C. J., Zito, A., Sang, H., & Dunson, D. B. (2025). Logistic-Beta Processes for Dependent Random Probabilities with Beta Marginals.

Bayesian Analysis , 1(1), 1-25.

https://doi.org/10.1214/25-BA1541 ^ a b Halliwell, L. J. (2021). The log-gamma distribution and non-normal error.

Variance 2(13).

https://www.casact.org/abstract/log-gamma-distribution-and-non-normal-error ^ Barndorff-Nielsen, O., Kent, J., & Sørensen, M. (1982). Normal variance-mean mixtures and z distributions.

International Statistical Review , 145-159.

https://doi.org/10.2307/1402598 ^ C.M.Bishop, Pattern Recognition and Machine Learning , Springer 2006.

v t e Probability distributions ( list ) Discrete univariate with finite support Benford Bernoulli Beta-binomial Binomial Categorical Hypergeometric Negative Poisson binomial Rademacher Soliton Discrete uniform Zipf Zipf–Mandelbrot with infinite support Beta negative binomial Borel Conway–Maxwell–Poisson Discrete phase-type Delaporte Extended negative binomial Flory–Schulz Gauss–Kuzmin Geometric Logarithmic Mixed Poisson Negative binomial Panjer Parabolic fractal Poisson Skellam Yule–Simon Zeta Continuous univariate supported on a bounded interval Arcsine ARGUS Balding–Nichols Bates Beta Generalized Beta rectangular Continuous Bernoulli Irwin–Hall Kumaraswamy Logit-normal Noncentral beta PERT Raised cosine Reciprocal Triangular U-quadratic Uniform Wigner semicircle supported on a semi-infinite interval Benini Benktander 1st kind Benktander 2nd kind Beta prime Burr Chi Chi-squared Noncentral Inverse Scaled Dagum Davis Erlang Hyper Exponential Hyperexponential Hypoexponential Logarithmic F Noncentral Folded normal Fréchet Gamma Generalized Inverse gamma/Gompertz Gompertz Shifted Half-logistic Half-normal Hotelling's T -squared Hartman–Watson Inverse Gaussian Generalized Kolmogorov Lévy Log-Cauchy Log-Laplace Log-logistic Log-normal Log-t Lomax Matrix-exponential Maxwell–Boltzmann Maxwell–Jüttner Mittag-Leffler Nakagami Pareto Phase-type Poly-Weibull Rayleigh Relativistic Breit–Wigner Rice Truncated normal type-2 Gumbel Weibull Discrete Wilks's lambda supported on the whole real line Cauchy Exponential power Fisher's z Kaniadakis κ-Gaussian Gaussian q Generalized hyperbolic Generalized logistic (logistic-beta) Generalized normal Geometric stable Gumbel Holtsmark Hyperbolic secant Johnson's S U Landau Laplace Asymmetric Logistic Noncentral t Normal (Gaussian) Normal-inverse Gaussian Skew normal Slash Stable Student's t Tracy–Widom Variance-gamma Voigt with support whose type varies Generalized chi-squared Generalized extreme value Generalized Pareto Marchenko–Pastur Kaniadakis κ -exponential Kaniadakis κ -Gamma Kaniadakis κ -Weibull Kaniadakis κ -Logistic Kaniadakis κ -Erlang q -exponential q -Gaussian q -Weibull Shifted log-logistic Tukey lambda Mixed univariate continuous- discrete Rectified Gaussian Multivariate (joint) Discrete: Ewens Multinomial Dirichlet Negative Continuous: Dirichlet Generalized Multivariate Laplace Multivariate normal Multivariate stable Multivariate t Normal-gamma Inverse Matrix-valued: LKJ Matrix beta Matrix normal Matrix t Matrix gamma Inverse Wishart Normal Inverse Normal-inverse Complex Uniform distribution on a Stiefel manifold Directional Univariate (circular) directional Circular uniform Univariate von Mises Wrapped normal Wrapped Cauchy Wrapped exponential Wrapped asymmetric Laplace Wrapped Lévy Bivariate (spherical) Kent Bivariate (toroidal) Bivariate von Mises Multivariate von Mises–Fisher Bingham Degenerate and singular Degenerate Dirac delta function Singular Cantor Families Circular Compound Poisson Elliptical Exponential Natural exponential Location–scale Maximum entropy Mixture Pearson Tweedie Wrapped Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=Generalized_logistic_distribution&oldid=1301351060 " Category : Continuous distributions Hidden categories: Articles with short description Short description matches Wikidata This page was last edited on 19 July 2025, at 09:33 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Generalized logistic distribution 1 language Add topic

