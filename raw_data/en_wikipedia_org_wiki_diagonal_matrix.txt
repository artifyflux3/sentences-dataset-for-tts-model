Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Definition 2 Vector-to-matrix diag operator 3 Matrix-to-vector diag operator 4 Scalar matrix 5 Vector operations 6 Matrix operations 7 Operator matrix in eigenbasis 8 Properties 9 Applications 10 Operator theory 11 See also 12 Notes 13 References 14 Sources Toggle the table of contents Diagonal matrix 38 languages العربية Català Чӑвашла Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Bahasa Indonesia Interlingua Íslenska Italiano עברית Latina Magyar Nederlands 日本語 Polski Português Română Русский Slovenščina Српски / srpski Suomi Svenska தமிழ் ไทย Türkçe Українська اردو 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Matrix whose only nonzero elements are on its main diagonal This article includes a list of general references , but it lacks sufficient corresponding inline citations .

Please help to improve this article by introducing more precise citations.

( June 2025 ) ( Learn how and when to remove this message ) In linear algebra , a diagonal matrix is a matrix in which the entries outside the main diagonal are all zero; the term usually refers to square matrices . Elements of the main diagonal can either be zero or nonzero. An example of a 2×2 diagonal matrix is [ 3 0 0 2 ] {\displaystyle \left[{\begin{smallmatrix}3&0\\0&2\end{smallmatrix}}\right]} , while an example of a 3×3 diagonal matrix is [ 6 0 0 0 5 0 0 0 4 ] {\displaystyle \left[{\begin{smallmatrix}6&0&0\\0&5&0\\0&0&4\end{smallmatrix}}\right]} . An identity matrix of any size, or any multiple of it is a diagonal matrix called a scalar matrix , for example, [ 0.5 0 0 0.5 ] {\displaystyle \left[{\begin{smallmatrix}0.5&0\\0&0.5\end{smallmatrix}}\right]} . 
In geometry , a diagonal matrix may be used as a scaling matrix , since matrix multiplication with it results in changing scale (size) and possibly also shape ; only a scalar matrix results in uniform change in scale.

Definition [ edit ] As stated above, a diagonal matrix is a matrix in which all off-diagonal entries are zero. That is, the matrix D = ( d i , j ) with n columns and n rows is diagonal if ∀ ∀ i , j ∈ ∈ { 1 , 2 , … … , n } , i ≠ ≠ j ⟹ ⟹ d i , j = 0.

{\displaystyle \forall i,j\in \{1,2,\ldots ,n\},i\neq j\implies d_{i,j}=0.} However, the main diagonal entries are unrestricted.

The term diagonal matrix may sometimes refer to a rectangular diagonal matrix , which is an m -by- n matrix with all the entries not of the form d i , i being zero. For example: [ 1 0 0 0 4 0 0 0 − − 3 0 0 0 ] or [ 1 0 0 0 0 0 4 0 0 0 0 0 − − 3 0 0 ] {\displaystyle {\begin{bmatrix}1&0&0\\0&4&0\\0&0&-3\\0&0&0\\\end{bmatrix}}\quad {\text{or}}\quad {\begin{bmatrix}1&0&0&0&0\\0&4&0&0&0\\0&0&-3&0&0\end{bmatrix}}} More often, however, diagonal matrix refers to square matrices, which can be specified explicitly as a square diagonal matrix . A square diagonal matrix is a symmetric matrix , so this can also be called a symmetric diagonal matrix .

The following matrix is square diagonal matrix: [ 1 0 0 0 4 0 0 0 − − 2 ] {\displaystyle {\begin{bmatrix}1&0&0\\0&4&0\\0&0&-2\end{bmatrix}}} If the entries are real numbers or complex numbers , then it is a normal matrix as well.

In the remainder of this article we will consider only square diagonal matrices, and refer to them simply as "diagonal matrices".

Vector-to-matrix diag operator [ edit ] A diagonal matrix D can be constructed from a vector a = [ a 1 … … a n ] T {\displaystyle \mathbf {a} ={\begin{bmatrix}a_{1}&\dots &a_{n}\end{bmatrix}}^{\textsf {T}}} using the diag {\displaystyle \operatorname {diag} } operator: D = diag ⁡ ⁡ ( a 1 , … … , a n ) .

{\displaystyle \mathbf {D} =\operatorname {diag} (a_{1},\dots ,a_{n}).} This may be written more compactly as D = diag ⁡ ⁡ ( a ) {\displaystyle \mathbf {D} =\operatorname {diag} (\mathbf {a} )} .

The same operator is also used to represent block diagonal matrices as A = diag ⁡ ⁡ ( A 1 , … … , A n ) {\displaystyle \mathbf {A} =\operatorname {diag} (\mathbf {A} _{1},\dots ,\mathbf {A} _{n})} where each argument A i is a matrix.

The diag operator may be written as diag ⁡ ⁡ ( a ) = ( a 1 T ) ∘ ∘ I , {\displaystyle \operatorname {diag} (\mathbf {a} )=\left(\mathbf {a} \mathbf {1} ^{\textsf {T}}\right)\circ \mathbf {I} ,} where ∘ ∘ {\displaystyle \circ } represents the Hadamard product , and 1 is a constant vector with elements 1.

Matrix-to-vector diag operator [ edit ] The inverse matrix-to-vector diag operator is sometimes denoted by the identically named diag ⁡ ⁡ ( D ) = [ a 1 … … a n ] T , {\displaystyle \operatorname {diag} (\mathbf {D} )={\begin{bmatrix}a_{1}&\dots &a_{n}\end{bmatrix}}^{\textsf {T}},} where the argument is now a matrix, and the result is a vector of its diagonal entries.

The following property holds: diag ⁡ ⁡ ( A B ) = ∑ ∑ j ( A ∘ ∘ B T ) i j = ( A ∘ ∘ B T ) 1 .

{\displaystyle \operatorname {diag} (\mathbf {A} \mathbf {B} )=\sum _{j}\left(\mathbf {A} \circ \mathbf {B} ^{\textsf {T}}\right)_{ij}=\left(\mathbf {A} \circ \mathbf {B} ^{\textsf {T}}\right)\mathbf {1} .} Scalar matrix [ edit ] A diagonal matrix with equal diagonal entries is a scalar matrix ; that is, a scalar multiple λ of the identity matrix I . Its effect on a vector is scalar multiplication by λ . For example, a 3×3 scalar matrix has the form: [ λ λ 0 0 0 λ λ 0 0 0 λ λ ] ≡ ≡ λ λ I 3 {\displaystyle {\begin{bmatrix}\lambda &0&0\\0&\lambda &0\\0&0&\lambda \end{bmatrix}}\equiv \lambda {\boldsymbol {I}}_{3}} The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.

[ a ] By contrast, over a field (like the real numbers), a diagonal matrix with all diagonal elements distinct only commutes with diagonal matrices (its centralizer is the set of diagonal matrices). That is because if a diagonal matrix D = diag ⁡ ⁡ ( a 1 , … … , a n ) {\displaystyle \mathbf {D} =\operatorname {diag} (a_{1},\dots ,a_{n})} has a i ≠ ≠ a j , {\displaystyle a_{i}\neq a_{j},} then given a matrix M with m i j ≠ ≠ 0 , {\displaystyle m_{ij}\neq 0,} the ( i , j ) term of the products are: ( D M ) i j = a i m i j {\displaystyle (\mathbf {DM} )_{ij}=a_{i}m_{ij}} and ( M D ) i j = m i j a j , {\displaystyle (\mathbf {MD} )_{ij}=m_{ij}a_{j},} and a j m i j ≠ ≠ m i j a i {\displaystyle a_{j}m_{ij}\neq m_{ij}a_{i}} (since one can divide by m ij ), so they do not commute unless the off-diagonal terms are zero.

[ b ] Diagonal matrices where the diagonal entries are not all equal or all distinct have centralizers intermediate between the whole space and only diagonal matrices.

[ 1 ] For an abstract vector space V (rather than the concrete vector space K n ), the analog of scalar matrices are scalar transformations . This is true more generally for a module M over a ring R , with the endomorphism algebra End( M ) (algebra of linear operators on M ) replacing the algebra of matrices. Formally, scalar multiplication is a linear map, inducing a map R → → End ⁡ ⁡ ( M ) , {\displaystyle R\to \operatorname {End} (M),} (from a scalar λ to its corresponding scalar transformation, multiplication by λ ) exhibiting End( M ) as a R - algebra . For vector spaces, the scalar transforms are exactly the center of the endomorphism algebra, and, similarly, scalar invertible transforms are the center of the general linear group GL( V ) . The former is more generally true free modules M ≅ ≅ R n , {\displaystyle M\cong R^{n},} for which the endomorphism algebra is isomorphic to a matrix algebra.

Vector operations [ edit ] Multiplying a vector by a diagonal matrix multiplies each of the terms by the corresponding diagonal entry. Given a diagonal matrix D = diag ⁡ ⁡ ( a 1 , … … , a n ) {\displaystyle \mathbf {D} =\operatorname {diag} (a_{1},\dots ,a_{n})} and a vector v = [ x 1 ⋯ ⋯ x n ] T {\displaystyle \mathbf {v} ={\begin{bmatrix}x_{1}&\dotsm &x_{n}\end{bmatrix}}^{\textsf {T}}} , the product is: D v = diag ⁡ ⁡ ( a 1 , … … , a n ) [ x 1 ⋮ ⋮ x n ] = [ a 1 ⋱ ⋱ a n ] [ x 1 ⋮ ⋮ x n ] = [ a 1 x 1 ⋮ ⋮ a n x n ] .

{\displaystyle \mathbf {D} \mathbf {v} =\operatorname {diag} (a_{1},\dots ,a_{n}){\begin{bmatrix}x_{1}\\\vdots \\x_{n}\end{bmatrix}}={\begin{bmatrix}a_{1}\\&\ddots \\&&a_{n}\end{bmatrix}}{\begin{bmatrix}x_{1}\\\vdots \\x_{n}\end{bmatrix}}={\begin{bmatrix}a_{1}x_{1}\\\vdots \\a_{n}x_{n}\end{bmatrix}}.} This can be expressed more compactly by using a vector instead of a diagonal matrix, d = [ a 1 ⋯ ⋯ a n ] T {\displaystyle \mathbf {d} ={\begin{bmatrix}a_{1}&\dotsm &a_{n}\end{bmatrix}}^{\textsf {T}}} , and taking the Hadamard product of the vectors (entrywise product), denoted d ∘ ∘ v {\displaystyle \mathbf {d} \circ \mathbf {v} } : D v = d ∘ ∘ v = [ a 1 ⋮ ⋮ a n ] ∘ ∘ [ x 1 ⋮ ⋮ x n ] = [ a 1 x 1 ⋮ ⋮ a n x n ] .

{\displaystyle \mathbf {D} \mathbf {v} =\mathbf {d} \circ \mathbf {v} ={\begin{bmatrix}a_{1}\\\vdots \\a_{n}\end{bmatrix}}\circ {\begin{bmatrix}x_{1}\\\vdots \\x_{n}\end{bmatrix}}={\begin{bmatrix}a_{1}x_{1}\\\vdots \\a_{n}x_{n}\end{bmatrix}}.} This is mathematically equivalent, but avoids storing all the zero terms of this sparse matrix . This product is thus used in machine learning , such as computing products of derivatives in backpropagation or multiplying IDF weights in TF-IDF , [ 2 ] since some BLAS frameworks, which multiply matrices efficiently, do not include Hadamard product capability directly.

[ 3 ] Matrix operations [ edit ] The operations of matrix addition and matrix multiplication are especially simple for diagonal matrices. Write diag( a 1 , ..., a n ) for a diagonal matrix whose diagonal entries starting in the upper left corner are a 1 , ..., a n . Then, for addition , we have diag ⁡ ⁡ ( a 1 , … … , a n ) + diag ⁡ ⁡ ( b 1 , … … , b n ) = diag ⁡ ⁡ ( a 1 + b 1 , … … , a n + b n ) {\displaystyle \operatorname {diag} (a_{1},\,\ldots ,\,a_{n})+\operatorname {diag} (b_{1},\,\ldots ,\,b_{n})=\operatorname {diag} (a_{1}+b_{1},\,\ldots ,\,a_{n}+b_{n})} and for matrix multiplication , diag ⁡ ⁡ ( a 1 , … … , a n ) diag ⁡ ⁡ ( b 1 , … … , b n ) = diag ⁡ ⁡ ( a 1 b 1 , … … , a n b n ) .

{\displaystyle \operatorname {diag} (a_{1},\,\ldots ,\,a_{n})\operatorname {diag} (b_{1},\,\ldots ,\,b_{n})=\operatorname {diag} (a_{1}b_{1},\,\ldots ,\,a_{n}b_{n}).} The diagonal matrix diag( a 1 , ..., a n ) is invertible if and only if the entries a 1 , ..., a n are all nonzero. In this case, we have diag ⁡ ⁡ ( a 1 , … … , a n ) − − 1 = diag ⁡ ⁡ ( a 1 − − 1 , … … , a n − − 1 ) .

{\displaystyle \operatorname {diag} (a_{1},\,\ldots ,\,a_{n})^{-1}=\operatorname {diag} (a_{1}^{-1},\,\ldots ,\,a_{n}^{-1}).} In particular, the diagonal matrices form a subring of the ring of all n -by- n matrices.

Multiplying an n -by- n matrix A from the left with diag( a 1 , ..., a n ) amounts to multiplying the i -th row of A by a i for all i ; multiplying the matrix A from the right with diag( a 1 , ..., a n ) amounts to multiplying the i -th column of A by a i for all i .

Operator matrix in eigenbasis [ edit ] Main articles: Transformation matrix § Finding the matrix of a transformation , and Eigenvalues and eigenvectors As explained in determining coefficients of operator matrix , there is a special basis, e 1 , ..., e n , for which the matrix A takes the diagonal form. Hence, in the defining equation A e j = ∑ ∑ i a i , j e i {\textstyle \mathbf {Ae} _{j}=\sum _{i}a_{i,j}\mathbf {e} _{i}} , all coefficients a i, j with i ≠ j are zero, leaving only one term per sum. The surviving diagonal elements, a i, j , are known as eigenvalues and designated with λ i in the equation, which reduces to A e i = λ λ i e i .

{\displaystyle \mathbf {Ae} _{i}=\lambda _{i}\mathbf {e} _{i}.} The resulting equation is known as eigenvalue equation [ 4 ] and used to derive the characteristic polynomial and, further, eigenvalues and eigenvectors .

In other words, the eigenvalues of diag( λ 1 , ..., λ n ) are λ 1 , ..., λ n with associated eigenvectors of e 1 , ..., e n .

Properties [ edit ] The determinant of diag( a 1 , ..., a n ) is the product a 1 ⋯ a n .

The adjugate of a diagonal matrix is again diagonal.

Where all matrices are square, A matrix is diagonal if and only if it is triangular and normal .

A matrix is diagonal if and only if it is both upper- and lower-triangular .

A diagonal matrix is symmetric .

The identity matrix I n and zero matrix are diagonal.

A 1×1 matrix is always diagonal.

The square of a 2×2 matrix with zero trace is always diagonal.

Applications [ edit ] Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.

In fact, a given n -by- n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X −1 AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable .

Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA ∗ = A ∗ A then there exists a unitary matrix U such that UAU ∗ is diagonal). Furthermore, the singular value decomposition implies that for any matrix A , there exist unitary matrices U and V such that U ∗ AV is diagonal with positive entries.

Operator theory [ edit ] In operator theory , particularly the study of PDEs , operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation . Therefore, a key technique to understanding operators is a change of coordinates—in the language of operators, an integral transform —which changes the basis to an eigenbasis of eigenfunctions : which makes the equation separable. An important example of this is the Fourier transform , which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation .

Especially easy are multiplication operators , which are defined as multiplication by (the values of) a fixed function–the values of the function at each point correspond to the diagonal entries of a matrix.

See also [ edit ] Anti-diagonal matrix Banded matrix Bidiagonal matrix Diagonally dominant matrix Diagonalizable matrix Jordan normal form Multiplication operator Tridiagonal matrix Toeplitz matrix Toral Lie algebra Circulant matrix Notes [ edit ] ^ Proof: given the elementary matrix e i j {\displaystyle e_{ij}} , M e i j {\displaystyle Me_{ij}} is the matrix with only the i -th row of M and e i j M {\displaystyle e_{ij}M} is the square matrix with only the M j -th column, so the non-diagonal entries must be zero, and the i th diagonal entry much equal the j th diagonal entry.

^ Over more general rings, this does not hold, because one cannot always divide.

References [ edit ] ^ "Do Diagonal Matrices Always Commute?" . Stack Exchange. March 15, 2016 . Retrieved August 4, 2018 .

^ Sahami, Mehran (2009-06-15).

Text Mining: Classification, Clustering, and Applications . CRC Press. p. 14.

ISBN 9781420059458 .

^ "Element-wise vector-vector multiplication in BLAS?" .

stackoverflow.com . 2011-10-01 . Retrieved 2020-08-30 .

^ Nearing, James (2010).

"Chapter 7.9: Eigenvalues and Eigenvectors" (PDF) .

Mathematical Tools for Physics . Dover Publications.

ISBN 978-0486482125 . Retrieved January 1, 2012 .

Sources [ edit ] Horn, Roger Alan ; Johnson, Charles Royal (1985), Matrix Analysis , Cambridge University Press , ISBN 978-0-521-38632-6 v t e Matrix classes Explicitly constrained entries Alternant Anti-diagonal Anti-Hermitian Anti-symmetric Arrowhead Band Bidiagonal Bisymmetric Block-diagonal Block Block tridiagonal Boolean Cauchy Centrosymmetric Conference Complex Hadamard Copositive Diagonally dominant Diagonal Discrete Fourier Transform Elementary Equivalent Frobenius Generalized permutation Hadamard Hankel Hermitian Hessenberg Hollow Integer Logical Matrix unit Metzler Moore Nonnegative Pentadiagonal Permutation Persymmetric Polynomial Quaternionic Signature Skew-Hermitian Skew-symmetric Skyline Sparse Sylvester Symmetric Toeplitz Triangular Tridiagonal Vandermonde Walsh Z Constant Exchange Hilbert Identity Lehmer Of ones Pascal Pauli Redheffer Shift Zero Conditions on eigenvalues or eigenvectors Companion Convergent Defective Definite Diagonalizable Hurwitz-stable Positive-definite Stieltjes Satisfying conditions on products or inverses Congruent Idempotent or Projection Invertible Involutory Nilpotent Normal Orthogonal Unimodular Unipotent Unitary Totally unimodular Weighing With specific applications Adjugate Alternating sign Augmented Bézout Carleman Cartan Circulant Cofactor Commutation Confusion Coxeter Distance Duplication and elimination Euclidean distance Fundamental (linear differential equation) Generator Gram Hessian Householder Jacobian Moment Payoff Pick Random Rotation Routh-Hurwitz Seifert Shear Similarity Symplectic Totally positive Transformation Used in statistics Centering Correlation Covariance Design Doubly stochastic Fisher information Hat Precision Stochastic Transition Used in graph theory Adjacency Biadjacency Degree Edmonds Incidence Laplacian Seidel adjacency Tutte Used in science and engineering Cabibbo–Kobayashi–Maskawa Density Fundamental (computer vision) Fuzzy associative Gamma Gell-Mann Hamiltonian Irregular Overlap S State transition Substitution Z (chemistry) Related terms Jordan normal form Linear independence Matrix exponential Matrix representation of conic sections Perfect matrix Pseudoinverse Row echelon form Wronskian Mathematics portal List of matrices Category:Matrices (mathematics) NewPP limit report
Parsed by mw‐web.codfw.main‐6cc77c66b8‐78t98
Cached time: 20250812015748
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.461 seconds
Real time usage: 0.635 seconds
Preprocessor visited node count: 3601/1000000
Revision size: 16984/2097152 bytes
Post‐expand include size: 53975/2097152 bytes
Template argument size: 5704/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 32462/5000000 bytes
Lua time usage: 0.249/10.000 seconds
Lua memory usage: 4914191/52428800 bytes
Number of Wikibase entities loaded: 0/500 Transclusion expansion time report (%,ms,calls,template)
100.00%  460.149      1 -total
 25.13%  115.647      2 Template:Reflist
 22.13%  101.812      1 Template:Matrix_classes
 21.69%   99.803      1 Template:Navbox
 18.75%   86.295      2 Template:Cite_web
 14.01%   64.446      1 Template:Short_description
 11.05%   50.843      1 Template:More_footnotes_needed
 10.40%   47.861      1 Template:Ambox
  9.66%   44.462     42 Template:Math
  7.97%   36.683      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:174080:|#|:idhash:canonical and timestamp 20250812015748 and revision id 1297704246. Rendering was triggered because: page-view Retrieved from " https://en.wikipedia.org/w/index.php?title=Diagonal_matrix&oldid=1297704246 " Categories : Matrix normal forms Sparse matrices Hidden categories: Use American English from March 2019 All Wikipedia articles written in American English Articles with short description Short description matches Wikidata Articles lacking in-text citations from June 2025 All articles lacking in-text citations This page was last edited on 27 June 2025, at 23:43 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Diagonal matrix 38 languages Add topic

