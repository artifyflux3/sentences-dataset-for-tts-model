Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk CentralNotice Contents move to sidebar hide (Top) 1 Short history 2 First and second law 3 Examples of irreversible processes 4 Performance of heat engines and refrigerators Toggle Performance of heat engines and refrigerators subsection 4.1 Engines 4.2 Refrigerators 4.3 Power dissipation 5 Equivalence with other formulations 6 Expressions for the entropy production Toggle Expressions for the entropy production subsection 6.1 Heat flow 6.2 Flow of mass 6.3 Entropy of mixing 6.4 Joule expansion 6.5 Microscopic interpretation 7 Basic inequalities and stability conditions 8 Homogeneous systems 9 Entropy production in stochastic processes 10 See also 11 References 12 Further reading Toggle the table of contents Entropy production 3 languages Español فارسی Polski Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Development of entropy in a thermodynamic system This article's lead section may be too short to adequately summarize the key points .

Please consider expanding the lead to provide an accessible overview of all important aspects of the article.

( February 2024 ) Entropy production (or generation) is the amount of entropy which is produced during heat process to evaluate the efficiency of the process.

Rudolf Clausius Short history [ edit ] Entropy is produced in irreversible processes . The importance of avoiding irreversible processes (hence reducing the entropy production) was recognized as early as 1824 by Carnot .

[ 1 ] In 1865 Rudolf Clausius expanded his previous work from 1854 [ 2 ] on the concept of "unkompensierte Verwandlungen" (uncompensated transformations), which, in our modern nomenclature, would be called the entropy production. In the same article in which he introduced the name entropy, [ 3 ] Clausius gives the expression for the entropy production for a cyclical process in a closed system, which he denotes by N , in equation (71) which reads N = S − − S 0 − − ∫ ∫ d Q T .

{\displaystyle N=S-S_{0}-\int {\frac {dQ}{T}}.} Here S is the entropy in the final state and S 0 the entropy in the initial state; S 0 -S is the entropy difference for the backwards part of the process.  The integral is to be taken from the initial state to the final state, giving the entropy difference for the forwards part of the process. From the context, it is clear that N = 0 if the process is reversible and N > 0 in case of an irreversible process.

First and second law [ edit ] Fig. 1 General representation of an inhomogeneous system that consists of a number of subsystems. The interaction of the system with the surroundings is through exchange of heat and other forms of energy, flow of matter, and changes of shape. The internal interactions between the various subsystems are of a similar nature and lead to entropy production.

The laws of thermodynamics apply to well-defined systems. Fig. 1 is a general representation of a thermodynamic system . We consider systems which, in general, are inhomogeneous. Heat and mass are transferred across the boundaries (nonadiabatic, open systems), and the boundaries are moving (usually through pistons). In our formulation we assume that heat and mass transfer and volume changes take place only separately at well-defined regions of the system boundary. The expression, given here, are not the most general formulations of the first and second law. E.g.

kinetic energy and potential energy terms are missing and exchange of matter by diffusion is excluded.

The rate of entropy production, denoted by S ˙ ˙ i {\displaystyle {\dot {S}}_{\text{i}}} , is a key element of the second law of thermodynamics for open inhomogeneous systems which reads d S d t = ∑ ∑ k Q ˙ ˙ k T k + ∑ ∑ k S ˙ ˙ k + ∑ ∑ k S ˙ ˙ i k with S ˙ ˙ i k ≥ ≥ 0.

{\displaystyle {\frac {\mathrm {d} S}{\mathrm {d} t}}=\sum _{k}{\frac {{\dot {Q}}_{k}}{T_{k}}}+\sum _{k}{\dot {S}}_{k}+\sum _{k}{\dot {S}}_{{\text{i}}k}{\text{   with  }}{\dot {S}}_{{\text{i}}k}\geq 0.} Here S is the entropy of the system; T k is the temperature at which the heat enters the system at heat flow rate Q ˙ ˙ k {\displaystyle {\dot {Q}}_{k}} ; S ˙ ˙ k = n ˙ ˙ k S m k = m ˙ ˙ k s k {\displaystyle {\dot {S}}_{k}={\dot {n}}_{k}S_{{\text{m}}k}={\dot {m}}_{k}s_{k}} represents the entropy flow into the system at position k , due to matter flowing into the system ( n ˙ ˙ k , m ˙ ˙ k {\displaystyle {\dot {n}}_{k},{\dot {m}}_{k}} are the molar flow rate and mass flow rate and S m k and s k are the molar entropy (i.e. entropy per unit amount of substance ) and specific entropy (i.e. entropy per unit mass) of the matter, flowing into the system, respectively); S ˙ ˙ i k {\displaystyle {\dot {S}}_{{\text{i}}k}} represents the entropy production rates due to internal processes. The subscript 'i' in S ˙ ˙ i k {\displaystyle {\dot {S}}_{{\text{i}}k}} refers to the fact that the entropy is produced due to irreversible processes. The entropy-production rate of every process in nature is always positive or zero. This is an essential aspect of the second law.

The Σ's indicate the algebraic sum of the respective contributions if there are more heat flows, matter flows, and internal processes.

In order to demonstrate the impact of the second law, and the role of entropy production, it has to be combined with the first law which reads d U d t = ∑ ∑ k Q ˙ ˙ k + ∑ ∑ k H ˙ ˙ k − − ∑ ∑ k p k d V k d t + P , {\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} t}}=\sum _{k}{\dot {Q}}_{k}+\sum _{k}{\dot {H}}_{k}-\sum _{k}p_{k}{\frac {\mathrm {d} V_{k}}{\mathrm {d} t}}+P,} with U the internal energy of the system; H ˙ ˙ k = n ˙ ˙ k H m k = m ˙ ˙ k h k {\displaystyle {\dot {H}}_{k}={\dot {n}}_{k}H_{{\text{m}}k}={\dot {m}}_{k}h_{k}} the enthalpy flows into the system due to the matter that flows into the system ( H m k its molar enthalpy, h k the specific enthalpy (i.e. enthalpy per unit mass)), and d V k /d t are the rates of change of the volume of the system due to a moving boundary at position k while p k is the pressure behind that boundary; P represents all other forms of power application (such as electrical).

The first and second law have been formulated in terms of time derivatives of U and S rather than in terms of total differentials d U and d S where it is tacitly assumed that d t > 0. So, the formulation in terms of time derivatives is more elegant. An even bigger advantage of this formulation is, however, that it emphasizes that heat flow rate and power are the basic thermodynamic properties and that heat and work are derived quantities being the time integrals of the heat flow rate and the power respectively.

Examples of irreversible processes [ edit ] Entropy is produced in irreversible processes . Some important irreversible processes are: heat flow through a thermal resistor fluid flow through a flow resistance such as in the Joule expansion or the Joule–Thomson effect heat transfer Joule heating friction between solid surfaces fluid viscosity within a system.

The expression for the rate of entropy production in the first two cases will be derived in separate sections.

Fig.2 a : Schematic diagram of a heat engine. A heating power Q ˙ ˙ H {\displaystyle {\dot {Q}}_{\text{H}}} enters the engine at the high temperature T H , and Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{a}}} is released at ambient temperature T a . A power P is produced and the entropy production rate is S ˙ ˙ i {\displaystyle {\dot {S}}_{\text{i}}} .

b : Schematic diagram of a refrigerator.

Q ˙ ˙ L {\displaystyle {\dot {Q}}_{\text{L}}} is the cooling power at the low temperature T L , and Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{a}}} is released at ambient temperature. The power P is supplied and S ˙ ˙ i {\displaystyle {\dot {S}}_{\text{i}}} is the entropy production rate. The arrows define the positive directions of the flows of heat and power in the two cases. They are positive under normal operating conditions.

Performance of heat engines and refrigerators [ edit ] Most heat engines and refrigerators are closed cyclic machines.

[ 4 ] In the steady state the internal energy and the entropy of the machines after one cycle are the same as at the start of the cycle. Hence, on average, d U /d t = 0 and d S /d t = 0 since U and S are functions of state.  Furthermore, they are closed systems ( n ˙ ˙ = 0 {\displaystyle {\dot {n}}=0} ) and the volume is fixed (d V /d t = 0). This leads to a significant simplification of the first and second law: 0 = ∑ ∑ k Q ˙ ˙ k + P {\displaystyle 0=\sum _{k}{\dot {Q}}_{k}+P} and 0 = ∑ ∑ k Q ˙ ˙ k T k + S ˙ ˙ i .

{\displaystyle 0=\sum _{k}{\frac {{\dot {Q}}_{k}}{T_{k}}}+{\dot {S}}_{\text{i}}.} The summation is over the (two) places where heat is added or removed.

Engines [ edit ] For a heat engine (Fig. 2a) the first and second law obtain the form 0 = Q ˙ ˙ H − − Q ˙ ˙ a − − P {\displaystyle 0={\dot {Q}}_{\text{H}}-{\dot {Q}}_{\text{a}}-P} and 0 = Q ˙ ˙ H T H − − Q ˙ ˙ a T a + S ˙ ˙ i .

{\displaystyle 0={\frac {{\dot {Q}}_{\text{H}}}{T_{\text{H}}}}-{\frac {{\dot {Q}}_{\text{a}}}{T_{\text{a}}}}+{\dot {S}}_{\text{i}}.} Here Q ˙ ˙ H {\displaystyle {\dot {Q}}_{\text{H}}} is the heat supplied at the high temperature T H , Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{a}}} is the heat removed at ambient temperature T a , and P is the power delivered by the engine. Eliminating Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{a}}} gives P = T H − − T a T H Q ˙ ˙ H − − T a S ˙ ˙ i .

{\displaystyle P={\frac {T_{\text{H}}-T_{\text{a}}}{T_{\text{H}}}}{\dot {Q}}_{\text{H}}-T_{\text{a}}{\dot {S}}_{\text{i}}.} The efficiency is defined by η η = P Q ˙ ˙ H .

{\displaystyle \eta ={\frac {P}{{\dot {Q}}_{\text{H}}}}.} If S ˙ ˙ i = 0 {\displaystyle {\dot {S}}_{\text{i}}=0} the performance of the engine is at its maximum and the efficiency is equal to the Carnot efficiency η η C = T H − − T a T H .

{\displaystyle \eta _{\text{C}}={\frac {T_{\text{H}}-T_{\text{a}}}{T_{\text{H}}}}.} Refrigerators [ edit ] For refrigerators (Fig. 2b) holds 0 = Q ˙ ˙ L − − Q ˙ ˙ a + P {\displaystyle 0={\dot {Q}}_{\text{L}}-{\dot {Q}}_{\text{a}}+P} and 0 = Q ˙ ˙ L T L − − Q ˙ ˙ a T a + S ˙ ˙ i .

{\displaystyle 0={\frac {{\dot {Q}}_{\text{L}}}{T_{\text{L}}}}-{\frac {{\dot {Q}}_{\text{a}}}{T_{\text{a}}}}+{\dot {S}}_{\text{i}}.} Here P is the power, supplied to produce the cooling power Q ˙ ˙ L {\displaystyle {\dot {Q}}_{\text{L}}} at the low temperature T L . Eliminating Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{a}}} now gives Q ˙ ˙ L = T L T a − − T L ( P − − T a S ˙ ˙ i ) .

{\displaystyle {\dot {Q}}_{\text{L}}={\frac {T_{\text{L}}}{T_{\text{a}}-T_{\text{L}}}}(P-T_{\text{a}}{\dot {S}}_{\text{i}}).} The coefficient of performance of refrigerators is defined by ξ ξ = Q ˙ ˙ L P .

{\displaystyle \xi ={\frac {{\dot {Q}}_{\text{L}}}{P}}.} If S ˙ ˙ i = 0 {\displaystyle {\dot {S}}_{\text{i}}=0} the performance of the cooler is at its maximum. The COP is then given by the Carnot coefficient of performance ξ ξ C = T L T a − − T L .

{\displaystyle \xi _{\text{C}}={\frac {T_{\text{L}}}{T_{\text{a}}-T_{\text{L}}}}.} Power dissipation [ edit ] In both cases we find a contribution T a S ˙ ˙ i {\displaystyle T_{\text{a}}{\dot {S}}_{\text{i}}} which reduces the system performance. This product of ambient temperature and the (average) entropy production rate P diss = T a S ˙ ˙ i {\displaystyle P_{\text{diss}}=T_{\text{a}}{\dot {S}}_{\text{i}}} is called the dissipated power.

Equivalence with other formulations [ edit ] It is interesting to investigate how the above mathematical formulation of the second law relates with other well-known formulations of the second law.

We first look at a heat engine, assuming that Q ˙ ˙ a = 0 {\displaystyle {\dot {Q}}_{\text{a}}=0} . In other words: the heat flow rate Q ˙ ˙ H {\displaystyle {\dot {Q}}_{\text{H}}} is completely converted into power. In this case the second law would reduce to 0 = Q ˙ ˙ H T H + S ˙ ˙ i .

{\displaystyle 0={\frac {{\dot {Q}}_{\text{H}}}{T_{\text{H}}}}+{\dot {S}}_{\text{i}}.} Since Q ˙ ˙ H ≥ ≥ 0 {\displaystyle {\dot {Q}}_{\text{H}}\geq 0} and T H > 0 {\displaystyle T_{\text{H}}>0} this would result in S ˙ ˙ i ≤ ≤ 0 {\displaystyle {\dot {S}}_{\text{i}}\leq 0} which violates the condition that the entropy production is always positive. Hence: No process is possible in which the sole result is the absorption of heat from a reservoir and its complete conversion into work.

This is the Kelvin statement of the second law.

Now look at the case of the refrigerator and assume that the input power is zero. In other words: heat is transported from a low temperature to a high temperature without doing work on the system. The first law with P = 0 would give Q ˙ ˙ L = Q ˙ ˙ a {\displaystyle {\dot {Q}}_{\text{L}}={\dot {Q}}_{\text{a}}} and the second law then yields 0 = Q ˙ ˙ L T L − − Q ˙ ˙ L T a + S ˙ ˙ i {\displaystyle 0={\frac {{\dot {Q}}_{\text{L}}}{T_{\text{L}}}}-{\frac {{\dot {Q}}_{\text{L}}}{T_{\text{a}}}}+{\dot {S}}_{\text{i}}} or S ˙ ˙ i = Q ˙ ˙ L ( 1 T a − − 1 T L ) .

{\displaystyle {\dot {S}}_{\text{i}}={\dot {Q}}_{\text{L}}\left({\frac {1}{T_{\text{a}}}}-{\frac {1}{T_{\text{L}}}}\right).} Since Q ˙ ˙ L ≥ ≥ 0 {\displaystyle {\dot {Q}}_{\text{L}}\geq 0} and T a > T L {\displaystyle T_{\text{a}}>T_{\text{L}}} this would result in S ˙ ˙ i ≤ ≤ 0 {\displaystyle {\dot {S}}_{\text{i}}\leq 0} which again violates the condition that the entropy production is always positive. Hence: No process is possible whose sole result is the transfer of heat from a body of lower temperature to a body of higher temperature.

This is the Clausius statement of the second law.

Expressions for the entropy production [ edit ] Heat flow [ edit ] In case of a heat flow rate Q ˙ ˙ {\displaystyle {\dot {Q}}} from T 1 to T 2 (with T 1 ≥ ≥ T 2 {\displaystyle T_{1}\geq T_{2}} ) the rate of entropy production is given by S ˙ ˙ i = Q ˙ ˙ ( 1 T 2 − − 1 T 1 ) .

{\displaystyle {\dot {S}}_{\text{i}}={\dot {Q}}\left({\frac {1}{T_{2}}}-{\frac {1}{T_{1}}}\right).} If the heat flow is in a bar with length L , cross-sectional area A , and thermal conductivity κ , and the temperature difference is small Q ˙ ˙ = κ κ A L ( T 1 − − T 2 ) {\displaystyle {\dot {Q}}=\kappa {\frac {A}{L}}(T_{1}-T_{2})} the entropy production rate is S ˙ ˙ i = κ κ A L ( T 1 − − T 2 ) 2 T 1 T 2 .

{\displaystyle {\dot {S}}_{\text{i}}=\kappa {\frac {A}{L}}{\frac {(T_{1}-T_{2})^{2}}{T_{1}T_{2}}}.} Flow of mass [ edit ] In case of a volume flow rate V ˙ ˙ {\displaystyle {\dot {V}}} from a pressure p 1 to p 2 S ˙ ˙ i = − − ∫ ∫ p 1 p 2 V ˙ ˙ T d p .

{\displaystyle {\dot {S}}_{\text{i}}=-\int _{p_{1}}^{p_{2}}{\frac {\dot {V}}{T}}\mathrm {d} p.} For small pressure drops and defining the flow conductance C by V ˙ ˙ = C ( p 1 − − p 2 ) {\displaystyle {\dot {V}}=C(p_{1}-p_{2})} we get S ˙ ˙ i = C ( p 1 − − p 2 ) 2 T .

{\displaystyle {\dot {S}}_{\text{i}}=C{\frac {(p_{1}-p_{2})^{2}}{T}}.} The dependences of S ˙ ˙ i {\displaystyle {\dot {S}}_{\text{i}}} on T 1 − T 2 and on p 1 − p 2 are quadratic.

This is typical for expressions of the entropy production rates in general. They guarantee that the entropy production is positive.

Entropy of mixing [ edit ] In this Section we will calculate the entropy of mixing when two ideal gases diffuse into each other. Consider a volume V t divided in two volumes V a and V b so that V t = V a + V b . The volume V a contains amount of substance n a of an ideal gas a and V b contains amount of substance n b of gas b. The total amount of substance is n t = n a + n b . The temperature and pressure in the two volumes is the same. The entropy at the start is given by S t1 = S a1 + S b1 .

{\displaystyle S_{\text{t1}}=S_{\text{a1}}+S_{\text{b1}}.} When the division between the two gases is removed the two gases expand, comparable to a Joule–Thomson expansion. In the final state the temperature is the same as initially but the two gases now both take the volume V t . The relation of the entropy of an amount of substance n of an ideal gas is S = n C V ln ⁡ ⁡ T T 0 + n R ln ⁡ ⁡ V V 0 {\displaystyle S=nC_{\text{V}}\ln {\frac {T}{T_{0}}}+nR\ln {\frac {V}{V_{0}}}} where C V is the molar heat capacity at constant volume and R is the molar gas constant.
The system is an adiabatic closed system, so the entropy increase during the mixing of the two gases is equal to the entropy production. It is given by S Δ Δ = S t2 − − S t1 .

{\displaystyle S_{\Delta }=S_{\text{t2}}-S_{\text{t1}}.} As the initial and final temperature are the same, the temperature terms cancel, leaving only the volume terms. The result is S Δ Δ = n a R ln ⁡ ⁡ V t V a + n b R ln ⁡ ⁡ V t V b .

{\displaystyle S_{\Delta }=n_{\text{a}}R\ln {\frac {V_{\text{t}}}{V_{\text{a}}}}+n_{\text{b}}R\ln {\frac {V_{\text{t}}}{V_{\text{b}}}}.} Introducing the concentration x = n a / n t = V a / V t we arrive at the well-known expression S Δ Δ = − − n t R [ x ln ⁡ ⁡ x + ( 1 − − x ) ln ⁡ ⁡ ( 1 − − x ) ] .

{\displaystyle S_{\Delta }=-n_{\text{t}}R[x\ln x+(1-x)\ln(1-x)].} Joule expansion [ edit ] The Joule expansion is similar to the mixing described above. It takes place in an adiabatic system consisting of a gas and two rigid vessels a and b of equal volume, connected by a valve. Initially, the valve is closed. Vessel a contains the gas while the other vessel b is empty. When the valve is opened, the gas flows from vessel a into b until the pressures in the two vessels are equal. The volume, taken by the gas, is doubled while the internal energy of the system is constant (adiabatic and no work done). Assuming that the gas is ideal, the molar internal energy is given by U m = C V T . As C V is constant, constant U means constant T . The molar entropy of an ideal gas, as function of the molar volume V m and T , is given by S m = C V ln ⁡ ⁡ T T 0 + R ln ⁡ ⁡ V m V 0 .

{\displaystyle S_{\text{m}}=C_{\text{V}}\ln {\frac {T}{T_{0}}}+R\ln {\frac {V_{\text{m}}}{V_{0}}}.} The system consisting of the two vessels and the gas is closed and adiabatic, so the entropy production during the process is equal to the increase of the entropy of the gas. So, doubling the volume with T constant gives that the molar entropy produced is S mi = R ln ⁡ ⁡ 2.

{\displaystyle S_{\text{mi}}=R\ln 2.} Microscopic interpretation [ edit ] The Joule expansion provides an opportunity to explain the entropy production in statistical mechanical (i.e., microscopic) terms. At the expansion, the volume that the gas can occupy is doubled. This means that, for every molecule there are now two possibilities: it can be placed in container a or b. If the gas has amount of substance n , the number of molecules is equal to n ⋅ N A , where N A is the Avogadro constant . The number of microscopic possibilities increases by a factor of 2 per molecule due to the doubling of volume, so in total the factor is 2 n ⋅ N A . Using the well-known Boltzmann expression for the entropy S = k ln ⁡ ⁡ Ω Ω , {\displaystyle S=k\ln \Omega ,} where k is the Boltzmann constant and Ω is the number of microscopic possibilities to realize the macroscopic state. This gives change in molar entropy of S m Δ Δ = S Δ Δ / n = k ln ⁡ ⁡ ( 2 n ⋅ ⋅ N A ) / n = k N A ln ⁡ ⁡ 2 = R ln ⁡ ⁡ 2.

{\displaystyle S_{{\text{m}}\Delta }=S_{\Delta }/n=k\ln(2^{n\cdot N_{\text{A}}})/n=kN_{\text{A}}\ln 2=R\ln 2.} So, in an irreversible process, the number of microscopic possibilities to realize the macroscopic state is increased by a certain factor.

Basic inequalities and stability conditions [ edit ] In this section we derive the basic inequalities and stability conditions for closed systems. For closed systems the first law reduces to d U d t = Q ˙ ˙ − − p d V d t + P .

{\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} t}}={\dot {Q}}-p{\frac {\mathrm {d} V}{\mathrm {d} t}}+P.} The second law we write as d S d t − − Q ˙ ˙ T ≥ ≥ 0.

{\displaystyle {\frac {\mathrm {d} S}{\mathrm {d} t}}-{\frac {\dot {Q}}{T}}\geq 0.} For adiabatic systems Q ˙ ˙ = 0 {\displaystyle {\dot {Q}}=0} so d S /d t ≥ 0 . In other words: the entropy of adiabatic systems cannot decrease. In equilibrium the entropy is at its maximum. Isolated systems are a special case of adiabatic systems, so this statement is also valid for isolated systems.

Now consider systems with constant temperature and volume . In most cases T is the temperature of the surroundings with which the system is in good thermal contact. Since V is constant the first law gives Q ˙ ˙ = d U / d t − − P {\displaystyle {\dot {Q}}=\mathrm {d} U/\mathrm {d} t-P} . Substitution in the second law, and using that T is constant, gives d ( T S ) d t − − d U d t + P ≥ ≥ 0.

{\displaystyle {\frac {\mathrm {d} (TS)}{\mathrm {d} t}}-{\frac {\mathrm {d} U}{\mathrm {d} t}}+P\geq 0.} With the Helmholtz free energy, defined as F = U − − T S , {\displaystyle F=U-TS,} we get d F d t − − P ≤ ≤ 0.

{\displaystyle {\frac {\mathrm {d} F}{\mathrm {d} t}}-P\leq 0.} If P = 0 this is the mathematical formulation of the general property that the free energy of systems with fixed temperature and volume tends to a minimum. The expression can be integrated from the initial state i to the final state f resulting in W S ≤ ≤ F i − − F f {\displaystyle W_{\text{S}}\leq F_{\text{i}}-F_{\text{f}}} where W S is the work done by the system. If the process inside the system is completely reversible the equality sign holds. Hence the maximum work, that can be extracted from the system, is equal to the free energy of the initial state minus the free energy of the final state.

Finally we consider systems with constant temperature and pressure and take P = 0 . As p is constant the first laws gives d U d t = Q ˙ ˙ − − d ( p V ) d t .

{\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} t}}={\dot {Q}}-{\frac {\mathrm {d} (pV)}{\mathrm {d} t}}.} Combining with the second law, and using that T is constant, gives d ( T S ) d t − − d U d t − − d ( p V ) d t ≥ ≥ 0.

{\displaystyle {\frac {\mathrm {d} (TS)}{\mathrm {d} t}}-{\frac {\mathrm {d} U}{\mathrm {d} t}}-{\frac {\mathrm {d} (pV)}{\mathrm {d} t}}\geq 0.} With the Gibbs free energy, defined as G = U + p V − − T S , {\displaystyle G=U+pV-TS,} we get d G d t ≤ ≤ 0.

{\displaystyle {\frac {\mathrm {d} G}{\mathrm {d} t}}\leq 0.} Homogeneous systems [ edit ] In homogeneous systems the temperature and pressure are well-defined and all internal processes are reversible. Hence S ˙ ˙ i = 0 {\displaystyle {\dot {S}}_{\text{i}}=0} . As a result, the second law, multiplied by T , reduces to T d S d t = Q ˙ ˙ + n ˙ ˙ T S m .

{\displaystyle T{\frac {\mathrm {d} S}{\mathrm {d} t}}={\dot {Q}}+{\dot {n}}TS_{\text{m}}.} With P = 0 the first law becomes d U d t = Q ˙ ˙ + n ˙ ˙ H m − − p d V d t .

{\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} t}}={\dot {Q}}+{\dot {n}}H_{\text{m}}-p{\frac {\mathrm {d} V}{\mathrm {d} t}}.} Eliminating Q ˙ ˙ {\displaystyle {\dot {Q}}} and multiplying with d t gives d U = T d S − − p d V + ( H m − − T S m ) d n .

{\displaystyle \mathrm {d} U=T\mathrm {d} S-p\mathrm {d} V+(H_{\text{m}}-TS_{\text{m}})\mathrm {d} n.} Since H m − − T S m = G m = μ μ {\displaystyle H_{\text{m}}-TS_{\text{m}}=G_{\text{m}}=\mu } with G m the molar Gibbs free energy and μ the molar chemical potential we obtain the well-known result d U = T d S − − p d V + μ μ d n .

{\displaystyle \mathrm {d} U=T\mathrm {d} S-p\mathrm {d} V+\mu \mathrm {d} n.} Entropy production in stochastic processes [ edit ] Since physical processes can be described by stochastic processes, such as Markov chains and diffusion processes, entropy production can be defined mathematically in such processes.

[ 5 ] For a continuous-time Markov chain with instantaneous probability distribution p i ( t ) {\displaystyle p_{i}(t)} and transition rate q i j {\displaystyle q_{ij}} , the instantaneous entropy production rate is e p ( t ) = 1 2 ∑ ∑ i , j [ p i ( t ) q i j − − p j ( t ) q j i ] log ⁡ ⁡ p i ( t ) q i j p j ( t ) q j i .

{\displaystyle e_{p}(t)={\frac {1}{2}}\sum _{i,j}[p_{i}(t)q_{ij}-p_{j}(t)q_{ji}]\log {\frac {p_{i}(t)q_{ij}}{p_{j}(t)q_{ji}}}.} The long-time behavior of entropy production is kept after a proper lifting of the process. This approach provides a dynamic explanation for the Kelvin statement and the Clausius statement of the second law of thermodynamics.

[ 6 ] Entropy production in diffusive-reactive system has also been studied, with interesting results emerging from diffusion, cross diffusion and reactions.

[ 7 ] For a continuous-time Gauss-Markov process, a multivariate Ornstein-Uhlenbeck process is a diffusion process defined by N {\displaystyle N} coupled linear Langevin equations of the form d x m ( t ) d t = − − ∑ ∑ n B m n x n ( t ) + η η m ( t ) .

{\displaystyle {\frac {dx_{m}(t)}{dt}}=-\sum _{n}B_{mn}x_{n}(t)+\eta _{m}(t).} ( m , n = 1 , … … , N ) {\displaystyle (m,n=1,\ldots ,N)} , i.e., in vector and matrix notations, d x ( t ) d t = − − B x ( t ) + η η ( t ) .

.

{\displaystyle {\frac {d\mathbf {x} (t)}{dt}}=-\mathbf {B} \mathbf {x} (t)+{\boldsymbol {\eta }}(t)..} The η η m ( t ) {\displaystyle \eta _{m}(t)} are Gaussian white noises such that ⟨ ⟨ η η m ( t ) η η n ( t ′ ) ⟩ ⟩ = 2 D m n δ δ ( t − − t ′ ) , .

{\displaystyle \langle \eta _{m}(t)\eta _{n}(t')\rangle =2D_{mn}\delta (t-t'),.} i.e., ⟨ ⟨ η η ( t ) η η T ( t ′ ) ⟩ ⟩ = 2 D δ δ ( t − − t ′ ) .

{\displaystyle \langle {\boldsymbol {\eta }}(t){\boldsymbol {\eta }}^{T}(t')\rangle =2\mathbf {D} \delta (t-t').} The stationary covariance matrix reads S = B − − 1 D = D ( B T ) − − 1 .

{\displaystyle \mathbf {S} =\mathbf {B} ^{-1}\mathbf {D} =\mathbf {D} \left(\mathbf {B} ^{\mathrm {T} }\right)^{-1}.} We can parametrize the matrices B {\displaystyle \mathbf {B} } , D {\displaystyle \mathbf {D} } , and S {\displaystyle \mathbf {S} } by setting L = B S = D + Q , L T = S B T = D − − Q .

{\displaystyle \mathbf {L} =\mathbf {B} \mathbf {S} =\mathbf {D} +\mathbf {Q} ,\quad \mathbf {L} ^{\mathrm {T} }=\mathbf {S} \mathbf {B} ^{\mathrm {T} }=\mathbf {D} -\mathbf {Q} .} Finally, the entropy production reads [ 8 ] e p = t r ( B T D − − 1 Q ) = − − t r ( D − − 1 B Q ) .

{\displaystyle e_{p}=\mathrm {tr} (\mathbf {B} ^{\mathrm {T} }\mathbf {D} ^{-1}\mathbf {Q} )=-\mathrm {tr} (\mathbf {D} ^{-1}\mathbf {B} \mathbf {Q} ).} A recent application of this formula is demonstrated in neuroscience, where it has been shown that entropy production of multivariate Ornstein-Uhlenbeck processes correlates with consciousness levels in the human brain.

[ 9 ] See also [ edit ] Thermodynamics First law of thermodynamics Second law of thermodynamics Irreversible process Non-equilibrium thermodynamics High entropy alloys General equation of heat transfer References [ edit ] ^ S. Carnot Reflexions sur la puissance motrice du feu Bachelier, Paris, 1824 ^ Clausius, R. (1854).

"Ueber eine veränderte Form des zweiten Hauptsatzes der mechanischen Wärmetheoriein" .

Annalen der Physik und Chemie .

93 (12): 481– 506.

Bibcode : 1854AnP...169..481C .

doi : 10.1002/andp.18541691202 . Retrieved 25 June 2012 .

.

Clausius, R. (August 1856).

"On a Modified Form of the Second Fundamental Theorem in the Mechanical Theory of Heat" .

Phil. Mag.

4.

12 (77): 81– 98.

doi : 10.1080/14786445608642141 . Retrieved 25 June 2012 .

^ R. Clausius Über verschiedene für die Anwendung bequeme Formen der Hauptgleigungen der mechanische Wärmetheorie in Abhandlungen über die Anwendung bequeme Formen der Haubtgleichungen der mechanischen Wärmetheorie Ann.Phys. [2] 125, 390 (1865). This paper is translated and can be found in: The second law of thermodynamics, Edited by J. Kestin, Dowden, Hutchinson, & Ross, Inc., Stroudsburg, Pennsylvania, pp. 162–193.

^ A.T.A.M. de Waele, Basic operation of cryocoolers and related thermal machines, Review article, Journal of Low Temperature Physics, Vol.164, pp. 179–236, (2011), DOI: 10.1007/s10909-011-0373-x.

^ Jiang, Da-Quan; Qian, Min; Qian, Min-Ping (2004).

Mathematical theory of nonequilibrium steady states: on the frontier of probability and dynamical systems . Berlin: Springer.

ISBN 978-3-540-40957-1 .

^ Wang, Yue; Qian, Hong (2020).

"Mathematical Representation of Clausius' and Kelvin's Statements of the Second Law and Irreversibility" .

Journal of Statistical Physics .

179 (3): 808– 837.

arXiv : 1805.09530 .

Bibcode : 2020JSP...179..808W .

doi : 10.1007/s10955-020-02556-6 .

S2CID 254745126 .

^ Mátyás, László; Gaspard, Pierre (2005).

"Entropy production in diffusion-reaction systems: The reactive random Lorentz gas" .

Phys. Rev. E .

71 (3): 036147.

arXiv : nlin/0411041 .

doi : 10.1103/PhysRevE.71.036147 .

PMID 15903533 .

^ Godrèche, Claude; Luck, Jean-Marc (2018).

"Characterising the nonequilibrium stationary states of Ornstein–Uhlenbeck processes" .

J. Phys. A: Math. Theor .

52 : 035002.

arXiv : 1807.00694 .

doi : 10.1088/1751-8121/aaf190 .

^ Gilson, Matthieu; Cofré, Rodrigo (2023).

"Entropy production of multivariate Ornstein-Uhlenbeck processes correlates with consciousness levels in the human brain" .

Phys. Rev. E .

107 : 024121.

arXiv : 2207.05197 .

doi : 10.1103/PhysRevE.107.024121 .

Further reading [ edit ] Crooks, G. (1999). "Entropy production fluctuation theorem and the non-equilibrium work relation for free energy differences".

Physical Review E (Free PDF).

60 (3): 2721– 2726.

arXiv : cond-mat/9901352 .

Bibcode : 1999PhRvE..60.2721C .

doi : 10.1103/PhysRevE.60.2721 .

PMID 11970075 .

S2CID 1813818 .

Seifert, Udo (2005). "Entropy Production along a Stochastic Trajectory and an Integral Fluctuation Theorem".

Physical Review Letters (Free PDF).

95 (4): 040602.

arXiv : cond-mat/0503686 .

Bibcode : 2005PhRvL..95d0602S .

doi : 10.1103/PhysRevLett.95.040602 .

PMID 16090792 .

S2CID 31706268 .

Retrieved from " https://en.wikipedia.org/w/index.php?title=Entropy_production&oldid=1287701496 " Categories : Cooling technology Cryogenics Heat pumps Thermodynamic entropy Hidden categories: Articles with short description Short description matches Wikidata Wikipedia introduction cleanup from February 2024 All pages needing cleanup Articles covered by WikiProject Wikify from February 2024 All articles covered by WikiProject Wikify This page was last edited on 27 April 2025, at 22:22 (UTC) .

Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Entropy production 3 languages Add topic

